{
    "Id": 70810857,
    "PostTypeId": 1,
    "Title": "split geometric progression efficiently in Python (Pythonic way)",
    "Body": "I am trying to achieve a calculation involving geometric progression (split). Is there any effective/efficient way of doing it. The data set has millions of rows.\nI need the column \"Traded_quantity\"\n\n\n\n\n\n\nMarker\nAction\nTraded_quantity\n\n\n\n\n2019-11-05\n09:25\n0\n\n0\n\n\n\n09:35\n2\nBUY\n3\n\n\n\n09:45\n0\n\n0\n\n\n\n09:55\n1\nBUY\n4\n\n\n\n10:05\n0\n\n0\n\n\n\n10:15\n3\nBUY\n56\n\n\n\n10:24\n6\nBUY\n8128\n\n\n\n\nturtle = 2\n(User defined)\nbase_quantity = 1\n(User defined)\n    def turtle_split(row):\n        if row['Action'] == 'BUY':\n            return base_quantity * (turtle ** row['Marker'] - 1) // (turtle - 1)\n        else:\n            return 0\n    df['Traded_quantity'] = df.apply(turtle_split, axis=1).round(0).astype(int)\n\nCalculation\nFor 0th Row, Traded_quantity should be zero (because the Marker is zero)\nFor 1st Row, Traded_quantity should be (1x1) + (1x2) = 3 (Marker 2 will be split into 1 and 1, First 1 will be multiplied with the base_quantity>>1x1, Second 1 will be multiplied with the result from first 1 times turtle>>1x2), then we make a sum of these two numbers)\nFor 2nd Row, Traded_quantity should be zero (because the Marker is zero)\nFor 3rd Row, Traded_quantity should be (2x2) = 4(Marker 1 will be multiplied with the last split from row 1 time turtle i.e 2x2)\nFor 4th Row, Traded_quantity should be zero(because the Marker is zero)\nFor 5th Row, Traded_quantity should be (4x2)+(4x2x2)+(4x2x2x2) = 56(Marker 3 will be split into 1,1 and 1, First 1 will be multiplied with the last split from row3 times turtle >>4x2, Second 1 will be multiplied with the result from first 1 with turtle>>8x2), third 1 will be multiplied with the result from second 1 with turtle>>16x2) then we make a sum of these three numbers)\nFor 6th Row, Traded_quantity should be (32x2)+(32x2x2)+(32x2x2x2)+(32x2x2x2x2)+(32x2x2x2x2x2) = 8128\nWhenever there will be a BUY, the traded quantity will be calculated using the last batch from Traded_quantity times turtle.\nTurns out the code is generating correct Traded_quantity when there is no zero in Marker. Once there is a gap with a couple of zeros geometric progression will not help, I would require the previous fig(from Cache) to recalculate Traded_q. tried with lru_cache for recursion, didn't work.\n",
    "AcceptedAnswerId": 70811799,
    "AcceptedAnswer": "This should work\ndef turtle_split(row):\n        global base_quantity\n        if row['Action'] == 'BUY':\n            summation = base_quantity * (turtle ** row['Marker'] - 1) // (turtle - 1)\n            base_quantity = base_quantity * (turtle ** (row['Marker'] - 1))*turtle\n            return summation\n        else:\n            return 0\n\n"
}
{
    "Id": 70552775,
    "PostTypeId": 1,
    "Title": "Multiprocess inherently shared memory in no longer working on python 3.10 (coming from 3.6)",
    "Body": "I understand there are a variety of techniques for sharing memory and data structures between processes in python. This question is specifically about this inherently shared memory in python scripts that existed in python 3.6 but seems to no longer exist in 3.10.  Does anyone know why and if it's possible to bring this back in 3.10?  Or what this change that I'm observing is?  I've upgraded my Mac to Monterey and it no longer supports python 3.6, so I'm forced to upgrade to either 3.9 or 3.10+.\nNote:  I tend to develop on Mac and run production on Ubuntu.  Not sure if that factors in here.  Historically with 3.6, everything behaved the same regardless of OS.\nMake a simple project with the following python files\nmyLibrary.py\nMyDict = {}\n\ntest.py\nimport threading\nimport time\nimport multiprocessing\n\nimport myLibrary\n\n\ndef InitMyDict():\n    myLibrary.MyDict = {'woot': 1, 'sauce': 2}\n    print('initialized myLibrary.MyDict to ', myLibrary.MyDict)\n\n\ndef MainLoop():\n    numOfSubProcessesToStart = 3\n    for i in range(numOfSubProcessesToStart):\n        t = threading.Thread(\n            target=CoolFeature(),\n            args=())\n        t.start()\n\n    while True:\n        time.sleep(1)\n\n\ndef CoolFeature():\n    MyProcess = multiprocessing.Process(\n        target=SubProcessFunction,\n        args=())\n    MyProcess.start()\n\n\ndef SubProcessFunction():\n    print('SubProcessFunction: ', myLibrary.MyDict)\n\n\nif __name__ == '__main__':\n    InitMyDict()\n    MainLoop()\n\nWhen I run this on 3.6 it has a significantly different behavior than 3.10.  I do understand that a subprocess cannot modify the memory of the main process, but it is still super convenient to access the main process' data structure that was previously set up as opposed to moving every little tiny thing into shared memory just to read a simple dictionary/int/string/etc.\nPython 3.10 output:\npython3.10 test.py \ninitialized myLibrary.MyDict to  {'woot': 1, 'sauce': 2}\nSubProcessFunction:  {}\nSubProcessFunction:  {}\nSubProcessFunction:  {}\n\nPython 3.6 output:\npython3.6 test.py \ninitialized myLibrary.MyDict to  {'woot': 1, 'sauce': 2}\nSubProcessFunction:  {'woot': 1, 'sauce': 2}\nSubProcessFunction:  {'woot': 1, 'sauce': 2}\nSubProcessFunction:  {'woot': 1, 'sauce': 2}\n\nObservation:\nNotice that in 3.6, the subprocess can view the value that was set from the main process.  But in 3.10, the subprocess sees an empty dictionary.\n",
    "AcceptedAnswerId": 70552892,
    "AcceptedAnswer": "In short, since 3.8, CPython uses the spawn start method on MacOs. Before it used the fork method.\nOn UNIX platforms, the fork start method is used which means that every new multiprocessing process is an exact copy of the parent at the time of the fork.\nThe spawn method means that it starts a new Python interpreter for each new multiprocessing process. According to the documentation:\n\nThe child process will only inherit those resources necessary to run the process object\u2019s run() method.\n\nIt will import your program into this new interpreter, so starting processes et cetera sould only be done from within the if __name__ == '__main__':-block!\nThis means you cannot count on variables from the parent process being available in the children, unless they are module level constants which would be imported.\nSo the change is significant.\nWhat can be done?\nIf the required information could be a module-level constant, that would solve the problem in the simplest way.\nIf that is not possible (e.g. because the data needs to be generated at runtime) you could have the parent write the information to be shared to a file. E.g. in JSON format and before it starts other processes. Then the children could simply read this. That is probably the next simplest solution.\nUsing a multiprocessing.Manager would allow you to share a dict between processes. There is however a certain amount of overhead associated with this.\nOr you could try calling multiprocessing.set_start_method(\"fork\") before creating processes or pools and see if it doesn't crash in your case. That would revert to the pre-3.8 method on MacOs. But as documented in this bug, there are real problems with using the fork method on MacOs.\nReading the issue indicates that fork might be OK as long as you don't use threads.\n"
}
{
    "Id": 70596809,
    "PostTypeId": 1,
    "Title": "Can a class attribute shadow a built-in in Python?",
    "Body": "If have some code like this:\nclass Foo():\n   def open(self, bar):\n       # Doing some fancy stuff here, i.e. opening \"bar\"\n       pass\n\nWhen I run flake8 with the flake8-builtins plug-in I get the error\nA003 class attribute \"open\" is shadowing a python builtin\n\nI don't understand how the method could possibly shadow the built-in open-function, because the method can only be called using an instance (i.e. self.open(\"\") or someFoo.open(\"\")). Is there some other way code expecting to call the built-in ends up calling the method? Or is this a false positive of the flake8-builtins plug-in?\n",
    "AcceptedAnswerId": 70597023,
    "AcceptedAnswer": "Not really a practical case, but your code would fail if you wanted to use the built-it functions on the class level after your shadowed function has been initialized:\nclass Foo:\n    def open(self, bar):\n        pass\n\n    with open('myfile.txt'):\n        print('did I get here?')\n\n>>> TypeError: open() missing 1 required positional argument: 'bar'\n\nThe same would also be true with other built-in functions, such as print\nclass Foo:\n    def print(self, bar):\n        pass\n\n    print('did I get here?')\n\n>>> TypeError: print() missing 1 required positional argument: 'bar'\n\n"
}
{
    "Id": 70587271,
    "PostTypeId": 1,
    "Title": "Is there a Pythonic way of filtering substrings of strings in a list?",
    "Body": "I have a list with strings as below.\ncandidates = [\"Hello\", \"World\", \"HelloWorld\", \"Foo\", \"bar\", \"ar\"]\n\nAnd I want the list to be filtered as [\"HelloWorld\", \"Foo\", \"Bar\"], because others are substrings. I can do it like this, but don't think it's fast or elegant.\ndef filter_not_substring(candidates):\n    survive = []\n    for a in candidates:\n        for b in candidates:\n            if a == b:\n                continue\n            if a in b:\n                break\n        else:\n            survive.append(a)\n    return survive\n\nIs there any fast way to do it?\n",
    "AcceptedAnswerId": 70587308,
    "AcceptedAnswer": "How about:\ncandidates = [\"Hello\", \"World\", \"HelloWorld\", \"Foo\", \"bar\", \"ar\"]\nresult = [c for c in candidates if not any(c in o and len(o) > len(c) for o in candidates)]\nprint(result)\n\nCounter to what was suggested in the comments:\nfrom timeit import timeit\n\n\ndef filter_not_substring(candidates):\n    survive = []\n    for a in candidates:\n        for b in candidates:\n            if a == b:\n                continue\n            if a in b:\n                break\n        else:\n            survive.append(a)\n    return survive\n\n\ndef filter_not_substring2a(candidates):\n    return [c for c in candidates if not any(len(o) > len(c) and c in o for o in candidates)]\n\n\ndef filter_not_substring2b(candidates):\n    return [c for c in candidates if not any(c in o and len(o) > len(c) for o in candidates)]\n\n\nxs = [\"Hello\", \"World\", \"HelloWorld\", \"Foo\", \"bar\", \"ar\", \"bar\"]\nprint(filter_not_substring(xs), filter_not_substring2a(xs), filter_not_substring2b(xs))\nprint(timeit(lambda: filter_not_substring(xs)))\nprint(timeit(lambda: filter_not_substring2a(xs)))\nprint(timeit(lambda: filter_not_substring2b(xs)))\n\nResult:\n['HelloWorld', 'Foo', 'bar', 'bar'] ['HelloWorld', 'Foo', 'bar', 'bar'] ['HelloWorld', 'Foo', 'bar', 'bar']\n1.5163685\n4.6516653\n3.8334089999999996\n\nSo, OP's solution is substantially faster, but filter_not_substring2b is still about 20% faster than 2a. So, putting the len comparison first doesn't save time.\nFor any production scenario, OP's function is probably optimal - a way to speed it up might be to bring the whole problem into C, but I doubt that would show great gains, since the logic is pretty straightforward already and I'd expect Python to do a fairly good job of it as well.\nUser @ming noted that OP's solution can be improved a bit:\ndef filter_not_substring_b(candidates):\n    survive = []\n    for a in candidates:\n        for b in candidates:\n            if a in b and a != b:\n                break\n        else:\n            survive.append(a)\n    return survive\n\nThis version of the function is somewhat faster, for me about 10-15%\nFinally, note that this is only just faster than 2b, even though it is very similar to the optimised solution by @ming, but almost 3x slower than their solution. It's unclear to me why that would be - if anyone has fairly certain thoughts on that, please share in the comments:\ndef filter_not_substring_c(candidates):\n    return [a for a in candidates if all(a not in b or a == b for b in candidates)]\n\n"
}
{
    "Id": 70923969,
    "PostTypeId": 1,
    "Title": "how to remove the \"User-Agent\" header when send request in python",
    "Body": "I'm using python requests library, I need send a request without a user-agent header.\nI found this question, but it's for Urllib2.\nI'm trying to simulate an Android app which does this when calling a private API.\nI try to set User-Agent to None as in the following code, but it doesn't work. It still sends User-Agent: python-requests/2.27.1.\nIs there any way?\nheaders = requests.utils.default_headers()\nheaders['User-Agent'] = None\nrequests.post(url, *args, headers=headers, **kwargs)\n\n",
    "AcceptedAnswerId": 70924222,
    "AcceptedAnswer": "The requests library is built on top of the urllib3 library.  So, when you pass None User-Agent header to the requests's post method, the urllib3 set their own default User-Agent\nimport requests\n\nr = requests.post(\"https://httpbin.org/post\", headers={\n    \"User-Agent\": None,\n})\n\nprint(r.json()[\"headers\"][\"User-Agent\"])\n\nOutput\npython-urllib3/1.26.7\n\nHere the urllib3 source of connection.py\nclass HTTPConnection(_HTTPConnection, object):\n    ...\n\n    def request(self, method, url, body=None, headers=None):\n        if headers is None:\n            headers = {}\n        else:\n            # Avoid modifying the headers passed into .request()\n            headers = headers.copy()\n        if \"user-agent\" not in (six.ensure_str(k.lower()) for k in headers):\n            headers[\"User-Agent\"] = _get_default_user_agent()\n        super(HTTPConnection, self).request(method, url, body=body, headers=headers) \n\nSo, you can monkey patch it to disable default User-Agent header\nimport requests\nfrom urllib3 import connection\n\n\ndef request(self, method, url, body=None, headers=None):\n    if headers is None:\n        headers = {}\n    else:\n        # Avoid modifying the headers passed into .request()\n        headers = headers.copy()\n    super(connection.HTTPConnection, self).request(method, url, body=body, headers=headers)\n\nconnection.HTTPConnection.request = request\n\n\nr = requests.post(\"https://httpbin.org/post\", headers={\n    \"User-Agent\": None,\n})\n\nprint(r.json()[\"headers\"])\n\nOutput\n{\n'Accept': '*/*', \n'Accept-Encoding': 'gzip, deflate', \n'Content-Length': '0', \n'Host': 'httpbin.org', \n'X-Amzn-Trace-Id': 'Root=1-61f7b53b-26c4c8f6498c86a24ff05940'\n}\n\nAlso, consider to provide browser-like User-Agent like this Mozilla/5.0 (Macintosh; Intel Mac OS X 12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36. Maybe it solves your task with less effort\n"
}
{
    "Id": 70587544,
    "PostTypeId": 1,
    "Title": "\"brew install python\" installs 3.9. Why not 3.10?",
    "Body": "My understanding is that \"brew install python\" installs the latest version of python. Why isn't it pulling 3.10? 3.10 is marked as a stable release.\nI can install 3.10 with \"brew install python@3.10 just fine and can update my PATH so that python and pip point to the right versions. But I am curious why \"brew install python\" its not installing 3.10.\nMy other understanding is that 3.10 is directly compatible with the M1 chips so that is why I want 3.10.\nPlease let me know if I am mistaken.\n",
    "AcceptedAnswerId": 70589077,
    "AcceptedAnswer": "As Henry Schreiner have specified now Python 3.10 is the new default in Brew. Thx for pointing it\n--- Obsolete ---\nThe \"python3\" formula is still 3.9 in the brew system\ncheck the doc here:\nhttps://formulae.brew.sh/formula/python@3.9#default\nThe latest version of the formula for 3.9 also support apple silicon.\nIf you want to use python3.10 you need to run as you described brew install python@3.10\nThe reason why 3.9 is still the official python3 formula is that generally user using the vanilla python3 are not looking for the latest revision but the more stable. in some months the transition will done.\n"
}
{
    "Id": 70610919,
    "PostTypeId": 1,
    "Title": "Installing python in Dockerfile without using python image as base",
    "Body": "I have a python script that uses DigitalOcean tools (doctl and kubectl) I want to containerize. This means my container will need python, doctl, and kubectl installed. The trouble is, I figure out how to install both python and DigitalOcean tools in the dockerfile.\nI can install python using the base image \"python:3\" and I can also install the DigitalOcean tools using the base image \"alpine/doctl\". However, the rule is you can only use one base image in a dockerfile.\nSo I can include the python base image and install the DigitalOcean tools another way:\nFROM python:3\nRUN \nRUN pip install firebase-admin\nCOPY script.py\nCMD [\"python\", \"script.py\"]\n\nOr I can include the alpine/doctl base image and install python3 another way.\nFROM alpine/doctl\nRUN \nRUN pip install firebase-admin\nCOPY script.py\nCMD [\"python\", \"script.py\"]\n\nUnfortunately, I'm not sure how I would do this. Any help in how I can get all these tools installed would be great!\n",
    "AcceptedAnswerId": 70611018,
    "AcceptedAnswer": "just add this with any other thing you want to apt-get install:\nRUN apt-get update && apt-get install -y \\\n    python3.6 &&\\\n    python3-pip &&\\\n\nin alpine it should be something like:\nRUN apk add --update --no-cache python3 && ln -sf python3 /usr/bin/python &&\\\n    python3 -m ensurepip &&\\\n    pip3 install --no-cache --upgrade pip setuptools &&\\\n\n"
}
{
    "Id": 71053839,
    "PostTypeId": 1,
    "Title": "VSCode Jupyter not connecting to python kernel",
    "Body": "Launching a cell will make this message appear: Connecting to kernel: Python 3.9.6 64-bit: Activating Python Environment 'Python 3.9.6 64-bit'. This message will then stay up loading indefinitely, without anything happening. No actual error message.\nI've already tried searching for this problem, but every other post seem to obtain at least an error message, which isn't the case here. I still looked at some of these, which seemed to indicate the problem might have come from the traitlets package. I tried to downgrade it to what was recommended, but it didn't solve anything, so I reverted the downgrade.\nThe main problem here is that I have no idea what could cause such a problem, without even an error message. If you think additional info could help, please do ask, I have no idea what could be of use right now.\n",
    "AcceptedAnswerId": 71092223,
    "AcceptedAnswer": "Not sure what did the trick but downgrading VSCode to November version and after that reinstalling Jupyter extension worked for me.\n"
}
{
    "Id": 70721360,
    "PostTypeId": 1,
    "Title": "Python/Selenium web scrap how to find hidden src value from a links?",
    "Body": "Scrapping links should be a simple feat, usually just grabbing the src value of the a tag.\nI recently came across this website (https://sunteccity.com.sg/promotions) where the href value of a tags of each item cannot be found, but the redirection still works. I'm trying to figure out a way to grab the items and their corresponding links. My typical python selenium code looks something as such\nall_items = bot.find_elements_by_class_name('thumb-img')\nfor promo in all_items:\n    a = promo.find_elements_by_tag_name(\"a\")\n    print(\"a[0]: \", a[0].get_attribute(\"href\"))\n\nHowever, I can't seem to retrieve any href, onclick attributes, and I'm wondering if this is even possible. I noticed that I couldn't do a right-click, open link in new tab as well.\nAre there any ways around getting the links of all these items?\nEdit: Are there any ways to retrieve all the links of the items on the pages?\ni.e.\nhttps://sunteccity.com.sg/promotions/724\nhttps://sunteccity.com.sg/promotions/731\nhttps://sunteccity.com.sg/promotions/751\nhttps://sunteccity.com.sg/promotions/752\nhttps://sunteccity.com.sg/promotions/754\nhttps://sunteccity.com.sg/promotions/280\n...\n\n\nEdit:\nAdding an image of one such anchor tag for better clarity:\n\n",
    "AcceptedAnswerId": 70725182,
    "AcceptedAnswer": "By reverse-engineering the Javascript that takes you to the promotions pages (seen in https://sunteccity.com.sg/_nuxt/d4b648f.js) that gives you a way to get all the links, which are based on the HappeningID. You can verify by running this in the JS console, which gives you the first promotion:\nwindow.__NUXT__.state.Promotion.promotions[0].HappeningID\n\nBased on that, you can create a Python loop to get all the promotions:\nitems = driver.execute_script(\"return window.__NUXT__.state.Promotion;\")\nfor item in items[\"promotions\"]:\n    base = \"https://sunteccity.com.sg/promotions/\"\n    happening_id = str(item[\"HappeningID\"])\n    print(base + happening_id)\n\nThat generated the following output:\nhttps://sunteccity.com.sg/promotions/724\nhttps://sunteccity.com.sg/promotions/731\nhttps://sunteccity.com.sg/promotions/751\nhttps://sunteccity.com.sg/promotions/752\nhttps://sunteccity.com.sg/promotions/754\nhttps://sunteccity.com.sg/promotions/280\nhttps://sunteccity.com.sg/promotions/764\nhttps://sunteccity.com.sg/promotions/766\nhttps://sunteccity.com.sg/promotions/762\nhttps://sunteccity.com.sg/promotions/767\nhttps://sunteccity.com.sg/promotions/732\nhttps://sunteccity.com.sg/promotions/733\nhttps://sunteccity.com.sg/promotions/735\nhttps://sunteccity.com.sg/promotions/736\nhttps://sunteccity.com.sg/promotions/737\nhttps://sunteccity.com.sg/promotions/738\nhttps://sunteccity.com.sg/promotions/739\nhttps://sunteccity.com.sg/promotions/740\nhttps://sunteccity.com.sg/promotions/741\nhttps://sunteccity.com.sg/promotions/742\nhttps://sunteccity.com.sg/promotions/743\nhttps://sunteccity.com.sg/promotions/744\nhttps://sunteccity.com.sg/promotions/745\nhttps://sunteccity.com.sg/promotions/746\nhttps://sunteccity.com.sg/promotions/747\nhttps://sunteccity.com.sg/promotions/748\nhttps://sunteccity.com.sg/promotions/749\nhttps://sunteccity.com.sg/promotions/750\nhttps://sunteccity.com.sg/promotions/753\nhttps://sunteccity.com.sg/promotions/755\nhttps://sunteccity.com.sg/promotions/756\nhttps://sunteccity.com.sg/promotions/757\nhttps://sunteccity.com.sg/promotions/758\nhttps://sunteccity.com.sg/promotions/759\nhttps://sunteccity.com.sg/promotions/760\nhttps://sunteccity.com.sg/promotions/761\nhttps://sunteccity.com.sg/promotions/763\nhttps://sunteccity.com.sg/promotions/765\nhttps://sunteccity.com.sg/promotions/730\nhttps://sunteccity.com.sg/promotions/734\nhttps://sunteccity.com.sg/promotions/623\n\n"
}
{
    "Id": 70729502,
    "PostTypeId": 1,
    "Title": "F2 rename variable doesn't work in vscode + jupyter notebook + python",
    "Body": "I can use the normal F2 rename variable functionality in regular python files in vscode. But not when editing python in a jupyter notebook.\nWhen I press F2 on a variable in a jupyter notebook in vscode I get the familiar change variable window but when I press enter the variable is not changed and I get this error message:\n\nNo result. No result.\n\nIs there a way to get the F2 change variable functionality to work in jupyter notebooks?\nHere's my system info:\njupyter module version\n(adventofcode) C:\\git\\leetcode>pip show jupyter\nName: jupyter\nVersion: 1.0.0\nSummary: Jupyter metapackage. Install all the Jupyter components in one go.\nHome-page: http://jupyter.org\nAuthor: Jupyter Development Team\nAuthor-email: jupyter@googlegroups.org\nLicense: BSD\nLocation: c:\\users\\johan\\anaconda3\\envs\\adventofcode\\lib\\site-packages\nRequires: ipykernel, qtconsole, nbconvert, jupyter-console, notebook, ipywidgets\nRequired-by:\n\nPython version:\n(adventofcode) C:\\git\\leetcode>python --version\nPython 3.10.0\n\nvscode version:\n1.63.2 (user setup)\n\nvscode Jupyter extension version (from the changelog in the extensions window):\n2021.11.100 (November Release on 8 December 2021)\n\n",
    "AcceptedAnswerId": 70736000,
    "AcceptedAnswer": "Notice that you put up a bug report in GitHub and see this issue: Renaming variables didn't work, the programmer replied:\n\nSome language features are currently not supported in notebooks, but\nwe are making plans now to hopefully bring more of those online soon.\n\nSo please wait for this feature.\n"
}
{
    "Id": 70704285,
    "PostTypeId": 1,
    "Title": "Can no longer fold python dictionaries in VS Code",
    "Body": "I used to be able to collapse (fold) python dictionaries just fine in my VS Code.  Randomly I am not able to do that anymore.  I can still fold classes and functions just fine, but dictionaries cannot fold, the arrow on the left hand side just isn't there.  I've checked my settings but I can't figure out what would've changed.  I'm not sure the best forum to go to for help, so I'm hoping this is ok.  Any ideas?\n",
    "AcceptedAnswerId": 70714478,
    "AcceptedAnswer": "It's caused by Pylance v2022.1.1. Use v2022.1.0 instead.\nIssue #2248\n"
}
{
    "Id": 70751249,
    "PostTypeId": 1,
    "Title": "Which are safe methods and practices for string formatting with user input in Python 3?",
    "Body": "My Understanding\nFrom various sources, I have come to the understanding that there are four main techniques of string formatting/interpolation in Python 3 (3.6+ for f-strings):\n\nFormatting with %, which is similar to C's printf\nThe str.format() method\nFormatted string literals/f-strings\nTemplate strings from the standard library string module\n\nMy knowledge of usage mainly comes from Python String Formatting Best Practices (source A):\n\nstr.format() was created as a better alternative to the %-style, so the latter is now obsolete\n\nHowever, str.format() is vulnerable to attacks if user-given format strings are not properly handled\n\n\nf-strings allow str.format()-like behavior only for string literals but are shorter to write and are actually somewhat-optimized syntactic sugar for concatenation\nTemplate strings are safer than str.format() (demonstrated in the first source) and the other two methods (implied in the first source) when dealing with user input\n\nI understand that the aforementioned vulnerability in str.format() comes from the method being usable on any normal strings where the delimiting braces are part of the string data itself. Malicious user input containing brace-delimited replacement fields can be supplied to the method to access environment attributes. I believe this is unlike the other ways of formatting where the programmer is the only one that can supply variables to the pre-formatted string. For example, f-strings have similar syntax to str.format() but, because f-strings are literals and the inserted values are evaluated separately through concatenation-like behavior, they are not vulnerable to the same attack (source B). Both %-formatting and Template strings also seem to only be supplied variables for substitution by the programmer; the main difference pointed out is Template's more limited functionality.\nMy Confusion\nI have seen a lot of emphasis on the vulnerability of str.format() which leaves me with questions of what I should be wary of when using the other techniques. Source A describes Template strings as the safest of the above methods \"due to their reduced complexity\":\n\nThe more complex formatting mini-languages of the other string formatting techniques might introduce security vulnerabilities to your programs.\n\n\nYes, it seems like f-strings are not vulnerable in the same way str.format() is, but are there known concerns about f-string security as is implied by source A? Is the concern more like risk mitigation for unknown exploits and unintended interactions?\n\nI am not familiar with C and I don't plan on using the clunkier %/printf-style formatting, but I have heard that C's printf had its own potential vulnerabilities. In addition, both sources A and B seem to imply a lack of security with this method. The top answer in Source B says,\n\nString formatting may be dangerous when a format string depends on untrusted data. So, when using str.format() or %-formatting, it's important to use static format strings, or to sanitize untrusted parts before applying the formatter function.\n\n\nDo %-style strings have known security concerns?\nLastly, which methods should be used and how can user input-based attacks be prevented (e.g. filtering input with regex)?\n\nMore specifically, are Template strings really the safer option? and Can f-strings be used just as easily and safely while granting more functionality?\n\n\n\n",
    "AcceptedAnswerId": 70755916,
    "AcceptedAnswer": "It doesn't matter which format you choose, any format and library can have its own downsides and vulnerabilities. The bigger questions you need to ask yourself is what is the risk factor and the scenario you are facing with, and what are you going to do about it.\nFirst ask yourself: will there be a scenario where a user or an external entity of some kind (for example - an external system) sends you a format string? If the answer is no, there is no risk. If the answer is yes, you need to see whether this is needed or not. If not - remove it to eliminate the risk.\nIf you need it - you can perform whitelist-based input validation and exclude all format-specific special characters from the list of permitted characters, in order to eliminate the risk. For example, no format string can pass the ^[a-zA-Z0-9\\s]*$ generic regular expression.\nSo the bottom line is: it doesn't matter which format string type you use, what's really important is what do you do with it and how can you reduce and eliminate the risk of it being tampered.\n"
}
{
    "Id": 70773526,
    "PostTypeId": 1,
    "Title": "Why do we need a dict.update() method in python instead of just assigning the values to the corresponding keys?",
    "Body": "I have been working with dictionaries that I have to modify within different parts of my code. I am trying to make sure if I do not miss anything about there is no need for dict_update() in any scenario.\nSo the reasons to use update() method is either to add a new key-value pair to current dictionary, or update the value of your existing ones.\nBut wait!?\nAren't they already possible by just doing:\n>>>test_dict = {'1':11,'2':1445}\n>>>test_dict['1'] = 645\n>>>test_dict\n{'1': 645, '2': 1445}\n>>>test_dict[5]=123\n>>>test_dict\n{'1': 645, '2': 1445, 5: 123}\n\nIn what case it would be crucial to use it ? I am curious.\nMany thanks\n",
    "AcceptedAnswerId": 70773868,
    "AcceptedAnswer": "1. You can update many keys on the same statement.\nmy_dict.update(other_dict)\n\nIn this case you don't have to know how many keys are in the other_dict. You'll just be sure that all of them will be updated on my_dict.\n2. You can use any iterable of key/value pairs with dict.update\nAs per the documentation you can use another dictionary, kwargs, list of tuples, or even generators that yield tuples of len 2.\n3. You can use the update method as an argument for functions that expect a function argument.\nExample:\ndef update_value(key, value, update_function):\n    update_function([(key, value)])\n\nupdate_value(\"k\", 3, update_on_the_db)  # suppose you have a update_on_the_db function\nupdate_value(\"k\", 3, my_dict.update)  # this will update on the dict\n\n"
}
{
    "Id": 71238822,
    "PostTypeId": 1,
    "Title": "Why is setuptools not available in environment Ubuntu docker image with Python & dev tools installed?",
    "Body": "I'm trying to build a Ubuntu 18.04 Docker image running Python 3.7 for a machine learning project. When installing specific Python packages with pip from requirements.txt, I get the following error:\nCollecting sklearn==0.0\n  Downloading sklearn-0.0.tar.gz (1.1 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'error'\n  error: subprocess-exited-with-error\n  \n  \u00d7 python setup.py egg_info did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [1 lines of output]\n      ERROR: Can not execute `setup.py` since setuptools is not available in the build environment.\n      [end of output]\n\nAlthough here the error arises in the context of sklearn, the issue is not specific to one library; when I remove that libraries and try to rebuild the image, the error arises with other libraries.\nHere is my Dockerfile:\nFROM ubuntu:18.04\n\n# install python\nRUN apt-get update && \\\n    apt-get install --no-install-recommends -y \\\n    python3.7 python3-pip python3.7-dev\n\n# copy requirements\nWORKDIR /opt/program\nCOPY requirements.txt requirements.txt\n\n# install requirements\nRUN python3.7 -m pip install --upgrade pip && \\\n    python3.7 -m pip install -r requirements.txt\n\n# set up program in image\nCOPY . /opt/program\n\nWhat I've tried:\n\ninstalling python-devtools, both instead of and alongside, python3.7-dev before installing requirements with pip;\ninstalling setuptools in requirements.txt before affected libraries are installed.\n\nIn both cases the same error arose.\nDo you know how I can ensure setuptools is available in my environment when installing libraries like sklearn?\n",
    "AcceptedAnswerId": 71239956,
    "AcceptedAnswer": "As mentioned in comment, install setuptools with pip before running pip install -r requirements.txt.\nIt is different than putting setuptools higher in the requirements.txt because it forces the order while the requirements file collect all the packages and installs them after so you don't control the order.\n"
}
{
    "Id": 70879159,
    "PostTypeId": 1,
    "Title": "Get datetime format from string python",
    "Body": "In Python there are multiple DateTime parsers which can parse a date string automatically without providing the datetime format. My problem is that I don't need to cast the datetime, I only need the datetime format.\nExample:\nFrom \"2021-01-01\", I want something like \"%Y-%m-%d\" or \"yyyy-MM-dd\".\nMy only idea was to try casting with different formats and get the successful one, but I don't want to list every possible format.\nI'm working with pandas, so I can use methods that work either with series or the string DateTime parser.\nAny ideas?\n",
    "AcceptedAnswerId": 70879221,
    "AcceptedAnswer": "In pandas, this is achieved by pandas._libs.tslibs.parsing.guess_datetime_format\nfrom pandas._libs.tslibs.parsing import guess_datetime_format\n\nguess_datetime_format('2021-01-01')\n\n# '%Y-%m-%d'\n\nAs there will always be an ambiguity on the day/month, you can specify the dayfirst case:\nguess_datetime_format('2021-01-01', dayfirst=True)\n# '%Y-%d-%m'\n\n"
}
{
    "Id": 70967266,
    "PostTypeId": 1,
    "Title": "what exactly is python typing.Callable?",
    "Body": "I have seen typing.Callable, but I didn't find any useful docs about it. What exactly is typing.Callable?\n",
    "AcceptedAnswerId": 70967371,
    "AcceptedAnswer": "typing.Callable is the type you use to indicate a callable. Most python types that support the () operator are of the type collections.abc.Callable. Examples include functions, classmethods, staticmethods, bound methods and lambdas.\nIn summary, anything with a __call__ method (which is how () is implemented), is a callable.\nPEP 677 attempted to introduce implicit tuple-with-arrow syntax, so that something like Callable[[int, str], list[float]] could be expressed much more intuitively as (int, str) -> list[float]. The PEP was rejected because the benefits of the new syntax were not deemed sufficient given the added maintenance burden and possible room for confusion.\n"
}
{
    "Id": 71500756,
    "PostTypeId": 1,
    "Title": "What is Python's \"Namespace\" object?",
    "Body": "I know what namespaces are. But when running\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument('bar')\nparser.parse_args(['XXX']) # outputs:  Namespace(bar='XXX')\n\nWhat kind of object is Namespace(bar='XXX')? I find this totally confusing.\nReading the argparse docs, it says \"Most ArgumentParser actions add some value as an attribute of the object returned by parse_args()\".  Shouldn't this object then appear when running globals()? Or how can I introspect it?\n",
    "AcceptedAnswerId": 71500890,
    "AcceptedAnswer": "Samwise's answer is very good, but let me answer the other part of the question.\n\nOr how can I introspect it?\n\nBeing able to introspect objects is a valuable skill in any language, so let's approach this as though Namespace is a completely unknown type.\n>>> obj = parser.parse_args(['XXX']) # outputs:  Namespace(bar='XXX')\n\nYour first instinct is good. See if there's a Namespace in the global scope, which there isn't.\n>>> Namespace\nTraceback (most recent call last):\n  File \"\", line 1, in \nNameError: name 'Namespace' is not defined\n\nSo let's see the actual type of the thing. The Namespace(bar='XXX') printer syntax is coming from a __str__ or __repr__ method somewhere, so let's see what the type actually is.\n>>> type(obj)\n\n\nand its module\n>>> type(obj).__module__\n'argparse'\n\nNow it's a pretty safe bet that we can do from argparse import Namespace and get the type. Beyond that, we can do\n>>> help(argparse.Namespace)\n\nin the interactive interpreter to get detailed documentation on the Namespace class, all with no Internet connection necessary.\n"
}
{
    "Id": 70863543,
    "PostTypeId": 1,
    "Title": "Can a Python docstring be calculated (f-string or %-expression)?",
    "Body": "Is it possible to have a Python docstring calculated? I have a lot of repetitive things in my docstrings, so I'd like to either use f-strings or a %-style format expression.\nWhen I use an f-string at the place of a docstring\n\nimporting the module invokes the processing\nbut when I check the __doc__ of such a function it is empty\nsphinx barfs when the docstring is an f-string\n\nI do know how to process the docstrings after the import, but that doesn't work for object 'doc' strings which is recognized by sphinx but is not a real __doc__'s of the object.\n",
    "AcceptedAnswerId": 70865657,
    "AcceptedAnswer": "Docstrings in Python must be regular string literals.\nThis is pretty easy to test - the following program does not show the docstring:\nBAR = \"Hello world!\"\n\ndef foo():\n        f\"\"\"This is {BAR}\"\"\"\n        pass\n\nassert foo.__doc__ is None\nhelp(foo)\n\n\nThe Python syntax docs say that the docstring must be a \"string literal\", and the tail end of the f-string reference says they \"cannot be used as docstrings\".\nSo unfortunately you must use the __doc__ attribute.\nHowever, you should be able to use a decorator to read the __doc__ attribute and replace it with whatever you want.\n"
}
{
    "Id": 70987896,
    "PostTypeId": 1,
    "Title": "Why is this task faster in Python than Julia?",
    "Body": "I ran the following code in RStudio:\nexo <- read.csv('exoplanets.csv',TRUE,\",\")\ndf <- data.frame(exo)\n\nranks <- 570\nfiles <- 3198\ndatas <- vector()\n\nfor ( w in 2:files ) {\n    listas <-vector()\n    for ( i in 1:ranks) {\n            name <- as.character(df[i,w])\n            listas <- append (listas, name)\n    }\n    datas <- append (datas, listas)\n}\n\nIt reads a huge NASA CSV file, converts it to a dataframe,\nconverts each element to string, and adds them to a vector.\nRStudio took 4 min and 15 seconds.\nSo I decided to implement the same code in Julia.\nI ran the following in VS Code:\nusing CSV, DataFrames\n\ndf = CSV.read(\"exoplanets.csv\", DataFrame)\n\nfil, col = 570, 3198\narr = []\n\nfor i in 2:fil\n        for j in 1:col\n            push!(arr, string(df[i, j]))\n        end\nend\n\nThe result was good.\nThe Julia code took only 1 minute and 25 seconds!\nThen for pure curiosity I implemented the same code\nthis time in Python to compare.\nI ran the following in VS Code:\nimport numpy as np\nimport pandas as pd\n\nexo = pd.read_csv(\"exoplanets.csv\")\narr = np.array(exo)\n\nfil, col = 570, 3198\nlis = []\n\nfor i in range(1, fil):\n        for j in range(col):\n            lis.append(arr[i][j].astype('str'))\n\nThe result shocked me! Only 35 seconds!!!\nAnd in Spyder from Anaconda only 26 seconds!!!\nAlmost 2 million floats!!!\nIs Julia slower than Python in data analysis?\nCan I improve the Julia code?\n",
    "AcceptedAnswerId": 70988453,
    "AcceptedAnswer": "NOTE: I wrote the below assuming you want the other column order (as in the Python and R examples).  It is more efficient in Julia this way; to make it work equivalently to your original behaviour, permute the logic or your data at the right places (left as an exercise). Bogumi\u0142's anwer does the right thing already.\n\nPut stuff into functions, preallocate where possible, iterate in stride order, use views, and use builtin functions and broadcasting:\nfunction tostringvector(d)\n    r, c = size(d)\n    result = Vector{String}(undef, r*c)\n    v = reshape(result, r, c)\n    for (rcol, dcol) in zip(eachcol(v), eachcol(d))\n        @inbounds rcol .= string.(dcol)\n    end\n    return result\nend\n\nWhich certainly can be optimized harder.\nOr shorter, making use of what DataFrames already provides:\ntostringvector(d) = vec(Matrix(string.(d)))\n\n"
}
{
    "Id": 71518406,
    "PostTypeId": 1,
    "Title": "How to bypass cloudflare browser checking selenium Python",
    "Body": "I am trying to access a site using selenium Python.\nBut the site is checking and checking continuously by cloudflare.\nNo other page is coming.\nCheck the screenshot here.\n\nI have tried undetected chrome but it is not working at all.\n",
    "AcceptedAnswerId": 71518481,
    "AcceptedAnswer": "By undetected chrome do you mean undetected chromedriver?:\nAnyways, undetected-chromedriver works for me:\nUndetected chromedriver\nGithub: https://github.com/ultrafunkamsterdam/undetected-chromedriver\npip install undetected-chromedriver\n\nCode that gets a cloudflare protected site:\nimport undetected_chromedriver as uc\ndriver = uc.Chrome(use_subprocess=True)\ndriver.get('https://nowsecure.nl')\n\nMy POV\n\n\n\nQuick setup code that logs into your google account:\nGithub: https://github.com/xtekky/google-login-bypass\nimport undetected_chromedriver as uc\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n#  ---------- EDIT ----------\nemail = 'email\\n' # replace email\npassword = 'password\\n' # replace password\n#  ---------- EDIT ----------\n\ndriver = uc.Chrome(use_subprocess=True)\nwait = WebDriverWait(driver, 20)\nurl = 'https://accounts.google.com/ServiceLogin?service=accountsettings&continue=https://myaccount.google.com%3Futm_source%3Daccount-marketing-page%26utm_medium%3Dgo-to-account-button'\ndriver.get(url)\n\n\nwait.until(EC.visibility_of_element_located((By.NAME, 'identifier'))).send_keys(email)\nwait.until(EC.visibility_of_element_located((By.NAME, 'password'))).send_keys(password)\nprint(\"You're in!! enjoy\")\n\n# [ ---------- paste your code here ---------- ]\n\n"
}
{
    "Id": 70872276,
    "PostTypeId": 1,
    "Title": "FastAPI python: How to run a thread in the background?",
    "Body": "I'm making a server in python using FastAPI, and I want a function that is not related to my API, to run in background every 5 minutes (like checking stuff from an API and printing stuff depending on the response)\nI've tried to make a thread that runs the function start_worker, but it doesn't print anything.\nDoes anyone know how to do so ?\ndef start_worker():\n    print('[main]: starting worker...')\n    my_worker = worker.Worker()\n    my_worker.working_loop() # this function prints \"hello\" every 5 seconds\n\nif __name__ == '__main__':\n    print('[main]: starting...')\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000, reload=True)\n    _worker_thread = Thread(target=start_worker, daemon=False)\n    _worker_thread.start()\n\n",
    "AcceptedAnswerId": 70873984,
    "AcceptedAnswer": "You should start your Thread before calling uvicorn.run, as uvicorn.run is blocking the thread.\nPS: In your question you state that you would like the background task to run every 5 minutes, but in your code you say every 5 seconds. The below examples assume that is the latter you want. If you want it to be executed every 5 minutes instead, then adjust the time to 60 * 5.\nOption 1\nimport time\nimport threading\nfrom fastapi import FastAPI\nimport uvicorn\n\napp = FastAPI()\nclass BackgroundTasks(threading.Thread):\n    def run(self,*args,**kwargs):\n        while True:\n            print('Hello')\n            time.sleep(5)\n  \nif __name__ == '__main__':\n    t = BackgroundTasks()\n    t.start()\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\nYou could also start your thread using FastAPI's startup event, as long as it is ok to run before the application starts.\n@app.on_event(\"startup\")\nasync def startup_event():\n    t = BackgroundTasks()\n    t.start()\n\nOption 2\nYou could instead use a repeating Event scheduler for the background task, as below:\nimport sched, time\nfrom threading import Thread\nfrom fastapi import FastAPI\nimport uvicorn\n\napp = FastAPI()\ns = sched.scheduler(time.time, time.sleep)\n\ndef print_event(sc): \n    print(\"Hello\")\n    sc.enter(5, 1, print_event, (sc,))\n\ndef start_scheduler():\n    s.enter(5, 1, print_event, (s,))\n    s.run()\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    thread = Thread(target = start_scheduler)\n    thread.start()\n\nif __name__ == '__main__':\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\n"
}
{
    "Id": 70966298,
    "PostTypeId": 1,
    "Title": "Python Black code formatter doesn't format docstring line length",
    "Body": "I am running the Black code formatter against a Python script however it doesn't reformat the line length for docstrings. For example, given the following code:\ndef my_func():\n    \"\"\"\n    This is a really long docstring. This is a really long docstring. This is a really long docstring. This is a really long docstring. This is a really long docstring. This is a really long docstring.\n    \"\"\"\n    return\n\nWhen running Black against this script, the line length does not change. How can I ensure docstrings get formatted when running Black?\n",
    "AcceptedAnswerId": 71041192,
    "AcceptedAnswer": "maintainer here! :wave:\nThe short answer is no you cannot configure Black to fix line length issues in docstrings currently.\nIt's not likely Black will split or merge lines in docstrings as it would be far too risky, structured data can and does exist in docstrings. While I would hope the added newlines wouldn't break the consumers it's still a valid concern.\nThere's currently an open issue asking for this (although it also wants the line length limit for docstrings and strings to be 79) GH-2289, and specifically for docstrings GH-2865. You can also read GH-1713 which is about splitting comments (and likewise has mixed feelings from maintainers).\nFor the time being, perhaps you can look into https://github.com/PyCQA/docformatter which does seem to wrap docstrings (see the --wrap-descriptions and --wrap-summaries options)\n\nP.S. if you're curious whether we'll add a flag to split docstrings or comments, it's once again unlikely since we seek to minimize formatting configurability. Especially as the pre-existing flags only disable certain elements of Black's style (barring --line-length which exists as there's no real consensus what it should be). Feel free to state your arguments in the linked issues tho!\n"
}
{
    "Id": 70669213,
    "PostTypeId": 1,
    "Title": "gyp ERR! stack Error: Command failed: python -c import sys; print \"%s.%s.%s\" % sys.version_info[:3]",
    "Body": "I'm trying to npm install in a Vue project, and even if I just ran vue create (name)\nit gives me this err:\nnpm ERR! gyp verb check python checking for Python executable \"c:\\Python310\\python.exe\" in the PATH\nnpm ERR! gyp verb `which` succeeded c:\\Python310\\python.exe c:\\Python310\\python.exe\nnpm ERR! gyp ERR! configure error\nnpm ERR! gyp ERR! stack Error: Command failed: c:\\Python310\\python.exe -c import sys; print \"%s.%s.%s\" % sys.version_info[:3];\nnpm ERR! gyp ERR! stack   File \"\", line 1\nnpm ERR! gyp ERR! stack     import sys; print \"%s.%s.%s\" % sys.version_info[:3];\nnpm ERR! gyp ERR! stack                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnpm ERR! gyp ERR! stack SyntaxError: Missing parentheses in call to 'print'. Did you mean print(...)?\nnpm ERR! gyp ERR! stack\nnpm ERR! gyp ERR! stack     at ChildProcess.exithandler (node:child_process:397:12)\nnpm ERR! gyp ERR! stack     at ChildProcess.emit (node:events:390:28)\nnpm ERR! gyp ERR! stack     at maybeClose (node:internal/child_process:1064:16)\nnpm ERR! gyp ERR! stack     at Process.ChildProcess._handle.onexit (node:internal/child_process:301:5)\nnpm ERR! gyp ERR! System Windows_NT 10.0.19044\nnpm ERR! gyp ERR! command \"C:\\\\Program Files\\\\nodejs\\\\node.exe\" \"C:\\\\Upwork\\\\contact_book\\\\node_modules\\\\node-gyp\\\\bin\\\\node-gyp.js\" \"rebuild\" \"--verbose\" \"--libsass_ext=\" \"--libsass_cflags=\" \"--libsass_ldflags=\" \"--libsass_library=\"\nnpm ERR! gyp ERR! cwd C:\\Upwork\\contact_book\\node_modules\\node-sass\nnpm ERR! gyp ERR! node -v v16.13.1\nnpm ERR! gyp ERR! node-gyp -v v3.8.0\nnpm ERR! gyp ERR! not ok\nnpm ERR! Build failed with error code: 1\n\nI tried it in another PC but it is working fine, I think it is because I need to install something (since the PC is new)\n",
    "AcceptedAnswerId": 70968862,
    "AcceptedAnswer": "As @MehdiMamas pointed out in the comments, downgrading Node to v14 should solve the problem\nnvm install 14\nnvm use 14\n\n"
}
{
    "Id": 71048280,
    "PostTypeId": 1,
    "Title": "Upgrade python to 3.10 in windows; Do I have to reinstall all site-packages manually?",
    "Body": "I have in windows 10 64 bit installed python 3.9 with site-packages. I would like to install python 3.10.2 on windows 10 64 bit and find a way to install packages automatically in python 3.10.2, the same ones I currently have installed in python 3.9. I am also interested in the answer to this question for windows 11 64 bit.\n",
    "AcceptedAnswerId": 71048281,
    "AcceptedAnswer": "I upgraded to python 3.10.2 in windows 10 64 bit. To properly install the packages, install the appropriate version of the Microsoft Visual C++ compiler if necessary. Details can be read https://wiki.python.org/moin/WindowsCompilers . With the upgrade to python 3.10.2 from 3.9, it turned out that I had to do it, due to errors that are appearing during the installation of the packages. Before the installing python 3.10.2, type and execute the following command in the windows command prompt:\npip freeze > reqs.txt\n\nThis command writes to the reqs.txt file the names of all installed packages in the version suitable for pip. If you run the command prompt with administrator privileges, the reqs.txt file will be saved in the directory C:\\WINDOWS\\system32.\nThen, after the installing of python 3.10.2 and the adding it to the paths in PATH, with the help of the command prompt you need to issue the command:\npip install -r reqs.txt\n\nThis will start the installing of the packages in the same versions as for python 3.9. If problems occur, e.g. an installation error appears during the installation of lxml, then you can remove from the regs.txt file the entry with the name of the package whose installation is causing the problem and then install it manually. To edit the reqs.txt file you need the administrator privileges. The easiest way is to run the command prompt in the administrator mode, type reqs.txt and click Enter to edit it.\nI decided later to update the missing packages to the latest version, because I suspected that with python 3.10.2 older versions were not compatible.\nThis means that when upgrading to python 3.10.2 it is worth asking yourself whether it is better to upgrade for all packages. To do this, you can generate the list of the outdated packages using the command:\npip list \u2013-outdated\n\nAfter the printing of the list in the command prompt, you can upgrade the outdated packages using the command:\npip install --upgrade \n\nThis can be automated by the editing of the reqs.txt file and the changing of the mark == to > which will speed up the upgrade. The mark >  should only be changed for the outdated packages or you will get an error: \"Could not find a version that satisfies the requirement ... \".\nSupplement to virtual environments:\nWhen you enter a virtual environment directory (in the windows command prompt):, such as D:\\python_projects\\data_visualization\\env\\Scripts, type activate to activate it. Then create the reqs.txt file analogous to the description above. Then, copy the file to a temporary directory. After this delete the virtual environment, e.g. using the windows explorator by the deleting of the contents of the env directory. Then, using the version of python in windows of our choice, create a virtual environment using the env directory (see: https://docs.python.org/3/library/venv.html). Copy the regs.txt file to the newly created D:\\python_projects\\data_visualization\\env\\Scripts directory. Install site-packages with the support of the regs.txt file as described above.\n"
}
{
    "Id": 71019671,
    "PostTypeId": 1,
    "Title": "VSCode Python Debugger stops suddenly",
    "Body": "after installing Windows updates today, debugging is not working anymore.\nThis is my active debug configuration:\n\"launch\": {\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n    {\n      \"name\": \"DEBUG CURR\",\n      \"type\": \"python\",\n      \"request\": \"launch\",\n      \"program\": \"${file}\",\n      \"console\": \"internalConsole\",\n      \"justMyCode\": false,\n      \"stopOnEntry\": false,\n    }...\n\nWhen I start the debugger, the menu pops up briefly for 1-2 seconds. But then it closes. There is no output in the console.\nIt does not stop at set breakpoints.\nDoes anybody have the same problem? Is there a solution?\nSystem settings\n\nOS: Microsoft Windows 10 Enterprise (10.0.17763 Build 17763)\nVSCode version 1.64.0\nPython version: 3.8.11 (in the active Anaconda Environment)\n\nInstalled VSCode extensions:\n\nPython (Microsoft) version: v2022.0.1786462952\nPylance (Microsoft) version: v2022.2.0\n\n",
    "AcceptedAnswerId": 71020430,
    "AcceptedAnswer": "It's an issue with the latest Python Extension for VSCode.\nDowngrading the python extension to v2021.12.1559732655 fixes the problem.\n\n"
}
{
    "Id": 71034111,
    "PostTypeId": 1,
    "Title": "How to set default python3 to python 3.9 instead of python 3.8 in Ubuntu 20.04 LTS",
    "Body": "I have installed Python 3.9 in the Ubuntu 20.04 LTS. Now the system has both Python 3.8 and Python 3.9.\n# which python\n# which python3\n/usr/bin/python3\n# which python3.8\n/usr/bin/python3.8\n# which python3.9\n/usr/bin/python3.9\n# ls -alith /usr/bin/python3\n12583916 lrwxrwxrwx 1 root root 9 Jul 19  2021 /usr/bin/python3 -> python3.8\n\nBut the pip3 command will still install everything into the Python 3.8 directory.\n# pip3 install --upgrade --find-links file:///path/to/directory \n\nI want to change that default pip3 behavior by updating the symbolic link /usr/bin/python3 to /usr/bin/python3.9.\nHow to do that?\n# update-alternatives --set python3 /usr/bin/python3.9\nThis command will not work as expected.\n\nHere is the pip3 info:\n# which pip3\n/usr/bin/pip3\n# ls -alith /usr/bin/pip3\n12589712 -rwxr-xr-x 1 root root 367 Jul 13  2021 /usr/bin/pip3\n# pip3 -V\npip 20.0.2 from /usr/lib/python3/dist-packages/pip (python 3.8)\n# \n\nThe alias command will not work:\n# alias python3=python3.9\n# ls -alith /usr/bin/python3\n12583916 lrwxrwxrwx 1 root root 9 Jul 19  2021 /usr/bin/python3 -> python3.8\n\n",
    "AcceptedAnswerId": 71034427,
    "AcceptedAnswer": "You should be able to use python3.9 -m pip install  to run pip with a specific python version, in this case 3.9.\nThe full docs on this are here: https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/\nIf you want python3 to point to python3.9 you could use the quick and dirty.\nalias python3=python3.9\n\nEDIT:\nTried to recreate your problem,\n# which python3\n/usr/bin/python3\n# python3 --version\nPython 3.8.10\n# which python3.8\n/usr/bin/python3.8\n# which python3.9\n/usr/bin/python3.9\n\nThen update the alternatives, and set new priority:\n# sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1\n# sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2\n# sudo update-alternatives --config python3\nThere are 2 choices for the alternative python3 (providing /usr/bin/python3).\n\n  Selection    Path                Priority   Status\n------------------------------------------------------------\n  0            /usr/bin/python3.9   2         auto mode\n  1            /usr/bin/python3.8   2         manual mode\n* 2            /usr/bin/python3.9   2         manual mode\n\nPress  to keep the current choice[*], or type selection number: 0\n\nCheck new version:\n# ls -alith /usr/bin/python3\n3338 lrwxrwxrwx 1 root root 25 Feb  8 14:33 /usr/bin/python3 -> /etc/alternatives/python3\n# python3 -V\nPython 3.9.5\n# ls -alith /usr/bin/pip3\n48482 -rwxr-xr-x 1 root root 367 Jul 13  2021 /usr/bin/pip3\n# pip3 -V\npip 20.0.2 from /usr/lib/python3/dist-packages/pip (python 3.9)\n\nHope this helps (tried it in wsl2 Ubuntu 20.04 LTS)\n"
}
{
    "Id": 71078751,
    "PostTypeId": 1,
    "Title": "VS Code Python Formatting: Change max line-length with autopep8 / yapf / black",
    "Body": "I am experimenting with different python formatters and would like to increase the max line length. Ideally without editing the settings.json file. Is there a way to achieve that?\n\n",
    "AcceptedAnswerId": 71078792,
    "AcceptedAnswer": "For all three formatters, the max line length can be increased with additional arguments passed in from settings, i.e.:\n\nautopep8 args: --max-line-length=120\nblack args: --line-length=120\nyapf args: --style={based_on_style: google, column_limit: 120, indent_width: 4}\n\nHope that helps someone in the future!\n\n"
}
{
    "Id": 71121056,
    "PostTypeId": 1,
    "Title": "Plotly Python update figure with dropMenu",
    "Body": "i am currently working with plotly i have a function called plotChart that takes a dataframe as input and plots a candlestick chart. I am trying to figure out a way to pass a list of dataframes  to the function plotChart and use a plotly dropdown menu to show the options on the input list by the stock name. The drop down menu will have the list of dataframe and when an option is clicked on it will update the figure in plotly is there away to do this. below is the code i have to plot a single dataframe\ndef make_multi_plot(df):\n    \n    fig = make_subplots(rows=2, cols=2,\n                        shared_xaxes=True,\n                        vertical_spacing=0.03,\n                        subplot_titles=('OHLC', 'Volume Profile'),\n                        row_width=[0.2, 0.7])\n\n    for s in df.name.unique():\n        \n        trace1 = go.Candlestick(\n            x=df.loc[df.name.isin([s])].time,\n            open=df.loc[df.name.isin([s])].open,\n            high=df.loc[df.name.isin([s])].high,\n            low=df.loc[df.name.isin([s])].low,\n            close=df.loc[df.name.isin([s])].close,\n            name = s)\n        fig.append_trace(trace1,1,1)\n        \n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].BbandsMid, mode='lines',name='MidBollinger'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].BbandsUpp, mode='lines',name='UpperBollinger'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].BbandsLow, mode='lines',name='LowerBollinger'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].vwap, mode='lines',name='VWAP'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].STDEV_1, mode='lines',name='UPPERVWAP'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].STDEV_N1, mode='lines',name='LOWERVWAP'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].KcMid, mode='lines',name='KcMid'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].KcUpper, mode='lines',name='KcUpper'),1,1)\n        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].KcLow, mode='lines',name='KcLow'),1,1)\n        \n\n        trace2 = go.Bar(\n                x=df.loc[df.name.isin([s])].time,\n                y=df.loc[df.name.isin([s])].volume,\n                name = s)\n        fig.append_trace(trace2,2,1)\n        # fig.update_layout(title_text=s)\n        \n        \n        \n    graph_cnt=len(fig.data)\n\n        \n    tr = 11\n    symbol_cnt =len(df.name.unique())\n    for g in range(tr, graph_cnt):\n        fig.update_traces(visible=False, selector=g)\n        #print(g)\n    def create_layout_button(k, symbol):\n        \n        start, end = tr*k, tr*k+2\n        visibility = [False]*tr*symbol_cnt\n        visibility[start:end] = [True,True,True,True,True,True,True,True,True,True,True]\n        return dict(label = symbol,\n                    method = 'restyle',\n                    args = [{'visible': visibility[:-1],\n                             'title': symbol,\n                             'showlegend': False}])    \n    \n    fig.update(layout_xaxis_rangeslider_visible=False)\n    fig.update_layout(\n        updatemenus=[go.layout.Updatemenu(\n            active = 0,\n            buttons = [create_layout_button(k, s) for k, s in enumerate(df.name.unique())]\n            )\n        ])\n    \n    fig.show()\n\ni am trying to add annotations to the figure it will be different for each chart below is how i had it setup for the single chart df['superTrend'] is a Boolean column\nfor i in range(df.first_valid_index()+1,len(df.index)):\n        prev = i - 1\n        if df['superTrend'][i] != df['superTrend'][prev] and not np.isnan(df['superTrend'][i]) :\n            #print(i,df['inUptrend'][i])\n            fig.add_annotation(x=df['time'][i], y=df['open'][i],\n            text= 'Buy' if df['superTrend'][i] else 'Sell',\n            showarrow=True,\n            arrowhead=6,\n            font=dict(\n                #family=\"Courier New, monospace\",\n                size=20,\n                #color=\"#ffffff\"\n            ),)\n\n",
    "AcceptedAnswerId": 71155096,
    "AcceptedAnswer": "I adapted an example from the plotly community to your example and created the code. The point of creation is to create the data for each subplot and then switch between them by means of buttons. The sample data is created using representative companies of US stocks. one issue is that the title is set but not displayed. We are currently investigating this issue.\nimport yfinance as yf\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport pandas as pd\n\nsymbols = ['AAPL','GOOG','TSLA']\nstocks = pd.DataFrame()\nfor s in symbols:\n    data = yf.download(s, start=\"2021-01-01\", end=\"2021-12-31\")\n    data['mean'] = data['Close'].rolling(20).mean()\n    data['std'] = data['Close'].rolling(20).std()\n    data['upperBand'] = data['mean'] + (data['std'] * 2)\n    data.reset_index(inplace=True)\n    data['symbol'] = s\n    stocks = stocks.append(data, ignore_index=True)\n\ndef make_multi_plot(df):\n    \n    fig = make_subplots(rows=2, cols=1,\n                        shared_xaxes=True,\n                        vertical_spacing=0.03,\n                        subplot_titles=('OHLC', 'Volume Profile'),\n                        row_width=[0.2, 0.7])\n\n    for s in df.symbol.unique():\n        trace1 = go.Candlestick(\n            x=df.loc[df.symbol.isin([s])].Date,\n            open=df.loc[df.symbol.isin([s])].Open,\n            high=df.loc[df.symbol.isin([s])].High,\n            low=df.loc[df.symbol.isin([s])].Low,\n            close=df.loc[df.symbol.isin([s])].Close,\n            name=s)\n        fig.append_trace(trace1,1,1)\n        \n        trace2 = go.Scatter(\n            x=df.loc[df.symbol.isin([s])].Date,\n            y=df.loc[df.symbol.isin([s])].upperBand,\n            name=s)\n        fig.append_trace(trace2,1,1)\n        \n        trace3 = go.Bar(\n            x=df.loc[df.symbol.isin([s])].Date,\n            y=df.loc[df.symbol.isin([s])].Volume,\n            name=s)\n        fig.append_trace(trace3,2,1)\n        # fig.update_layout(title_text=s)\n    \n    # Calculate the total number of graphs\n    graph_cnt=len(fig.data)\n    # Number of Symbols\n    symbol_cnt =len(df.symbol.unique())\n    # Number of graphs per symbol\n    tr = 3\n    # Hide setting for initial display\n    for g in range(tr, graph_cnt): \n        fig.update_traces(visible=False, selector=g)\n\n    def create_layout_button(k, symbol):\n        start, end = tr*k, tr*k+2\n        visibility = [False]*tr*symbol_cnt\n        # Number of graphs per symbol, so if you add a graph, add True.\n        visibility[start:end] = [True,True,True]\n        return dict(label = symbol,\n                    method = 'restyle',\n                    args = [{'visible': visibility[:-1],\n                             'title': symbol,\n                             'showlegend': True}])    \n    \n    fig.update(layout_xaxis_rangeslider_visible=False)\n    fig.update_layout(\n        updatemenus=[go.layout.Updatemenu(\n            active = 0,\n            buttons = [create_layout_button(k, s) for k, s in enumerate(df.symbol.unique())]\n            )\n        ])\n    \n    fig.show()\n    return fig.layout\n    \nmake_multi_plot(stocks)\n\n\n\n\n"
}
{
    "Id": 71193085,
    "PostTypeId": 1,
    "Title": "Creating nested columns in python dataframe",
    "Body": "I have 3 columns namely Models(should be taken as index), Accuracy without normalization, Accuracy with normalization (zscore, minmax, maxabs, robust) and these are required to be created as:\n ------------------------------------------------------------------------------------\n|   Models  |  Accuracy without normalization    |      Accuracy with normalization  |\n|           |                                    |-----------------------------------|\n|           |                                    | zscore | minmax | maxabs | robust |\n ------------------------------------------------------------------------------------\n\n\ndfmod-> Models column\ndfacc-> Accuracy without normalization\ndfacc1-> Accuracy with normalization - zscore\ndfacc2-> Accuracy with normalization - minmax\ndfacc3-> Accuracy with normalization - maxabs\ndfacc4-> Accuracy with normalization - robust\n\ndfout=pd.DataFrame({('Accuracy without Normalization'):{dfacc},\n     ('Accuracy using Normalization','zscore'):{dfacc1},\n     ('Accuracy using Normalization','minmax'):{dfacc2},\n     ('Accuracy using Normalization','maxabs'):{dfacc3},\n     ('Accuracy using Normalization','robust'):{dfacc4},\n   },index=dfmod\n)\n\nI was trying to do something like this but i can't figure out any further\nTest data:\nqda    0.6333       0.6917      0.5917      0.6417     0.5833\nsvm    0.5333       0.6917      0.5333      0.575      0.575\nlda    0.5333       0.6583      0.5333      0.5667     0.5667\nlr     0.5333       0.65        0.4917      0.5667     0.5667\ndt     0.5333       0.65        0.4917      0.5667     0.5667\nrc     0.5083       0.6333      0.4917      0.525      0.525\nnb     0.5          0.625       0.475       0.5        0.4833\nrfc    0.5          0.625       0.4417      0.4917     0.4583\nknn    0.3917       0.6         0.4417      0.4833     0.45\net     0.375        0.5333      0.4333      0.4667     0.45\ndc     0.375        0.5333      0.4333      0.4667     0.425\nqds    0.3417       0.5333      0.4         0.4583     0.3667\nlgt    0.3417       0.525       0.3917      0.45       0.3583\nlt     0.2333       0.45        0.3917      0.4167     0.3417\n\nThese are values for respective subcolumns in order specified in the table above\n",
    "AcceptedAnswerId": 71194341,
    "AcceptedAnswer": "There's a dirty way to do this, I'll write about it till someone answers with a better idea. Here we go:\nimport pandas as pd\n\n# I assume that you can read raw data named test.csv by pandas and\n# set header = None cause you mentioned the Test data without any headers, so:\ndf = pd.read_csv(\"test.csv\", header = None)\n\n# Then define preferred Columns! \nMyColumns = pd.MultiIndex.from_tuples([(\"Models\" , \"\"),\n                                       (\"Accuracy without normalization\" , \"\"),\n                                       (\"Accuracy with normalization\" , \"zscore\"),\n                                       (\"Accuracy with normalization\" , \"minmax\"),\n                                       (\"Accuracy with normalization\" , \"maxabs\"),\n                                       (\"Accuracy with normalization\" , \"robust\")])\n\n# Create new DataFrame with specified Columns, after this you should pass values \nNew_DataFrame = pd.DataFrame(df , columns = MyColumns)\n\n# a loop for passing values\nfor item in range(len(MyColumns)):\n    New_DataFrame.loc[: , MyColumns[item]] = df.iloc[: , item]\n\nThis gives me:\n\nafter all, if you want to set Models as the index of New_DataFrame, You can continue with:\nNew_DataFrame.set_index(New_DataFrame.columns[0][0] , inplace=True)\nNew_DataFrame\n\nThis gives me:\n\n"
}
{
    "Id": 71758114,
    "PostTypeId": 1,
    "Title": "Python list comprehension with complex data structures",
    "Body": "I'm trying to flatten some mixed arrays in Python using LC. I'm having some trouble figuring out how to structure it.\nHere's the array's i'm trying to flatten\narr_1 = [1, [2, 3], 4, 5]\narr_2 = [1,[2,3],[[4,5]]]\n\nI tried this methods for arr_1 but get \"TypeError: 'int' object is not iterable\"\nprint([item if type(items) is list else items for items in arr_1 for item in items])\n\nSo I decided to break it into parts to see where it's failing by using this\ndef check(item):\nreturn item;\n\nprint([check(item) if type(items) is list else check(items) for items in [1, [2, 3], 4, 5] for items in arr_2]) \n\nThrough the debugger I found that it's failing at the 2d array in\nfor items in [1, [2, 3], 4, 5]\n\nI don't need the LC to be in one line but I just wanted to know how to do it in a single nested LC if its even possible.\n",
    "AcceptedAnswerId": 71758467,
    "AcceptedAnswer": "Using an internal stack and iter's second form to simulate a while loop:\ndef flatten(obj):\n    return [x\n            for stack in [[obj]]\n            for x, in iter(lambda: stack and [stack.pop()], [])\n            if isinstance(x, int)\n            or stack.extend(reversed(x))]\n\nprint(flatten([1, [2, 3], 4, 5]))\nprint(flatten([1, [2, 3], [[4, 5]]]))\nprint(flatten([1, [2, [], 3], [[4, 5]]]))\n\nOutput (Try it online!):\n[1, 2, 3, 4, 5]\n[1, 2, 3, 4, 5]\n[1, 2, 3, 4, 5]\n\nSlight variation, splitting the \"long\" line into two (Try it online!):\ndef flatten(obj):\n    return [x\n            for stack in [[obj]]\n            for _ in iter(lambda: stack, [])\n            for x in [stack.pop()]\n            if isinstance(x, int)\n            or stack.extend(reversed(x))]\n\nTo explain it a bit, here's roughly the same with ordinary code:\ndef flatten(obj):\n    result = []\n    stack = [obj]\n    while stack:\n        x = stack.pop()\n        if isinstance(x, int):\n            result.append(x)\n        else:\n            stack.extend(reversed(x))\n    return result\n\nIf the order doesn't matter, we can use a queue instead (inspired by 0x263A's comment), although it's less memory-efficient (Try it online!):\ndef flatten(obj):\n    return [x\n            for queue in [[obj]]\n            for x in queue\n            if isinstance(x, int) or queue.extend(x)]\n\nWe can fix the order if instead of putting each list's contents at the end of the queue, we insert them right after the list (which is less time-efficient) in the \"priority\" queue (Try it online!):\ndef flatten(obj):\n    return [x\n            for pqueue in [[obj]]\n            for i, x in enumerate(pqueue, 1)\n            if isinstance(x, int) or pqueue.__setitem__(slice(i, i), x)]\n\n"
}
{
    "Id": 71079342,
    "PostTypeId": 1,
    "Title": "How can I take comma separated inputs for python AnyTree module?",
    "Body": "Community. I need to accept multiple comma-separated inputs to produce a summary of information ( specifically, how many different employees participated in each group/project)? The program takes employees, managers and groups in the form of strings.\nI'm using anytree python library to be able to search/count the occurrence of each employee per group. However, this program is only accepting one value/cell at a time instead of multiple values. \nHere is the tree structure and how I accept input values?\nPress q to exit, Enter your data: Joe\nPress q to exit, Enter your data: Manager1\nPress q to exit, Enter your data: Group1\nPress q to exit, Enter your data: Charles \nPress q to exit, Enter your data: Manager1\nPress q to exit, Enter your data: Group2\nPress q to exit, Enter your data: Joe\nPress q to exit, Enter your data: Manager3\nPress q to exit, Enter your data: Group1\nPress q to exit, Enter your data: Charles\nPress q to exit, Enter your data: Manager3\nPress q to exit, Enter your data: Group1\nPress q to exit, Enter your data: Joe\nPress q to exit, Enter your data: Manager5\nPress q to exit, Enter your data: Group2\nPress q to exit, Enter your data: q\nEmployee   No of groups\n   JOE       2\n   CHARLES       2\nGroup\n\u251c\u2500\u2500 GROUP1\n\u2502   \u251c\u2500\u2500 JOE\n\u2502   \u2502   \u2514\u2500\u2500 MANAGER1\n\u2502   \u251c\u2500\u2500 JOE\n\u2502   \u2502   \u2514\u2500\u2500 MANAGER3\n\u2502   \u2514\u2500\u2500 CHARLES\n\u2502       \u2514\u2500\u2500 MANAGER3\n\u2514\u2500\u2500 GROUP2\n    \u251c\u2500\u2500 CHARLES\n    \u2502   \u2514\u2500\u2500 MANAGER1\n    \u2514\u2500\u2500 JOE\n        \u2514\u2500\u2500 MANAGER5\n\nI need help with this code so that It can accept comma-separated values; for example, to enter Joe, Manager1, Group1 at a time.\nimport anytree\n\nfrom anytree import Node, RenderTree, LevelOrderIter, LevelOrderGroupIter, PreOrderIter\n\nimport sys\n\n# user input\nio=''\nlst_input = []\nwhile (io!='q'):\n    io=input('Press q to exit, Enter your data: ')\n    if io!='q':\n        lst_input.append(io.upper())\n\n# change list in to matrix\nlst=[]\nfor i in range(0, len(lst_input), 3):\n    lst.append(lst_input[i:i + 3])\n\nlst\n\n# create tree structure from lst\ngroup = Node('Group')\nstoreGroup = {}\nfor i in range(len(lst)):\n    if lst[i][2] in [x.name for x in group.children]: # parent already exist, append childrens\n        storeGroup[lst[i][0]] = Node(lst[i][0], parent=storeGroup[lst[i][2]])\n        storeGroup[lst[i][1]] = Node(lst[i][1], parent=storeGroup[lst[i][0]])\n    else: # create parent and append childreds\n        storeGroup[lst[i][2]] = Node(lst[i][2], parent=group)\n        storeGroup[lst[i][0]] = Node(lst[i][0], parent=storeGroup[lst[i][2]])\n        storeGroup[lst[i][1]] = Node(lst[i][1], parent=storeGroup[lst[i][0]])\n\n\nstore = {}\nfor children in LevelOrderIter(group, maxlevel=3):\n    if children.parent!=None and children.parent.name!='Group':\n        if children.name not in store:\n            store[children.name] = {children.parent.name}\n        else:\n            store[children.name] = store[children.name] | {children.parent.name}\n\nprint('Employee', '  No of groups')\nfor i in store:\n    print('   '+i+'      ', len(store[i]))\n\n\nfor pre,fill, node in RenderTree(group):\n    print('{}{}'.format(pre,node.name))\n\n Thank you! Any thoughts are welcomed.\n",
    "AcceptedAnswerId": 71110010,
    "AcceptedAnswer": "Leverage unpacking to extract elements. Then the if statement can be re-written this way.\nif io!='q':\n    name, role, grp = io.upper(). split(',')\n    lst_input.append([name,role, grp]) \n\nyou also need to change lst.append(lst_input[i:i + 3]) in the for loop to this.\nlst.append(lst_input[0][i:i + 3])\n\n"
}
{
    "Id": 71295840,
    "PostTypeId": 1,
    "Title": "python pip: \"error: legacy-install-failure\"",
    "Body": "I want to install gensim python package via pip install gensim\nBut this error occurs and I have no idea what should I do to solve it.\n      running build_ext\n      building 'gensim.models.word2vec_inner' extension\n      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: legacy-install-failure\n\n\u00d7 Encountered error while trying to install package.\n\u2570\u2500> gensim\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for output from the failure.\n\n",
    "AcceptedAnswerId": 71296224,
    "AcceptedAnswer": "If you fail to install plugins,\nyou can download it from other repositories like this one:\nrepository depends on the version of python and the system.\nfor example: for  windows 11(x64) and python 3.10 you should take this file: gensim\u20114.1.2\u2011cp310\u2011cp310\u2011win_amd64.whl\n"
}
{
    "Id": 71297077,
    "PostTypeId": 1,
    "Title": "Python regex replace every 2nd occurrence in a string",
    "Body": "I have a string with data that looks like this:\nstr1 = \"[2.4],[5],[2.54],[4],[3.36],[4.46],[3.36],[4],[3.63],[4.86],[4],[4.63]\"\n\nI would want to replace every second iteration of \"],[\" with \",\" so it will look like this:\nstr2 = \"[2.4,5],[2.54,4],[3.36,4.46],[3.36,4],[3.63,4.86],[4,4.63]\"\n\nHere is was I have so far:\nstr1 = \"[2.4],[5],[2.54],[4],[3.36],[4.46],[3.36],[4],[3.63],[4.86],[4],[4.63]\"\ns2 = re.sub(r\"],\\[\", ',', str1)\nprint(s2)\n\nI was trying to mess around with this:\n(.*?],\\[){2}\n\nBut it does not seem to yield me the desired results.\nI tried using loops but I only managed to replace only the second occurrence and nothing after using this sample code I found here. And the code is:\nimport re\n\ndef replacenth(string, sub, wanted, n):\n    where = [m.start() for m in re.finditer(sub, string)][n-1]\n    before = string[:where]\n    after = string[where:]\n    after = after.replace(sub, wanted, 1)\n    newString = before + after\n    print(newString)\nFor these variables:\n\nstring = 'ababababababababab'\nsub = 'ab'\nwanted = 'CD'\nn = 5\n\nThank you.\n",
    "AcceptedAnswerId": 71297176,
    "AcceptedAnswer": "You can use\nimport re\nfrom itertools import count\n\nstr1 = \"[2.4],[5],[2.54],[4],[3.36],[4.46],[3.36],[4],[3.63],[4.86],[4],[4.63]\"\nc = count(0)\nprint( re.sub(r\"],\\[\", lambda x: \",\" if next(c) % 2 == 0 else x.group(), str1) )\n# => [2.4,5],[2.54,4],[3.36,4.46],[3.36,4],[3.63,4.86],[4,4.63]\n\nSee the Python demo.\nThe regex is the same, ],\\[, it matches a literal ],[ text.\nThe c = count(0)  initializes the counter whose value is incremented upon each match inside a lambda expression used as the replacement argument. When the  counter is even, the match is replaced with a comma, else, it is kept as is.\n"
}
{
    "Id": 71805426,
    "PostTypeId": 1,
    "Title": "how to tell a python type checker that an optional definitely exists?",
    "Body": "I'm used to typescript, in which one can use a ! to tell the type-checker to assume a value won't be null. Is there something analogous when using type annotations in python?\nA (contrived) example:\nWhen executing the expression m.maybe_num + 3 in the code below, the enclosing if guarantees that maybe_num won't be None.  But the type-checker doesn't know that, and returns an error.  (Verified in https://mypy-play.net/?mypy=latest&python=3.10.) How can I tell the type-checker that I know better?\nfrom typing import Optional\n\nclass MyClass:\n\n    def __init__(self, maybe_num: Optional[int]):\n        self.maybe_num = maybe_num\n        \n    def has_a_num(self) -> bool:\n        return self.maybe_num is not None\n\n    def three_more(self) -> Optional[int]:\n        if self.has_a_num:\n            # mypy error: Unsupported operand types for + (\"None\" and \"int\")\n            return self.maybe_num + 3\n        else:\n            return None\n\n",
    "AcceptedAnswerId": 71806921,
    "AcceptedAnswer": "Sadly there's no clean way to infer the type of something from a function call like this, but you can work some magic with TypeGuard annotations for the has_a_num() method, although the benefit from those annotations won't really be felt unless the difference is significantly more major than the type of a single int. If it's just a single value, you should just use a standard  is not None check.\nif self.maybe_num is not None:\n    ...\n\nYou can define a subclass of your primary subclass, where the types of any parameters whose types are affected are explicitly redeclared.\nclass MyIntClass(MyClass):\n    maybe_num: int\n\nFrom there, your checker function should still return a boolean, but the annotated return type tells MyPy that it should use it for type narrowing to the listed type.\nSadly it will only do this for proper function parameters, rather than the implicit self argument, but this can be fixed easily enough by providing self explicitly as follows:\nif MyClass.has_a_num(self):\n    ...\n\nThat syntax is yucky, but it works with MyPy.\nThis makes the full solution be as follows\n# Parse type annotations as strings to avoid \n# circular class references\nfrom __future__ import annotations\nfrom typing import Optional, TypeGuard\n\nclass MyClass:\n    def __init__(self, maybe_num: Optional[int]):\n        self.maybe_num = maybe_num\n\n    def has_a_num(self) -> TypeGuard[_MyClass_Int]:\n        # This annotation defines a type-narrowing operation,\n        # such that if the return value is True, then self\n        # is (from MyPy's perspective) _MyClass_Int, and \n        # otherwise it isn't\n        return self.maybe_num is not None\n\n    def three_more(self) -> Optional[int]:\n        if MyClass.has_a_num(self):\n            # No more mypy error\n            return self.maybe_num + 3\n        else:\n            return None\n\nclass _MyClass_Int(MyClass):\n    maybe_num: int\n\nTypeGuard was added in Python 3.10, but can be used in earlier versions using the typing_extensions module from pip.\n"
}
{
    "Id": 71232879,
    "PostTypeId": 1,
    "Title": "How to speed up async requests in Python",
    "Body": "I want to download/scrape 50 million log records from a site. Instead of downloading 50 million in one go, I was trying to download it in parts like 10 million at a time using the following code but it's only handling 20,000 at a time (more than that throws an error) so it becomes time-consuming to download that much data. Currently, it takes 3-4 mins to download 20,000 records with the speed of 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20000/20000 [03:48 so how to speed it up?\nimport asyncio\nimport aiohttp\nimport time\nimport tqdm\nimport nest_asyncio\n\nnest_asyncio.apply()\n\n\nasync def make_numbers(numbers, _numbers):\n    for i in range(numbers, _numbers):\n        yield i\n\n\nn = 0\nq = 10000000\n\n\nasync def fetch():\n    # example\n    url = \"https://httpbin.org/anything/log?id=\"\n\n    async with aiohttp.ClientSession() as session:\n        post_tasks = []\n        # prepare the coroutines that poat\n        async for x in make_numbers(n, q):\n            post_tasks.append(do_get(session, url, x))\n        # now execute them all at once\n\n        responses = [await f for f in tqdm.tqdm(asyncio.as_completed(post_tasks), total=len(post_tasks))]\n\n\nasync def do_get(session, url, x):\n    headers = {\n        'Content-Type': \"application/x-www-form-urlencoded\",\n        'Access-Control-Allow-Origin': \"*\",\n        'Accept-Encoding': \"gzip, deflate\",\n        'Accept-Language': \"en-US\"\n    }\n\n    async with session.get(url + str(x), headers=headers) as response:\n        data = await response.text()\n        print(data)\n\n\ns = time.perf_counter()\ntry:\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(fetch())\nexcept:\n    print(\"error\")\n\nelapsed = time.perf_counter() - s\n# print(f\"{__file__} executed in {elapsed:0.2f} seconds.\")\n\nTraceback (most recent call last):\nFile \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\connector.py\", line 986, in _wrap_create_connection\n    return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1056, in create_connection\n    raise exceptions[0]\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1041, in create_connection\n    sock = await self._connect_sock(\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 955, in _connect_sock\n    await self.sock_connect(sock, address)\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\proactor_events.py\", line 702, in sock_connect\n    return await self._proactor.connect(sock, address)\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\tasks.py\", line 328, in __wakeup\n    future.result()\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\windows_events.py\", line 812, in _poll\n    value = callback(transferred, key, ov)\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\windows_events.py\", line 599, in finish_connect\n    ov.getresult()\nOSError: [WinError 121] The semaphore timeout period has expired\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\SGM\\Desktop\\xnet\\x3stackoverflow.py\", line 136, in \n    loop.run_until_complete(fetch())\n  File \"C:\\Users\\SGM\\AppData\\Roaming\\Python\\Python39\\site-packages\\nest_asyncio.py\", line 81, in run_until_complete\n    return f.result()\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\futures.py\", line 201, in result\n    raise self._exception\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\tasks.py\", line 256, in __step\n    result = coro.send(None)\n  File \"C:\\Users\\SGM\\Desktop\\xnet\\x3stackoverflow.py\", line 88, in fetch\n    response = await f\n  File \"C:\\Users\\SGM\\Desktop\\xnet\\x3stackoverflow.py\", line 37, in _wait_for_one\n    return f.result()\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\futures.py\", line 201, in result\n    raise self._exception\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\tasks.py\", line 258, in __step\n    result = coro.throw(exc)\n  File \"C:\\Users\\SGM\\Desktop\\xnet\\x3stackoverflow.py\", line 125, in do_get\n    async with session.get(url + str(x), headers=headers) as response:\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\client.py\", line 1138, in __aenter__\n    self._resp = await self._coro\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\client.py\", line 535, in _request\n    conn = await self._connector.connect(\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\connector.py\", line 542, in connect\n    proto = await self._create_connection(req, traces, timeout)\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\connector.py\", line 907, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\connector.py\", line 1206, in _create_direct_connection\n    raise last_exc\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\connector.py\", line 1175, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n  File \"C:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\aiohttp\\connector.py\", line 992, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host example.com:80 ssl:default [The semaphore timeout period has expired]\n\n",
    "AcceptedAnswerId": 71285322,
    "AcceptedAnswer": "Bottleneck: number of simultaneous connections\nFirst, the bottleneck is the total number of simultaneous connections in the TCP connector.\nThat default for aiohttp.TCPConnector is limit=100. On most systems (tested on macOS), you should be able to double that by passing a connector with limit=200:\n# async with aiohttp.ClientSession() as session:\nasync with aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit=200)) as session:\n\nThe time taken should decrease significantly. (On macOS: q = 20_000 decreased 43% from 58 seconds to 33 seconds, and q = 10_000 decreased 42% from 31 to 18 seconds.)\nThe limit you can configure depends on the number of file descriptors that your machine can open. (On macOS: You can run ulimit -n to check, and ulimit -n 1024 to increase to 1024 for the current terminal session, and then change to limit=1000. Compared to limit=100, q = 20_000 decreased 76% to 14 seconds, and q = 10_000 decreased 71% to 9 seconds.)\nSupporting 50 million requests: async generators\nNext, the reason why 50 million requests appears to hang is simply because of its sheer number.\nJust creating 10 million coroutines in post_tasks takes 68-98 seconds (varies greatly on my machine), and then the event loop is further burdened with that many tasks, 99.99% of which are blocked by the TCP connection pool.\nWe can defer the creation of coroutines using an async generator:\nasync def make_async_gen(f, n, q):\n    async for x in make_numbers(n, q):\n        yield f(x)\n\nWe need a counterpart to asyncio.as_completed() to handle async_gen and concurrency:\nfrom asyncio import ensure_future, events\nfrom asyncio.queues import Queue\n\ndef as_completed_for_async_gen(fs_async_gen, concurrency):\n    done = Queue()\n    loop = events.get_event_loop()\n    # todo = {ensure_future(f, loop=loop) for f in set(fs)}  # -\n    todo = set()                                             # +\n\n    def _on_completion(f):\n        todo.remove(f)\n        done.put_nowait(f)\n        loop.create_task(_add_next())  # +\n\n    async def _wait_for_one():\n        f = await done.get()\n        return f.result()\n\n    async def _add_next():  # +\n        try:\n            f = await fs_async_gen.__anext__()\n        except StopAsyncIteration:\n            return\n        f = ensure_future(f, loop=loop)\n        f.add_done_callback(_on_completion)\n        todo.add(f)\n\n    # for f in todo:                           # -\n    #     f.add_done_callback(_on_completion)  # -\n    # for _ in range(len(todo)):               # -\n    #     yield _wait_for_one()                # -\n    for _ in range(concurrency):               # +\n        loop.run_until_complete(_add_next())   # +\n    while todo:                                # +\n        yield _wait_for_one()                  # +\n\nThen, we update fetch():\nfrom functools import partial\n\nCONCURRENCY = 200  # +\n\nn = 0\nq = 50_000_000\n\nasync def fetch():\n    # example\n    url = \"https://httpbin.org/anything/log?id=\"\n\n    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit=CONCURRENCY)) as session:\n        # post_tasks = []                                                # -\n        # # prepare the coroutines that post                             # -\n        # async for x in make_numbers(n, q):                             # -\n        #     post_tasks.append(do_get(session, url, x))                 # -\n        # Prepare the coroutines generator                               # +\n        async_gen = make_async_gen(partial(do_get, session, url), n, q)  # +\n\n        # now execute them all at once                                                                         # -\n        # responses = [await f for f in tqdm.asyncio.tqdm.as_completed(post_tasks, total=len(post_tasks))]     # -\n        # Now execute them with a specified concurrency                                                        # +\n        responses = [await f for f in tqdm.tqdm(as_completed_for_async_gen(async_gen, CONCURRENCY), total=q)]  # +\n\nOther limitations\nWith the above, the program can start processing 50 million requests but:\n\nit will still take 8 hours or so with CONCURRENCY = 1000, based on the estimate from tqdm.\nyour program may run out of memory for responses and crash.\n\nFor point 2, you should probably do:\n# responses = [await f for f in tqdm.tqdm(as_completed_for_async_gen(async_gen, CONCURRENCY), total=q)]\nfor f in tqdm.tqdm(as_completed_for_async_gen(async_gen, CONCURRENCY), total=q):\n    response = await f\n    \n    # Do something with response, such as writing to a local file\n    # ...\n\n\nAn error in the code\ndo_get() should return data:\nasync def do_get(session, url, x):\n    headers = {\n        'Content-Type': \"application/x-www-form-urlencoded\",\n        'Access-Control-Allow-Origin': \"*\",\n        'Accept-Encoding': \"gzip, deflate\",\n        'Accept-Language': \"en-US\"\n    }\n\n    async with session.get(url + str(x), headers=headers) as response:\n        data = await response.text()\n        # print(data)  # -\n        return data    # +\n\n"
}
{
    "Id": 71343002,
    "PostTypeId": 1,
    "Title": "Downloading files from public Google Drive in python: scoping issues?",
    "Body": "Using my answer to my question on how to download files from a public Google drive I managed in the past to download images using their IDs from a python script and Google API v3 from a public drive using the following bock of code:\nfrom google_auth_oauthlib.flow import Flow, InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaFileUpload, MediaIoBaseDownload\nfrom google.auth.transport.requests import Request\nimport io\nimport re\nSCOPES = ['https://www.googleapis.com/auth/drive']\nCLIENT_SECRET_FILE = \"myjson.json\"\nauthorized_port = 6006 # authorize URI redirect on the console\nflow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRET_FILE, SCOPES)\ncred = flow.run_local_server(port=authorized_port)\ndrive_service = build(\"drive\", \"v3\", credentials=cred)\nregex = \"(?<=https://drive.google.com/file/d/)[a-zA-Z0-9]+\"\nfor i, l in enumerate(links_to_download):\n    url = l\n    file_id = re.search(regex, url)[0]\n    request = drive_service.files().get_media(fileId=file_id)\n    fh = io.FileIO(f\"file_{i}\", mode='wb')\n    downloader = MediaIoBaseDownload(fh, request)\n    done = False\n    while done is False:\n        status, done = downloader.next_chunk()\n        print(\"Download %d%%.\" % int(status.progress() * 100))\n\nIn the mean time I discovered pydrive and pydrive2, two wrappers around Google API v2 that allows to do very useful things such as listing files from folders and basically allows to do the same thing with a lighter syntax:\nfrom pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\nimport io\nimport re\nCLIENT_SECRET_FILE = \"client_secrets.json\"\n\ngauth = GoogleAuth()\ngauth.LocalWebserverAuth()\ndrive = GoogleDrive(gauth)\nregex = \"(?<=https://drive.google.com/file/d/)[a-zA-Z0-9]+\"\nfor i, l in enumerate(links_to_download):\n    url = l\n    file_id = re.search(regex, url)[0]\n    file_handle = drive.CreateFile({'id': file_id})\n    file_handle.GetContentFile(f\"file_{i}\")\n\nHowever now whether I use pydrive or the raw API I cannot seem to be able to download the same files and instead I am met with:\ngoogleapiclient.errors.HttpError: \n\nI tried everything and registered 3 different apps using Google console it seems it might be (or not) a question of scoping (see for instance this answer, with apps having access to only files in my Google drive or created by this app). However I did not have this issue before (last year).\nWhen going to the Google console explicitly giving https://www.googleapis.com/auth/drive as a scope to the API mandates filling a ton of fields with application's website/conditions of use/confidentiality rules/authorized domains and youtube videos explaining the app. However I will be the sole user of this script.\nSo I could only give explicitly the following scopes:\n/auth/drive.appdata\n/auth/drive.file\n/auth/drive.install\n\nIs it because of scoping ? Is there a solution that doesn't require creating a homepage and a youtube video ?\nEDIT 1:\nHere is an example of links_to_download:\nlinks_to_download = [\"https://drive.google.com/file/d/fileID/view?usp=drivesdk&resourcekey=0-resourceKeyValue\"]\n\nEDIT 2:\nIt is super instable sometimes it works without a sweat sometimes it doesn't. When I relaunch the script multiple times I get different results. Retry policies are working to a certain extent but sometimes it fails multiple times for hours.\n",
    "AcceptedAnswerId": 71351780,
    "AcceptedAnswer": "Well thanks to the security update released by Google few months before. This makes the link sharing stricter and you need resource key as well to access the file in-addition to the fileId.\nAs per the documentation , You need to provide the resource key as well for newer links, if you want to access it in the header X-Goog-Drive-Resource-Keys as fileId1/resourceKey1.\nIf you apply this change in your code, it will work as normal. Example edit below:\nregex = \"(?<=https://drive.google.com/file/d/)[a-zA-Z0-9]+\"\nregex_rkey = \"(?<=resourcekey=)[a-zA-Z0-9-]+\"\nfor i, l in enumerate(links_to_download):\n    url = l\n    file_id = re.search(regex, url)[0]\n    resource_key = re.search(regex_rkey, url)[0]\n    request = drive_service.files().get_media(fileId=file_id)\n    request.headers[\"X-Goog-Drive-Resource-Keys\"] = f\"{file_id}/{resource_key}\"\n    fh = io.FileIO(f\"file_{i}\", mode='wb')\n    downloader = MediaIoBaseDownload(fh, request)\n    done = False\n    while done is False:\n        status, done = downloader.next_chunk()\n        print(\"Download %d%%.\" % int(status.progress() * 100))\n\nWell, the regex for resource key was something I quickly made, so cannot be sure on if it supports every case. But this provides you the solution.\nNow, you may have to listen to old and new links based on this and set the changes.\n"
}
{
    "Id": 71029876,
    "PostTypeId": 1,
    "Title": "How can I perform a type guard on a property of an object in Python",
    "Body": "PEP 647 introduced type guards to perform complex type narrowing operations using functions. If I have a class where properties can have various types, is there a way that I can perform a similar type narrowing operation on the property of an object given as the function argument?\nclass MyClass:\n    a: Optional[int]\n    b: Optional[str]\n    # Some other things\n\ndef someTypeGuard(my_obj: MyClass) -> ???:\n    return my_obj.a is not None\n\nI'm thinking it might be necessary for me to implement something to do with square brackets in type hints, but I really don't know where to start on this.\n",
    "AcceptedAnswerId": 71252167,
    "AcceptedAnswer": "TypeGuard annotations can be used to annotate subclasses of a class. If parameter types are specified for those classes, then MyPy will recognise the type narrowing operation successfully.\nclass MyClass:\n    a: Optional[int]\n    b: Optional[str]\n    # Some other things\n\n# Two hidden classes for the different types\nclass _MyClassInt(MyClass):\n    a: int\n    b: None\nclass _MyClassStr(MyClass):\n    a: None\n    b: str\n\n\ndef someTypeGuard(my_obj: MyClass) -> TypeGuard[_MyClassInt]:\n    \"\"\"Check if my_obj's `a` property is NOT `None`\"\"\"\n    return my_obj.a is not None\n\ndef someOtherTypeGuard(my_obj: MyClass) -> TypeGuard[_MyClassStr]:\n    \"\"\"Check if my_obj's `b` property is NOT `None`\"\"\"\n    return my_obj.b is not None\n\nSadly failure to narrow to one type doesn't automatically narrow to the other type, and I can't find an easy way to do this other than an assert someOtherTypeGuard(obj) in your else block.\nEven still this seems to be the best solution.\n"
}
{
    "Id": 71372066,
    "PostTypeId": 1,
    "Title": "Docker fails to install cffi with python:3.9-alpine in Dockerfile",
    "Body": "Im trying to run the below Dockerfile using docker-compose.\nI searched around but I couldnt find a solution on how to install cffi with python:3.9-alpine.\nI also read this post which states that pip 21.2.4 or greater can be a possible solution but it didn't work out form me\nhttps://www.pythonfixing.com/2021/09/fixed-why-i-getting-this-error-while.html\nDocker file\nFROM python:3.9-alpine\n\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\nCOPY ./requirements.txt .\n\nRUN apk add --update --no-cache postgresql-client\n\nRUN apk add --update --no-cache --virtual .tmp-build-deps \\\n    gcc libc-dev linux-headers postgresql-dev\nRUN pip3 install --upgrade pip && pip3 install -r /requirements.txt\n\nRUN apk del .tmp-build-deps\n\nRUN mkdir /app\nWORKDIR /app\nCOPY . /app\n\nRUN adduser -D user\n\nUSER user\n\nThis is the requirements.txt file.\nasgiref==3.5.0\nbackports.zoneinfo==0.2.1\ncertifi==2021.10.8\ncffi==1.15.0\ncfgv==3.3.1\n...\n\nError message:\nprocess-exited-with-error\n#9 47.99   \n#9 47.99   \u00d7 Running setup.py install for cffi did not run successfully.\n#9 47.99   \u2502 exit code: 1\n#9 47.99   \u2570\u2500> [58 lines of output]\n#9 47.99       Package libffi was not found in the pkg-config search path.\n#9 47.99       Perhaps you should add the directory containing `libffi.pc'\n#9 47.99       to the PKG_CONFIG_PATH environment variable\n#9 47.99       Package 'libffi', required by 'virtual:world', not found\n#9 47.99       Package libffi was not found in the pkg-config search path.\n#9 47.99       Perhaps you should add the directory containing `libffi.pc'\n#9 47.99       to the PKG_CONFIG_PATH environment variable\n#9 47.99       Package 'libffi', required by 'virtual:world', not found\n#9 47.99       Package libffi was not found in the pkg-config search path.\n#9 47.99       Perhaps you should add the directory containing `libffi.pc'\n#9 47.99       to the PKG_CONFIG_PATH environment variable\n#9 47.99       Package 'libffi', required by 'virtual:world', not found\n#9 47.99       Package libffi was not found in the pkg-config search path.\n#9 47.99       Perhaps you should add the directory containing `libffi.pc'\n#9 47.99       to the PKG_CONFIG_PATH environment variable\n#9 47.99       Package 'libffi', required by 'virtual:world', not found\n#9 47.99       Package libffi was not found in the pkg-config search path.\n#9 47.99       Perhaps you should add the directory containing `libffi.pc'\n#9 47.99       to the PKG_CONFIG_PATH environment variable\n#9 47.99       Package 'libffi', required by 'virtual:world', not found\n#9 47.99       running install\n#9 47.99       running build\n#9 47.99       running build_py\n#9 47.99       creating build\n#9 47.99       creating build/lib.linux-aarch64-3.9\n#9 47.99       creating build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/__init__.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/cffi_opcode.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/commontypes.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/vengine_gen.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/vengine_cpy.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/backend_ctypes.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/api.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/ffiplatform.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/verifier.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/error.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/setuptools_ext.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/lock.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/recompiler.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/pkgconfig.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/cparser.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/model.py -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/_cffi_include.h -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/parse_c_type.h -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/_embedding.h -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       copying cffi/_cffi_errors.h -> build/lib.linux-aarch64-3.9/cffi\n#9 47.99       warning: build_py: byte-compiling is disabled, skipping.\n#9 47.99       \n#9 47.99       running build_ext\n#9 47.99       building '_cffi_backend' extension\n#9 47.99       creating build/temp.linux-aarch64-3.9\n#9 47.99       creating build/temp.linux-aarch64-3.9/c\n#9 47.99       gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -DTHREAD_STACK_SIZE=0x100000 -fPIC -DUSE__THREAD -DHAVE_SYNC_SYNCHRONIZE -I/usr/include/ffi -I/usr/include/libffi -I/usr/local/include/python3.9 -c c/_cffi_backend.c -o build/temp.linux-aarch64-3.9/c/_cffi_backend.o\n#9 47.99       c/_cffi_backend.c:15:10: fatal error: ffi.h: No such file or directory\n#9 47.99          15 | #include \n#9 47.99             |          ^~~~~~~\n#9 47.99       compilation terminated.\n#9 47.99       error: command '/usr/bin/gcc' failed with exit code 1\n#9 47.99       [end of output]\n#9 47.99   \n#9 47.99   note: This error originates from a subprocess, and is likely not a problem with pip.\n#9 47.99 error: legacy-install-failure\n#9 47.99 \n#9 47.99 \u00d7 Encountered error while trying to install package.\n#9 47.99 \u2570\u2500> cffi\n#9 47.99 \n#9 47.99 note: This is an issue with the package mentioned above, not pip.\n#9 47.99 hint: See above for output from the failure.\n\n",
    "AcceptedAnswerId": 71372163,
    "AcceptedAnswer": "@Klaus D.'s comment helped a lot.\nI updated Dockerfile:\nRUN apk add --update --no-cache --virtual .tmp-build-deps \\\n    gcc libc-dev linux-headers postgresql-dev \\\n    && apk add libffi-dev\n\n"
}
{
    "Id": 71938799,
    "PostTypeId": 1,
    "Title": "Python asyncio.create_task() - really need to keep a reference?",
    "Body": "The documentation of asyncio.create_task() states the following warning:\n\nImportant: Save a reference to the result of this function, to avoid a task disappearing mid execution. (source)\n\nMy question is: Is this really true?\nI have several IO bound \"fire and forget\" tasks which I want to run concurrently using asyncio by submitting them to the event loop using asyncio.create_task(). However, I do not really care for the return value of the coroutine or even if they run successfully, only that they do run eventually. One use case is writing data from an \"expensive\" calculation back to a Redis data base. If Redis is available, great. If not, oh well, no harm. This is why I do not want/need to await those tasks.\nHere a generic example:\nimport asyncio\n\nasync def fire_and_forget_coro():\n    \"\"\"Some random coroutine waiting for IO to complete.\"\"\"\n    print('in fire_and_forget_coro()')\n    await asyncio.sleep(1.0)\n    print('fire_and_forget_coro() done')\n\n\nasync def async_main():\n    \"\"\"Main entry point of asyncio application.\"\"\"\n    print('in async_main()')\n    n = 3\n    for _ in range(n):\n        # create_task() does not block, returns immediately.\n        # Note: We do NOT save a reference to the submitted task here!\n        asyncio.create_task(fire_and_forget_coro(), name='fire_and_forget_coro')\n\n    print('awaiting sleep in async_main()')\n    await asycnio.sleep(2.0) # <-- note this line\n    print('sleeping done in async_main()')\n\n    print('async_main() done.')\n\n    # all references of tasks we *might* have go out of scope when returning from this coroutine!\n    return\n\nif __name__ == '__main__':\n    asyncio.run(async_main())\n\nOutput:\nin async_main()\nawaiting sleep in async_main()\nin fire_and_forget_coro()\nin fire_and_forget_coro()\nin fire_and_forget_coro()\nfire_and_forget_coro() done\nfire_and_forget_coro() done\nfire_and_forget_coro() done\nsleeping done in async_main()\nasync_main() done.\n\nWhen commenting out the await asyncio.sleep() line, we never see fire_and_forget_coro() finish. This is to be expected: When the event loop started with asyncio.run() closes, tasks will not be excecuted anymore. But it appears that as long as the event loop is still running, all tasks will be taken care of, even when I never explicitly created references to them. This seem logical to me, as the event loop itself must have a reference to all scheduled tasks in order to run them. And we can even get them all using asyncio.all_tasks()!\nSo, I think I can trust Python to have at least one strong reference to every scheduled tasks as long as the event loop it was submitted to is still running, and thus I do not have to manage references myself. But I would like a second opinion here. Am I right or are there pitfalls I have not yet recognized?\nIf I am right, why the explicit warning in the documentation? It is a usual Python thing that stuff is garbage-collected if you do not keep a reference to it. Are there situations where one does not have a running event loop but still some task objects to reference? Maybe when creating an event loop manually (never did this)?\n",
    "AcceptedAnswerId": 71956673,
    "AcceptedAnswer": "There is an open issue at the cpython bug tracker at github about this topic I just found:\nhttps://github.com/python/cpython/issues/88831\nQuote:\n\nasyncio will only keep weak references to alive tasks (in _all_tasks). If a user does not keep a reference to a task and the task is not currently executing or sleeping, the user may get \"Task was destroyed but it is pending!\".\n\nSo the answer to my question is, unfortunately, yes. One has to keep around a reference to the scheduled task.\nHowever, the github issue also describes a relatively simple workaround: Keep all running tasks in a set() and add a callback to the task which removes itself from the set() again.\nrunning_tasks = set()\n# [...]\ntask = asyncio.create_task(some_background_function())\nrunning_tasks.add(task)\ntask.add_done_callback(lambda t: running_tasks.remove(t))\n\n"
}
{
    "Id": 71862398,
    "PostTypeId": 1,
    "Title": "Install python 3.6.* on Mac M1",
    "Body": "I'm trying to run an old app that requires python \nI've installed pyenv-virtualenv and pyenv and successfully installed python 3.7.13. However, when I try to install 3.6.*, I get this:\n$ pyenv install 3.6.13\npython-build: use openssl@1.1 from homebrew\npython-build: use readline from homebrew\nDownloading Python-3.6.13.tar.xz...\n-> https://www.python.org/ftp/python/3.6.13/Python-3.6.13.tar.xz\nInstalling Python-3.6.13...\npython-build: use tcl-tk from homebrew\npython-build: use readline from homebrew\npython-build: use zlib from xcode sdk\n\nBUILD FAILED (OS X 12.3.1 using python-build 2.2.5-11-gf0f2cdd1)\n\nInspect or clean up the working tree at /var/folders/r5/xz73mp557w30h289rr6trb800000gp/T/python-build.20220413143259.33773\nResults logged to /var/folders/r5/xz73mp557w30h289rr6trb800000gp/T/python-build.20220413143259.33773.log\n\nLast 10 log lines:\nchecking for --with-cxx-main=... no\nchecking for clang++... no\nconfigure:\n\n  By default, distutils will build C++ extension modules with \"clang++\".\n  If this is not intended, then set CXX on the configure command line.\n  \nchecking for the platform triplet based on compiler characteristics... darwin\nconfigure: error: internal configure error for the platform triplet, please file a bug report\nmake: *** No targets specified and no makefile found.  Stop.\n\nIs there a way to solve this? I've looked and it seems like Mac M1 doesn't allow installing 3.6.*\n",
    "AcceptedAnswerId": 71957981,
    "AcceptedAnswer": "Copying from a GitHub issue.\n\nI successfully installed Python 3.6 on an Apple M1 MacBook Pro running Monterey using the following setup. There is probably some things in here that can be removed/refined... but it worked for me!\n#Install Rosetta\n/usr/sbin/softwareupdate --install-rosetta --agree-to-license\n\n# Install x86_64 brew\narch -x86_64 /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"\n\n# Set up x86_64 homebrew and pyenv and temporarily set aliases\nalias brew86=\"arch -x86_64 /usr/local/bin/brew\"\nalias pyenv86=\"arch -x86_64 pyenv\"\n\n# Install required packages and flags for building this particular python version through emulation\nbrew86 install pyenv gcc libffi gettext\nexport CPPFLAGS=\"-I$(brew86 --prefix libffi)/include -I$(brew86 --prefix openssl)/include -I$(brew86 --prefix readline)/lib\"\nexport CFLAGS=\"-I$(brew86 --prefix openssl)/include -I$(brew86 --prefix bzip2)/include -I$(brew86 --prefix readline)/include -I$(xcrun --show-sdk-path)/usr/include -Wno-implicit-function-declaration\" \nexport LDFLAGS=\"-L$(brew86 --prefix openssl)/lib -L$(brew86 --prefix readline)/lib -L$(brew86 --prefix zlib)/lib -L$(brew86 --prefix bzip2)/lib -L$(brew86 --prefix gettext)/lib -L$(brew86 --prefix libffi)/lib\"\n\n# Providing an incorrect openssl version forces a proper openssl version to be downloaded and linked during the build\nexport PYTHON_BUILD_HOMEBREW_OPENSSL_FORMULA=openssl@1.0\n\n# Install Python 3.6\npyenv86 install --patch 3.6.15 <<(curl -sSL https://raw.githubusercontent.com/pyenv/pyenv/master/plugins/python-build/share/python-build/patches/3.6.15/Python-3.6.15/0008-bpo-45405-Prevent-internal-configure-error-when-runn.patch\\?full_index\\=1)\n\nNote, the build succeeds but gives the following warning\nWARNING: The Python readline extension was not compiled. Missing the GNU readline lib?\n\nrunning pyenv versions shows that 3.6.15 can be used normally by the system\n"
}
{
    "Id": 71391946,
    "PostTypeId": 1,
    "Title": "Does Raku has Python's Union type?",
    "Body": "In Python, Python has Union type, which is convenient when a method can accept multi types:\nfrom typing import Union\n\ndef test(x: Union[str,int,float,]):\n    print(x)\n\nif __name__ == '__main__':\n    test(1)\n    test('str')\n    test(3.1415926)\n\nRaku probably doesn't have Union type as Python, but a where clause can achieve a similar effect:\nsub test(\\x where * ~~ Int | Str | Rat) {\n    say(x)\n}\n\nsub MAIN() {\n    test(1);\n    test('str');\n    test(3.1415926);\n}\n\nI wander if Raku have a possibility to provide the Union type as Python?\n#        vvvvvvvvvvvvvvvvvvvv - the Union type doesn't exist in Raku now.\nsub test(Union[Int, Str, Rat] \\x) {\n    say(x)\n}\n\n",
    "AcceptedAnswerId": 71402432,
    "AcceptedAnswer": "My answer (which is very similar to your first solution ;) would be:\nsubset Union where Int | Rat | Str;\n\nsub test(Union \\x) {\n   say(x) \n}\n\nsub MAIN() {\n    test(1);\n    test('str');\n    test(pi);\n}\n\nConstraint type check failed in binding to parameter 'x'; \nexpected Union but got Num (3.141592653589793e0)\n\n(or you can put a where clause in the call signature, as you have it)\nIn contrast to Python:\n\nthis is native in raku and does not rely on a package like \"typing\" to be imported\nPython Union / SumTypes are used for static hinting, which is good for eg. IDEs\nbut these types are unenforced in Python (per @freshpaste comment and this SO), in raku they are checked and will fail at runtime\n\nSo - the raku syntax is there to do what you ask ... sure, it's a different language so it does it in a different way.\nPersonally I think that a typed language should fail if type checks are breached. It seems to me that type hinting that is not always enforced is a false comfort blanket.\nOn a wider point, raku also offers built in Allomorph types for IntStr, RatStr, NumStr and ComplexStr - so you can work in a mixed mode using both string and math functions\n"
}
{
    "Id": 71150313,
    "PostTypeId": 1,
    "Title": "python-docx adding bold and non-bold strings to same cell in table",
    "Body": "I'm using python-docx to create a document with a table I want to populate from textual data. My text looks like this:\n01:02:10.3 \na: Lorem ipsum dolor sit amet,  \nb: consectetur adipiscing elit.\na: Mauris a turpis erat. \n01:02:20.4 \na: Vivamus dignissim aliquam\nb: Nam ultricies\n(etc.)\n\nI need to organize it in a table like this (using ASCII for visualization):\n+---+--------------------+---------------------------------+\n|   |         A          |                B                |\n+---+--------------------+---------------------------------+\n| 1 | 01:02:10.3         | a: Lorem ipsum dolor sit amet,  |\n| 2 |                    | b: consectetur adipiscing elit. |\n| 3 |                    | a: Mauris a turpis erat.        |\n| 4 | ------------------ | ------------------------------- |\n| 5 | 01:02:20.4         | a: Vivamus dignissim aliqua     |\n| 6 |                    | b: Nam ultricies                |\n+---+--------------------+---------------------------------+\n\nhowever, I need to make it so everything after \"a: \" is bold, and everything after \"b: \" isn't, while they both occupy the same cell. It's pretty easy to iterate and organize this the way I want, but I'm really unsure about how to make only some of the lines bold:\nIS_BOLD = { \n    'a': True\n    'b': False\n}\n\nrow_cells = table.add_row().cells\n\nfor line in lines: \n    if is_timestamp(line): # function that uses regex to discern between columns\n        if row_cells[1]:\n            row_cells = table.add_row().cells\n\n        row_cells[0].text = line\n\n    else \n        row_cells[1].text += line\n\n        if IS_BOLD[ line.split(\":\")[0] ]:\n            # make only this line within the cell bold, somehow.\n\n(this is sort of pseudo-code, I'm doing some more textual processing but that's kinda irrelevant here). I found one probably relevant question where someone uses something called run but I'm finding it hard to understand how to apply it to my case.\nAny help?\nThanks.\n",
    "AcceptedAnswerId": 71280321,
    "AcceptedAnswer": "You need to add run in the cell's paragraph. This way you can control the specific text you wish to bold\nFull example:\nfrom docx import Document\nfrom docx.shared import Inches\nimport os\nimport re\n\n\ndef is_timestamp(line):\n    # it's flaky, I saw you have your own method and probably you did a better job parsing this.\n    return re.match(r'^\\d{2}:\\d{2}:\\d{2}', line) is not None\n\n\ndef parse_raw_script(raw_script):\n    current_timestamp = ''\n    current_content = ''\n    for line in raw_script.splitlines():\n        line = line.strip()\n        if is_timestamp(line):\n            if current_timestamp:\n                yield {\n                    'timestamp': current_timestamp,\n                    'content': current_content\n                }\n\n            current_timestamp = line\n            current_content = ''\n            continue\n\n        if current_content:\n            current_content += '\\n'\n\n        current_content += line\n\n    if current_timestamp:\n        yield {\n            'timestamp': current_timestamp,\n            'content': current_content\n        }\n\n\ndef should_bold(line):\n    # i leave it to you to replace with your logic\n    return line.startswith('a:')\n\n\ndef load_raw_script():\n    # I placed here the example from your question. read from file instead I presume\n\n    return '''01:02:10.3 \na: Lorem ipsum dolor sit amet,  \nb: consectetur adipiscing elit.\na: Mauris a turpis erat. \n01:02:20.4 \na: Vivamus dignissim aliquam\nb: Nam ultricies'''\n\n\ndef convert_raw_script_to_docx(raw_script, output_file_path):\n    document = Document()\n    table = document.add_table(rows=1, cols=3, style=\"Table Grid\")\n\n    # add header row\n    header_row = table.rows[0]\n    header_row.cells[0].text = ''\n    header_row.cells[1].text = 'A'\n    header_row.cells[2].text = 'B'\n\n    # parse the raw script into something iterable\n    script_rows = parse_raw_script(raw_script)\n\n    # create a row for each timestamp row\n    for script_row in script_rows:\n        timestamp = script_row['timestamp']\n        content = script_row['content']\n\n        row = table.add_row()\n        timestamp_cell = row.cells[1]\n        timestamp_cell.text = timestamp\n\n        content_cell = row.cells[2]\n        content_paragraph = content_cell.paragraphs[0]  # using the cell's default paragraph here instead of creating one\n\n        for line in content.splitlines():\n            run = content_paragraph.add_run(line)\n            if should_bold(line):\n                run.bold = True\n\n            run.add_break()\n\n    # resize table columns (optional)\n    for row in table.rows:\n        row.cells[0].width = Inches(0.2)\n        row.cells[1].width = Inches(1.9)\n        row.cells[2].width = Inches(3.9)\n\n    document.save(output_file_path)\n\n\ndef main():\n    script_dir = os.path.dirname(__file__)\n    dist_dir = os.path.join(script_dir, 'dist')\n\n    if not os.path.isdir(dist_dir):\n        os.makedirs(dist_dir)\n\n    output_file_path = os.path.join(dist_dir, 'so-template.docx')\n    raw_script = load_raw_script()\n    convert_raw_script_to_docx(raw_script, output_file_path)\n\n\nif __name__ == '__main__':\n    main()\n\n\nResult (file should be in ./dist/so-template.docx):\n\n\nBTW - if you prefer sticking with your own example, this is what needs to be changed:\nIS_BOLD = {\n    'a': True,\n    'b': False\n}\n\nrow_cells = table.add_row().cells\n\nfor line in lines:\n    if is_timestamp(line):\n        if row_cells[1]:\n            row_cells = table.add_row().cells\n        row_cells[0].text = line\n\n    else:\n        run = row_cells[1].paragraphs[0].add_run(line)\n        if IS_BOLD[line.split(\":\")[0]]:\n            run.bold = True\n\n        run.add_break()\n\n"
}
{
    "Id": 71969299,
    "PostTypeId": 1,
    "Title": "How to disable code formatting in ipython?",
    "Body": "IPython has this new feature that reformats my prompt. Unfortunately, it is really buggy, so I want to disable it. I managed to do it when starting IPython from the command line by adding the following line in my ipython_config.py:\nc.TerminalInteractiveShell.autoformatter = None\n\nHowever, it does not work when I run it from a python script. I start IPython from my script the following way:\nc = traitlets.config.get_config()\nc.InteractiveShellEmbed.colors = \"Linux\"\nc.TerminalInteractiveShell.autoformatter = None\nc.InteractiveShellEmbed.loop_runner = lambda coro: loop.run_until_complete(coro)\nIPython.embed(display_banner='', using='asyncio', config=c)\n\nIf I change the colors value, the colors change accordingly, so the configuration itself works. However, no matter what I do with autoformatter, IPython autoformats my code regardless. What am I doing wrong?\n",
    "AcceptedAnswerId": 71995927,
    "AcceptedAnswer": "Apparently, the answer is:\nc.InteractiveShellEmbed.autoformatter = None\n\n"
}
{
    "Id": 71424233,
    "PostTypeId": 1,
    "Title": "How do I list my scheduled queries via the Python google client API?",
    "Body": "I have set up my service account and I can run queries on bigQuery using client.query().\nI could just write all my scheduled queries into this new client.query() format but I already have many scheduled queries so I was wondering if there is a way I can get/list the scheduled queries and then use that information to run those queries from a script.\n\n",
    "AcceptedAnswerId": 71428499,
    "AcceptedAnswer": "Yes, you can use the APIs. When you don't know which one to use, I have a tip. Use the command proposed by @Yev\nbq ls --transfer_config --transfer_location=US --format=prettyjson\nBut log the API calls. for that use the --apilog  parameter like that\nbq --apilog ./log ls --transfer_config --transfer_location=US --format=prettyjson\nAnd, magically, you can find the API called by the command:\nhttps://bigquerydatatransfer.googleapis.com/v1/projects//locations/US/transferConfigs?alt=json\nThen, a simple google search leads you to the correct documentation\n\nIn python, add that dependencies in your requirements.txt: google-cloud-bigquery-datatransfer and use that code\nfrom google.cloud import bigquery_datatransfer\n\nclient = bigquery_datatransfer.DataTransferServiceClient()\nparent = client.common_project_path(\"\")\nresp = client.list_transfer_configs(parent=parent)\nprint(resp)\n\n"
}
{
    "Id": 71583528,
    "PostTypeId": 1,
    "Title": "Python extracting string",
    "Body": "I have a dataframe where one of the columns which is in string format looks like this\n    filename\n 0  Machine02-2022-01-28_00-21-45.blf.424\n 1  Machine02-2022-01-28_00-21-45.blf.425\n 2  Machine02-2022-01-28_00-21-45.blf.426\n 3  Machine02-2022-01-28_00-21-45.blf.427\n 4  Machine02-2022-01-28_00-21-45.blf.428\n\nI want my column to look like this\n      filename\n 0    2022-01-28 00-21-45 424\n 1    2022-01-28 00-21-45 425\n 2    2022-01-28 00-21-45 426\n 3    2022-01-28 00-21-45 427\n 4    2022-01-28 00-21-45 428\n\nI tried this code\ndf['filename'] = df['filename'].str.extract(r\"(\\d{4}-\\d{1,2}-\\d{1,2})_(\\d{2}-\\d{2}-\\d{2}).*\\.(\\d+)\", r\"\\1 \\2 \\3\")\n\nI am getting this error, unsupported operand type(s) for &: 'str' and 'int'.\nCan anyone please tell me where I am doing wrong ?\n",
    "AcceptedAnswerId": 71583643,
    "AcceptedAnswer": "please try this:\ndf['filename'] = df['filename'].str.split('-',1).apply(lambda x:' '.join(x[1].split('_')).replace('.blf.',' '))\n\n"
}
{
    "Id": 71452013,
    "PostTypeId": 1,
    "Title": "Does Python not reuse memory here? What does tracemalloc's output mean?",
    "Body": "I create a list of a million int objects, then replace each with its negated value. tracemalloc reports 28 MB extra memory (28 bytes per new int object). Why? Does Python not reuse the memory of the garbage-collected int objects for the new ones? Or am I misinterpreting the tracemalloc results? Why does it say those numbers, what do they really mean here?\nimport tracemalloc\n\nxs = list(range(10**6))\ntracemalloc.start()\nfor i, x in enumerate(xs):\n    xs[i] = -x\nprint(tracemalloc.get_traced_memory())\n\nOutput (Try it online!):\n(27999860, 27999972)\n\nIf I replace xs[i] = -x with x = -x (so the new object rather than the original object gets garbage-collected), the output is a mere (56, 196) (try it). How does it make any difference which of the two objects I keep/lose?\nAnd if I do the loop twice, it still only reports (27992860, 27999972) (try it). Why not 56 MB? How is the second run any different for this than the first?\n",
    "AcceptedAnswerId": 71481334,
    "AcceptedAnswer": "Short Answer\ntracemalloc was started too late to track the inital block of memory, so it\ndidn't realize it was a reuse. In the example you gave, you free 27999860 bytes\nand allocate 27999860 bytes, but tracemalloc can't 'see' the free. Consider the\nfollowing, slightly modified example:\nimport tracemalloc\n\ntracemalloc.start()\n\nxs = list(range(10**6))\nprint(tracemalloc.get_traced_memory())\nfor i, x in enumerate(xs):\n    xs[i] = -x\nprint(tracemalloc.get_traced_memory())\n\nOn my machine (python 3.10, but same allocator), this displays:\n(35993436, 35993436)\n(36000576, 36000716)\n\nAfter we allocate xs, the system has allocated 35993436 bytes, and after we run\nthe loop we have a net total of 36000576. This shows that the memory usage isn't\nactually increasing by 28 Mb.\nWhy does it behave this way?\nTracemalloc works by overriding the standard internal methods for allocating\nwith tracemalloc_alloc, and the similar free and realloc methods. Taking a\npeek at the source:\nstatic void*\ntracemalloc_alloc(int use_calloc, void *ctx, size_t nelem, size_t elsize)\n{\n    PyMemAllocatorEx *alloc = (PyMemAllocatorEx *)ctx;\n    void *ptr;\n\n    assert(elsize == 0 || nelem <= SIZE_MAX / elsize);\n\n    if (use_calloc)\n        ptr = alloc->calloc(alloc->ctx, nelem, elsize);\n    else\n        ptr = alloc->malloc(alloc->ctx, nelem * elsize);\n    if (ptr == NULL)\n        return NULL;\n\n    TABLES_LOCK();\n    if (ADD_TRACE(ptr, nelem * elsize) < 0) {\n        /* Failed to allocate a trace for the new memory block */\n        TABLES_UNLOCK();\n        alloc->free(alloc->ctx, ptr);\n        return NULL;\n    }\n    TABLES_UNLOCK();\n    return ptr;\n}\n\nWe see that the new allocator does two things:\n1.) Call out to the \"old\" allocator to get memory\n2.) Add a trace to a special table, so we can track this memory\nIf we look at the associated free functions, it's very similar:\n1.) free the memory\n2.) Remove the trace from the table\nIn your example, you allocated xs before you called tracemalloc.start(), so\nthe trace records for this allocation are never put in the memory tracking\ntable. Therefore, when you call free on the initial array data, the traces aren't removed, and thus your weird allocation behavior.\nWhy is the total memory usage 36000000 bytes and not 28000000\nLists in python are weird. They're actually a list of pointer to individually\nallocated objects. Internally, they look like this:\ntypedef struct {\n    PyObject_HEAD\n    Py_ssize_t ob_size;\n\n    /* Vector of pointers to list elements.  list[0] is ob_item[0], etc. */\n    PyObject **ob_item;\n\n    /* ob_item contains space for 'allocated' elements.  The number\n     * currently in use is ob_size.\n     * Invariants:\n     *     0 <= ob_size <= allocated\n     *     len(list) == ob_size\n     *     ob_item == NULL implies ob_size == allocated == 0\n     */\n    Py_ssize_t allocated;\n} PyListObject;\n\nPyObject_HEAD is a macro that expands to some header information all python\nvariables have. It is just 16 bytes, and contains pointers to type data.\nImportantly, a list of integers is actually a list of pointer to PyObjects\nthat happen to be ints. On the line xs = list(range(10**6)), we expect to\nallocate:\n\n1 PyListObject with internal size 1000000 -- true size:\n\nsizeof(PyObject_HEAD) + sizeof(PyObject *) * 1000000 + sizeof(Py_ssize_t)\n(     16 bytes      ) + (    8 bytes     ) * 1000000 + (     8 bytes    )\n8000024 bytes\n\n\n1000000 PyObject ints (A PyLongObject in the underlying implmentation)\n\n1000000 * sizeof(PyLongObject)\n1000000 * (     28 bytes     )\n28000000 bytes\n\nFor a grand total of 36000024 bytes. That number looks pretty farmiliar!\nWhen you overwrite a value in the array, your just freeing the old value, and updating the pointer in PyListObject->ob_item. This means the array structure is allocated once, takes up 8000024 bytes, and lives to the end of the program. Additionally, 1000000 Integer objects are each allocated, and references are put in the array. They take up the 28000000 bytes. One by one, they are deallocated, and then the memory is used to reallocate a new object in the loop. This is why multiple loops don't increase the amount of memory.\n"
}
{
    "Id": 71486255,
    "PostTypeId": 1,
    "Title": "How can I make Python re work like grep for repeating groups?",
    "Body": "I have the following string:\nseq = 'MNRYLNRQRLYNMYRNKYRGVMEPMSRMTMDFQGRYMDSQGRMVDPRYYDHYGRMHDYDRYYGRSMFNQGHSMDSQRYGGWMDNPERYMDMSGYQMDMQGRWMDAQGRYNNPFSQMWHSRQGH'\n\nalso saved in a file called seq.dat. If I use the following grep command\ngrep '\\([MF]D.\\{4,6\\}\\)\\{3,10\\}' seq.dat\n\nI get the following matching string:\nMDNPERYMDMSGYQMDMQGRWMDAQGRYN\n\nwhich is what I want. In words, what I want to match is as many consecutive repeats as the string has of [MF]D.{4,6}. I don't want to match cases where it has less than 3 consecutive repeats, but I want it to be able to capture up to 6.\nNow, I'm trying to do this with python. I have\np = re.compile(\"(?:[MF]D.{4,6}){3,10}\")\n\nTrying search() returns\nMDNPERYMDMSGYQMDMQGRWM\n\nIt is the close to the answer I seek, but is still missing the last MDAQGRYN. I'm guessing this is because .{4,6} matches the M, which in turn prevents {3,10} from capturing this 4th occurence of ([MF]D.{4,6}), but since I asked for at least 3, it's happy and it stops.\nHow do I make Python regex behave like grep does?\n",
    "AcceptedAnswerId": 71487029,
    "AcceptedAnswer": "There is a fundamental difference between POSIX (\"text-directed\") and NFA (\"regex-directed\") engines. POSIX engines (grep here uses a POSIX BRE regex flavor, it is the flavor used by default) will parse the input text applying the regex to it and return the longest match possible. NFA engine (Python re engine is an NFA engine) here does not re-consume (backtrack) when the subsequent pattern parts match.\nSee reference on regex-directed and text-directed engines:\n\nA regex-directed engine walks through the regex, attempting to match the next token in the regex to the next character. If a match is found, the engine advances through the regex and the subject string. If a token fails to match, the engine backtracks to a previous position in the regex and the subject string where it can try a different path through the regex... Modern regex flavors using regex-directed engines have lots of features such as atomic grouping and possessive quantifiers that allow you to control this backtracking.\nA text-directed engine walks through the subject string, attempting all permutations of the regex before advancing to the next character in the string. A text-directed engine never backtracks. Thus, there isn\u2019t much to discuss about the matching process of a text-directed engine. In most cases, a text-directed engine finds the same matches as a regex-directed engine.\n\nThe last sentence says \"in most cases\", but not all cases, and yours is a good illustration that discrepances may occur.\nTo avoid consuming M or F that are immediately followed with D, I'd suggest using\n(?:[MF]D(?:(?![MF]D).){4,6}){3,10}\n\nSee the regex demo. Details:\n\n(?: - start of an outer non-capturing container group:\n\n[MF]D - M or F and then D\n(?:(?![MF]D).){4,6} - any char (other than a line break) repeated four to six times, that does not start an MD or FD char sequence\n\n\n){3,10} - end of the outer group, repeat 3 to 10 times.\n\nBy the way, if you only want to match uppercase ASCII letters, replace the . with [A-Z].\n"
}
{
    "Id": 71178416,
    "PostTypeId": 1,
    "Title": "Can you safely change a Python object's type in a C extension?",
    "Body": "Question\nSuppose that I have implemented two Python types using the C extension API and that the types are identical (same data layouts/C struct) with the exception of their names and a few methods. Assuming that all methods respect the data layout, can you safely change the type of an object from one of these types into the other in a C function?\nNotably, as of Python 3.9, there appears to be a function Py_SET_TYPE, but the documentation is not clear as to whether/when this is safe to do. I'm interested in knowing both how to use this function safely and whether types can be safely changed prior to version 3.9.\nMotivation\nI'm writing a Python C extension to implement a Persistent Hash Array Mapped Trie (PHAMT); in case it's useful, the source code is here (as of writing, it is at this commit). A feature I would like to add is the ability to create a Transient Hash Array Mapped Trie (THAMT) from a PHAMT. THAMTs can be created from PHAMTs in O(1) time and can be mutated in-place efficiently. Critically, THAMTs have the exact same underlying C data-structure as PHAMTs\u2014the only real difference between a PHAMT and a THAMT is a few methods encapsulated by their Python types. This common structure allows one to very efficiently turn a THAMT back into a PHAMT once one has finished performing a set of edits. (This pattern typically reduces the number of memory allocations when performing a large number of updates to a PHAMT).\nA very convenient way to implement the conversion from THAMT to PHAMT would be to simply change the type pointers of the THAMT objects from the THAMT type to the PHAMT type. I am confident that I can write code that safely navigates this change, but I can imagine that doing so might, for example, break the Python garbage collector.\n(To be clear: the motivation is just context as to how the question arose. I'm not looking for help implementing the structures described in the Motivation, I'm looking for an answer to the Question, above.)\n",
    "AcceptedAnswerId": 71316603,
    "AcceptedAnswer": "The supported way\nIt is officially possible to change an object's type in Python, as long as the memory layouts are compatible... but this is mostly limited to types not implemented in C. With some restrictions, it is possible to do\n# Python attribute assignment, not C struct member assignment\nobj.__class__ = some_new_class\n\nto change an object's class, with one of the restrictions being that both the old and new classes must be \"heap types\", which all classes implemented in Python are and most classes implemented in C are not. (types.ModuleType and subclasses of that type are also specifically permitted, despite types.ModuleType not being a heap type. See the source for exact restrictions.)\nIf you want to create a heap type from C, you can, but the interface is pretty different from the normal way of defining Python types from C. Plus, for __class__ assignment to work, you have to not set the Py_TPFLAGS_IMMUTABLETYPE flag, and that means that people will be able to monkey-patch your classes in ways you might not like (or maybe you see that as an upside).\nIf you want to go that route, I suggest looking at the CPython 3.10 _functools module source code for an example. (They set the Py_TPFLAGS_IMMUTABLETYPE flag, which you'll have to make sure not to do.)\n\nThe unsupported way\nThere was an attempt at one point to allow __class__ assignment for non-heap types, as long as the memory layouts worked. It got abandoned because it caused problems with some built-in immutable types, where the interpreter likes to reuse instances. For example, allowing (1).__class__ = SomethingElse would have caused a lot of problems. You can read more in the big comment in the source code for the __class__ setter. (The comment is slightly out of date, particularly regarding the Py_TPFLAGS_IMMUTABLETYPE flag, which was added after the comment was written.)\nAs far as I know, this was the only problem, and I don't think any more problems have been added since then. The interpreter isn't going to aggressively reuse instances of your classes, so as long as you're not doing anything like that, and the memory layouts are compatible, I think changing the type of your objects should work for now, even for non-heap-types. However, it is not officially supported, so even if I'm right about this working for now, there's no guarantee it'll keep working.\nPy_SET_TYPE only sets an object's type pointer. It doesn't do any refcount fixing that might be needed. It's a very low-level operation. If neither the old class nor the new class are heap types, no extra refcount fixing is needed, but if the old class is a heap type, you will have to decref the old class, and if the new class is a heap type, you will have to incref the new class.\nIf you need to decref the old class, make sure to do it after changing the object's class and possibly incref'ing the new class.\n"
}
{
    "Id": 72071447,
    "PostTypeId": 1,
    "Title": "Python Enum and Pydantic : accept enum member's composition",
    "Body": "I have an enum :\nfrom enum import Enum\n\nclass MyEnum(Enum):\n    val1 = \"val1\"\n    val2 = \"val2\"\n    val3 = \"val3\"\n\nI would like to validate a pydantic field based on that enum.\nfrom pydantic import BaseModel\n\nclass MyModel(BaseModel):\n    my_enum_field: MyEnum\n\nBUT I would like this validation to also accept string that are composed by the Enum members.\nSo for example : \"val1_val2_val3\" or \"val1_val3\" are valid input.\nI cannot make this field as a string field with a validator since I use a test library (hypothesis and pydantic-factories) that needs this type in order to render one of the values from the enum (for mocking random inputs)\nSo this :\nfrom pydantic import BaseModel, validator\n\nclass MyModel(BaseModel):\n    my_enum_field: str\n\n    @validator('my_enum_field', pre=True)\n    def validate_my_enum_field(cls, value):\n        split_val = str(value).split('_')\n        if not all(v in MyEnum._value2member_map_ for v in split_val):\n            raise ValueError()\n        return value\n\nCould work, but break my test suites because the field is anymore of enum types.\nHow to keep this field as an Enum type (to make my mock structures still valid) and make pydantic accept composite values in the same time ?\nSo far, I tried to dynamically extend the enum, with no success.\n",
    "AcceptedAnswerId": 72072103,
    "AcceptedAnswer": "I looked at this a bit further, and I believe something like this could be helpful. You can create a new class to define the property that is a list of enum values.\nThis class can supply a customized validate method and supply a __modify_schema__ to keep the information present about being a string in the json schema.\nWe can define a base class for generic lists of concatenated enums like this:\nfrom typing import Generic, TypeVar, Type\nfrom enum import Enum\n\nT = TypeVar(\"T\", bound=Enum)\n\n\nclass ConcatenatedEnum(Generic[T], list[T]):\n    enum_type: Type[T]\n\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, value: str):\n        return list(map(cls.enum_type, value.split(\"_\")))\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: dict):\n        all_values = ', '.join(f\"'{ex.value}'\" for ex in cls.enum_type)\n        field_schema.update(\n            title=f\"Concatenation of {cls.enum_type.__name__} values\",\n            description=f\"Underscore delimited list of values {all_values}\",\n            type=\"string\",\n        )\n        if \"items\" in field_schema:\n            del field_schema[\"items\"]\n\nIn the __modify_schema__ method I also provide a way to generate a description of which values are valid.\nTo use this in your application:\nclass MyEnum(Enum):\n    val1 = \"val1\"\n    val2 = \"val2\"\n    val3 = \"val3\"\n\n\nclass MyEnumList(ConcatenatedEnum[MyEnum]):\n    enum_type = MyEnum\n\n\nclass MyModel(BaseModel):\n    my_enum_field: MyEnumList\n\nExamples Models:\nprint(MyModel.parse_obj({\"my_enum_field\": \"val1\"}))\nprint(MyModel.parse_obj({\"my_enum_field\": \"val1_val2\"}))\n\nmy_enum_field=[]\nmy_enum_field=[, ]\n\nExample Schema:\nprint(json.dumps(MyModel.schema(), indent=2))\n\n{\n  \"title\": \"MyModel\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"my_enum_field\": {\n      \"title\": \"Concatenation of MyEnum values\",\n      \"description\": \"Underscore delimited list of values 'val1', 'val2', 'val3'\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"my_enum_field\"\n  ]\n}\n\n"
}
{
    "Id": 71292505,
    "PostTypeId": 1,
    "Title": "TK python checkbutton RTL",
    "Body": "I have a checkbutton:\nfrom tkinter import *\nmaster = Tk()\nCheckbutton(master, text=\"Here...\").grid(row=0, sticky=W)\nmainloop()\n\nWhich looks like this:\n\nI tried to move the checkbutton to the other side (to support RTL languages), so it'll be like:\nHere...[]\nI know that I can draw a label next to the checkbutton, but this way clicking the text won't effect the checkbutton.\nHow can I do it?\n",
    "AcceptedAnswerId": 71348390,
    "AcceptedAnswer": "You can bind the left mouse button click event of the label, to a lambda construct that toggles the checkbutton -:\nlabel.bind(\"\", lambda x : check_button.toggle())\n\nThe label can then be placed before the checkbutton using grid(as mentioned in the OP at the end) -:\nfrom tkinter import *\n\nmaster = Tk()\n\nl1 = Label(master, text = \"Here...\")\ncb = Checkbutton(master)\nl1.grid(row = 0, column = 0)\ncb.grid(row = 0, column = 1, sticky=W)\n\nl1.bind(\"\", lambda x : cb.toggle())\nmainloop()\n\nThis will toggle, the checkbutton even if the label is clicked.\nOUTPUT -:\n\n\nNOTE:\nThe checkbutton, has to now be fetched as an object(cb), to be used in the lambda construct for the label's bind function callback argument. Thus, it is gridded in the next line. It is generally a good practice to manage the geometry separately, which can prevent error such as this one.\n\nAlso, as mentioned in the post linked by @Alexander B. in the comments, if this assembly is to be used multiple times, it can also be made into a class of it's own that inherits from the tkinter.Frame class -:\nclass LabeledCheckbutton(Frame):\n    def __init__(self, root, text = \"\"):\n        Frame.__init__(self, root)\n        self.checkbutton = Checkbutton(self)\n        self.label = Label(self, text = text)\n        self.label.grid(row = 0, column = 0)\n        self.checkbutton.grid(row = 0, column = 1)\n        self.label.bind('', lambda x : self.checkbutton.toggle())\n        return\n    \n    pass\n\nUsing this with grid as the geometry manager, would make the full code look like this -:\nfrom tkinter import *\n\nclass LabeledCheckbutton(Frame):\n    def __init__(self, root, text = \"\"):\n        Frame.__init__(self, root)\n        self.checkbutton = Checkbutton(self)\n        self.label = Label(self, text = text)\n        self.label.grid(row = 0, column = 0)\n        self.checkbutton.grid(row = 0, column = 1)\n        self.label.bind('', lambda x : self.checkbutton.toggle())\n        return\n    \n    pass\n\nmaster = Tk()\nlcb = LabeledCheckbutton(master, text = \"Here...\")\nlcb.grid(row = 0, sticky = W)\n\nmainloop()\n\nThe output of the above code remains consistent with that of the first approach. The only difference is that it is now more easily scalable, as an object can be created whenever needed and the same lines of code need not be repeated every time.\n"
}
{
    "Id": 72011315,
    "PostTypeId": 1,
    "Title": "PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: after installing python-certifi-win32",
    "Body": "I installed python-certifi-win32 package and after that, I am getting below error, when I import anything or pip install anything, the fail with the final error of PermissionError.\nI tried rebooting the box. It didn't work. I am unable to uninstall the package as pip is erroring out too.\nI am unable to figure out the exact reason why this error is happening. It doesn't seem to be code specific, seems related to the library I installed\nPS C:\\Users\\visha\\PycharmProjects\\master_test_runner> pip install python-certifi-win32                                                                \nTraceback (most recent call last):\n  File \"C:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\_common.py\", line 89, in _tempfile\n    os.write(fd, reader())\n  File \"C:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\abc.py\", line 371, in read_bytes\n    with self.open('rb') as strm:\n  File \"C:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\_adapters.py\", line 54, in open\n    raise ValueError()\nValueError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\Scripts\\pip.exe\\__main__.py\", line 4, in \n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_internal\\cli\\main.py\", line 9, in \n    from pip._internal.cli.autocompletion import autocomplete\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_internal\\cli\\autocompletion.py\", line 10, in \n    from pip._internal.cli.main_parser import create_main_parser\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_internal\\cli\\main_parser.py\", line 8, in \n    from pip._internal.cli import cmdoptions\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_internal\\cli\\cmdoptions.py\", line 23, in \n    from pip._internal.cli.parser import ConfigOptionParser\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_internal\\cli\\parser.py\", line 12, in \n    from pip._internal.configuration import Configuration, ConfigurationError\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_internal\\configuration.py\", line 21, in \n    from pip._internal.exceptions import (\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_internal\\exceptions.py\", line 8, in \n    from pip._vendor.requests.models import Request, Response\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_vendor\\requests\\__init__.py\", line 123, in \n    from . import utils\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\pip\\_vendor\\requests\\utils.py\", line 25, in \n    from . import certs\n  File \"\", line 1027, in _find_and_load\n  File \"\", line 1006, in _find_and_load_unlocked\n  File \"\", line 688, in _load_unlocked\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\wrapt\\importer.py\", line 170, in exec_module\n    notify_module_loaded(module)\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\wrapt\\decorators.py\", line 470, in _synchronized\n    return wrapped(*args, **kwargs)\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\wrapt\\importer.py\", line 136, in notify_module_loaded\n    hook(module)\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\certifi_win32\\wrapt_pip.py\", line 35, in apply_patches\n    import certifi\n  File \"\", line 1027, in _find_and_load\n  File \"\", line 1006, in _find_and_load_unlocked\n  File \"\", line 688, in _load_unlocked\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\wrapt\\importer.py\", line 170, in exec_module\n    notify_module_loaded(module)\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\wrapt\\decorators.py\", line 470, in _synchronized\n    return wrapped(*args, **kwargs)\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\wrapt\\importer.py\", line 136, in notify_module_loaded\n    hook(module)\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\certifi_win32\\wrapt_certifi.py\", line 20, in apply_patches\n    certifi_win32.wincerts.CERTIFI_PEM = certifi.where()\n  File \"C:\\Users\\visha\\PycharmProjects\\GUI_Automation\\venv\\lib\\site-packages\\certifi\\core.py\", line 37, in where\n    _CACERT_PATH = str(_CACERT_CTX.__enter__())\n  File \"C:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py\", line 135, in __enter__\n    return next(self.gen)\n  File \"C:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\_common.py\", line 95, in _tempfile\n    os.remove(raw_path)\nPermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\visha\\\\AppData\\\\Local\\\\Temp\\\\tmpy_tb8siv'\nPS C:\\Users\\visha\\PycharmProjects\\master_test_runner> \n\n",
    "AcceptedAnswerId": 72087091,
    "AcceptedAnswer": "I ran into the same issue today.  I corrected it by removing two *.pth files that were created when I had installed python-certifi-win32.  This prevents python-certifi-win32 from loading when python is run.\nThe files are listed below, and were located here:\nC:\\Users\\\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\n\nFiles:\npython-certifi-win32-init.pth\ndistutils-precedence.pth\n\nRemoving these files allowed me to install/uninstall other modules.\n"
}
{
    "Id": 71564200,
    "PostTypeId": 1,
    "Title": "Python how to revert the pattern of a list rearrangement",
    "Body": "So I am rearranging a list based on an index pattern and would like to find a way to calculate the pattern I need to revert the list back to its original order.\nfor my example I am using a list of 5 items as I can work out the pattern needed to revert the list back to its original state.\nHowever this isn't so easy when dealing with 100's of list items.\ndef rearrange(pattern: list, L: list):\n    new_list = []\n    for i in pattern:\n        new_list.append(L[i-1])\n    return new_list\n\nprint(rearrange([2,5,1,3,4], ['q','t','g','x','r']))\n\n#['t', 'r', 'q', 'g', 'x']\n\nand in order to set it back to the original pattern\nI would use\nprint(rearrange([3,1,4,5,2],['t', 'r', 'q', 'g', 'x']))\n#['q', 't', 'g', 'x', 'r']\n\nWhat I am looking for is a way to calculate the pattern \"[3,1,4,5,2]\"\nregarding the above example.\nwhist running the script so that I can set the list back to its original order.\nUsing a larger example:\nprint(rearrange([18,20,10,11,13,1,9,12,16,6,15,5,3,7,17,2,19,8,14,4],['e','p','b','i','s','r','q','h','m','f','c','g','d','k','l','t','a','n','j','o']))\n#['n', 'o', 'f', 'c', 'd', 'e', 'm', 'g', 't', 'r', 'l', 's', 'b', 'q', 'a', 'p', 'j', 'h', 'k', 'i']\n\nbut I need to know the pattern to use with this new list in order to return it to its original state.\nprint(rearrange([???],['n', 'o', 'f', 'c', 'd', 'e', 'm', 'g', 't', 'r', 'l', 's', 'b', 'q', 'a', 'p', 'j', 'h', 'k', 'i']))\n#['e','p','b','i','s','r','q','h','m','f','c','g','d','k','l','t','a','n','j','o']\n\n",
    "AcceptedAnswerId": 71564272,
    "AcceptedAnswer": "This is commonly called \"argsort\". But since you're using 1-based indexing, you're off-by-one. You can get it with numpy:\n>>> pattern\n[2, 5, 1, 3, 4]\n>>> import numpy as np\n>>> np.argsort(pattern) + 1\narray([3, 1, 4, 5, 2])\n\nWithout numpy:\n>>> [1 + i for i in sorted(range(len(pattern)), key=pattern.__getitem__)]\n[3, 1, 4, 5, 2]\n\n"
}
{
    "Id": 72161257,
    "PostTypeId": 1,
    "Title": "Exclude default fields from python `dataclass` `__repr__`",
    "Body": "Summary\nI have a dataclass with 10+ fields. print()ing them buries interesting context in a wall of defaults - let's make them friendlier by not needlessly repeating those.\nDataclasses in Python\nPython's @dataclasses.dataclass() (PEP 557) provides automatic printable representations (__repr__()).\nAssume this example, based on python.org's:\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass InventoryItem:\n    name: str\n    unit_price: float = 1.00\n    quantity_on_hand: int = 0\n\nThe decorator, through @dataclass(repr=True) (default) will print() a nice output:\nInventoryItem(name='Apple', unit_price='1.00', quantity_on_hand=0)\n\nWhat I want: Skip printing the defaults\nrepr It prints all the fields, including implied defaults you wouldn't want to show.\nprint(InventoryItem(\"Apple\"))\n\n# Outputs: InventoryItem(name='Apple', unit_price='1.00', quantity_on_hand=0)\n# I want: InventoryItem(name='Apple')\n\nprint(InventoryItem(\"Apple\", unit_price=\"1.05\"))\n\n# Outputs: InventoryItem(name='Apple', unit_price='1.05', quantity_on_hand=0)\n# I want: InventoryItem(name='Apple', unit_price='1.05')\n\nprint(InventoryItem(\"Apple\", quantity_on_hand=3))\n\n# Outputs: InventoryItem(name='Apple', unit_price=1.00, quantity_on_hand=3)\n# I want: InventoryItem(name='Apple', quantity_on_hand=3)\n\nprint(InventoryItem(\"Apple\", unit_price='2.10', quantity_on_hand=3))\n\n# Output is fine (everything's custom):\n# InventoryItem(name='Apple', unit_price=2.10, quantity_on_hand=3)\n\nDiscussion\nInternally, here's the machinery of dataclass repr-generator as of python 3.10.4: cls.__repr__=_repr_fn(flds, globals)) -> _recursive_repr(fn)\nIt may be the case that @dataclass(repr=False) be switched off and def __repr__(self): be added.\nIf so, what would that look like? We don't want to include the optional defaults.\nContext\nTo repeat, in practice, my dataclass has 10+ fields.\nI'm print()ing instances via running the code and repl, and @pytest.mark.parametrize when running pytest with -vvv.\nBig dataclass' non-defaults (sometimes the inputs) are impossible to see as they're buried in the default fields and worse, each one is disproportionately and distractingly huge: obscuring other valuable stuff bring printed.\nRelated questions\nAs of today there aren't many dataclass questions yet (this may change):\n\nExtend dataclass' __repr__ programmatically: This is trying to limit the repr. It should show less fields unless they're explicitly overridden.\nPython dataclass generate hash and exclude unsafe fields: This is for hashing and not related to defaults.\n\n",
    "AcceptedAnswerId": 72161437,
    "AcceptedAnswer": "You could do it like this:\nimport dataclasses\nfrom dataclasses import dataclass\nfrom operator import attrgetter\n\n\n@dataclass(repr=False)\nclass InventoryItem:\n    name: str\n    unit_price: float = 1.00\n    quantity_on_hand: int = 0\n\n    def __repr__(self):\n        nodef_f_vals = (\n            (f.name, attrgetter(f.name)(self))\n            for f in dataclasses.fields(self)\n            if attrgetter(f.name)(self) != f.default\n        )\n\n        nodef_f_repr = \", \".join(f\"{name}={value}\" for name, value in nodef_f_vals)\n        return f\"{self.__class__.__name__}({nodef_f_repr})\"\n        \n\n# Prints: InventoryItem(name=Apple)\nprint(InventoryItem(\"Apple\"))\n\n# Prints: InventoryItem(name=Apple,unit_price=1.05)\nprint(InventoryItem(\"Apple\", unit_price=\"1.05\"))\n\n# Prints: InventoryItem(name=Apple,unit_price=2.10,quantity_on_hand=3)\nprint(InventoryItem(\"Apple\", unit_price='2.10', quantity_on_hand=3))\n\n"
}
{
    "Id": 72005302,
    "PostTypeId": 1,
    "Title": "Completely uninstall Python 3 on Mac",
    "Body": "I installed Python 3 on Mac and installed some packages as well. But then I see AWS lamda does not support Python 3 so I decided to downgrade. I removed Python3 folder in Applications and cleared the trash. But still I see a folder named 3 in /Library/Frameworks/Python.framework/Versions which is causing problems, such as this:\n  $ python3 -m pip install virtualenv\n Requirement already satisfied: virtualenv in      /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (20.14.1)\n Requirement already satisfied: platformdirs=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from virtualenv) (2.5.2) \n\nSo my question is how do I completely uninstall python 3 from my Mac?\n",
    "AcceptedAnswerId": 72005684,
    "AcceptedAnswer": "Removing the app does not completely uninstall that version of Python. You will need to remove the framework directories and their symbolic links.\nDeleting the frameworks\nsudo rm -rf /Library/Frameworks/Python.framework/Versions/[version number]\nreplacing [version number] with 3.10 in your case.\nRemoving symbolic links\nTo list the broken symbolic links.\nls -l /usr/local/bin | grep '../Library/Frameworks/Python.framework/Versions/[version number]'\nAnd to remove these links:\ncd /usr/local/bin\nls -l /usr/local/bin | grep '../Library/Frameworks/Python.framework/Versions/[version number]' | awk '{print $9}' | tr -d @ | xargs rm*\nAs always, please be wary of copying these commands. Please make sure the directories in the inputs are actual working directories before you execute anything.\nThe general idea in the end is to remove the folders and symlinks, and you're good to go.\nHere is another response addressing this process: How to uninstall Python 2.7 on a Mac OS X 10.6.4?\n"
}
{
    "Id": 72238460,
    "PostTypeId": 1,
    "Title": "Python ImportError: sys.meta_path is None, Python is likely shutting down",
    "Body": "When using __del__\ndatetime.date.today() throws ImportError: sys.meta_path is None, Python is likely shutting down\nimport datetime\nimport time\nimport sys\n\n\nclass Bug(object):\n\n    def __init__(self):\n        print_meta_path()\n\n    def __del__(self):\n        print_meta_path()\n        try_date('time')\n        try_date('datetime')\n\n\ndef print_meta_path():\n    print(f'meta_path: {sys.meta_path}')\n\n\ndef try_date(date_type):\n    try:\n        print('----------------------------------------------')\n        print(date_type)\n        if date_type == 'time':\n            print(datetime.date.fromtimestamp(time.time()))\n        if date_type == 'datetime':\n            print(datetime.date.today())\n    except Exception as ex:\n        print(ex)\n\n\nif __name__ == '__main__':\n    print(sys.version)\n    bug = Bug()\n\noutput with different envs (3.10, 3.9, 3.7):\n3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:39:04) [GCC 10.3.0]\nmeta_path: [, , , ]\nmeta_path: None\n----------------------------------------------\ntime\n2022-05-17\n----------------------------------------------\ndatetime\nsys.meta_path is None, Python is likely shutting down\n\n3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:22:55)\n[GCC 10.3.0]\nmeta_path: [, , , ]\nmeta_path: None\n----------------------------------------------\ntime\n2022-05-17\n----------------------------------------------\ndatetime\nsys.meta_path is None, Python is likely shutting down\n\n3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53)\n[GCC 9.4.0]\nmeta_path: [, , ]\nmeta_path: None\n----------------------------------------------\ntime\n2022-05-17\n----------------------------------------------\ndatetime\nsys.meta_path is None, Python is likely shutting down\n\n3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]\nmeta_path: [, , ]\nmeta_path: None\n----------------------------------------------\ntime\n2022-05-17\n----------------------------------------------\ndatetime\nsys.meta_path is None, Python is likely shutting down\n\nWhy is that happening?\nI need to use requests which use urllib3 connection.py\n380:  is_time_off = datetime.date.today() \n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/api.py\", line 117, in post\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/api.py\", line 61, in request\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/sessions.py\", line 529, in request\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/sessions.py\", line 645, in send\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/adapters.py\", line 440, in send\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 386, in _make_request\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 1040, in _validate_conn\n  File \"/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/urllib3/connection.py\", line 380, in connect\nImportError: sys.meta_path is None, Python is likely shutting down\n\nswitching the line to\n380:  is_time_off = datetime.date.fromtimestamp(time.time())  solve it.\nOS Linux-5.13.0-41-generic-x86_64-with-glibc2.31\nurllib3 1.26.9\n\nI already tried to rebind __del__ arguments default\ndef __del__(self, datetime=datetime):....\nDoes anyone have an idea? thanks\n",
    "AcceptedAnswerId": 72275619,
    "AcceptedAnswer": "Using atexit provide the same behavior as __del__ but works\nimport datetime\nimport time\nimport sys\nimport atexit\n\n\nclass Bug(object):\n\n    def __init__(self):\n        print_meta_path()\n        atexit.register(self.__close)\n\n    def __close(self):\n        print_meta_path()\n        try_date('time')\n        try_date('datetime')\n\n\ndef print_meta_path():\n    print(f'meta_path: {sys.meta_path}')\n\n\ndef try_date(date_type):\n    try:\n        print('----------------------------------------------')\n        print(date_type)\n        if date_type == 'time':\n            print(datetime.date.fromtimestamp(time.time()))\n        if date_type == 'datetime':\n            print(datetime.date.today())\n    except ImportError:\n        print('')\n\n\nif __name__ == '__main__':\n    print(sys.version)\n    bug = Bug()\n\noutput:\n3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:38:57) [GCC 10.3.0]\nmeta_path: [, , , ]\nmeta_path: [, , , ]\n----------------------------------------------\ntime\n2022-05-17\n----------------------------------------------\ndatetime\n2022-05-17\n\nProcess finished with exit code 0\n\n"
}
{
    "Id": 72280762,
    "PostTypeId": 1,
    "Title": "pip broke after downlading python-certifi-win32",
    "Body": "I have downloaded python for the first time in a new computer(ver 3.10.4).\nI have download the package python-certifi-win32, after someone suggested it as a solution to a SSL certificate problem in a similar question to a problem I had.\nSince then, pip has completely stopped working, to the point where i can't not run pip --version\nEvery time the same error is printed, it is mostly seemingly junk(just a deep stack trace), but the file at the end is different.\nstart of the printed log:\nTraceback (most recent call last):\n  File \"C:\\Users\\---\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\_common.py\", line 89, in _tempfile\n    os.write(fd, reader())\n  File \"C:\\Users\\---\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\abc.py\", line 371, in read_bytes\n    with self.open('rb') as strm:\n  File \"C:\\Users\\---\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\_adapters.py\", line 54, in open\n    raise ValueError()\nValueError\n\nDuring handling of the above exception, another exception occurred:\n\nlast row of the printed log:\nPermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\-----\\\\AppData\\\\Local\\\\Temp\\\\tmpunox3fhw'\n\n",
    "AcceptedAnswerId": 72293534,
    "AcceptedAnswer": "I found the answer in another question -\nPermissionError: [WinError 32] The process cannot access the file because it is being used by another process: after installing python-certifi-win32\nbasically, you should remove two files that initialize python-certifi-win32 when running pip. the files are located in the directory:\nC:\\Users\\\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\n\nand their names are:\npython-certifi-win32-init.pth\ndistutils-precedence.pth\n\nShoutout to Richard from the mentioned post :)\n"
}
{
    "Id": 71737743,
    "PostTypeId": 1,
    "Title": "How can I change playback speed of an audio file in python whilst it is playing?",
    "Body": "I've done alot of searching to try and find a way to achieve this but the solutions I've found either don't do what I need or I don't understand them.\nI'm looking for a way of playing a sound in python (non-blocking) that allows me to change the playback speed in real time, as it's playing, with no gaps or cutouts.\nChanging the pitch is fine. Audio quality isn't even that important.\nMost of the solutions I've found only allow setting the playback speed once, before the file is played.\n",
    "AcceptedAnswerId": 71748928,
    "AcceptedAnswer": "I've found a solution, using python-mpv, a wrapper for mpv.io\nfrom pynput.keyboard import Key, Listener\nimport mpv\nspeed=1\n\n#quick function to change speed via keyboard. \ndef on_press(key):\n\n    global speed\n\n    if key.char == 'f' :\n        speed=speed-0.1\n        player.speed=speed\n    if key.char == 'g' :\n        speed=speed+0.1\n        player.speed=speed\n\nplayer = mpv.MPV(ytdl=True)\nplayer.play('/Users/regvardy/mediapipe_faceswap-main/test.wav')\nwith Listener(\n        on_press=on_press) as listener:\n    listener.join()\nwhile True:\n    \n    player.speed=speed\n\nI haven't tested it for stability yet.\nIt feels like a workaround rather than me actually finding out how to do it so I may try and find a different solution.\n"
}
{
    "Id": 72277275,
    "PostTypeId": 1,
    "Title": "How to monitor per-process network usage in Python?",
    "Body": "I'm looking to troubleshoot my internet issue so I need a way to track both my latency and which application is using how much network bandwidth.\nI've already sorted out checking latency, but now I need a way to monitor each process' network usage (KB/s), like how it appears in Windows Task Manager.\nBefore you suggest a program, unless it's able to record the values with a timestamp then that's not what I'm looking for. I'm asking for a Pythonic way because I need to record the network bandwidth and latency values at the same time so I can figure out if a specific process is causing latency spikes.\nSo here's the info I need:\nTime | Process ID | Process Name | Down Usage | Up Usage | Network Latency |\nAlso, please don't link to another Stackoverflow question unless you know their solution works. I've looked through plenty already and none of them work, which is why I'm asking again.\n",
    "AcceptedAnswerId": 72310057,
    "AcceptedAnswer": "Following the third section of this guide provided me with all of the information listed in the post, minus latency. Given that you said you already had measuring latency figured out, I assume this isn't an issue.\nLogging this to csv/json/whatever is pretty easy, as all of the information is stored in panda data frames.\nAs this shows the time the process was created, you can use datetime to generate a new timestamp at the time of logging.\nI tested this by logging to a csv after the printing_df variable was initialized, and had no issues.\n"
}
{
    "Id": 71102876,
    "PostTypeId": 1,
    "Title": "in ipython how do I accept and use an autocomplete suggestion?",
    "Body": "I'm using Python 3.8.9 with IPython 8.0.1 on macOS. When I type anything whatsoever, it displays a predicted suggestion based on past commands. Cool.\nHowever, how do I actually accept that suggestion? I tried the obvious: tab, which does not accept the suggestion, but rather opens up a menu with different suggestions, while the original suggestion is still there (see screenshot).\nI also tried space, and return, but both of those act as if the suggestion was never made. How the heck do I actually use the ipython autosuggestion? Or is tab supposed to work and something is wrong with my ipython build or something?\n\n",
    "AcceptedAnswerId": 71459528,
    "AcceptedAnswer": "CTRL-E, CTRL-F, or Right Arrow Key\nhttps://ipython.readthedocs.io/en/6.x/config/shortcuts/index.html\n"
}
{
    "Id": 71803409,
    "PostTypeId": 1,
    "Title": "VSCode: how to interrupt a running Python test?",
    "Body": "I'm using VSCode Test Explorer to run my Python unit tests. There was a bug in my code and my tested method never finishes.\nHow do I interrupt my test? I can't find how to do it using the GUI. I had to close VSCode to interrupt it.\nI'm using pytest framework.\n",
    "AcceptedAnswerId": 71803605,
    "AcceptedAnswer": "Silly me, here is the Stop button at the top right of the the Testing tab:\n\n"
}
{
    "Id": 71630563,
    "PostTypeId": 1,
    "Title": "Syntax for making objects callable in python",
    "Body": "I understand that in python user-defined objects can be made callable by defining a __call__() method in the class definition. For example,\nclass MyClass:\n  def __init__(self):\n    pass\n\n  def __call__(self, input1):\n    self.my_function(input1)\n\n  def my_function(self, input1):\n    print(f\"MyClass - print {input1}\")\n\nmy_obj = MyClass()\n# same as calling my_obj.my_function(\"haha\")\nmy_obj(\"haha\") # prints \"MyClass - print haha\"\n\nI was looking at how pytorch makes the forward() method of a nn.Module object be called implicitly when the object is called and saw some syntax I didn't understand.\nIn the line that supposedly defines the __call__ method the syntax used is,\n__call__ : Callable[..., Any] = _call_impl\n\nThis seemed like a combination of an annotation (keyword Callable[ following : ignored by python) and a value of _call_impl which we want to be called when __call__ is invoked, and my guess is that this is a shorthand for,\ndef __call__(self, *args, **kwargs):\n    return self._call_impl(*args, **kwargs)\n\nbut wanted to understand clearly how this method of defining functions worked.\nMy question is: When would we want to use such a definition of callable attributes of a class instead of the usual def myfunc(self, *args, **kwargs)\n",
    "AcceptedAnswerId": 71630606,
    "AcceptedAnswer": "Functions are normal first-class objects in python. The name to with which you define a function object, e.g. with a def statement, is not set in stone, any more than it would be for an int or list. Just as you can do\na = [1, 2, 3]\nb = a\n\nto access the elements of a through the name b, you can do the same with functions. In your first example, you could replace\ndef __call__(self, input1):\n    self.my_function(input1)\n\nwith the much simpler\n__call__ = my_function\n\nYou would need to put this line after the definition of my_function.\nThe key differences between the two implementations is that def __call__(... creates a new function. __call__ = ... simply binds the name __call__ to the same object as my_function. The noticeable difference is that if you do __call__.__name__, the first version will show __call__, while the second will show my_function, since that's what gets assigned by a def statement.\n"
}
{
    "Id": 71823279,
    "PostTypeId": 1,
    "Title": "Python Read huge file line per line and send it to multiprocessing or thread",
    "Body": "I have been trying to get my code to work for many days,\nI am desperate.\nI've scoured the internet, but I still can't find it.\nI have a text file encoded in \"latin-1\" of 9GB -> 737 022 387 lines, each line contains a string.\nI would like to read each line and send them in an http PUT request that waits for a response, and returns TRUE or FALSE if the response is 200 or 400\nThe PUT request takes about 1 to 3 seconds, so to speed up the processing time I would like to use either a Thread or a multiprocessing.\nTo start, I simulate my PUT request with a sleep of 3 seconds.\nand even that I can't get it to work\nThis code split my string into char, i don't know why...\nfrom multiprocessing import Pool\nfrom time import sleep\n\n\ndef process_line(line):\n   sleep(3)\n   print(line)\n   return True\n\nif __name__ == \"__main__\":\n    pool = Pool(2)\n    peon =  open(r'D:\\txtFile',encoding=\"latin-1\")\n    for line in peon:\n        res = pool.map(process_line,line )\n        print(res)\n\nThis give error : TypeError: process_line() takes 1 positional argument but 17 were given\nimport multiprocessing\nfrom multiprocessing import Pool\nfrom time import sleep\n\n\ndef process_line(line):\n   sleep(3)\n   print(line)\n   return True\n\nif __name__ == \"__main__\":\n    pool = Pool(2)\n    with open(r\"d:\\txtFile\",encoding=\"latin-1\") as file:\n        res = pool.apply(process_line,file.readline() )\n        print(res)\n\nthat : Crash the computer\nfrom multiprocessing import Pool\nfrom time import sleep\n\n\ndef process_line(line):\n   sleep(3)\n   print(line)\n   return True\n\nif __name__ == \"__main__\":\n    pool = Pool(2)\n    peon =  open(r'D:\\txtFile',encoding=\"latin-1\")\n    for line in peon:\n        res = pool.map(process_line,peon )\n        print(res)\n\n",
    "AcceptedAnswerId": 71824107,
    "AcceptedAnswer": "Although the problem seems unrealistic though. shooting 737,022,387 requests! calculate how many months it'll take from single computer!!\nStill, Better way to do this task is to read line by line from file in a separate thread and insert into a queue. And then multi-process the queue.\nSolution 1:\nfrom multiprocessing import Queue, Process\nfrom threading import Thread\nfrom time import sleep\n\nurls_queue = Queue()\nmax_process = 4\n\ndef read_urls():\n    with open('urls_file.txt', 'r') as f:\n        for url in f:\n            urls_queue.put(url.strip())\n            print('put url: {}'.format(url.strip()))\n\n    # put DONE to tell send_request_processor to exit\n    for i in range(max_process):\n        urls_queue.put(\"DONE\")\n\n\ndef send_request(url):\n    print('send request: {}'.format(url))\n    sleep(1)\n    print('recv response: {}'.format(url))\n\n\ndef send_request_processor():\n    print('start send request processor')\n    while True:\n        url = urls_queue.get()\n        if url == \"DONE\":\n            break\n        else:\n            send_request(url)\n\n\ndef main():\n    file_reader_thread = Thread(target=read_urls)\n    file_reader_thread.start()\n\n    procs = []\n    for i in range(max_process):\n        p = Process(target=send_request_processor)\n        procs.append(p)\n        p.start()\n\n    for p in procs:\n        p.join()\n\n    print('all done')\n    # wait for all tasks in the queue\n    file_reader_thread.join()\n\n\nif __name__ == '__main__':\n    main()\n\nDemo: https://onlinegdb.com/Elfo5bGFz\nSolution 2:\nYou can use tornado asynchronous networking library\nfrom tornado import gen\nfrom tornado.ioloop import IOLoop\nfrom tornado.queues import Queue\n\nq = Queue(maxsize=2)\n\nasync def consumer():\n    async for item in q:\n        try:\n            print('Doing work on %s' % item)\n            await gen.sleep(0.01)\n        finally:\n            q.task_done()\n\nasync def producer():\n    with open('urls_file.txt', 'r') as f:\n        for url in f:\n            await q.put(url)\n            print('Put %s' % item)\n\nasync def main():\n    # Start consumer without waiting (since it never finishes).\n    IOLoop.current().spawn_callback(consumer)\n    await producer()     # Wait for producer to put all tasks.\n    await q.join()       # Wait for consumer to finish all tasks.\n    print('Done')\n    # producer and consumer can run in parallel\n\nIOLoop.current().run_sync(main)\n\n"
}
{
    "Id": 71656644,
    "PostTypeId": 1,
    "Title": "Python type hint for Iterable[str] that isn't str",
    "Body": "In Python, is there a way to distinguish between strings and other iterables of strings?\nA str is valid as an Iterable[str] type, but that may not be the correct input for a function. For example, in this trivial example that is intended to operate on sequences of filenames:\nfrom typing import Iterable\n\ndef operate_on_files(file_paths: Iterable[str]) -> None:\n    for path in file_paths:\n        ...\n\nPassing in a single filename would produce the wrong result but would not be caught by type checking. I know that I can check for string or byte types at runtime, but I want to know if it's possible to catch silly mistakes like that with a type-checking tool.\nI've looked over the collections.abc module and there doesn't seem to be any abc that would include typical iterables (e.g. lists, tuples) but exclude strings. Similarly, for the typing module, there doesn't seem to be a type for iterables that don't include strings.\n",
    "AcceptedAnswerId": 71657094,
    "AcceptedAnswer": "As of March 2022, the answer is no.\nThis issue has been discussed since at least July 2016. On a proposal to distinguish between str and Iterable[str], Guido van Rossum writes:\n\nSince str is a valid iterable of str this is tricky. Various proposals have been made but they don't fit easily in the type system.\n\nYou'll need to list out all of the types that you want your functions to accept explicitly, using Union (pre-3.10) or | (3.10 and higher).\ne.g. For pre-3.10, use:\nfrom typing import Union\n## Heading ##\ndef operate_on_files(file_paths: Union[TypeOneName, TypeTwoName, etc.]) -> None:\n    for path in file_paths:\n        ...\n\nFor 3.10 and higher, use:\n## Heading ##\ndef operate_on_files(file_paths: TypeOneName | TypeTwoName | etc.) -> None:\n    for path in file_paths:\n        ...\n\nIf you happen to be using Pytype, it will not treat str as an Iterable[str] (as pointed out by Kelly Bundy). But, this behavior is typechecker-specific, and isn't widely supported in other typecheckers.\n"
}
{
    "Id": 71818149,
    "PostTypeId": 1,
    "Title": "POST request gets blocked on Python backend. GET request works fine",
    "Body": "I am building a web app where the front-end is done with Flutter while the back-end is with Python.\nGET requests work fine while POST requests get blocked because of CORS, I get this error message:\nAccess to XMLHttpRequest at 'http://127.0.0.1:8080/signal' from origin 'http://localhost:57765' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource.\n\nBelow is my flutter function I used to send GET and POST requests:\n  Future sendResponse() async {\n    final url = 'http://127.0.0.1:8080/signal';\n    var data = {\n      \"signal\": '8',\n    };\n    var header = {\n      'Access-Control-Allow-Origin': '*',\n      \"Accept\": \"application/x-www-form-urlencoded, '*'\"\n    };\n\n\n    http.Response response = await http.post(Uri.parse(url), body: data, headers: header);//http.post(Uri.parse(url), body: data, headers: header);//http.get(Uri.parse(url));\n    if (response.statusCode == 200) {\n      print(json.decode(response.body));\n      return jsonDecode(response.body);\n      //print(json.decode(credentials.body));\n    } else {\n      print(response.statusCode);\n      throw Exception('Failed to load Entry');\n    }\n\n   // var ResponseFromPython = await response.body;//jsonDecode(credentials.body);\n\n   // return ResponseFromPython;\n  }\n\nBelow is my Python backend code using Flask:\n   from flask import Flask,jsonify, request, make_response\n   import json\n\n\n   from flask_cors import CORS, cross_origin\n\n\n   #declared an empty variable for reassignment\n   response = ''\n\n   app = Flask(__name__)\n\n   #CORS(app, resources={r\"/signal\": {\"origins\": \"*, http://localhost:59001\"}}) \n   #http://localhost:52857\n   #CORS(app, origins=['*'])\n   app.config['CORS_HEADERS'] = ['Content-Type','Authorization']\n\n\n\n   @app.route(\"/\")\n   def index():\n    \n    return \"Congratulations, it worked\"\n\n   @app.route(\"/signal\", methods = ['POST', 'GET']) #,\n   @cross_origin(origins='http://localhost:57765',headers=['Content-Type','Authorization', \n   'application/x-www-form-urlencoded','*'], upports_credentials=True)# allow all origins all \n   methods.\n   def multbytwo():\n       \"\"\"multiple signal by 2 just to test.\"\"\"\n       global response\n       if (request.method=='POST'):\n       # request.headers.add(\"Access-Control-Allow-Origin\", \"*\")\n           request_data = request.data #getting the response data\n           request_data = json.loads(request_data.decode('utf-8')) #converting it from json to key \n   value pair\n           comingSignal = request_data['signal']\n           response = make_response(comingSignal, 201)#jsonify(comingSignal*2)\n           response.headers.add('Access-Control-Allow-Origin', '*')\n           response.headers.add('Access-Control-Allow-Methods\", \"DELETE, POST, GET, OPTIONS')\n           response.headers.add('Access-Control-Allow-Headers\", \"Content-Type, Authorization, X- \n  Requested-With')\n           return response\n       else:\n           try:\n        #scaler = request.args.get(\"signal\")\n               out = 9 * 2 \n         \n               response = jsonify(out)\n               response.headers.add(\"Access-Control-Allow-Origin\", \"*\") \n               return response #sending data back to your frontend app\n\n           except ValueError:\n               return \"invalid input xyz\"\n\n   if __name__ == \"__main__\":\n       app.run(host=\"127.0.0.1\", port=8080, debug=True)\n\nBelow are the troubleshooting steps I made:\n-Added the flask_CORS package in python\nI tried here different combination from using general parameters like CORS(app, resources={r\"/signal\": {\"origins\": \"*\"}})  did not help. Also tried the decorator @cross-origin and did not help\n-Added some headers to the response itself to indicate that it accepts cross-origin\nYou see in my python code I tried adding a lot of headers to the response, nothing seem to respond.\n-Tried installing an extension in Chrome that by-passes the CORS check\nI tried the allow CORS and CORS unblock extensions and I used the steps described in this answer: How chrome extensions be enabled when flutter web debugging?. Although these extensions are supposed to add the CORS allow header to the response, I still got the same error.\nI still do not fully understand the CORS concept but I tried a lot of work-arounds and nothing works! please help.\n",
    "AcceptedAnswerId": 71882248,
    "AcceptedAnswer": "I finally figured out what was going on.\nFirst I disabled the same origin policy in chrome using this command: this is run clicking the start button in windows and typing this command directly..\nchrome.exe  --disable-site-isolation-trials --disable-web-security --user-data-dir=\"D:\\anything\"\n\nThis fired a separate chrome window that does not block cross-origin, we will call this the CORS free window. This allowed me to finally communicate with my python code and understand what is going on.\n\nYou can see that the chrome default setting were not even showing me anything related to the response, just showing a 500 code error.\nI copied the localhost link and port and pasted them in my other CORS free chrome window\nThe other CORS free chrome window showed helpful information:\n\nIt was a simple JSON decoding error! I went back to my flutter code and I changed the http post request, adding a jsonEncode function on the post body:\nhttp.Response response = await http.post(Uri.parse(url), body:jsonEncode(data), headers: header);\n\nNow the post request returns a correct response on the default chrome settings.\n\nIt was just this CORS blocking the response completely that made me handi-capped.\n"
}
{
    "Id": 72782100,
    "PostTypeId": 1,
    "Title": "For loop in c# vs For loop in python",
    "Body": "I was writing a method that would calculate the value of e^x. The way I implemented this in python was as follows.\nimport math\n\ndef exp(x):\n    return sum([\n        x**n/math.factorial(n)\n        for n in range(0, 100)\n    ])\n\nThis would return the value of e^x very well. But when I tried to implement the same method in c#, it didn't output the same value as it did in python. The following was the implementation in c#.\nstatic double exp(int x)\n{\n    double FinalAnswer = 0;\n    for (int j = 0; j <= 100; j++)\n    {\n        FinalAnswer += (Math.Pow(x, j))/Factorial(j);\n    }\n    return FinalAnswer;\n}\n\nThe output for this code was an infinity symbol at first. To resolve this I just reduced the number of times the loop ran. The output of the code in c# where the loop only ran 10 times was pretty close to the output in python where the loop ran 100 times. My question is that what is going on between the two loops in different programming languages. At first I thought that the expression that I was using in my method to calculate e^x was converging quickly. But how does a loop that runs 10 times produce an output that matches the output of a loop that runs 100 times.\nAlso, When I increased the for loop in c# to 20 and 30, the values of e^x for x > 3 were way off. Could someone explain what is going on here?\n",
    "AcceptedAnswerId": 72782395,
    "AcceptedAnswer": "What you're likely running into here is integer overflow with the C# version of the Factorial function (at least your implementation of it, or wherever its coming from).\nIn C#, an int is a numerical type stored in 32 bits of memory, which means it's bounded by -2^31  which is around +/- 2.1 billion. You could try using a long type, which is a 64 bit numerical type, however for even larger upper bounds in your for loop, like getting close to 100, you're going to overflow long as well.\nWhen you run the Factorial function in C#, it starts off normally for the first little while, however if you keep going, you'll see that it all of a sudden jumps into negative numbers, and if you keep going even further than that, it'll get to 0 and stop changing. You're seeing the output of infinity due to division by 0, and C# has a way of handling that with doubles; that being to just return double.PositiveInfinity.\nThe reason why this doesn't happen in python is that it uses a variable number of bits to store its numerical values.\nAdded note: What you might also want to try is using a Factorial function that works with the double type instead of int or long, however by doing this, you'll lose precision on what the exact value is, but you get more range as the magnitude of the number you can store is larger\nFurther Note: As mentioned in the comments, C# has a type called BigInteger which is designed to handle huge numbers like the values you would expect from large inputs to a Factorial function. You can find a reference to the BigInteger docs here\n\nWhat you can do is calculate each component of the factorial function separately with the power you're using. Here's what I mean:\npublic decimal Exp(decimal power, int accuracy = 100)\n{\n    decimal runningTotal = 1;\n    decimal finalValue = 1;\n    for (int i = 1; i <= accuracy; i++)\n    {\n        runningTotal *= power/i;\n        finalValue += runningTotal;\n    }\n    return finalValue;\n}\n\n"
}
{
    "Id": 71671866,
    "PostTypeId": 1,
    "Title": "Python: What is the difference between `lambda` and `lambda_`?",
    "Body": "I know the function of lambda: and lambda var: , but what does lambda_: means acutally?\n",
    "AcceptedAnswerId": 71671890,
    "AcceptedAnswer": "lambda_ is just a variable name, like any other. Like foo or x.\nIf you saw:\nlambda_: Something\n\nThen that is actually a variable annotation, for type hints, so the same as:\nnum: int\nnum = 0\n\n"
}
{
    "Id": 71641609,
    "PostTypeId": 1,
    "Title": "How does CPython implement os.environ?",
    "Body": "I was looking through source and noticed that it references a variable environ in methods before its defined:\ndef _createenviron():\n    if name == 'nt':\n        # Where Env Var Names Must Be UPPERCASE\n        def check_str(value):\n            if not isinstance(value, str):\n                raise TypeError(\"str expected, not %s\" % type(value).__name__)\n            return value\n        encode = check_str\n        decode = str\n        def encodekey(key):\n            return encode(key).upper()\n        data = {}\n        for key, value in environ.items():\n            data[encodekey(key)] = value\n    else:\n        # Where Env Var Names Can Be Mixed Case\n        encoding = sys.getfilesystemencoding()\n        def encode(value):\n            if not isinstance(value, str):\n                raise TypeError(\"str expected, not %s\" % type(value).__name__)\n            return value.encode(encoding, 'surrogateescape')\n        def decode(value):\n            return value.decode(encoding, 'surrogateescape')\n        encodekey = encode\n        data = environ\n    return _Environ(data,\n        encodekey, decode,\n        encode, decode)\n\n# unicode environ\nenviron = _createenviron()\ndel _createenviron\n\nSo how does environ get setup? I cant seem to reason about where its initialized and declared so that _createenviron can use it?\n",
    "AcceptedAnswerId": 71682620,
    "AcceptedAnswer": "TLDR search for from posix import * in os module content.\nThe os module imports all public symbols from posix (Unix) or nt (Windows) low-level module at the beginning of os.py.\nposix exposes environ as a plain Python dict.\nos wraps it with _Environ dict-like object that updates environment variables on _Environ items changing.\n"
}
{
    "Id": 72235819,
    "PostTypeId": 1,
    "Title": "How can I redirect module imports with modern Python?",
    "Body": "I am maintaining a python package in which I did some restructuring. Now, I want to support clients who still do from my_package.old_subpackage.foo import Foo instead of the new from my_package.new_subpackage.foo import Foo, without explicitly reintroducing many files that do the forwarding.  (old_subpackage still exists, but no longer contains foo.py.)\nI have learned that there are \"loaders\" and \"finders\", and my impression was that I should implement a loader for my purpose, but I only managed to implement a finder so far:\nRENAMED_PACKAGES = {\n    'my_package.old_subpackage.foo': 'my_package.new_subpackage.foo',\n}\n\n# TODO: ideally, we would not just implement a \"finder\", but also a \"loader\"\n# (using the importlib.util.module_for_loader decorator); this would enable us\n# to get module contents that also pass identity checks\nclass RenamedFinder:\n\n    @classmethod\n    def find_spec(cls, fullname, path, target=None):\n        renamed = RENAMED_PACKAGES.get(fullname)\n        if renamed is not None:\n            sys.stderr.write(\n                f'WARNING: {fullname} was renamed to {renamed}; please adapt import accordingly!\\n')\n            return importlib.util.find_spec(renamed)\n        return None\n\nsys.meta_path.append(RenamedFinder())\n\nhttps://docs.python.org/3.5/library/importlib.html#importlib.util.module_for_loader and related functionality, however, seem to be deprecated.  I know it's not a very pythonic thing I am trying to achieve, but I would be glad to learn that it's achievable.\n",
    "AcceptedAnswerId": 72244240,
    "AcceptedAnswer": "On import of your package's __init__.py, you can place whatever objects you want into sys.modules, the values you put in there will be returned by import statements:\nfrom . import new_package\nfrom .new_package import module1, module2\nimport sys\n\nsys.modules[\"my_lib.old_package\"] = new_package\nsys.modules[\"my_lib.old_package.module1\"] = module1\nsys.modules[\"my_lib.old_package.module2\"] = module2\n\nIf someone now uses import my_lib.old_package or import my_lib.old_package.module1 they will obtain a reference to my_lib.new_package.module1. Since the import machinery already finds the keys in the sys.modules dictionary, it never even begins looking for the old files.\nIf you want to avoid importing all the submodules immediately, you can emulate a bit of lazy loading by placing a module with a __getattr__ in sys.modules:\nfrom types import ModuleType\nimport importlib\nimport sys\n\nclass LazyModule(ModuleType):\n def __init__(self, name, mod_name):\n  super().__init__(name)\n  self.__mod_name = name\n\n def __getattr__(self, attr):\n  if \"_lazy_module\" not in self.__dict__:\n    self._lazy_module = importlib.import(self.__mod_name, package=\"my_lib\")\n  return self._lazy_module.__getattr__(attr)\n\nsys.modules[\"my_lib.old_package\"] = LazyModule(\"my_lib.old_package\", \"my_lib.new_package\")\n\n"
}
{
    "Id": 72795799,
    "PostTypeId": 1,
    "Title": "How to solve 403 error with Flask in Python?",
    "Body": "I made a simple server using python flask in mac. Please find below the code.\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET', 'POST'])\ndef hello():\n    print(\"request received\")\n    return \"Hello world!\"\n\n    \nif __name__ == \"__main__\":\n    app.run(debug=True)\n\nI run it using python3 main.py command.\nWhile calling above API on url  http://localhost:5000/ from Postman using GET / POST method, it always returns http 403 error.\n\nPython version : 3.8.9\nOS : Mac OS 12.4\nFlask : 2.1.2\n\n",
    "AcceptedAnswerId": 72797062,
    "AcceptedAnswer": "Mac OSX Monterey (12.x) currently uses ports 5000 and 7000 for its Control centre hence the issue.\nTry running your app from port other than 5000 and 7000\nuse this:\nif __name__ == \"__main__\":\n    app.run(port=8000, debug=True)\n\nand then run your flask file, eg: app.py\npython app.py\nYou can also run using the flask command line interface using this command provided you have set the environment variable necessary for flask CLI.\nflask run --port 8000\nYou can also turn off AirPlay Receiver in the Sharing via System Preference.\nRelated discussion here: https://developer.apple.com/forums/thread/682332\nUpdate(November 2022):\nMac OSX Ventura(13.x) still has this problem and is fixed with the change in default port as mentioned above.\n"
}
{
    "Id": 71739870,
    "PostTypeId": 1,
    "Title": "How to install Python 2 on macOS 12.3+",
    "Body": "macOS 12.3 update drops Python 2 and replaces it with version 3:\nhttps://developer.apple.com/documentation/macos-release-notes/macos-12_3-release-notes\n\nPython\nDeprecations\nPython 2.7 was removed from macOS in this update. Developers should use Python 3 or an alternative language instead. (39795874)\n\nI understand we need to migrate to version 3, but in the meantime we still need version 2. Homebrew does not seem to have it anymore:\nbrew install python@2.7\nWarning: No available formula with the name \"python@2.7\". Did you mean python@3.7, python@3.9, python@3.8, python@3.10 or python-yq?\n\nbrew install python2\nWarning: No available formula with the name \"python2\". Did you mean ipython, bpython, jython or cython?\n\nWhat gives?\n",
    "AcceptedAnswerId": 71740144,
    "AcceptedAnswer": "You can get any Python release, including the last Python 2, from the official download site:\nhttps://www.python.org/downloads/release/python-2718/ \u2192 macOS 64-bit installer\n"
}
{
    "Id": 72236445,
    "PostTypeId": 1,
    "Title": "How can I wrap a python function in a way that works with with inspect.signature?",
    "Body": "Some uncontroversial background experimentation up front:\nimport inspect\n\ndef func(foo, bar):\n  pass\n\nprint(inspect.signature(func))  # Prints \"(foo, bar)\" like you'd expect\n\ndef decorator(fn):\n  def _wrapper(baz, *args, *kwargs):\n    fn(*args, **kwargs)\n\n  return _wrapper\n\nwrapped = decorator(func)\nprint(inspect.signature(wrapped))  # Prints \"(baz, *args, **kwargs)\" which is totally understandable\n\nThe Question\nHow can implement my decorator so that print(inspect.signature(wrapped)) spits out \"(baz, foo, bar)\"?  Can I build _wrapper dynamically somehow by adding the arguments of whatever fn is passed in, then gluing baz on to the list?\nThe answer is NOT\ndef decorator(fn):\n  @functools.wraps(fn)\n  def _wrapper(baz, *args, *kwargs):\n    fn(*args, **kwargs)\n\n  return _wrapper\n\nThat give \"(foo, bar)\" again - which is totally wrong.  Calling wrapped(foo=1, bar=2) is a type error - \"Missing 1 required positional argument: 'baz'\"\nI don't think it's necessary to be this pedantic, but\ndef decorator(fn):\n  def _wrapper(baz, foo, bar):\n    fn(foo=foo, bar=bar)\n\n  return _wrapper\n\nIs also not the answer I'm looking for - I'd like the decorator to work for all functions.\n",
    "AcceptedAnswerId": 72242606,
    "AcceptedAnswer": "You can use __signature__ (PEP) attribute to modify returned signature of wrapped object. For example:\nimport inspect\n\n\ndef func(foo, bar):\n    pass\n\n\ndef decorator(fn):\n    def _wrapper(baz, *args, **kwargs):\n        fn(*args, **kwargs)\n\n    f = inspect.getfullargspec(fn)\n\n    fn_params = []\n    if f.args:\n        for a in f.args:\n            fn_params.append(\n                inspect.Parameter(a, inspect.Parameter.POSITIONAL_OR_KEYWORD)\n            )\n\n    if f.varargs:\n        fn_params.append(\n            inspect.Parameter(f.varargs, inspect.Parameter.VAR_POSITIONAL)\n        )\n\n    if f.varkw:\n        fn_params.append(\n            inspect.Parameter(f.varkw, inspect.Parameter.VAR_KEYWORD)\n        )\n\n    _wrapper.__signature__ = inspect.Signature(\n        [\n            inspect.Parameter(\"baz\", inspect.Parameter.POSITIONAL_OR_KEYWORD),\n            *fn_params,\n        ]\n    )\n    return _wrapper\n\n\nwrapped = decorator(func)\nprint(inspect.signature(wrapped))\n\nPrints:\n(baz, foo, bar)\n\n\nIf the func is:\ndef func(foo, bar, *xxx, **yyy):\n    pass\n\nThen print(inspect.signature(wrapped)) prints:\n(baz, foo, bar, *xxx, **yyy)\n\n"
}
{
    "Id": 72193393,
    "PostTypeId": 1,
    "Title": "Find the value of variables to maximize return of function in Python",
    "Body": "I'd want to achieve similar result as how the Solver-function in Excel is working. I've been reading of Scipy optimization and been trying to build a function which outputs what I would like to find the maximal value of. The equation is based on four different variables which, see my code below:\nimport pandas as pd\nimport numpy as np\nfrom scipy import optimize\n\ncols = {\n    'Dividend2': [9390, 7448, 177], \n    'Probability': [341, 376, 452], \n    'EV': [0.53, 0.60, 0.55], \n    'Dividend': [185, 55, 755], \n    'EV2': [123, 139, 544],\n}\n\ndf = pd.DataFrame(cols)\n\ndef myFunc(params):\n    \"\"\"myFunc metric.\"\"\"\n    (ev, bv, vc, dv) = params\n    df['Number'] = np.where(df['Dividend2'] <= vc, 1, 0) \\\n                    + np.where(df['EV2'] <= dv, 1, 0)\n    df['Return'] =  np.where(\n        df['EV'] <= ev, 0, np.where(\n            df['Probability'] >= bv, 0, df['Number'] * df['Dividend'] - (vc + dv)\n        )\n    )\n    return -1 * (df['Return'].sum())\n\nb1 = [(0.2,4), (300,600), (0,1000), (0,1000)]\nstart = [0.2, 600, 1000, 1000]\nresult = optimize.minimize(fun=myFunc, bounds=b1, x0=start)\nprint(result)\n\nSo I'd like to find the maximum value of the column Return in df when changing the variables ev,bv,vc & dv. I'd like them to be between in the intervals of ev: 0.2-4, bv: 300-600, vc: 0-1000 & dv: 0-1000.\nWhen running my code it seem like the function stops at x0.\n",
    "AcceptedAnswerId": 72252081,
    "AcceptedAnswer": "Solution\nI will use optuna library to give you a solution to the type of problem you are trying to solve. I have tried using scipy.optimize.minimize and it appears that the loss-landscape is probably quite flat in most places, and hence the tolerances enforce the minimizing algorithm (L-BFGS-B) to stop prematurely.\n\nOptuna Docs: https://optuna.readthedocs.io/en/stable/index.html\n\nWith optuna, it rather straight forward. Optuna only requires an objective function and a study. The study send various trials to the objective function, which in turn, evaluates the metric of your choice.\nI have defined another metric function myFunc2 by mostly removing the np.where calls, as you can do-away with them (reduces number of steps) and make the function slightly faster.\n# install optuna with pip\npip install -Uqq optuna\n\nAlthough I looked into using a rather smooth loss landscape, sometimes it is necessary to visualize the landscape itself. The answer in section B elaborates on visualization. But, what if you want to use a smoother metric function? Section D sheds some light on this.\nOrder of code-execution should be:\n\nSections: C >> B >> B.1 >> B.2 >> B.3 >> A.1 >> A.2 >> D\n\nA. Building Intuition\nIf you create a hiplot (also known as a plot with parallel-coordinates) with all the possible parameter values as mentioned in the search_space for Section B.2, and plot the lowest 50 outputs of myFunc2, it would look like this:\n\nPlotting all such points from the search_space would look like this:\n\nA.1. Loss Landscape Views for Various Parameter-Pairs\nThese figures show that mostly the loss-landscape is flat for any two of the four parameters (ev, bv, vc, dv). This could be a reason why, only GridSampler (which brute-forces the searching process) does better, compared to the other two samplers (TPESampler and RandomSampler). Please click on any of the images below to view them enlarged. This could also be the reason why scipy.optimize.minimize(method=\"L-BFGS-B\") fails right off the bat.\n\n\n\n\n  01. dv-vc\n  02. dv-bv\n  03. dv-ev\n\n\n\n\n  04. bv-ev\n  05. cv-ev\n  06. vc-bv\n\n\n\n\n# Create contour plots for parameter-pairs\nstudy_name = \"GridSampler\"\nstudy = studies.get(study_name)\n\nviews = [(\"dv\", \"vc\"), (\"dv\", \"bv\"), (\"dv\", \"ev\"), \n         (\"bv\", \"ev\"), (\"vc\", \"ev\"), (\"vc\", \"bv\")]\n\nfor i, (x, y) in enumerate(views):\n    print(f\"Figure: {i}/{len(views)}\")\n    study_contour_plot(study=study, params=(x, y))\n\nA.2. Parameter Importance\n\nstudy_name = \"GridSampler\"\nstudy = studies.get(study_name)\n\nfig = optuna.visualization.plot_param_importances(study)\nfig.update_layout(title=f'Hyperparameter Importances: {study.study_name}', \n                  autosize=False,\n                  width=800, height=500,\n                  margin=dict(l=65, r=50, b=65, t=90))\nfig.show()\n\nB. Code\nSection B.3. finds the lowest metric -88.333 for:\n\n{'ev': 0.2, 'bv': 500.0, 'vc': 222.2222, 'dv': 0.0}\n\nimport warnings\nfrom functools import partial\nfrom typing import Iterable, Optional, Callable, List\n\nimport pandas as pd\nimport numpy as np\nimport optuna\nfrom tqdm.notebook import tqdm\n\nwarnings.filterwarnings(\"ignore\", category=optuna.exceptions.ExperimentalWarning)\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\nPARAM_NAMES: List[str] = [\"ev\", \"bv\", \"vc\", \"dv\",]\nDEFAULT_METRIC_FUNC: Callable = myFunc2\n\n\ndef myFunc2(params):\n    \"\"\"myFunc metric v2 with lesser steps.\"\"\"\n    global df # define as a global variable\n    (ev, bv, vc, dv) = params\n    df['Number'] = (df['Dividend2'] <= vc) * 1 + (df['EV2'] <= dv) * 1\n    df['Return'] =  (\n        (df['EV'] > ev) \n        * (df['Probability'] < bv) \n        * (df['Number'] * df['Dividend'] - (vc + dv))\n    )\n    return -1 * (df['Return'].sum())\n\n\ndef make_param_grid(\n        bounds: List[Tuple[float, float]], \n        param_names: Optional[List[str]]=None, \n        num_points: int=10, \n        as_dict: bool=True,\n    ) -> Union[pd.DataFrame, Dict[str, List[float]]]:\n    \"\"\"\n    Create parameter search space.\n\n    Example:\n    \n        grid = make_param_grid(bounds=b1, num_points=10, as_dict=True)\n    \n    \"\"\"\n    if param_names is None:\n        param_names = PARAM_NAMES # [\"ev\", \"bv\", \"vc\", \"dv\"]\n    bounds = np.array(bounds)\n    grid = np.linspace(start=bounds[:,0], \n                       stop=bounds[:,1], \n                       num=num_points, \n                       endpoint=True, \n                       axis=0)\n    grid = pd.DataFrame(grid, columns=param_names)\n    if as_dict:\n        grid = grid.to_dict()\n        for k,v in grid.items():\n            grid.update({k: list(v.values())})\n    return grid\n\n\ndef objective(trial, \n              bounds: Optional[Iterable]=None, \n              func: Optional[Callable]=None, \n              param_names: Optional[List[str]]=None):\n    \"\"\"Objective function, necessary for optimizing with optuna.\"\"\"\n    if param_names is None:\n        param_names = PARAM_NAMES\n    if (bounds is None):\n        bounds = ((-10, 10) for _ in param_names)\n    if not isinstance(bounds, dict):\n        bounds = dict((p, (min(b), max(b))) \n                        for p, b in zip(param_names, bounds))\n    if func is None:\n        func = DEFAULT_METRIC_FUNC\n\n    params = dict(\n        (p, trial.suggest_float(p, bounds.get(p)[0], bounds.get(p)[1])) \n        for p in param_names        \n    )\n    # x = trial.suggest_float('x', -10, 10)\n    return func((params[p] for p in param_names))\n\n\ndef optimize(objective: Callable, \n             sampler: Optional[optuna.samplers.BaseSampler]=None, \n             func: Optional[Callable]=None, \n             n_trials: int=2, \n             study_direction: str=\"minimize\",\n             study_name: Optional[str]=None,\n             formatstr: str=\".4f\",\n             verbose: bool=True):\n    \"\"\"Optimizing function using optuna: creates a study.\"\"\"\n    if func is None:\n        func = DEFAULT_METRIC_FUNC\n    study = optuna.create_study(\n        direction=study_direction, \n        sampler=sampler, \n        study_name=study_name)\n    study.optimize(\n        objective, \n        n_trials=n_trials, \n        show_progress_bar=True, \n        n_jobs=1,\n    )\n    if verbose:\n        metric = eval_metric(study.best_params, func=myFunc2)\n        msg = format_result(study.best_params, metric, \n                            header=study.study_name, \n                            format=formatstr)\n        print(msg)\n    return study\n\n\ndef format_dict(d: Dict[str, float], format: str=\".4f\") -> Dict[str, float]:\n    \"\"\"\n    Returns formatted output for a dictionary with \n    string keys and float values.\n    \"\"\"\n    return dict((k, float(f'{v:{format}}')) for k,v in d.items())\n\n\ndef format_result(d: Dict[str, float], \n                  metric_value: float, \n                  header: str='', \n                  format: str=\".4f\"): \n    \"\"\"Returns formatted result.\"\"\"\n    msg = f\"\"\"Study Name: {header}\\n{'='*30}\n    \n    \u2705 study.best_params: \\n\\t{format_dict(d)}\n    \u2705 metric: {metric_value} \n    \"\"\"\n    return msg\n\n\ndef study_contour_plot(study: optuna.Study, \n                       params: Optional[List[str]]=None, \n                       width: int=560, \n                       height: int=500):\n    \"\"\"\n    Create contour plots for a study, given a list or \n    tuple of two parameter names.\n    \"\"\"\n    if params is None:\n        params = [\"dv\", \"vc\"]\n    fig = optuna.visualization.plot_contour(study, params=params)\n    fig.update_layout(\n        title=f'Contour Plot: {study.study_name} ({params[0]}, {params[1]})', \n        autosize=False,\n        width=width, \n        height=height,\n        margin=dict(l=65, r=50, b=65, t=90))\n    fig.show()\n\n\nbounds = [(0.2, 4), (300, 600), (0, 1000), (0, 1000)]\nparam_names = PARAM_NAMES # [\"ev\", \"bv\", \"vc\", \"dv\",]\npobjective = partial(objective, bounds=bounds)\n\n# Create an empty dict to contain \n# various subsequent studies.\nstudies = dict()\n\nOptuna comes with a few different types of Samplers. Samplers provide the strategy of how optuna is going to sample points from the parametr-space and evaluate the objective function.\n\nhttps://optuna.readthedocs.io/en/stable/reference/samplers.html\n\nB.1 Use TPESampler\nfrom optuna.samplers import TPESampler\n\nsampler = TPESampler(seed=42)\n\nstudy_name = \"TPESampler\"\nstudies[study_name] = optimize(\n    pobjective, \n    sampler=sampler, \n    n_trials=100, \n    study_name=study_name,\n)\n\n# Study Name: TPESampler\n# ==============================\n#    \n#     \u2705 study.best_params: \n#   {'ev': 1.6233, 'bv': 585.2143, 'vc': 731.9939, 'dv': 598.6585}\n#     \u2705 metric: -0.0 \n\nB.2. Use GridSampler\nGridSampler requires a parameter search grid. Here we are using the following search_space.\n\nfrom optuna.samplers import GridSampler\n\n# create search-space\nsearch_space = make_param_grid(bounds=bounds, num_points=10, as_dict=True)\n\nsampler = GridSampler(search_space)\n\nstudy_name = \"GridSampler\"\nstudies[study_name] = optimize(\n    pobjective, \n    sampler=sampler, \n    n_trials=2000, \n    study_name=study_name,\n)\n\n# Study Name: GridSampler\n# ==============================\n#    \n#     \u2705 study.best_params: \n#   {'ev': 0.2, 'bv': 500.0, 'vc': 222.2222, 'dv': 0.0}\n#     \u2705 metric: -88.33333333333337 \n\nB.3. Use RandomSampler\nfrom optuna.samplers import RandomSampler\n\nsampler = RandomSampler(seed=42)\n\nstudy_name = \"RandomSampler\"\nstudies[study_name] = optimize(\n    pobjective, \n    sampler=sampler, \n    n_trials=300, \n    study_name=study_name,\n)\n\n# Study Name: RandomSampler\n# ==============================\n#    \n#     \u2705 study.best_params: \n#   {'ev': 1.6233, 'bv': 585.2143, 'vc': 731.9939, 'dv': 598.6585}\n#     \u2705 metric: -0.0 \n\nC. Dummy Data\nFor the sake of reproducibility, I am keeping a record of the dummy data used here.\nimport pandas as pd\nimport numpy as np\nfrom scipy import optimize\n\ncols = {\n    'Dividend2': [9390, 7448, 177], \n    'Probability': [341, 376, 452], \n    'EV': [0.53, 0.60, 0.55], \n    'Dividend': [185, 55, 755], \n    'EV2': [123, 139, 544],\n}\n\ndf = pd.DataFrame(cols)\n\ndef myFunc(params):\n    \"\"\"myFunc metric.\"\"\"\n    (ev, bv, vc, dv) = params\n    df['Number'] = np.where(df['Dividend2'] <= vc, 1, 0) \\\n                    + np.where(df['EV2'] <= dv, 1, 0)\n    df['Return'] =  np.where(\n        df['EV'] <= ev, 0, np.where(\n            df['Probability'] >= bv, 0, df['Number'] * df['Dividend'] - (vc + dv)\n        )\n    )\n    return -1 * (df['Return'].sum())\n\nb1 = [(0.2,4), (300,600), (0,1000), (0,1000)]\nstart = [0.2, 600, 1000, 1000]\nresult = optimize.minimize(fun=myFunc, bounds=b1, x0=start)\nprint(result)\n\nC.1. An Observation\nSo, it seems at first glance that the code executed properly and did not throw any error. It says it had success in finding the minimized solution.\n      fun: -0.0\n hess_inv: \n      jac: array([0., 0., 3., 3.])\n  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL' # \ud83d\udca1\n     nfev: 35\n      nit: 2\n   status: 0\n  success: True\n        x: array([2.e-01, 6.e+02, 0.e+00, 0.e+00]) # \ud83d\udd25\n\nA close observation reveals that the solution (see \ud83d\udd25) is no different from the starting point [0.2, 600, 1000, 1000]. So, seems like nothing really happened and the algorithm just finished prematurely?!!\nNow look at the message above (see \ud83d\udca1). If we run a google search on this, you could find something like this:\n\nSummary\n\nb'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_\nIf the loss-landscape does not have a smoothely changing topography, the gradient descent algorithms will soon find that from one iteration to the next, there isn't much change happening and hence, will terminate further seeking. Also, if the loss-landscape is rather flat, this could see similar fate and get early-termination.\n\n\nscipy-optimize-minimize does not perform the optimization - CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_\n\n\n\nD. Making the Loss Landscape Smoother\nA binary evaluation of value = 1 if x>5 else 0 is essentially a step-function that assigns 1 for all values of x that are greater than 5 and 0 otherwise. But this introduces a kink - a discontinuity in smoothness and this could potentially introduce problems in traversing the loss-landscape.\nWhat if we use a sigmoid function to introduce some smoothness?\n\n\n\n\n# Define sigmoid function\ndef sigmoid(x):\n    \"\"\"Sigmoid function.\"\"\"\n    return 1 / (1 + np.exp(-x))\n\nFor the above example, we could modify it as follows.\n\n\n\n\nYou can additionally introduce another factor (gamma: \u03b3) as follows and try to optimize it to make the landscape smoother. Thus by controlling the gamma factor, you could make the function smoother and change how quickly it changes around x = 5\n\n\n\n\n\nThe above figure is created with the following code-snippet.\nimport matplotlib.pyplot as plt\n\n%matplotlib inline \n%config InlineBackend.figure_format = 'svg' # 'svg', 'retina' \nplt.style.use('seaborn-white')\n\ndef make_figure(figtitle: str=\"Sigmoid Function\"):\n    \"\"\"Make the demo figure for using sigmoid.\"\"\"\n\n    x = np.arange(-20, 20.01, 0.01)\n    y1 = sigmoid(x)\n    y2 = sigmoid(x - 5)\n    y3 = sigmoid((x - 5)/3)\n    y4 = sigmoid((x - 5)/0.3)\n    fig, ax = plt.subplots(figsize=(10,5))\n    plt.sca(ax)\n    plt.plot(x, y1, ls=\"-\", label=\"$\\sigma(x)$\")\n    plt.plot(x, y2, ls=\"--\", label=\"$\\sigma(x - 5)$\")\n    plt.plot(x, y3, ls=\"-.\", label=\"$\\sigma((x - 5) / 3)$\")\n    plt.plot(x, y4, ls=\":\", label=\"$\\sigma((x - 5) / 0.3)$\")\n    plt.axvline(x=0, ls=\"-\", lw=1.3, color=\"cyan\", alpha=0.9)\n    plt.axvline(x=5, ls=\"-\", lw=1.3, color=\"magenta\", alpha=0.9)\n    plt.legend()\n    plt.title(figtitle)\n    plt.show()\n\nmake_figure()\n\nD.1. Example of Metric Smoothing\nThe following is an example of how you could apply function smoothing.\nfrom functools import partial\n\ndef sig(x, gamma: float=1.):\n    return sigmoid(x/gamma)\n\ndef myFunc3(params, gamma: float=0.5):\n    \"\"\"myFunc metric v3 with smoother metric.\"\"\"\n    (ev, bv, vc, dv) = params\n    _sig = partial(sig, gamma=gamma)\n    df['Number'] = _sig(x = -(df['Dividend2'] - vc)) * 1 \\\n                    + _sig(x = -(df['EV2'] - dv)) * 1\n    df['Return'] = (\n        _sig(x = df['EV'] - ev) \n        * _sig(x = -(df['Probability'] - bv))\n        * _sig(x = df['Number'] * df['Dividend'] - (vc + dv))\n    )\n    return -1 * (df['Return'].sum())\n\n"
}
{
    "Id": 71814658,
    "PostTypeId": 1,
    "Title": "Python typing: Does TypedDict allow additional / extra keys?",
    "Body": "Does typing.TypedDict allow extra keys? Does a value pass the typechecker, if it has keys which are not present on the definition of the TypedDict?\n",
    "AcceptedAnswerId": 71814659,
    "AcceptedAnswer": "It depends.\nPEP-589, the specification of TypedDict, explicitely forbids extra keys:\n\nExtra keys included in TypedDict object construction should also be caught. In this example, the director key is not defined in Movie and is expected to generate an error from a type checker:\nm: Movie = dict(\n      name='Alien',\n      year=1979,\n      director='Ridley Scott')  # error: Unexpected key 'director'\n\n\n[highlighting by me]\nThe typecheckers mypy, pyre, pyright implement this according to the specification.\nHowever, it is possible that a value with extra keys is accepted. This is because subtyping of TypedDicts is allowed, and the subtype might implement the extra key. PEP-589 only forbids extra keys in object construction, i.e. in literal assignment. As any value that complies with a subtype is always deemed to comply with the parent type and can be upcasted from the subtype to the parent type, an extra key can be introduced through a subtype:\nfrom typing import TypedDict\n\nclass Movie(TypedDict):\n    name: str\n    year: int\n\nclass MovieWithDirector(Movie):\n    director: str\n\n\n# This is illegal:\nmovie: Movie = {\n    'name': 'Ash is purest white', \n    'year': 2018, \n    'director': 'Jia Zhangke',\n}    \n\n# This is legal:\nmovie_with_director: MovieWithDirector = {\n    'name': 'Ash is purest white', \n    'year': 2018, \n    'director': 'Jia Zhangke',\n}\n\n# This is legal, MovieWithDirector is a subtype of Movie\nmovie: Movie = movie_with_director  \n\nIn the example above, we see that the same value can sometimes be considered complying with Movie by the typing system, and sometimes not.\nAs a consequence of subtyping, typing a parameter as a certain TypedDict is not a safeguard against extra keys, because they could have been introduced through a subtype.\nIf your code is sensitive with regard to the presence of extra keys (for instance, if it makes use of param.keys(), param.values() or len(param) on the TypedDict parameter param), this could lead to problems when extra keys are present. A solution to this problem is to either handle the exceptional case that extra keys are actually present on the parameter or to make your code insensitive against extra keys.\nIf you want to test that your code is robust against extra keys, you cannot simply add a key in the test value:\ndef some_movie_function(movie: Movie):\n    # ...\n\ndef test_some_movie_function():\n    # this will not be accepted by the type checker:\n    result = some_movie_function({\n        'name': 'Ash is purest white', \n        'year': 2018, \n        'director': 'Jia Zhangke',\n        'genre': 'drama',\n    })    \n\nWorkarounds are to either make the type checkers ignore the line or to create a subtype for your test, introducing the extra keys only for your test:\nclass ExtendedMovie(Movie):\n     director: str\n     genre: str\n\n\ndef test_some_movie_function():\n    extended_movie: ExtendedMovie = {\n        'name': 'Ash is purest white', \n        'year': 2018, \n        'director': 'Jia Zhangke',\n        'genre': 'drama',\n    }\n\n    result = some_movie_function(test_some_movie_function)\n    # run assertions against result\n} \n\n"
}
{
    "Id": 72373093,
    "PostTypeId": 1,
    "Title": "How to define \"python_requires\" in pyproject.toml using setuptools?",
    "Body": "Setuptools allows you to specify the minimum python version as such:\nfrom setuptools import setup\n\n[...]\n\nsetup(name=\"my_package_name\",\n      python_requires='>3.5.2',\n      [...]\n\n\nHowever, how can you do this with the pyproject.toml? The following two things did NOT work:\n[project]\n...\n# ERROR: invalid key \npython_requires = \">=3\"\n\n# ERROR: no matching distribution found\ndependencies = [\"python>=3\"]\n\n",
    "AcceptedAnswerId": 72462723,
    "AcceptedAnswer": "According to PEP 621, the equivalent field in the [project] table is requires-python.\nMore information about the list of valid configuration fields can be found in: https://packaging.python.org/en/latest/specifications/declaring-project-metadata/.\nThe equivalent pyproject.toml of your example would be:\n[project]\nname = \"my_package_name\"\nrequires-python = \">3.5.2\"\n...\n\n"
}
{
    "Id": 73062386,
    "PostTypeId": 1,
    "Title": "Adding single integer to numpy array faster if single integer has python-native int type",
    "Body": "I add a single integer to an array of integers with 1000 elements. This is faster by 25% when I first cast the single integer from numpy.int64 to the python-native int.\nWhy? Should I, as a general rule of thumb convert the single number to native python formats for single-number-to-array operations with arrays of about this size?\nNote: may be related to my previous question Conjugating a complex number much faster if number has python-native complex type.\nimport numpy as np\n\nnnu = 10418\nnnu_use = 5210\na = np.random.randint(nnu,size=1000)\nb = np.random.randint(nnu_use,size=1)[0]\n\n%timeit a + b                            # --> 3.9 \u00b5s \u00b1 19.9 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n%timeit a + int(b)                       # --> 2.87 \u00b5s \u00b1 8.07 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\n\nNote that the speed-up can be enormous (factor 50) for scalar-to-scalar-operations as well, as seen below:\nnp.random.seed(100)\n\na = (np.random.rand(1))[0]\na_native = float(a)\nb = complex(np.random.rand(1)+1j*np.random.rand(1))\nc = (np.random.rand(1)+1j*np.random.rand(1))[0]\nc_native = complex(c)\n\n%timeit a * (b - b.conjugate() * c)                # 6.48 \u00b5s \u00b1 49.7 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n%timeit a_native * (b - b.conjugate() * c_native)  # 283 ns \u00b1 7.78 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n%timeit a * b                                      # 5.07 \u00b5s \u00b1 17.7 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n%timeit a_native * b                               # 94.5 ns \u00b1 0.868 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\n\n\nUpdate: Could it be that the latest numpy release fixes the speed difference? The release notes of numpy 1.23 mention that scalar operations are now much faster, see https://numpy.org/devdocs/release/1.23.0-notes.html#performance-improvements-and-changes and https://github.com/numpy/numpy/pull/21188. I am using python 3.7.6, numpy 1.21.2.\n",
    "AcceptedAnswerId": 73070306,
    "AcceptedAnswer": "On my Windows PC with CPython 3.8.1, I get:\n[Old] Numpy 1.22.4:\n - First test: 1.65 \u00b5s VS 1.43 \u00b5s\n - Second:     2.03 \u00b5s VS 0.17 \u00b5s\n\n[New] Numpy 1.23.1:\n - First test: 1.38 \u00b5s VS 1.24 \u00b5s    <----  A bit better than Numpy 1.22.4\n - Second:     0.38 \u00b5s VS 0.17 \u00b5s    <----  Much better than Numpy 1.22.4\n\n\nWhile the new version of Numpy gives a good boost, native type should always be faster than Numpy ones with the (default) CPython interpreter. Indeed, the interpreter needs to call C function of Numpy. This is not needed with native types. Additionally, the Numpy checks and wrapping is not optimal but Numpy is not designed for fast scalar computation in the first place (though the overhead was previously not reasonable). In fact, scalar computations are very inefficient and the interpreter prevent any fast execution.\nIf you plan to do many scalar operation you need to use a natively compiled code, possibly using Cython, Numba, or even a raw C/C++ module. Note that Cython do not optimize/inline Numpy calls but can operate on native types faster. A native code can do this certainly in one or even two order of magnitude less time.\nNote that in the first case, the path in Numpy functions is not the same and Numpy does additional check that are a bit more expensive then the value is not a CPython object. Still, it should be a constant overhead (and now relatively small). Otherwise, it would be a bug (and should be reported).\nRelated: Why is np.sum(range(N)) very slow?\n"
}
{
    "Id": 72258087,
    "PostTypeId": 1,
    "Title": "unexpected keyword argument 'tenant_id' while accessing Azure Key Vault in Python",
    "Body": "I was trying to accessing my key vault, but I got always the same error:\nAppServiceCredential.get_token failed: request() got an unexpected keyword argument 'tenant_id'\nManagedIdentityCredential.get_token failed: request() got an unexpected keyword argument 'tenant_id'\n\nThis was the code I used in an Azure Machine Learning notebook, copied from the docs:\nfrom azure.identity import ManagedIdentityCredential\nfrom azure.keyvault.secrets import SecretClient\n\ncredential = ManagedIdentityCredential()\nsecret_client = SecretClient(vault_url=\"https://XXXX.vault.azure.net/\", credential=credential)\n\nsecretName = 'test'\nretrieved_secret = secret_client.get_secret(secretName) # here's the error\nretrieved_secret\n\nWhat is wrong? Could you help me?\nThank you in advance.\n",
    "AcceptedAnswerId": 72262694,
    "AcceptedAnswer": "This error is because of a bug that has since been fixed in azure-identity's ManagedIdentityCredential. Key Vault clients in recent packages include a tenant ID in token requests to support cross-tenant authentication, but some azure-identity credentials didn't correctly handle this keyword argument until the bug was fixed in version 1.8.0. Installing azure-identity>=1.8.0 should fix the error you're getting.\n(Disclaimer: I work for the Azure SDK for Python)\n"
}
{
    "Id": 73136808,
    "PostTypeId": 1,
    "Title": "AWS Glue error - Invalid input provided while running python shell program",
    "Body": "I have Glue job, a python shell code. When I try to run it I end up getting the below error.\nJob Name : xxxxx Job Run Id : yyyyyy failed to execute with exception Internal service error : Invalid input provided\nIt is not specific to code, even if I just put\nimport boto3\nprint('loaded')\n\nI am getting the error right after clicking the run job option. What is the issue here?\n",
    "AcceptedAnswerId": 73162640,
    "AcceptedAnswer": "I think Quatermass is right, the jobs started working out of the blue the next day without any changes.\n"
}
{
    "Id": 72571235,
    "PostTypeId": 1,
    "Title": "Can I install node.js 18 on Centos 7 and do I need python 3 install too?",
    "Body": "I'm not sure if node.js 18 supports centos 7 and is it a requirement to install python 3 for node.js 18?\n",
    "AcceptedAnswerId": 72571789,
    "AcceptedAnswer": "Step 1 - curl --silent --location https://rpm.nodesource.com/setup_18.x | sudo bash -\nStep 2 - sudo yum -y install nodejs\nI don't think you need Python 3.\nReference - https://computingforgeeks.com/install-node-js-on-centos-rhel-rocky-linux/\n"
}
{
    "Id": 72059380,
    "PostTypeId": 1,
    "Title": "Python fuctional style iterative algoritm?",
    "Body": "In Haskell there is a simple list function available\niterate :: (a -> a) -> a -> [a]\niterate f x =  x : iterate f (f x)\n\nIn python it could be implemented as following:\ndef iterate(f, init):\n  while True:\n    yield init\n    init = f(init)\n\nI was kinda surprised that something basic like this is not part of the functools/itertools modules. Could it be simply costructed in functional style (i.e. without the loop) using the tools provided in these libraries? (Mostly code golf, trying to learn about functional style in Python.)\n",
    "AcceptedAnswerId": 72059725,
    "AcceptedAnswer": "You can do it using some of the functions in itertools:\nfrom itertools import accumulate, repeat\n\ndef iterate(func, initial):\n    return accumulate(repeat(None), func=lambda tot, _: func(tot), initial=initial)\n\nAlthough it's clearly not very clean. Itertools is missing some fundamental functions for constructing streams, like unfoldr. Most of the itertools functions could be defined in terms of unfoldr, as it happens, but functional programming is a little uncomfortable in Python anyways so that might not be much of a benefit.\n"
}
{
    "Id": 71861779,
    "PostTypeId": 1,
    "Title": "MWAA - Airflow - PythonVirtualenvOperator requires virtualenv",
    "Body": "I am using AWS's MWAA service (2.2.2) to run a variety of DAGs, most of which are implemented with standard PythonOperator types. I bundle the DAGs into an S3 bucket alongside any shared requirements, then point MWAA to the relevant objects & versions. Everything runs smoothly so far.\nI would now like to implement a DAG using the PythonVirtualenvOperator type, which AWS acknowledge is not supported out of the box. I am following their guide on how to patch the behaviour using a custom plugin, but continue to receive an error from Airflow, shown at the top of the dashboard in big red writing:\n\nDAG Import Errors (1)\n... ...\nAirflowException: PythonVirtualenvOperator requires virtualenv, please install it.\n\nI've confirmed that the plugin is indeed being picked up by Airflow (I see it referenced in the admin screen), and for the avoidance of doubt I am using the exact code provided by AWS in their examples for the DAG. AWS's documentation on this is pretty light and I've yet to stumble across any community discussion for the same.\nFrom AWS's docs, we'd expect the plugin to run at startup prior to any DAGs being processed. The plugin itself appears to effectively rewrite the venv command to use the pip-installed version, rather than that which is installed on the machine, however I've struggled to verify that things are happening in the order I expect. Any pointers on debugging the instance's behavior would be very much appreciated.\nHas anyone faced a similar issue? Is there a gap in the MWAA documentation that needs addressing? Am I missing something incredibly obvious?\nPossibly related, but I do see this warning in the scheduler's logs, which may indicate why MWAA is struggling to resolve the dependency?\n\nWARNING: The script virtualenv is installed in '/usr/local/airflow/.local/bin' which is not on PATH.\n\n",
    "AcceptedAnswerId": 72203130,
    "AcceptedAnswer": "Airflow uses shutil.which to look for virtualenv. The installed virtualenv via requirements.txt isn't on the PATH. Adding the path to virtualenv to PATH solves this.\nThe doc here is wrong https://docs.aws.amazon.com/mwaa/latest/userguide/samples-virtualenv.html\nimport os\nfrom airflow.plugins_manager import AirflowPlugin\nimport airflow.utils.python_virtualenv \nfrom typing import List\ndef _generate_virtualenv_cmd(tmp_dir: str, python_bin: str, system_site_packages: bool) -> List[str]:\n    cmd = ['python3','/usr/local/airflow/.local/lib/python3.7/site-packages/virtualenv', tmp_dir]\n    if system_site_packages:\n        cmd.append('--system-site-packages')\n    if python_bin is not None:\n        cmd.append(f'--python={python_bin}')\n    return cmd\nairflow.utils.python_virtualenv._generate_virtualenv_cmd=_generate_virtualenv_cmd\n#This is the added path code\nos.environ[\"PATH\"] = f\"/usr/local/airflow/.local/bin:{os.environ['PATH']}\"\nclass VirtualPythonPlugin(AirflowPlugin):                \n    name = 'virtual_python_plugin'\n\n"
}
{
    "Id": 73457379,
    "PostTypeId": 1,
    "Title": "Python regex and leading 0 in capturing group",
    "Body": "I'm writing a script in python 3 to automatically rename files. But I have a problem with the captured group in a regex.\nI have these kinds of files :\ntest tome 01 something.cbz\ntest tome 2 something.cbz\ntest tome 20 something.cbz\n\nAnd I would like to have :\ntest 001 something.cbz\ntest 002 something.cbz\ntest 020 something.cbz\n\nI tried several bits of code:\nExample 1:\nname = re.sub('tome [0]{0,1}(\\d{1,})', str('\\\\1').zfill(3), name)\n\nThe result is:\ntest 01 something.cbz\ntest 02 something.cbz\ntest 020 something.cbz\n\nExample 2:\nname = re.sub('tome (\\d{1,})', str('\\\\1').lstrip(\"0\").zfill(3), name)\n\nThe result is:\ntest 001 something.cbz\ntest 02 something.cbz\ntest 020 something.cbz\n\n",
    "AcceptedAnswerId": 73457472,
    "AcceptedAnswer": "You can run the zfill(3) on the .group(1) value after stripping the zeroes from the left side:\nimport re\n\ns = (\"test tome 01 something.cbz\\n\"\n            \"test tome 2 something.cbz\\n\"\n            \"test tome 20 something.cbz\")\n\nresult = re.sub(\n    r'tome (\\d+)',\n    lambda x: x.group(1).lstrip(\"0\").zfill(3),\n    s\n)\nprint(result)\n\nOutput\ntest 001 something.cbz\ntest 002 something.cbz\ntest 020 something.cbz\n\n"
}
{
    "Id": 72604922,
    "PostTypeId": 1,
    "Title": "How to convert Python dataclass to dictionary of string literal?",
    "Body": "Given a dataclass like below:\nclass MessageHeader(BaseModel):\n    message_id: uuid.UUID\n\n    def dict(self, **kwargs):\n        return json.loads(self.json())\n\nI would like to get a dictionary of string literal when I call dict on MessageHeader\nThe desired outcome of dictionary is like below:\n{'message_id': '383b0bfc-743e-4738-8361-27e6a0753b5a'}\n\nI want to avoid using 3rd party library like pydantic & I do not want to use json.loads(self.json()) as there are extra round trips\nIs there any better way to convert a dataclass to a dictionary with string literal like above?\n",
    "AcceptedAnswerId": 72605423,
    "AcceptedAnswer": "You can use dataclasses.asdict:\nfrom dataclasses import dataclass, asdict\n\nclass MessageHeader(BaseModel):\n    message_id: uuid.UUID\n\n    def dict(self):\n        return {k: str(v) for k, v in asdict(self).items()}\n\nIf you're sure that your class only has string values, you can skip the dictionary comprehension entirely:\nclass MessageHeader(BaseModel):\n    message_id: uuid.UUID\n\n    dict = asdict\n\n"
}
{
    "Id": 73206810,
    "PostTypeId": 1,
    "Title": "Faker Python generating chinese/pinyin names",
    "Body": "I am trying to generate random chinese names using Faker (Python), but it generates the names in chinese characters instead of pinyin.\nI found this :\n\nand it show that it generates them in pinyin, while when I try the same code, it gives me only chinese characters.\nhow to get the pinyin ??\n",
    "AcceptedAnswerId": 73206894,
    "AcceptedAnswer": "fake.romanized_name() worked for me.\nI got lucky by looking through dir(fake). Doesn't seem to have a method for pinyin address that I can see...\n"
}
{
    "Id": 73166250,
    "PostTypeId": 1,
    "Title": "Why does a recursive Python program not crash my system?",
    "Body": "I've written an R.py script which contains the following two lines:\nimport os\n\nos.system(\"python3 R.py\")\n\nI expected my system to run out of memory after running this script for a few minutes, but it is still surprisingly responsive. Does someone know, what kind of Python interpreter magic is happening here?\n",
    "AcceptedAnswerId": 73216511,
    "AcceptedAnswer": "Preface\nos.system() is actually a call to C\u2019s system().\nHere is what the documentation states:\n\nThe system() function shall behave as if a child process were created\nusing fork(), and the child process invoked the sh utility using\nexecl() as follows:\nexecl(, \"sh\", \"-c\", command, (char *)0);\nwhere  is an unspecified pathname for the sh utility. It\nis unspecified whether the handlers registered with pthread_atfork()\nare called as part of the creation of the child process.\nThe system() function shall ignore the SIGINT and SIGQUIT signals, and\nshall block the SIGCHLD signal, while waiting for the command to\nterminate. If this might cause the application to miss a signal that\nwould have killed it, then the application should examine the return\nvalue from system() and take whatever action is appropriate to the\napplication if the command terminated due to receipt of a signal.\nThe system() function shall not affect the termination status of any\nchild of the calling processes other than the process or processes it\nitself creates.\nThe system() function shall not return until the child process has\nterminated. [Option End]\nThe system() function need not be thread-safe.\n\nSolution\nsystem() creates a child process and exits, there is no stack to be resolved, therefore one would expect this to run as long as resources to do so are available. Furthermore, the operation being of creating a child process is not an intensive one\u2014 the processes aren't using up much resources, but if allowed to run long enough the script will to start to affect general performance and eventually run out of memory to spawn a new child process. Once this occurs the processes will exit.\nExample\nTo demonstrate this, set recursion depth limit to 10 and allow the program to run:\nimport os, sys, inspect\n\nsys.setrecursionlimit(10)\n\nargs = sys.argv[1:]\narg = int(args[0]) if len(args) else 0\n\nstack_depth = len(inspect.stack(0))\n\nprint(f\"Iteration {arg} - at stack depth of {stack_depth}\")\n\narg += 1\n\nos.system(f\"python3 main.py {arg}\")\n\n\nOutputs:\nIteration 0 - at stack depth of 1 - avaialable memory 43337904128 \nIteration 1 - at stack depth of 1 - avaialable memory 43370692608 \nIteration 2 - at stack depth of 1 - avaialable memory 43358756864 \nIteration 3 - at stack depth of 1 - avaialable memory 43339202560 \nIteration 4 - at stack depth of 1 - avaialable memory 43354894336 \nIteration 5 - at stack depth of 1 - avaialable memory 43314974720 \nIteration 6 - at stack depth of 1 - avaialable memory 43232366592 \nIteration 7 - at stack depth of 1 - avaialable memory 43188719616 \nIteration 8 - at stack depth of 1 - avaialable memory 43173384192 \nIteration 9 - at stack depth of 1 - avaialable memory 43286093824 \nIteration 10 - at stack depth of 1 - avaialable memory 43288162304\nIteration 11 - at stack depth of 1 - avaialable memory 43310637056\nIteration 12 - at stack depth of 1 - avaialable memory 43302408192\nIteration 13 - at stack depth of 1 - avaialable memory 43295440896\nIteration 14 - at stack depth of 1 - avaialable memory 43303870464\nIteration 15 - at stack depth of 1 - avaialable memory 43303870464\nIteration 16 - at stack depth of 1 - avaialable memory 43296256000\nIteration 17 - at stack depth of 1 - avaialable memory 43286032384\nIteration 18 - at stack depth of 1 - avaialable memory 43246657536\nIteration 19 - at stack depth of 1 - avaialable memory 43213336576\nIteration 20 - at stack depth of 1 - avaialable memory 43190259712\nIteration 21 - at stack depth of 1 - avaialable memory 43133902848\nIteration 22 - at stack depth of 1 - avaialable memory 43027984384\nIteration 23 - at stack depth of 1 - avaialable memory 43006255104\n...\n\nhttps://replit.com/@pygeek1/os-system-recursion#main.py\nReferences\nhttps://pubs.opengroup.org/onlinepubs/9699919799/functions/system.html\n"
}
{
    "Id": 73699500,
    "PostTypeId": 1,
    "Title": "python-polars split string column into many columns by delimiter",
    "Body": "In pandas, the following code will split the string from col1 into many columns. is there a way to do this in polars?\nd = {'col1': [\"a/b/c/d\", \"a/b/c/d\"]}\ndf= pd.DataFrame(data=d)\ndf[[\"a\",\"b\",\"c\",\"d\"]]=df[\"col1\"].str.split('/',expand=True)\n\n",
    "AcceptedAnswerId": 73703650,
    "AcceptedAnswer": "Here's an algorithm that will automatically adjust for the required number of columns -- and should be quite performant.\nLet's start with this data.  Notice that I've purposely added the empty string \"\" and a null value - to show how the algorithm handles these values.  Also, the number of split strings varies widely.\nimport polars as pl\ndf = pl.DataFrame(\n    {\n        \"my_str\": [\"cat\", \"cat/dog\", None, \"\", \"cat/dog/aardvark/mouse/frog\"],\n    }\n)\ndf\n\nshape: (5, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 my_str                      \u2502\n\u2502 ---                         \u2502\n\u2502 str                         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 cat                         \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 cat/dog                     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 null                        \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502                             \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 cat/dog/aardvark/mouse/frog \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nThe Algorithm\nThe algorithm below may be a bit more than you need, but you can edit/delete/add as you need.\n(\n    df\n    .with_row_count('id')\n    .with_column(pl.col(\"my_str\").str.split(\"/\").alias(\"split_str\"))\n    .explode(\"split_str\")\n    .with_column(\n        (\"string_\" + pl.arange(0, pl.count()).cast(pl.Utf8).str.zfill(2))\n        .over(\"id\")\n        .alias(\"col_nm\")\n    )\n    .pivot(\n        index=['id', 'my_str'],\n        values='split_str',\n        columns='col_nm',\n    )\n    .with_column(\n        pl.col('^string_.*$').fill_null(\"\")\n    )\n)\n\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 my_str                      \u2506 string_00 \u2506 string_01 \u2506 string_02 \u2506 string_03 \u2506 string_04 \u2502\n\u2502 --- \u2506 ---                         \u2506 ---       \u2506 ---       \u2506 ---       \u2506 ---       \u2506 ---       \u2502\n\u2502 u32 \u2506 str                         \u2506 str       \u2506 str       \u2506 str       \u2506 str       \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 cat                         \u2506 cat       \u2506           \u2506           \u2506           \u2506           \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 cat/dog                     \u2506 cat       \u2506 dog       \u2506           \u2506           \u2506           \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 null                        \u2506           \u2506           \u2506           \u2506           \u2506           \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3   \u2506                             \u2506           \u2506           \u2506           \u2506           \u2506           \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 cat       \u2506 dog       \u2506 aardvark  \u2506 mouse     \u2506 frog      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\nHow it works\nWe first assign a row number id (which we'll need later), and use split to separate the strings.  Note that the split strings form a list.\n(\n    df\n    .with_row_count('id')\n    .with_column(pl.col(\"my_str\").str.split(\"/\").alias(\"split_str\"))\n)\n\nshape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 my_str                      \u2506 split_str                  \u2502\n\u2502 --- \u2506 ---                         \u2506 ---                        \u2502\n\u2502 u32 \u2506 str                         \u2506 list[str]                  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 cat                         \u2506 [\"cat\"]                    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 cat/dog                     \u2506 [\"cat\", \"dog\"]             \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 null                        \u2506 null                       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3   \u2506                             \u2506 [\"\"]                       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 [\"cat\", \"dog\", ... \"frog\"] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nNext, we'll use explode to put each string on its own row.  (Notice how the id column tracks the original row that each string came from.)\n(\n    df\n    .with_row_count('id')\n    .with_column(pl.col(\"my_str\").str.split(\"/\").alias(\"split_str\"))\n    .explode(\"split_str\")\n)\n\nshape: (10, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 my_str                      \u2506 split_str \u2502\n\u2502 --- \u2506 ---                         \u2506 ---       \u2502\n\u2502 u32 \u2506 str                         \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 cat                         \u2506 cat       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 cat/dog                     \u2506 cat       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 cat/dog                     \u2506 dog       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 null                        \u2506 null      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3   \u2506                             \u2506           \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 cat       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 dog       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 aardvark  \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 mouse     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 frog      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nIn the next step, we're going to generate our column names.  I chose to call each column string_XX where XX is the offset with regards to the original string.\nI've used the handy zfill expression so that 1 becomes 01.  (This makes sure that string_02 comes before string_10 if you decide to sort your columns later.)\nYou can substitute your own naming in this step as you need.\n(\n    df\n    .with_row_count('id')\n    .with_column(pl.col(\"my_str\").str.split(\"/\").alias(\"split_str\"))\n    .explode(\"split_str\")\n    .with_column(\n        (\"string_\" + pl.arange(0, pl.count()).cast(pl.Utf8).str.zfill(2))\n        .over(\"id\")\n        .alias(\"col_nm\")\n    )\n)\n\nshape: (10, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 my_str                      \u2506 split_str \u2506 col_nm    \u2502\n\u2502 --- \u2506 ---                         \u2506 ---       \u2506 ---       \u2502\n\u2502 u32 \u2506 str                         \u2506 str       \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 cat                         \u2506 cat       \u2506 string_00 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 cat/dog                     \u2506 cat       \u2506 string_00 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 cat/dog                     \u2506 dog       \u2506 string_01 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 null                        \u2506 null      \u2506 string_00 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3   \u2506                             \u2506           \u2506 string_00 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 cat       \u2506 string_00 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 dog       \u2506 string_01 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 aardvark  \u2506 string_02 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 mouse     \u2506 string_03 \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 frog      \u2506 string_04 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nIn the next step, we'll use the pivot function to place each string in its own column.\n(\n    df\n    .with_row_count('id')\n    .with_column(pl.col(\"my_str\").str.split(\"/\").alias(\"split_str\"))\n    .explode(\"split_str\")\n    .with_column(\n        (\"string_\" + pl.arange(0, pl.count()).cast(pl.Utf8).str.zfill(2))\n        .over(\"id\")\n        .alias(\"col_nm\")\n    )\n    .pivot(\n        index=['id', 'my_str'],\n        values='split_str',\n        columns='col_nm',\n    )\n)\n\nshape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 my_str                      \u2506 string_00 \u2506 string_01 \u2506 string_02 \u2506 string_03 \u2506 string_04 \u2502\n\u2502 --- \u2506 ---                         \u2506 ---       \u2506 ---       \u2506 ---       \u2506 ---       \u2506 ---       \u2502\n\u2502 u32 \u2506 str                         \u2506 str       \u2506 str       \u2506 str       \u2506 str       \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 cat                         \u2506 cat       \u2506 null      \u2506 null      \u2506 null      \u2506 null      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 cat/dog                     \u2506 cat       \u2506 dog       \u2506 null      \u2506 null      \u2506 null      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 null                        \u2506 null      \u2506 null      \u2506 null      \u2506 null      \u2506 null      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3   \u2506                             \u2506           \u2506 null      \u2506 null      \u2506 null      \u2506 null      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4   \u2506 cat/dog/aardvark/mouse/frog \u2506 cat       \u2506 dog       \u2506 aardvark  \u2506 mouse     \u2506 frog      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAll that remains is to use fill_null to replace the null values with an empty string \"\".  Notice that I've used a regex expression in the col expression to target only those columns whose names start with \"string_\".  (Depending on your other data, you may not want to replace null with \"\" everywhere in your data.)\n"
}
{
    "Id": 73143854,
    "PostTypeId": 1,
    "Title": "Linking opencv-python to opencv-cuda in Arch",
    "Body": "I'm trying to to get OpenCV with CUDA to be used in Python open-cv on Arch Linux, but I'm not sure how to link it.\nArch provides a package opencv-cuda, which provides these files.\nGuides I've found said to link the python cv2.so to the one provided, but the package doesn't provide that. My python site_packages has cv2.abi3.so in it, and I've tried linking that to core.so and cvv.so to no avail.\nDo I need to build it differently to support Python? Or is there another step I'm missing?\n",
    "AcceptedAnswerId": 73227581,
    "AcceptedAnswer": "On Arch, opencv-cuda provides opencv=4.6.0, but you still need the python bindings. Fortunately though, installing python-opencv after installling opencv-cuda works, since it leverages it.\nI just set up my Python virtual environment to allow system site packages (python -m venv .venv --system-site-packages), and it works like a charm! Neural net image detection runs ~300% as fast now.\n"
}
{
    "Id": 72839263,
    "PostTypeId": 1,
    "Title": "Access python interpreter in VSCode version controll when using pre-commit",
    "Body": "I'm using pre-commit for most of my Python projects, and in many of them, I need to use pylint as a local repo. When I want to commit, I always have to activate python venv and then commit; otherwise, I'll get the following error:\nblack....................................................................Passed\npylint...................................................................Failed\n- hook id: pylint\n- exit code: 1\n\nExecutable `pylint` not found\n\nWhen I use vscode version control to commit, I get the same error; I searched about the problem and didn't find any solution to avoid the error in VSCode.\nThis is my typical .pre-commit-config.yaml:\nrepos:\n-   repo: https://github.com/ambv/black\n    rev: 21.9b0\n    hooks:\n    - id: black\n      language_version: python3.8\n      exclude: admin_web/urls\\.py\n-   repo: local\n    hooks:\n    -   id: pylint\n        name: pylint\n        entry: pylint\n        language: python\n        types: [python]\n        args: \n         - --rcfile=.pylintrc\n\n\n",
    "AcceptedAnswerId": 72839338,
    "AcceptedAnswer": "you have ~essentially two options here -- neither are great (language: system is kinda the unsupported escape hatch so it's on you to make those things available on PATH)\nyou could use a specific path to the virtualenv entry: venv/bin/pylint -- though that will reduce the portability.\nor you could start vscode with your virtualenv activated (usually code .) -- this doesn't always work if vscode is already running\n\ndisclaimer: I created pre-commit\n"
}
{
    "Id": 73271404,
    "PostTypeId": 1,
    "Title": "How to find the average of the differences between all the numbers of a Python List",
    "Body": "I have a python list like this,\narr = [110, 60, 30, 10, 5] \n\nWhat I need to do is actually find the difference of every number with all the other numbers and then find the average of all those differences.\nSo, for this case, it would first find the difference between 110 and then all the remaining elements, i.e. 60, 30, 10, 5, and then it will find the difference of 60 with the remaining elements, i.e. 30, 10, 5 and etc.\nAfter which, it will compute the Average of all these differences.\nNow, this can easily be done with two For Loops but in O(n^2) time complexity and also a little bit of \"messy\" code. I was wondering if there was a faster and more efficient way of doing this same thing?\n",
    "AcceptedAnswerId": 73271447,
    "AcceptedAnswer": "I'll just give the formula first:\nn = len(arr)\nout = np.sum(arr * np.arange(n-1, -n, -2) ) / (n*(n-1) / 2)\n# 52\n\nExplanation: You want to find the mean of\na[0] - a[1], a[0] - a[2],..., a[0] - a[n-1]\n             a[1] - a[2],..., a[1] - a[n-1]\n                         ...\n\nthere, your\n`a[0]` occurs `n-1` times with `+` sign, `0` with `-` -> `n-1` times\n`a[1]` occurs `n-2` times with `+` sign, `1` with `-` -> `n-3` times\n... and so on \n\n"
}
{
    "Id": 72497046,
    "PostTypeId": 1,
    "Title": "skipping a certain range of a list at time in python",
    "Body": "I have a array, I want to pick first 2 or range, skip the next 2, pick the next 2 and continue this until the end of the list\nlist = [2, 4, 6, 7, 9,10, 13, 11, 12,2]\nresults_wanted = [2,4,9,10,12,2] # note how it skipping 2. 2 is used here as and example\n\nIs there way to achieve this in python?\n",
    "AcceptedAnswerId": 72497107,
    "AcceptedAnswer": "Taking n number of elements and skipping the next n.\nl = [2, 4, 6, 7, 9, 10, 13, 11, 12, 2]\nn = 2\nwanted = [x for i in range(0, len(l), n + n) for x in l[i: i + n]]\n### Output : [2, 4, 9, 10, 12, 2]\n\n"
}
{
    "Id": 72409563,
    "PostTypeId": 1,
    "Title": "Unsupported hash type ripemd160 with hashlib in Python",
    "Body": "After a thorough search, I have not found a complete explanation and solution to this very common problem on the entire web. All scripts that need to encode with hashlib give me error:\nPython 3.10\nimport hashlib\nh = hashlib.new('ripemd160')\n\nreturn:\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/usr/lib/python3.10/hashlib.py\", line 166, in __hash_new\n    return __get_builtin_constructor(name)(data)\n  File \"/usr/lib/python3.10/hashlib.py\", line 123, in __get_builtin_constructor\n    raise ValueError('unsupported hash type ' + name)\nValueError: unsupported hash type ripemd160\n\nI already tried to check if that hash exists in the library, and if I have it:\nprint(hashlib.algorithms_available): {'md5', 'sm3', 'sha3_512', 'sha384', 'sha256', 'sha1', 'shake_128', 'sha224', 'sha512_224', 'sha512_256', 'blake2b', 'ripemd160', 'md5-sha1', 'sha512', 'sha3_256', 'shake_256', 'sha3_384', 'whirlpool', 'md4', 'blake2s', 'sha3_224'}\nI am having this problem in a vps with linux, but in my pc I use Windows and I don't have this problem.\nI sincerely appreciate any help or suggestion.\n",
    "AcceptedAnswerId": 72508879,
    "AcceptedAnswer": "Hashlib uses OpenSSL for ripemd160 and apparently OpenSSL disabled some older crypto algos around version 3.0 in November 2021. All the functions are still there but require manual enabling. See issue 16994 of OpenSSL github project for details.\nTo quickly enable it, find the directory that holds your OpenSSL config file or a symlink to it, by running the below command:\nopenssl version -d\n\nYou can now go to the directory and edit the config file (it may be necessary to use sudo):\nnano openssl.cnf\n\nMake sure that the config file contains following lines:\nopenssl_conf = openssl_init\n\n[openssl_init]\nproviders = provider_sect\n\n[provider_sect]\ndefault = default_sect\nlegacy = legacy_sect\n\n[default_sect]\nactivate = 1\n\n[legacy_sect]\nactivate = 1\n\nTested on: OpenSSL 3.0.2, Python 3.10.4, Linux Ubuntu 22.04 LTS aarch64, I have no access to other platforms at the moment.\n"
}
{
    "Id": 73739158,
    "PostTypeId": 1,
    "Title": "NodeJS convert to Byte Array code return different results compare to python",
    "Body": "I got the following Javascript code and I need to convert it to Python(I'm not an expert in hashing so sorry for my knowledge on this subject)\nfunction generateAuthHeader(dataToSign) {\n    let apiSecretHash = new Buffer(\"Rbju7azu87qCTvZRWbtGqg==\", 'base64');\n    let apiSecret = apiSecretHash.toString('ascii');\n    var hash = CryptoJS.HmacSHA256(dataToSign, apiSecret);\n    return hash.toString(CryptoJS.enc.Base64);\n}\n\nwhen I ran generateAuthHeader(\"abc\") it returned +jgBeooUuFbhMirhh1KmQLQ8bV4EXjRorK3bR/oW37Q=\nSo I tried writing the following Python code:\ndef generate_auth_header(data_to_sign):\n    api_secret_hash = bytearray(base64.b64decode(\"Rbju7azu87qCTvZRWbtGqg==\"))\n    hash = hmac.new(api_secret_hash, data_to_sign.encode(), digestmod=hashlib.sha256).digest()\n    return base64.b64encode(hash).decode()\n\nBut when I ran generate_auth_header(\"abc\") it returned a different result aOGo1XCa5LgT1CIR8C1a10UARvw2sqyzWWemCJBJ1ww=\nCan someone tell me what is wrong with my Python code and what I need to change?\nThe base64 is the string I generated myself for this post\nUPDATE:\nthis is the document I'm working with\n//Converting the Rbju7azu87qCTvZRWbtGqg== (key) into byte array \n//Converting the data_to_sign into byte array \n//Generate the hmac signature\n\nit seems like apiSecretHash and api_secret_hash is different, but I don't quite understand as the equivalent of new Buffer() in NodeJS is bytearray() in python\n",
    "AcceptedAnswerId": 73769662,
    "AcceptedAnswer": "It took me 2 days to look it up and ask for people in python discord and I finally got an answer. Let me summarize the problems:\n\nAPI secret hash from both return differents hash of the byte array\njavascript\n\nJavascript\napiSecret = \"E8nm,ns:\\u0002NvQY;F*\"\n\nPython\napi_secret_hash = b'E\\xb8\\xee\\xed\\xac\\xee\\xf3\\xba\\x82N\\xf6QY\\xbbF\\xaa'\n\nonce we replaced the hash with python code it return the same result\ndef generate_auth_header(data_to_sign):\n    api_secret_hash = \"E8nm,ns:\\u0002NvQY;F*\".encode()\n\n    hash = hmac.new(api_secret_hash, data_to_sign.encode(), digestmod=hashlib.sha256).digest()\n    return base64.b64encode(hash).decode()\n\nencoding for ASCII in node.js you can find here https://github.com/nodejs/node/blob/a2a32d8beef4d6db3a8c520572e8a23e0e51a2f8/src/string_bytes.cc#L636-L647\ncase ASCII:\n  if (contains_non_ascii(buf, buflen)) {\n    char* out = node::UncheckedMalloc(buflen);\n    if (out == nullptr) {\n      *error = node::ERR_MEMORY_ALLOCATION_FAILED(isolate);\n      return MaybeLocal();\n    }\n    force_ascii(buf, out, buflen);\n    return ExternOneByteString::New(isolate, out, buflen, error);\n  } else {\n    return ExternOneByteString::NewFromCopy(isolate, buf, buflen, error);\n  }\n\nthere is this force_ascii() function that is called when the data contains non-ASCII characters which is implemented here https://github.com/nodejs/node/blob/a2a32d8beef4d6db3a8c520572e8a23e0e51a2f8/src/string_bytes.cc#L531-L573\nso we need to check for the hash the same as NodeJS one, so we get the final version of the Python code:\ndef generate_auth_header(data_to_sign):\n    # convert to bytearray so the for loop below can modify the values\n    api_secret_hash = bytearray(base64.b64decode(\"Rbju7azu87qCTvZRWbtGqg==\"))\n    \n    # \"force\" characters to be in ASCII range\n    for i in range(len(api_secret_hash)):\n        api_secret_hash[i] &= 0x7f;\n\n    hash = hmac.new(api_secret_hash, data_to_sign.encode(), digestmod=hashlib.sha256).digest()\n    return base64.b64encode(hash).decode()\n\nnow it returned the same result as NodeJS one\nThank you Mark from the python discord for helping me understand and fix this!\nHope anyone in the future trying to convert byte array from javascript to python know about this different of NodeJS Buffer() function\n"
}
{
    "Id": 72199354,
    "PostTypeId": 1,
    "Title": "Python type hinting for a generic mutable tuple / fixed length sequence with multiple types",
    "Body": "I am currently working on adding type hints to a project and can't figure out how to get this right. I have a list of lists, with the nested list containing two elements of type int and float. The first element of the nested list is always an int and the second is always a float.\nmy_list = [[1000, 5.5], [1432, 2.2], [1234, 0.3]]\n\nI would like to type annotate it so that unpacking the inner list in for loops or loop comprehensions keeps the type information. I could change the inner lists to tuples and would get what I'm looking for:\ndef some_function(list_arg: list[tuple[int, float]]): pass\n\n\nHowever, I need the inner lists to be mutable. Is there a nice way to do this for lists? I know that abstract classes like Sequence and Collection do not support multiple types.\n",
    "AcceptedAnswerId": 73817809,
    "AcceptedAnswer": "I think the question highlights a fundamental difference between statically typed Python and dynamically typed Python. For someone who is used to dynamically typed Python (or Perl or JavaScript or any number of other scripting languages), it's perfectly normal to have diverse data types in a list. It's convenient, flexible, and doesn't require you to define custom data types. However, when you introduce static typing, you step into a tighter box that requires more rigorous design.\nAs several others have already pointed out, type annotations for lists require all elements of the list to be the same type, and don't allow you to specify a length. Rather than viewing this as a shortcoming of the type system, you should consider that the flaw is in your own design. What you are really looking for is a class with two data members. The first data member is named 0, and has type int, and the second is named 1, and has type float. As your friend, I would recommend that you define a proper class, with meaningful names for these data members. As I'm not sure what your data type represents, I'll make up names, for illustration.\nclass Sample:\n    def __init__(self, atomCount: int, atomicMass: float):\n        self.atomCount = atomCount\n        self.atomicMass = atomicMass\n\nThis not only solves the typing problem, but also gives a major boost to readability. Your code would now look more like this:\nmy_list = [Sample(1000, 5.5), Sample(1432, 2.2), Sample(1234, 0.3)]\n\ndef some_function(list_arg: list[Sample]): pass\n\nI do think it's worth highlighting Stef's comment, which points to this question. The answers given highlight two useful features related to this.\nFirst, as of Python 3.7, you can mark a class as a data class, which will automatically generate methods like __init__(). The Sample class would look like this, using the @dataclass decorator:\nfrom dataclasses import dataclass\n\n@dataclass\nclass Sample:\n    atomCount: int\n    atomicMass: float\n\nAnother answer to that question mentions a PyPi package called recordclass, which it says is basically a mutable namedtuple. The typed version is called RecordClass\nfrom recordclass import RecordClass\n\nclass Sample(RecordClass):\n    atomCount: int\n    atomicMass: float\n\n"
}
{
    "Id": 72876146,
    "PostTypeId": 1,
    "Title": "Handling GIL when calling python lambda from C++ function",
    "Body": "The question\nIs pybind11 somehow magically doing the work of PyGILState_Ensure() and PyGILState_Release()? And if not, how should I do it?\nMore details\nThere are many questions regarding passing a python function to C++ as a callback using pybind11, but I haven't found one that explains the use of the GIL with pybind11.\nThe documentation is pretty clear about the GIL:\n\n[...] However, when threads are created from C (for example by a third-party library with its own thread management), they don\u2019t hold the GIL, nor is there a thread state structure for them.\nIf you need to call Python code from these threads (often this will be part of a callback API provided by the aforementioned third-party library), you must first register these threads with the interpreter by creating a thread state data structure, then acquiring the GIL, and finally storing their thread state pointer, before you can start using the Python/C API.\n\nI can easily bind a C++ function that takes a callback:\npy::class_ some_api(m, \"SomeApi\"); \nsome_api\n    .def(py::init())\n    .def(\"mode\", &SomeApi::subscribe_mode, \"Subscribe to 'mode' updates.\");\n\nWith the corresponding C++ function being something like:\nvoid subscribe_mode(const std::function& mode_callback);\n\nBut because pybind11 cannot know about the threading happening in my C++ implementation, I suppose it cannot handle the GIL for me. Therefore, if mode_callback is called by a thread created from C++, does that mean that I should write a wrapper to SomeApi::subscribe_mode that uses PyGILState_Ensure() and PyGILState_Release() for each call?\nThis answer seems to be doing something similar, but still slightly different: instead of \"taking the GIL\" when calling the callback, it seems like it \"releases the GIL\" when starting/stopping the thread. Still I'm wondering if there exists something like py::call_guard() that would do exactly what I (believe I) need, i.e. wrapping my callback with PyGILState_Ensure() and PyGILState_Release().\n",
    "AcceptedAnswerId": 72933328,
    "AcceptedAnswer": "In general\npybind11 tries to do the Right Thing and the GIL will be held when pybind11 knows that it is calling a python function, or in C++ code that is called from python via pybind11. The only time that you need to explicitly acquire the GIL when using pybind11 is when you are writing C++ code that accesses python and will be called from other C++ code, or if you have explicitly dropped the GIL.\nstd::function wrapper\nThe wrapper for std::function always acquires the GIL via gil_scoped_acquire when the function is called, so your python callback will always be called with the GIL held, regardless which thread it is called from.\nIf gil_scoped_acquire is called from a thread that does not currently have a GIL thread state associated with it, then it will create a new thread state. As a side effect, if nothing else in the thread acquires the thread state and increments the reference count, then once your function exits the GIL will be released by the destructor of gil_scoped_acquire and then it will delete the thread state associated with that thread.\nIf you're only calling the function once from another thread, this isn't a problem. If you're calling the callback often, it will create/delete the thread state a lot, which probably isn't great for performance. It would be better to cause the thread state to be created when your thread starts (or even easier, start the thread from Python and call your C++ code from python).\n"
}
{
    "Id": 74097901,
    "PostTypeId": 1,
    "Title": "meaning of `__all__` inside Python class",
    "Body": "I am aware of the use of __all__ at module scope. However I came across the usage of __all__ inside classes. This is done e.g. in the Python standardlib:\nclass re(metaclass=_DeprecatedType):\n    \"\"\"Wrapper namespace for re type aliases.\"\"\"\n\n    __all__ = ['Pattern', 'Match']\n    Pattern = Pattern\n    Match = Match\n\nWhat does __all__ achieve in this context?\n",
    "AcceptedAnswerId": 74098046,
    "AcceptedAnswer": "The typing module does some unorthodox things to patch existing modules (like re). Basically, the built-in module re is being replaced with this class re defined using a custom metaclass that intercepts attribute lookups on the underlying object. __all__ doesn't really have any special meaning to the class (it's just another class attribute), but it effectively becomes the __all__ attribute of the re module. It's the metaclass's definition of __getattribute__ that accomplishes this.\n"
}
{
    "Id": 74202814,
    "PostTypeId": 1,
    "Title": "In python, create index from flat representation of nested structure in a list, sorting by alphabetical order",
    "Body": "I have lists where each entry is representing a nested structure, where / represents each level in the structure.\n['a','a/b/a','a/b','a/b/d',....]\n\nI want to take such a list and return an index list where each level is sorted in alphabetical order.\nIf we had the following list\n['a','a/b','a/b/a','a/c','a/c/a','b']\n\nIt represents the nested structure\n'a':                   #1\n\n    'b':               #1.1\n         'a': ...      #1.1.1\n    'c':               #1.2\n         'a': ...      #1.2.1\n'b' : ...              #2\n\nI am trying to get the output\n ['1','1.1','1.1.1', '1.2','1.2.1','2']\n\nBut I am having real issue on how to tackle the problem, would it be solved recursively? Or what would be a way to solve this for any generic list where each level is separated by /? The list is originally not necessarily sorted, and each level can be any generic word.\n",
    "AcceptedAnswerId": 74204355,
    "AcceptedAnswer": "Since the goal is to simply convert the paths to indices according to their respective positions against other paths of the same prefix, there is no need to build a tree at all. Instead, iterate over the paths in alphabetical order while using a dict of sets to keep track of the prefixes at each level of paths, and join the lengths of sets at each level for output:\ndef indices(paths):\n    output = {}\n    names = {}\n    for index, path in sorted(enumerate(paths), key=lambda t: t[1]):\n        counts = []\n        prefixes = tuple(path.split('/'))\n        for level, name in enumerate(prefixes):\n            prefix = prefixes[:level]\n            names.setdefault(prefix, set()).add(name)\n            counts.append(len(names[prefix]))\n        output[index] = '.'.join(map(str, counts))\n    return list(map(output.get, range(len(output))))\n\nso that:\nprint(indices(['a', 'a/b', 'a/b/a', 'a/c', 'a/c/a', 'b']))\nprint(indices(['a', 'c', 'b', 'a/b']))\nprint(indices(['a/b/c/d', 'a/b/d', 'a/b/c']))\nprint(indices(['abc/d', 'bcc/d']))\nprint(indices(['apple/cat','apple/dog', 'banana/dog']))\n\noutputs:\n['1', '1.1', '1.1.1', '1.2', '1.2.1', '2']\n['1', '3', '2', '1.1']\n['1.1.1.1', '1.1.2', '1.1.1']\n['1.1', '2.1']\n['1.1', '1.2', '2.1']\n\nDemo: https://replit.com/@blhsing/StainedMassivePi\n"
}
{
    "Id": 73647685,
    "PostTypeId": 1,
    "Title": "Why does a temporary variable in Python change how this Pass-By-Sharing variable behaves?",
    "Body": "first-time questioner here so do highlight my mistakes.\nI was grinding some Leetcode and came across a behavior (not related to the problem) in Python I couldn't quite figure out nor google-out. It's especially difficult because I'm not sure if my lack of understanding is in:\n\nrecursion\nthe += operator in Python or variable assignment in general\nor Python's pass-by-sharing behavior\nor just something else entirely\n\nHere's the simplified code:\nclass Holder:\n    def __init__(self, val=0):\n         self.val = val\n\nclass Solution:\n    def runThis(self):\n        holder = Holder()\n        self.diveDeeper(holder, 5)\n        return \n        \n    def diveDeeper(self, holder, n):\n        if n==0:\n            return 1\n\n        # 1) Doesn't result in mutation\n        holder.val += self.diveDeeper(holder, n-1)\n\n        # 2) Also doesn't result in mutation\n        # holder.val = holder.val + self.diveDeeper(holder, n-1)\n\n        # 3) !! Results in mutations\n        # returnVal = self.diveDeeper(holder, n-1)\n        # holder.val += returnVal\n\n        print(holder.val)\n        return 1\n\na = Solution()\na.runThis()\n\nSo yeah my main source of confusion is how (1) and (3) look semantically identical to me but results in two completely different outcomes:\n================ RESTART: Case 1 ===============\n1\n1\n1\n1\n1\n>>> \n================ RESTART: Case 3 ===============\n\n1\n2\n3\n4\n5\n>>> \n\nFrom (2), it doesn't seem related to the += operator and for brevity, I haven't included the tens of variations I've tried but none of them have given me any leads so far. Would really appreciate any pointers in the right direction (especially in case I get blindsided in job interviews lmao)\nPS: In case this is relevant, I'm using Python 3.8.2\n",
    "AcceptedAnswerId": 73648204,
    "AcceptedAnswer": "In Python, if you have expression1() + expression2(), expression1() is evaluated first.\nSo 1 and 2 are really equivalent to:\nleft = holder.val\nright = self.diveDeeper(holder, n - 1)\nholder.val = left + right\n\nNow, holder.val is only ever modified after the recursive call, but you use the value from before the recursive call, which means that no matter the iteration, left == 0.\nYour solution 3 is equivalent to:\nright = self.diveDeeper(holder, n - 1)\nleft = holder.val\nholder.val = left + right\n\nSo the recursive call is made before left = holder.val is evaluated, which means left is now the result of the sum of the previous iteration.\nThis is why you have to be careful with mutable state, you got to understand the order of operations perfectly.\n"
}
{
    "Id": 73157383,
    "PostTypeId": 1,
    "Title": "How do you create a fully-fledged Python package?",
    "Body": "When creating a Python package, you can simply write the code, build the package, and share it on PyPI. But how do you do that?\n\nHow do you create a Python package?\nHow do you publish it?\n\nAnd then, what if you want to go further?\n\nHow do you set up CI/CD for it?\nHow do you test it and check code coverage?\nHow do you lint it?\nHow do you automate everything you can?\n\n",
    "AcceptedAnswerId": 73157490,
    "AcceptedAnswer": "Preamble\nWhen you've published dozens of packages, you know how to answer these questions in ways that suit your workflow(s) and taste. But answering these questions for the first time can be quite difficult, time consuming, and frustrating!\nThat's why I spent days researching ways of doing these things, which I then published as a blog article called How to create a Python package in 2022.\nThat article, and this answer, document my findings for when I wanted to publish my package extendedjson\nOverview\nHere is an overview of some tools you can use and the steps you can take, in the order I followed them while discovering all of this.\nDisclaimer: other alternative tools exist (usually) & most of the steps here are not mandatory.\n\nUse Poetry for dependency management\nUse GitHub to host the code\nUse pre-commit to ensure committed code is linted & formatted well\nUse Test PyPI to test uploading your package (which will make it installable with pip)\nUse Scriv for changelog management\nUpload to the real PyPI\nUse pytest to test your Python code\nUse tox to automate linting, formatting, and testing across Python versions\n\nblack\nisort\npylint\nflake8 with mccabe\n\n\nAdd code coverage with coverage.py\nSet up CI/CD with GitHub Actions\n\nrun linters and tests\ntrigger automatically on pull requests and commits\nintegrate with Codecov for coverage reports\npublish to PyPI automatically\n\n\nAdd cool README badges\nTidy up a bit\n\nset tox to use pre-commit\nremove duplicate work between tox and pre-commit hooks\nremove some redundancy in CI/CD\n\n\n\nSteps\nHere is an overview of the things you can do and more or less how to do it. Again, thorough instructions plus the rationale of why I picked certain tools, methods, etc, can be found in the reference article.\n\nUse Poetry for dependency management.\n\npoetry init initialises a project in a directory or poetry new dirname creates a new directory structure for you\ndo poetry install to install all your dependencies\npoetry add packagename can be used to add packagename as a dependency, use -D if it's a development dependency (i.e., you need it while developing the package, but the package users won't need it. For example, black is a nice example of a development dependency)\n\n\nSet up a repository on GitHub to host your code.\n\nSet up pre-commit hooks to ensure your code is always properly formatted and it passes linting. This goes on .pre-commit-config.yaml. E.g., the YAML below checks TOML and YAML files, ensures all files end with a newline, makes sure the end-of-line marker is consistent across all files, and then runs black and isort on your code.\n\n\n# See https://pre-commit.com for more information\n# See https://pre-commit.com/hooks.html for more hooks\nrepos:\n\u00a0 - repo: https://github.com/pre-commit/pre-commit-hooks\n\u00a0 \u00a0 rev: v4.0.1\n\u00a0 \u00a0 hooks:\n\u00a0 \u00a0 \u00a0 - id: check-toml\n\u00a0 \u00a0 \u00a0 - id: check-yaml\n\u00a0 \u00a0 \u00a0 - id: end-of-file-fixer\n\u00a0 \u00a0 \u00a0 - id: mixed-line-ending\n\u00a0 - repo: https://github.com/psf/black\n\u00a0 \u00a0 rev: 22.3.0\n\u00a0 \u00a0 hooks:\n\u00a0 \u00a0 \u00a0 - id: black\n\u00a0 - repo: https://github.com/PyCQA/isort\n\u00a0 \u00a0 rev: 5.10.1\n\u00a0 \u00a0 hooks:\n\u00a0 \u00a0 \u00a0 - id: isort\n\u00a0 \u00a0 \u00a0 \u00a0 args: [\"--profile\", \"black\"]\n\n\nConfigure Poetry to use the Test PyPI to make sure you can publish a package and it is downloadable & installable.\n\nTell Poetry about Test PyPI with poetry config repositories.testpypi https://test.pypi.org/legacy/\nLog in to Test PyPI, get an API token, and tell Poetry to use it with poetry config http-basic.testpypi __token__ pypi-your-api-token-here (the __token__ is a literal and shouldn't be replaced, your token goes after that).\nBuild poetry build and upload your package poetry publish -r testpypi\n\n\nManage your CHANGELOG with Scriv\n\nrun scriv create before any substantial commit and edit the file that pops up\nrun scriv collect before any release to collect all fragments into one changelog\n\n\nConfigure Poetry to use PyPI\n\nlogin to PyPI and get an API token\ntell Poetry about it with poetry config pypi-token.pypi pypi-your-token-here\nbuild & publish your package in one fell swoop with poetry publish --build\n\n\nDo a victory lap: try pip install yourpackagename to make sure everything is going great ;)\n\nPublish a GH release that matches what you uploaded to PyPI\n\nWrite tests. There are many options out there. Pytest is simple, versatile, and not too verbose.\n\nwrite tests in a directory tests/\nstart test files with test_...\nactual tests are functions with a name starting with test_...\nuse assertions (assert) to check for things (tests fail when asserting something Falsy); notice sometimes you don't even need to import pytest in your test files; e.g.:\n\n\n\n# In tests/test_basic_example.py\n\ndef this_test_would_definitely_fail():\n\u00a0 \u00a0 assert 5 > 10\n\ndef this_test_would_definitely_pass():\n\u00a0 \u00a0 assert 5 > 0\n\n\nrun tests with the command pytest\n\nAutomate testing, linting, and formatting, with tox.\n\ntox creates virtual environments for separate Python versions and can run essentially what you tell it to. Configuration goes in tox.ini. You can also embed it in the file pyproject.toml, but as of writing this, that's only supported if you add a string that actually represents the .ini configuration, which is ugly. Example tox.ini:\n\n\n\n[tox]\nisolated_build = True\nenvlist = py38,py39,py310\n\n[testenv]\ndeps =\n\u00a0 \u00a0 black\n\u00a0 \u00a0 pytest\ncommands =\n\u00a0 \u00a0 black --check extendedjson\n\u00a0 \u00a0 pytest .\n\nThe environments py38 to py310 are automatically understood by tox to represent different Python versions (you guess which ones). The header [testenv] defines configurations for all those environments that tox knows about. We install the dependencies listed in deps = ... and then run the commands listed in commands = ....\n\nrun tox with tox for all environments or tox -e py39 to pick a specific environment\n\nAdd code coverage with coverage.py\n\nrun tests and check coverage with coverage run --source=yourpackage --branch -m pytest .\ncreate a nice HTML report with coverage html\nadd this to tox\n\n\nCreate a GitHub action that runs linting and testing on commits and pull requests\n\nGH Actions are just YAML files in .github/workflows\nthis example GH action runs tox on multiple Python versions\n\n\n\n# .github/workflows/build.yaml\nname: Your amazing CI name\n\n# Run automatically on...\non:\n\u00a0 push: \u00a0# pushes...\n\u00a0 \u00a0 branches: [ main ] \u00a0# to the main branch... and\n\u00a0 pull_request: \u00a0# on pull requests...\n\u00a0 \u00a0 branches: [ main ] \u00a0# against the main branch.\n\n# What jobs does this workflow run?\njobs:\n\u00a0 build: \u00a0# There's a job called \u201cbuild\u201d which\n\u00a0 \u00a0 runs-on: ubuntu-latest \u00a0# runs on an Ubuntu machine\n\u00a0 \u00a0 strategy:\n\u00a0 \u00a0 \u00a0 matrix: \u00a0# that goes through\n\u00a0 \u00a0 \u00a0 \u00a0 python-version: [\"3.8\", \"3.9\", \"3.10\"] \u00a0# these Python versions.\n\n\u00a0 \u00a0 steps: \u00a0# The job \u201cbuild\u201d has multiple steps:\n\u00a0 \u00a0 \u00a0 - name: Checkout sources\n\u00a0 \u00a0 \u00a0 \u00a0 uses: actions/checkout@v2 \u00a0# Checkout the repository into the runner,\n\n\u00a0 \u00a0 \u00a0 - name: Setup Python\n\u00a0 \u00a0 \u00a0 \u00a0 uses: actions/setup-python@v2 \u00a0# then set up Python,\n\u00a0 \u00a0 \u00a0 \u00a0 with:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 python-version: ${{ matrix.python-version }} \u00a0# with the version that is currently \u201cselected\u201d...\n\n\u00a0 \u00a0 \u00a0 - name: Install dependencies\n\u00a0 \u00a0 \u00a0 \u00a0 run: | \u00a0# Then run these commands\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 python -m pip install --upgrade pip\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 python -m pip install tox tox-gh-actions \u00a0# install two dependencies...\n\n\u00a0 \u00a0 \u00a0 - name: Run tox\n\u00a0 \u00a0 \u00a0 \u00a0 run: tox \u00a0# and finally run tox.\n\nNotice that, above, we installed tox and a plugin called tox-gh-actions.\nThis plugin will make tox aware of the Python version that is set up in the GH action runner, which will allow us to specify which environments tox will run in that case.\nWe just need to set a correspondence in the file tox.ini:\n# tox.ini\n# ...\n[gh-actions]\npython =\n\u00a0 \u00a0 3.8: py38\n\u00a0 \u00a0 3.9: py39\n\u00a0 \u00a0 3.10: py310\n\n\nIntegrate with Codecov for nice coverage reports in the pull requests.\n\nlog in to Codecov with GitHub and give permissions\nadd Codecov's action to the YAML from before after tox runs (it's tox that generates the local coverage report data) and add/change a coverage command to generate an xml report (it's a format that Codecov understands)\n\n\n\n# ...\n\u00a0 \u00a0 \u00a0 - name: Upload coverage to Codecov\n\u00a0 \u00a0 \u00a0 \u00a0 uses: codecov/codecov-action@v2\n\u00a0 \u00a0 \u00a0 \u00a0 with:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fail_ci_if_error: true\n\n\nAdd a GH Action to publish to PyPI automatically\n\njust set up a YAML file that does your manual steps of building and publishing with Poetry when a new release is made\ncreate a PyPI token to be used by GitHub\nadd it as a secret in your repository\nconfigure Poetry in the action to use that secret\n\n\n\nname: Publish to PyPI\n\non:\n\u00a0 release:\n\u00a0 \u00a0 types: [ published ]\n\u00a0 \u00a0 branches: [ main ]\n\u00a0 workflow_dispatch:\n\njobs:\n\u00a0 build-and-publish:\n\u00a0 \u00a0 runs-on: ubuntu-latest\n\n\u00a0 \u00a0 steps:\n\u00a0 \u00a0 \u00a0 # Checkout and set up Python\n\n\u00a0 \u00a0 \u00a0 - name: Install poetry and dependencies\n\u00a0 \u00a0 \u00a0 \u00a0 run: |\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 python -m pip install --upgrade pip\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 python -m pip install poetry\n\n\u00a0 \u00a0 \u00a0 - name: Configure poetry\n\u00a0 \u00a0 \u00a0 \u00a0 env:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pypi_token: ${{ secrets.PyPI_TOKEN }} \u00a0# You set this manually as a secret in your repository\n\u00a0 \u00a0 \u00a0 \u00a0 run: poetry config pypi-token.pypi $pypi_token\n\n\u00a0 \u00a0 \u00a0 - name: Build and publish\n\u00a0 \u00a0 \u00a0 \u00a0 run: poetry publish --build\n\n\nAdd cool badges to your README file like\n\n\n\n\n\n\nTidy up a bit\n\nrun linting through tox on pre-commit to deduplicate effort and run your preferred versions of the linters/formatters/...\nseparate linting/formatting from testing in tox as a separate environment\ncheck coverage only once as a separate tox environment\n\n\n\n"
}
{
    "Id": 74743233,
    "PostTypeId": 1,
    "Title": "What happens \"behind the scenes\" if I call `None == x` in Python?",
    "Body": "I am learning and playing around with Python and I came up with the following test code (please be aware that I would not write productive code like that, but when learning new languages I like to play around with the language's corner cases):\na = None    \nprint(None == a) # I expected True, I got True\n\nb = 1\nprint(None == b) # I expected False, I got False\n\nclass MyNone:\n    # Called if I compare some myMyNone == somethingElse\n    def __eq__(self, __o: object) -> bool:\n        return True\n\nc = MyNone()\nprint (None == c) # !!! I expected False, I got True !!!\n\nPlease see the very last line of the code example.\nHow can it be that None == something, where something is clearly not None, return True? I would have expected that result for something == None, but not for None == something.\nI expected that it would call None is something behind the scenes.\nSo I think the question boils down to: How does the __eq__ method of the None singleton object look like and how could I have found that out?\n\nPS: I am aware of PEP-0008 and its quote\n\nComparisons to singletons like None should always be done with is or is not, never the equality operators.\n\nbut I still would like to know why print (None == c) in the above example returns True.\n",
    "AcceptedAnswerId": 74743523,
    "AcceptedAnswer": "In fact, None's type does not have its own __eq__ method; within Python we can see that it apparently inherits from the base class object:\n>>> type(None).__eq__\n\n\nBut this is not really what's going on in the source code. The implementation of None can be found in Objects/object.c in the CPython source, where we see:\nPyTypeObject _PyNone_Type = {\n    PyVarObject_HEAD_INIT(&PyType_Type, 0)\n    \"NoneType\",\n    0,\n    0,\n    none_dealloc,       /*tp_dealloc*/ /*never called*/\n    0,                  /*tp_vectorcall_offset*/\n    0,                  /*tp_getattr*/\n    0,                  /*tp_setattr*/\n    // ...\n    0,                  /*tp_richcompare */\n    // ...\n    0,                  /*tp_init */\n    0,                  /*tp_alloc */\n    none_new,           /*tp_new */\n};\n\nI omitted most of the irrelevant parts. The important thing here is that _PyNone_Type's tp_richcompare is 0, i.e. a null pointer. This is checked for in the do_richcompare function:\n    if ((f = Py_TYPE(v)->tp_richcompare) != NULL) {\n        res = (*f)(v, w, op);\n        if (res != Py_NotImplemented)\n            return res;\n        Py_DECREF(res);\n    }\n    if (!checked_reverse_op && (f = Py_TYPE(w)->tp_richcompare) != NULL) {\n        res = (*f)(w, v, _Py_SwappedOp[op]);\n        if (res != Py_NotImplemented)\n            return res;\n        Py_DECREF(res);\n    }\n\nTranslating for those who don't speak C:\n\nIf the left-hand-side's tp_richcompare function is not null, call it, and if its result is not NotImplemented then return that result.\nOtherwise if the reverse hasn't already been checked*, and the right-hand-side's tp_richcompare function is not null, call it, and if the result is not NotImplemented then return that result.\n\nThere are some other branches in the code, to fall back to in case none of those branches returns a result. But these two branches are enough to see what's going on. It's not that type(None).__eq__ returns NotImplemented, rather the type doesn't have the corresponding function in the C source code at all. That means the second branch is taken, hence the result you observe.\n*The flag checked_reverse_op is set if the reverse direction has already been checked; this happens if the right-hand-side is a strict subtype of the left-hand-side, in which case it takes priority. That doesn't apply in this case since there is no subtype relation between type(None) and your class.\n"
}
{
    "Id": 73326570,
    "PostTypeId": 1,
    "Title": "Why is the float * int multiplication faster than int * float in CPython?",
    "Body": "Basically, the expression 0.4 * a is consistently, and surprisingly, significantly faster than a * 0.4. a being an integer. And I have no idea why.\nI speculated that it is a case of a LOAD_CONST LOAD_FAST bytecode pair being \"more specialized\" than the LOAD_FAST LOAD_CONST and I would be entirely satisfied with this explanation, except that this quirk seems to apply only to multiplications where types of multiplied variables differ. (By the way, I can no longer find the link to this \"bytecode instruction pair popularity ranking\" I once found on github, does anyone have a link?)\nAnyway, here are the micro benchmarks:\n$ python3.10 -m pyperf timeit -s\"a = 9\" \"a * 0.4\"\nMean +- std dev: 34.2 ns +- 0.2 ns\n\n$ python3.10 -m pyperf timeit -s\"a = 9\" \"0.4 * a\"\nMean +- std dev: 30.8 ns +- 0.1 ns\n\n$ python3.10 -m pyperf timeit -s\"a = 0.4\" \"a * 9\"\nMean +- std dev: 30.3 ns +- 0.3 ns\n\n$ python3.10 -m pyperf timeit -s\"a = 0.4\" \"9 * a\"\nMean +- std dev: 33.6 ns +- 0.3 ns\n\nAs you can see - in the runs where the float comes first (2nd and 3rd) - it is faster.\nSo my question is where does this behavior come from? I'm 90% sure that it is an implementation detail of CPython, but I'm not that familiar with low level instructions to state that for sure.\n",
    "AcceptedAnswerId": 73326827,
    "AcceptedAnswer": "It's CPython's implementation of the BINARY_MULTIPLY opcode. It has no idea what the types are at compile-time, so everything has to be figured out at run-time. Regardless of what a and b may be, BINARY_MULTIPLY ends up inoking a.__mul__(b).\nWhen a is of int type int.__mul__(a, b) has no idea what to do unless b is also of int type. It returns Py_RETURN_NOTIMPLEMENTED (an internal C constant). This is in longobject.c's CHECK_BINOP macro. The interpreter sess that, and effectively says \"OK, a.__mul__ has no idea what to do, so let's give b.__rmul__ a shot at it\". None of that is free - it all takes time.\nfloat.__mul__(b, a) (same as float.__rmul__) does know what to do with an int (converts it to float first), so that succeeds.\nBut when a is of float type to begin with, we go to float.__mul__ first, and that's the end of it. No time burned figuring out that the int type doesn't know what to do.\nThe actual code is quite a bit more involved than the above pretends, but that's the gist of it.\n"
}
{
    "Id": 73406581,
    "PostTypeId": 1,
    "Title": "python manage.py collectstatic error: cannot find rest_framework bootstrap.min.css.map (from book 'Django for APIs')",
    "Body": "I am reading the book 'Django for APIs' from 'William S. Vincent' (current edition for Django 4.0)\nIn chapter 4, I cannot run successfully the command python manage.py collectstatic.\nI get the following error:\nTraceback (most recent call last):\n  File \"/Users/my_name/Projects/django/django_for_apis/library/manage.py\", line 22, in \n    main()\n  File \"/Users/my_name/Projects/django/django_for_apis/library/manage.py\", line 18, in main\n    execute_from_command_line(sys.argv)\n  File \"/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/core/management/__init__.py\", line 446, in execute_from_command_line\n    utility.execute()\n  File \"/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/core/management/__init__.py\", line 440, in execute\n    self.fetch_command(subcommand).run_from_argv(self.argv)\n  File \"/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/core/management/base.py\", line 402, in run_from_argv\n    self.execute(*args, **cmd_options)\n  File \"/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/core/management/base.py\", line 448, in execute\n    output = self.handle(*args, **options)\n  File \"/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/contrib/staticfiles/management/commands/collectstatic.py\", line 209, in handle\n    collected = self.collect()\n  File \"/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/contrib/staticfiles/management/commands/collectstatic.py\", line 154, in collect\n    raise processed\nwhitenoise.storage.MissingFileError: The file 'rest_framework/css/bootstrap.min.css.map' could not be found with .\n\nThe CSS file 'rest_framework/css/bootstrap.min.css' references a file which could not be found:\n  rest_framework/css/bootstrap.min.css.map\n\nPlease check the URL references in this CSS file, particularly any\nrelative paths which might be pointing to the wrong location. \n\nI have the exact same settings like in the book in settings.py:\nSTATIC_URL = \"static/\"\nSTATICFILES_DIRS = [BASE_DIR / \"static\"]  # new\nSTATIC_ROOT = BASE_DIR / \"staticfiles\"  # new\nSTATICFILES_STORAGE = \"whitenoise.storage.CompressedManifestStaticFilesStorage\"  # new\n\nI couldn't find any explanation for it. maybe someone can point me in the right direction.\n",
    "AcceptedAnswerId": 73411956,
    "AcceptedAnswer": "Update: DRF 3.14.0 now supports Django 4.1. If you've added stubs to static as per below, be sure to remove them.\nThis appears to be related to Django 4.1: either downgrade to Django 4.0 or simply create the following empty files in one of your static directories:\nstatic/rest_framework/css/bootstrap-theme.min.css.map\nstatic/rest_framework/css/bootstrap.min.css.map\n\nThere's a recent change to ManifestStaticFilesStorage where it now attempts to replace source maps with their hashed counterparts.\nDjango REST framework has only recently added the bootstrap css source maps but is not yet released.\n"
}
{
    "Id": 72620996,
    "PostTypeId": 1,
    "Title": "Apple M1 - Symbol not found: _CFRelease while running Python app",
    "Body": "I am hoping to run my app without any problem, but I got this attached error. Could someone help or point me into the right direction as to why this is happening?\nTraceback (most recent call last):\n  File \"/Users/andre.sitorus/Documents/GitHub/nexus/automation-api/app/main.py\", line 4, in \n    from configurations import config  # noqa # pylint: disable=unused-import\n  File \"/Users/andre.sitorus/Documents/GitHub/nexus/automation-api/app/configurations/config.py\", line 7, in \n    from google.cloud import secretmanager\n  File \"/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/google/cloud/secretmanager.py\", line 20, in \n    from google.cloud.secretmanager_v1 import SecretManagerServiceClient\n  File \"/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/google/cloud/secretmanager_v1/__init__.py\", line 24, in \n    from google.cloud.secretmanager_v1.gapic import secret_manager_service_client\n  File \"/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/google/cloud/secretmanager_v1/gapic/secret_manager_service_client.py\", line 25, in \n    import google.api_core.gapic_v1.client_info\n  File \"/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/google/api_core/gapic_v1/__init__.py\", line 18, in \n    from google.api_core.gapic_v1 import config\n  File \"/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/google/api_core/gapic_v1/config.py\", line 23, in \n    import grpc\n  File \"/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/grpc/__init__.py\", line 22, in \n    from grpc import _compression\n  File \"/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/grpc/_compression.py\", line 15, in \n    from grpc._cython import cygrpc\nImportError: dlopen(/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/grpc/_cython/cygrpc.cpython-39-darwin.so, 2): Symbol not found: _CFRelease\n  Referenced from: /Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/grpc/_cython/cygrpc.cpython-39-darwin.so\n  Expected in: flat namespace\n in /Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/grpc/_cython/cygrpc.cpython-39-darwin.so\n\nI'm running  this in Apple M1.\nI already upgraded pip and setuptools before installing all the requirements in my virtual environment using conda. Here is my python, pip, and setuptools version:\npython 3.9.12\npip 21.2.4\nsetuptools 62.4.0\n\n",
    "AcceptedAnswerId": 73245207,
    "AcceptedAnswer": "Had the same issue; turned out it's because of the grpcio build. Doing this helped:\npip uninstall grpcio\nconda install grpcio\n\n(Make sure you use the conda-forge channel with conda; the community puts in work to make sure packages play well with M1/arm64)\n"
}
{
    "Id": 73464414,
    "PostTypeId": 1,
    "Title": "Why are generics in Python implemented using __class_getitem__ instead of __getitem__ on metaclass",
    "Body": "I was reading python documentation and peps and couldn't find an answer for this.\nGenerics in python are implemented by subscripting class objects. list[str] is a list where all elements are strings.\nThis behaviour is achieved by implementing a special (dunder) classmethod called __class_getitem__ which as the documentation states should return a GenericAlias.\nAn example:\nclass MyGeneric:\n    def __class_getitem__(cls, key):\n        # implement generics\n        ...\n\nThis seems weird to me because the documentation also shows some code similar to what the interpreter does when faced with subscripting objects and shows that defining both __getitem__ on object's metaclass and __class_getitem__ on the object itself always chooses the metaclass' __getitem__. This means that a class with the same functionality as the one above can be implemented without introducing a new special method into the language.\nAn example of a class with identical behaviour:\nclass GenericMeta(type):\n    def __getitem__(self, key):\n        # implement generics\n        ...\n\n\nclass MyGeneric(metaclass=GenericMeta):\n    ...\n\nLater the documentation also shows an example of Enums using a __getitem__ of a metaclass as an example of a __class_getitem__ not being called.\nMy question is why was the __class_getitem__ classmethod introduced in the first place?\nIt seems to do the exact same thing as the metaclass' __getitem__ but with the added complexity and the need for extra code in the interpreter for deciding which method to call. All of this comes with no extra benefit as defining both will simply call the same one every time unless specifically calling dunder methods (which should not be done in general).\nI know that implementing generics this way is discouraged. The general approach is to subclass a class that already defines a __class_getitem__ like typing.Generic but I'm still curious as to why that functionality was implemented that way.\n",
    "AcceptedAnswerId": 73464466,
    "AcceptedAnswer": "__class_getitem__ exists because using multiple inheritance where multiple metaclasses are involved is very tricky and sets limitations that can\u2019t always be met when using 3rd-party libraries.\nWithout __class_getitem__ generics requires a metaclass, as defining a  __getitem__ method on a class would only handle attribute access on instances, not on the class. Normally, object[...] syntax is handled by the type of object, not by object itself. For instances, that's the class, but for classes, that's the metaclass.\nSo, the syntax:\nClassObject[some_type]\n\nwould translate to:\ntype(ClassObject).__getitem__(ClassObject, some_type)\n\n__class_getitem__ exists to avoid having to give every class that needs to support generics, a metaclass.\nFor how __getitem__ and other special methods work, see the Special method lookup section in the Python Datamodel chapter:\n\nFor custom classes, implicit invocations of special methods are only guaranteed to work correctly if defined on an object\u2019s type, not in the object\u2019s instance dictionary.\n\nThe same chapter also explicitly covers __class_getitem__ versus __getitem__:\n\nUsually, the subscription of an object using square brackets will call the __getitem__() instance method defined on the object\u2019s class. However, if the object being subscribed is itself a class, the class method __class_getitem__() may be called instead.\n\nThis section also covers what will happen if the class has both a metaclass with a __getitem__ method, and a __class_getitem__ method defined on the class itself. You found this section, but it only applies in this specific corner-case.\nAs stated, using metaclasses can be tricky, especially when inheriting from classes with different metaclasses. See the original PEP 560 - Core support for typing module and generic types proposal:\n\nAll generic types are instances of GenericMeta, so if a user uses a custom metaclass, then it is hard to make a corresponding class generic. This is particularly hard for library classes that a user doesn\u2019t control.\n...\nWith the help of the proposed special attributes the GenericMeta metaclass will not be needed.\n\nWhen mixing multiple classes with different metaclasses, Python requires that the most specific metaclass derives from the other metaclasses, a requirement that can't easily be met if the metaclass is not your own; see the documentation on determining the appropriate metaclass.\nAs a side note, if you do use a metaclass, then __getitem__ should not be a classmethod:\nclass GenericMeta(type):\n    # not a classmethod! `self` here is a class, an instance of this\n    # metaclass.\n    def __getitem__(self, key):\n        # implement generics\n        ...\n\nBefore PEP 560, that's basically what the typing.GenericMeta metaclass did, albeit with a bit more complexity.\n"
}
{
    "Id": 74930922,
    "PostTypeId": 1,
    "Title": "How to load a custom Julia package in Python using Python's juliacall",
    "Body": "I already know How to import Julia packages into Python.\nHowever, now I have created my own simple Julia package with the following command:\nusing Pkg;Pkg.generate(\"MyPack\");Pkg.activate(\"MyPack\");Pkg.add(\"StatsBase\")\nwhere the file MyPack/src/MyPack.jl has the following contents:\nmodule MyPack\nusing StatsBase\n\nfunction f1(x, y)\n   return 3x + y\nend\ng(x) = StatsBase.std(x)\n\nexport f1\n\nend\n\nNow I would like to load this Julia package in Python via juliacall and call f1 and g functions.\nI have already run pip3 install juliacall from command line. How do I call the above functions from Python?\n",
    "AcceptedAnswerId": 74930923,
    "AcceptedAnswer": "You need to run the following code to load the MyPack package from Python via juliacall\nfrom juliacall import Main as jl\nfrom juliacall import Pkg as jlPkg\n\njlPkg.activate(\"MyPack\")  # relative path to the folder where `MyPack/Project.toml` should be used here \n\njl.seval(\"using MyPack\")\n\nNow you can use the function (note that calls to non exported functions require package name):\n>>> jl.f1(4,7)\n19\n\n>>> jl.f1([4,5,6],[7,8,9]).to_numpy()\narray([19, 23, 27], dtype=object)\n\n>>> jl.MyPack.g(numpy.arange(0,3))\n1.0\n\nNote another option for calling Julia from Python that seems to be more difficult to configure as of today is the pip install julia package which is described here: I have a high-performant function written in Julia, how can I use it from Python?\n"
}
{
    "Id": 73599734,
    "PostTypeId": 1,
    "Title": "Python dataclass, one attribute referencing other",
    "Body": "@dataclass\nclass Stock:\n    symbol: str\n    price: float = get_price(symbol)\n\nCan a dataclass attribute access to the other one? In the above example, one can create a Stock by providing a symbol and the price. If price is not provided, it defaults to a price which we get from some function get_price. Is there a way to reference symbol?\nThis example generates error NameError: name 'symbol' is not defined.\n",
    "AcceptedAnswerId": 73599883,
    "AcceptedAnswer": "You can use __post_init__ here. Because it's going to be called after __init__, you have your attributes already populated so do whatever you want to do there:\nfrom typing import Optional\nfrom dataclasses import dataclass\n\n\ndef get_price(name):\n    # logic to get price by looking at `name`.\n    return 1000.0\n\n\n@dataclass\nclass Stock:\n    symbol: str\n    price: Optional[float] = None\n\n    def __post_init__(self):\n        if self.price is None:\n            self.price = get_price(self.symbol)\n\n\nobj1 = Stock(\"boo\", 2000.0)\nobj2 = Stock(\"boo\")\nprint(obj1.price)  # 2000.0\nprint(obj2.price)  # 1000.0\n\nSo if user didn't pass price while instantiating, price is None. So you can check it in __post_init__ and ask it from get_price.\n"
}
{
    "Id": 73599970,
    "PostTypeId": 1,
    "Title": "How to solve \"wkhtmltopdf reported an error: Exit with code 1 due to network error: ProtocolUnknownError\" in python pdfkit",
    "Body": "I'm using Django. This is code is in views.py.\ndef download_as_pdf_view(request, doc_type, pk):\n    import pdfkit\n    file_name = 'invoice.pdf'\n    pdf_path = os.path.join(settings.BASE_DIR, 'static', 'pdf', file_name)\n\n    template = get_template(\"paypal/card_invoice_detail.html\")\n    _html = template.render({})\n    pdfkit.from_string(_html, pdf_path)\n\n    return FileResponse(open(pdf_path, 'rb'), filename=file_name, content_type='application/pdf')\n\nTraceback is below.\n\n[2022-09-05 00:56:35,785] ERROR [django.request.log_response:224] Internal Server Error: /paypal/download_pdf/card_invoice/MTE0Nm1vamlva29zaGkz/\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/django/core/handlers/exception.py\", line 47, in inner\n    response = get_response(request)\n  File \"/usr/local/lib/python3.8/site-packages/django/core/handlers/base.py\", line 181, in _get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/project/app/paypal/views.py\", line 473, in download_as_pdf_view\n    pdfkit.from_string(str(_html), pdf_path)\n  File \"/usr/local/lib/python3.8/site-packages/pdfkit/api.py\", line 75, in from_string\n    return r.to_pdf(output_path)\n  File \"/usr/local/lib/python3.8/site-packages/pdfkit/pdfkit.py\", line 201, in to_pdf\n    self.handle_error(exit_code, stderr)\n  File \"/usr/local/lib/python3.8/site-packages/pdfkit/pdfkit.py\", line 155, in handle_error\n    raise IOError('wkhtmltopdf reported an error:\\n' + stderr)\nOSError: wkhtmltopdf reported an error:\nExit with code 1 due to network error: ProtocolUnknownError\n\n[2022-09-05 00:56:35,797] ERROR [django.server.log_message:161] \"GET /paypal/download_pdf/card_invoice/MTE0Nm1vamlva29zaGkz/ HTTP/1.1\" 500 107486\n\n\nThis is work file.\npdfkit.from_url('https://google.com', 'google.pdf')\n\nHowever pdfkit.from_string and pdfkit.from_file return \"ProtocolUnknownError\"\nPlease help me!\nUpdate\nI tyied this code.\n    _html = '''Hello world'''\n    pdfkit.from_string(_html), pdf_path)\n\nIt worked fine. I saved above html as sample.html. Then run this code\n\nI added this parameter options={\"enable-local-file-access\": \"\"}\n\n    _html = render_to_string('path/to/sample.html')\n    pdfkit.from_string(str(_html), pdf_path, options={\"enable-local-file-access\": \"\"})\n\nIt worked fine! And the \"ProtocolUnknownError\" error is gone thanks to options={\"enable-local-file-access\": \"\"}.\nSo, I changed the HTML file path to the one I really want to use.\n    _html = render_to_string('path/to/invoice.html')\n    pdfkit.from_string(_html, pdf_path, options={\"enable-local-file-access\": \"\"})\n    return FileResponse(open(pdf_path, 'rb'), filename=file_name, content_type='application/pdf')\n\nIt does not finish convert pdf. When I run the code line by line.\nstdout, stderr = result.communicate(input=input) does not return.\nIt was processing long time.\n",
    "AcceptedAnswerId": 73603802,
    "AcceptedAnswer": "I solved this problem. Theare are 3 step to pass this problems.\n\nYou need to set options {\"enable-local-file-access\": \"\"}. pdfkit.from_string(_html, pdf_path, options={\"enable-local-file-access\": \"\"})\n\npdfkit.from_string() can't load css from URL. It's something like this.\n css path should be absolute path or write style in same file.\n\nIf css file load another file. ex: font file. It will be ContentNotFoundError.\n\n\nMy solution\nI used simple css file like this.\nbody {\n    font-size: 18px;\n    padding: 55px;\n}\n\nh1 {\n    font-size: 38px;\n}\n\nh2 {\n    font-size: 28px;\n}\n\nh3 {\n    font-size: 24px;\n}\n\nh4 {\n    font-size: 20px;\n}\n\ntable, th, td {\n    margin: auto;\n    text-align: center;\n    border: 1px solid;\n}\n\ntable {\n    width: 80%;\n}\n\n.text-right {\n    text-align: right;\n}\n\n\n.text-left {\n    text-align: left;\n}\n\n.text-center {\n    text-align: center;\n}\n\nThis code insert last css file as style in same html.\nimport os\n\nimport pdfkit\nfrom django.http import FileResponse\nfrom django.template.loader import render_to_string\n\nfrom paypal.models import Invoice\nfrom website import settings\n\n\ndef download_as_pdf_view(request, pk):\n    # create PDF from HTML template file with context.\n    invoice = Invoice.objects.get(pk=pk)\n    context = {\n        # please set your contexts as dict.\n    }\n    _html = render_to_string('paypal/card_invoice_detail.html', context)\n     # remove header\n    _html = _html[_html.find(''):]  \n\n    # create new header\n    new_header = '''\n    \n    \n    \n    \n    \n'''\n    # add style from css file. please change to your css file path.\n    css_path = os.path.join(settings.BASE_DIR, 'paypal', 'static', 'paypal', 'css', 'invoice.css')\n    with open(css_path, 'r') as f:\n        new_header += f.read()\n    new_header += '\\n'\n    print(new_header)\n\n    # add head to html\n    _html = new_header + _html[_html.find(''):]\n    with open('paypal/sample.html', 'w') as f: f.write(_html)  # for debug\n\n    # convert html to pdf\n    file_name = 'invoice.pdf'\n    pdf_path = os.path.join(settings.BASE_DIR, 'static', 'pdf', file_name)\n    pdfkit.from_string(_html, pdf_path, options={\"enable-local-file-access\": \"\"})\n    return FileResponse(open(pdf_path, 'rb'), filename=file_name, content_type='application/pdf')\n\n"
}
{
    "Id": 73660050,
    "PostTypeId": 1,
    "Title": "How to achieve \"resumption semantics\" for Python exceptions?",
    "Body": "I have a validator class with a method that performs multiple checks and may raise different exceptions:\nclass Validator:\n    def validate(something) -> None:\n        if a:\n            raise ErrorA()\n        if b:\n            raise ErrorB()\n        if c:\n            raise ErrorC()\n\nThere's a place in the outside (caller) code where I want to customize its behaviour and prevent ErrorB from being raised, without preventing ErrorC. Something like resumption semantics would be useful here. Hovewer, I haven't found a good way to achieve this.\nTo clarify: I have the control over Validator source code, but prefer to preserve its existing interface as much as possible.\nSome possible solutions that I've considered:\n\nThe obvious\ntry:\n    validator.validate(something)\nexcept ErrorB:\n    ...\n\nis no good because it also suppresses ErrorC in cases where both ErrorB and ErrorC should be raised.\n\nCopy-paste the method and remove the check:\n# In the caller module\n\nclass CustomValidator(Validator):\n    def validate(something) -> None:\n        if a:\n            raise ErrorA()\n        if c:\n            raise ErrorC()\n\nDuplicating the logic for a and c is a bad idea\nand will lead to bugs if Validator changes.\n\nSplit the method into separate checks:\nclass Validator:\n    def validate(something) -> None:\n        self.validate_a(something)\n        self.validate_b(something)\n        self.validate_c(something)\n\n    def validate_a(something) -> None:\n        if a:\n            raise ErrorA()\n\n    def validate_b(something) -> None:\n        if b:\n            raise ErrorB()\n\n    def validate_c(something) -> None:\n        if c:\n            raise ErrorC()\n\n# In the caller module\n\nclass CustomValidator(Validator):\n    def validate(something) -> None:\n        super().validate_a(something)\n        super().validate_c(something)\n\nThis is just a slightly better copy-paste.\nIf some validate_d() is added later, we have a bug in CustomValidator.\n\nAdd some suppression logic by hand:\nclass Validator:\n    def validate(something, *, suppress: list[Type[Exception]] = []) -> None:\n        if a:\n            self._raise(ErrorA(), suppress)\n        if b:\n            self._raise(ErrorB(), suppress)\n        if c:\n            self._raise(ErrorC(), suppress)\n\n    def _raise(self, e: Exception, suppress: list[Type[Exception]]) -> None:\n        with contextlib.suppress(*suppress):\n            raise e\n\nThis is what I'm leaning towards at the moment.\nThere's a new optional parameter and the raise syntax becomes kinda ugly,\nbut this is an acceptable cost.\n\nAdd flags that disable some checks:\nclass Validator:\n    def validate(something, *, check_a: bool = True,\n                 check_b: bool = True, check_c: bool = True) -> None:\n        if check_a and a:\n            raise ErrorA()\n        if check_b and b:\n            raise ErrorB()       \n        if check_c and c:\n            raise ErrorC()\n\nThis is good, because it allows to granually control different checks even\nif they raise the same exception.\nHowever, it feels verbose and will require additional maintainance\nas Validator changes. I actually have more than three checks there.\n\nYield exceptions by value:\nclass Validator:\n    def validate(something) -> Iterator[Exception]:\n        if a:\n            yield ErrorA()\n        if b:\n            yield ErrorB()\n        if c:\n            yield ErrorC()\n\nThis is bad, because it's a breaking change for existing callers\nand it makes propagating the exception (the typical use) way more verbose:\n# Instead of\n# validator.validate(something)\n\ne = next(validator.validate(something), None)\nif e is not None:\n    raise e\n\nEven if we keep everything backwards-compatible\nclass Validator:\n    def validate(something) -> None:\n        e = next(self.iter_errors(something), None)\n        if e is not None:\n            raise e\n\n    def iter_errors(something) -> Iterator[Exception]:\n        if a:\n            yield ErrorA()\n        if b:\n            yield ErrorB()\n        if c:\n            yield ErrorC()\n\nThe new suppressing caller still needs to write all this code:\nexceptions = validator.iter_errors(something)\ne = next(exceptions, None)\nif isinstance(e, ErrorB):\n    # Skip ErrorB, don't raise it.\n    e = next(exceptions, None)\nif e is not None:\n    raise e\n\nCompared to the previous two options:\nvalidator.validate(something, suppress=[ErrorB])\n\nvalidator.validate(something, check_b=False)\n\n\n\n",
    "AcceptedAnswerId": 73662557,
    "AcceptedAnswer": "With bare exceptions you are looking at the wrong tool for the job. In Python, to raise an exception means that execution hits an exceptional case in which resuming is not possible. Terminating the broken execution is an express purpose of exceptions.\n\nExecution Model: 4.3. Exceptions\nPython uses the \u201ctermination\u201d model of error handling: an exception handler can find out what happened and continue execution at an outer level, but it cannot repair the cause of the error and retry the failing operation (except by re-entering the offending piece of code from the top).\n\nTo get resumption semantics for exception handling, you can look at the generic tools for either resumption or for handling.\n\nResumption: Coroutines\nPython's resumption model are coroutines: yield coroutine-generators or async coroutines both allow to pause and explicitly resume execution.\ndef validate(something) -> Iterator[Exception]:\n    if a:\n        yield ErrorA()\n    if b:\n        yield ErrorB()\n    if c:\n        yield ErrorC()\n\nIt is important to distinguish between send-style \"proper\" coroutines and iterator-style \"generator\" coroutines. As long as no value must be sent into the coroutine, it is functionally equivalent to an iterator. Python has good inbuilt support for working with iterators:\nfor e in validator.iter_errors(something):\n    if isinstance(e, ErrorB):\n        continue  # continue even if ErrorB happens\n    raise e\n\nSimilarly, one could filter the iterator or use comprehensions. Iterators easily compose and gracefully terminate, making them suitable for iterating exception cases.\n\nEffect Handling\nException handling is just the common use case for the more generic effect handling. While Python has no builtin effect handling support, simple handlers that address only the origin or sink of an effect can be modelled just as functions:\ndef default_handler(failure: BaseException):\n    raise failure\n\ndef validate(something, failure_handler = default_handler) -> None:\n    if a:\n        failure_handler(ErrorA())\n    if b:\n        failure_handler(ErrorB())\n    if c:\n        failure_handler(ErrorC())\n\nThis allows the caller to change the effect handling by supplying a different handler.\ndef ignore_b_handler(failure: BaseException):\n    if not isinstance(failure, ErrorB):\n        raise failure\n\nvalidate(..., ignore_b_handler)\n\nThis might seem familiar to dependency inversion and is in fact related to it.\nThere are various stages of buying into effect handling, and it is possible to reproduce much if not all features via classes. Aside from technical functionality, one can implement ambient effect handlers (similar to how try \"connects\" to raise automatically) via thread local or context-local variables.\n"
}
{
    "Id": 73719101,
    "PostTypeId": 1,
    "Title": "Connecting a C++ program to a Python script with shared memory",
    "Body": "I'm trying to connect a C++ program to python using shared memory but I don't know how to pass the name of the memory segment to python.\nHere is my C++ code:\nkey_t key = ftok(\"address\", 1);\nint shm_o;\nchar* msg = \"hello there\";\nint len = strlen(msg) + 1;\nvoid* addr;\n\nshm_o = shmget(key, 20, IPC_CREAT | 0600);\nif(shm_o == -1)\n{\n    std::cout << \"Failed: shmget.\\n\";\n    return 1;\n}\n\naddr = shmat(shm_o, NULL, 0);\nif(addr == (void*) -1)\n{\n    std::cout << \"Failed: shmat.\\n\";\n    return 1;\n}\n\nstd::cout << \"Shared memory segment created successfully with id: \" << shm_o;\nmemcpy(addr, msg, len);\n\ngetchar();\nreturn 0;\n\nI'm trying to get python to read from the shared memory segment like so:\nshm_a = shared_memory.SharedMemory(name=\"address\", create=False, size=20)\n\nprint(bytes(shm_a.buf[:11]))\n\nbut it throws an exception saying there is no file or directory called 'address'.\nAm I going about this correctly or is there another way to attach python to the shared memory segment?\nAny help would be much appreciated.\n",
    "AcceptedAnswerId": 73720808,
    "AcceptedAnswer": "Taking the liberty to post a working example here for POSIX shared memory segments, which will work across C/C++ and Python on Linux/UNIX-like systems. This will not work on Windows.\nC++ code to create and write data into a shared memory segment (name provided on command line):\n#include \n#include \n#include \n#include \n#include \n\n#include \n#include \n\nint main(int argc, char * argv[])\n{\n    if (argc != 2) {\n         std::cerr  required\" << std::endl;\n         return 1;\n    }\n    const char * shmem_name = argv[1];\n    size_t shm_size = 4096;\n    int shmem_fd = shm_open(shmem_name, O_CREAT|O_RDWR, S_IRUSR|S_IWUSR|S_IRGRP|S_IWGRP);\n    if (shmem_fd == -1) {\n         perror(\"shm_open\");\n         return 1;\n    }\n    std::cout << \"Shared Memory segment created with fd \" << shmem_fd << std::endl;\n    if (ftruncate(shmem_fd, shm_size) == -1) {\n        perror(\"ftruncate\");\n        return 1;\n    }\n    std::cout << \"Shared Memory segment resized to \" << shm_size << std::endl;\n    void * addr = mmap(0, shm_size, PROT_WRITE, MAP_SHARED, shmem_fd, 0);\n    if (addr == MAP_FAILED) {\n        perror(\"mmap\");\n        return 1;\n    }\n    std::cout << \"Please enter some text to write to shared memory segment\\n\";\n    std::string text;\n    std::getline(std::cin, text);\n    while (! text.empty()) {\n        strncpy((char *)addr, text.data(), shm_size);\n        std::cout << \"Written '\" << text << \"' to shared memory segment\\n\";\n        std::getline(std::cin, text);\n    }\n    std::cout << \"Unlinking shared memory segment.\" << std::endl;\n    shm_unlink(shmem_name) ;\n}\n\nPython code to read any string from the beginning of the shared memory segment:\nimport sys\nfrom multiprocessing import shared_memory, resource_tracker\n\nif len(sys.argv) != 2:\n    print(\"Argument  required\")\n    sys.exit(1)\n\nshm_seg = shared_memory.SharedMemory(name=sys.argv[1])\nprint(bytes(shm_seg.buf).strip(b'\\x00').decode('ascii'))\nshm_seg.close()\n# Manually remove segment from resource_tracker, otherwise shmem segment\n# will be unlinked upon program exit\nresource_tracker.unregister(shm_seg._name, \"shared_memory\")\n\n"
}
{
    "Id": 73566474,
    "PostTypeId": 1,
    "Title": "Unable to locate package python-openssl",
    "Body": "I'm trying to install Pyenv, and I'm running on Ubuntu 22.04 LTS. but whenever I run this command\nsudo apt install -y make build-essential libssl-dev zlib1g-dev \\ libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev \\ libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python-openssl \\ git\n\nI get this error\nUnable to locate package python-openssl\n\nI've tried searching for solutions online, but I think they have encountered it on older versions of Ubuntu and not on the latest version.\n",
    "AcceptedAnswerId": 73566675,
    "AcceptedAnswer": "Make sure your list of packages is updated (sudo apt update). Python openssl bindings are available in 22.04 in python3-openssl (link), so you can install it by running\nsudo apt install python3-openssl\n\n"
}
{
    "Id": 74405180,
    "PostTypeId": 1,
    "Title": "Why cpython exposes 'PyTuple_SetItem' as C-API if tuple is immutable by design?",
    "Body": "Tuple in python is immutable by design, so if we try to mutate a tuple object python emits following TypeError which make sense.\n>>> a = (1, 2, 3)\n>>> a[0] = 12\nTraceback (most recent call last):\n  File \"\", line 1, in \nTypeError: 'tuple' object does not support item assignment\n\nSo my question is, if tuple is immutable by design why cpython exposes PyTuple_SetItem as C-API?.\nFrom the documentation it's described as\n\nint PyTuple_SetItem(PyObject *p, Py_ssize_t pos, PyObject *o)\nInsert a reference to object o at position pos of the tuple pointed to\nby p. Return 0 on success. If pos is out of bounds, return -1 and set\nan IndexError exception.\n\nIsn't this statement exactly equal to tuple[index] = value in python layer?. If the goal was to create a tuple from collection of items we could have use PyTuple_Pack.\nAdditional note:\nAfter lot of trial and error with ctypes.pythonapi I managed to mutate tuple object using PyTuple_SetItem\nimport ctypes\n\nfrom ctypes import py_object\n\nmy_tuple = (1, 2, 3)\nnewObj = py_object(my_tuple)\n\nm = \"hello\"\n\n# I don't know why I need to Py_DecRef here. \n# Although to reproduce this in your system,  no of times you have \n# to do `Py_DecRef` depends on no of ref count of `newObj` in your system\nctypes.pythonapi.Py_DecRef(newObj)\nctypes.pythonapi.Py_DecRef(newObj)\nctypes.pythonapi.Py_DecRef(newObj)\n\nctypes.pythonapi.Py_IncRef(m)\n\n\n\nPyTuple_SetItem = ctypes.pythonapi.PyTuple_SetItem\nPyTuple_SetItem.argtypes = ctypes.py_object, ctypes.c_size_t, ctypes.py_object\n\nPyTuple_SetItem(newObj, 0, m)\nprint(my_tuple) # this will print `('hello', 2, 3)`\n\n",
    "AcceptedAnswerId": 74405544,
    "AcceptedAnswer": "Similarly, there is a PyTuple_Resize function with the warning\n\nBecause tuples are supposed to be immutable, this should only be used\nif there is only one reference to the object. Do not use this if the\ntuple may already be known to some other part of the code. The tuple\nwill always grow or shrink at the end. Think of this as destroying the\nold tuple and creating a new one, only more efficiently.\n\nLooking at the source, there is a guard on the function\nif (!PyTuple_Check(op) || Py_REFCNT(op) != 1) {\n    .... error ....\n\nSure enough, this is only allowed when there is only 1 reference to the tuple - that reference being the thing that thinks its a good idea to change it. So, a tuple is \"mostly immutable\" but C code can change it in limited circumstances to avoid the penalty of creating a new tuple.\n"
}
{
    "Id": 73711633,
    "PostTypeId": 1,
    "Title": "How to calculate and store results based upon the Matching Rows of two different Pandas Dataframes in Python",
    "Body": "I have three DataFrames which I am importing from Excel Files.\nThe dataframes are given below as HTML Tables,\nSeason Wise Record (this contains a Column Reward which is initialized with 0 initially)\n\r\n\r\nUnnamed: 0NameTeamPositionGames PlayedPassingCompletionsPassingYardsPassingTouchdownsRushingYardsRushingTouchdownsReceivingYardsReceptionsTouchdownsTypeSacksSoloTacklesTacklesForLossFumblesForcedDefensiveTouchdownsInterceptionsPassesDefendedReceivingTouchdownsReward0Tom BradyTAMQB17485531643812002OFFENSE0000000001Justin HerbertLACQB174435014383023003OFFENSE0100000002Matthew StaffordLARQB17404488641430000OFFENSE0000000003Patrick MahomesKANQB174364839373812002OFFENSE0100000004Derek CarrLVRQB174284804231080000OFFENSE0000000005Joe BurrowCINQB163664611341182002OFFENSE0100000006Dak PrescottDALQB164104449371461001OFFENSE0000000007Josh AllenBUFQB174094407367636006OFFENSE01000000088Ezekiel ElliottDALRB171401002102874712OFFENSE01000002089Marcus MariotaLVRQB10140871001OFFENSE00000000090Johnny HekkerLARQB1712000000OFFENSE00000000091Greg WardPHIQB17120009573OFFENSE00000003092Kendall HintonDENWR1611000175151OFFENSE01000001093Keenan AllenLACWR160000011381066OFFENSE01000006094Danny AmendolaHOUQB800000248243OFFENSE01000003095Cole BeasleyBUFWR1600000693821OFFENSE000000010\r\n\r\n\r\n\nGame Wise Record (I am only adding some  sample rows, there are 20k+ rows in it)\n\r\n\r\nIndexWeekNameTeamStarterInterceptionsPassesDefendedSacksSoloTacklesTacklesForLossFumblesForcedPassesCompletionsPassingYardsPassingTouchdownsPassingInterceptionsRushingYardsRushingTouchdownsReceptionsReceivingYardsReceivingTouchdowns01Jourdan LewisDAL112000000000000011Trevon DiggsDAL112010000000000021Anthony BrownDAL100060000000000031Jayron KearseDAL000050000000000041Micah ParsonsDAL101030000000000051Keanu NealDAL100030000000000061DeMarcus LawrenceDAL100040100000000071Jaylon SmithDAL000020000000000081Dorance Armstrong Jr.DAL000000000000000091Tarell BashamDAL000000000000000051755Patrick MahomesKAN1000000332722261000051765Darrel WilliamsKAN00000000000270318051775Tyreek HillKAN10000000000150763051785Clyde Edwards-HelaireKAN10000000000130111051795Jerick McKinnonKAN0000000000020213051805Michael BurtonKAN000000000002000051815Mecole HardmanKAN1000000000000976051825Travis KelceKAN10000000000006571\r\n\r\n\r\n\nAnd lastly, there's a Player Goals File (this is an Excel File containing Sheets for each of the position, I am only sharing for QB sheet, to keep the question short. IF needed, I can share the rest too)\n\r\n\r\nGoalGoal TypePCC RewardTargetMin ValueMax ValueGames RequiredStartedLevel 99 PCC Reward x4 (current series)TImes achievedPCC Rewarded Throw 300-399 ydsGame25PassingYards300399001008200 Throw 400-499 ydsGame50PassingYards4004990020052501000Throw 500+ ydsGame150PassingYards5009999900600 00Throw 2 TDsGame50Touchdowns220020094501800Throw 3 TDsGame75Touchdowns330030043001200Throw 4 TDsGame100Touchdowns44004002200800Throw 5+ TDsGame300Touchdowns510000001200 0030-39 CompletionsGame50PassingCompletions3039002005250100040+ CompletionsGame200PassingCompletions4099990080012008000 INTs (must have been designated starter)Game200PassingInterceptions00018007140056003500-3999 Passing YDsSeason500PassingYards35003999002000 004000-4999 Passing YDSSeason750PassingYards40004999003000 005000+ Passing YDSSeason1250PassingYards500099999005000 0030-39 Passing TDSSeason750PassingTouchdowns3039003000 0040-45 Passing TDSSeason1250PassingTouchdowns4049005000 0050+ Passing TDSSeason2000PassingTouchdowns5099999008000 00\r\n\r\n\r\n\nWhat I want to do is analyze the Records of the Season Wise Records and the Game Wise Records, and based upon the Goals given in the Player Goals File, I want to add the Reward for all the players.\nThis is player position dependent so I made the following function to calculate Rewards for all the players (for the Season Records only)\ndef calculatePointsSeason(target, min_value, games_played_condition, max_value, tier_position, player_position, reward, games_played):\n    if player_position in positions[tier_position]:\n        if games_played > games_played_condition:\n            if target >= min_value and target <= max_value:\n                return reward \n    return 0 \n\nSimilarly, I made this function to calculate Game wise Record,\ndef calculatePointsGame(target, min_value, max_value, tier_position, player_position, reward, started, started_condition):\n    if player_position in positions[tier_position]:\n        if started == started_condition:\n            if target >= min_value and target <= max_value:\n                return reward \n    return 0 \n\nFollowing is the function in which I am applying these two functions to calculate the Reward for each player,\nfor key, value in positions.items(): # Positions has a list of all the positions \n    for (idx, row) in rewards[key].iterrows(): # Rewards is a Dict containing Pandas Dataframes against each position\n        if row['Goal Type'] == 'Season':\n            df = df.copy(deep=True) # df contains the Season Wise Record Dataframe\n            df['Reward'] += df.apply(lambda x: calculatePointsSeason(x[row['Target']], row['Min Value'], row['Games Required'],\n                                                               row['Max Value'], key, x['Position'],\n                                                                row['PCC Reward'], x['Games Played']), axis=1)\n        else: # For Game wise points\n            for (i, main_player) in df.iterrows():\n                for (j, game_player) in data.iterrows(): # data contains the Game Wise Record dataframe\n                    if main_player['Name'] == game_player['Name']:\n                        main_player['Reward'] += calculatePointsGame(main_player[row['Target']], \n                                                                    row['Min Value'], row['Max Value'], \n                                                                    key, main_player['Position'], row['PCC Reward'], \n                                                                    game_player['Starter'], row['Started'])\n\nThis function works well for the Season Wise Records, but for the Game Wise, I couldn't come up with any Pandas way to do it (eliminating the need of iteration of two Dataframes). I want some way to,\n\nMatch the Rows given in the Game Wise Record file with the Season Wise Record file, based upon the Name attribute\n\nSend the Values from the Game Wise Record to the Custom Function and the Position of the player from the Season Wise Record (so that, only the specific reward is calculated for the player, e.g. if player is QB, so only QB Rewards will be match with him and etc. There are Excel Sheets for each position rewards)\n\nGet the Reward Value back and add it to the Reward in the Season Wise Record against that specific player record.\n\n\nI previously tried to do it by comparing the Name of the Player in the Season Wise Record with the Game Wise Record, but it didn't work. Is there any Pandas way to solve this issue? (where you don't have to iterate all the rows two times)\n",
    "AcceptedAnswerId": 73819986,
    "AcceptedAnswer": "I hope I understood correctly your intentions. To avoid double for loops, you need to use groupby() method and then apply the desired function to every row of the group; finally the aggregation function (sum()) should be applied to the group. Although you can use the Name as a key for grouping, I recommend to add PlayerID.\nThe approach needs little preparation:\ndata = data.join(\n    df.reset_index().set_index(['Name', 'Team'], drop=False)[['index','Position']],\n    on=['Name','Team'],\n    how='left'\n).rename({'index':'PlayerID'}, axis=1)\n\nWe add 2 columns to data DataFrame, namely Position and PlayerID which is the index of the first DataFrame df. We search for the ID checking Name and Team that still may cause a collision (when there 2 players with identical name in the same team).\nWhen it's done the last part of the code will be like this:\nfor key, value in positions.items(): # Positions has a list of all the positions \n    for (_, row) in rewards[key].iterrows(): # Rewards is a Dict containing Pandas Dataframes against each position\n        if row['Goal Type'] == 'Season':\n            if row['Target'] in df.columns:\n                df['Reward'] += df.apply(lambda x: calculatePointsSeason(\n                    x[row['Target']], row['Min Value'], row['Games Required'],\n                    row['Max Value'], key, x['Position'],\n                    row['PCC Reward'], x['Games Played']\n                ), axis=1)\n        else: # For Game wise points\n            if row['Target'] in data.columns: # I added these 2 checks because sometimes target is not presented in the columns which raises the error\n                df['Reward'] = df['Reward'].add(\n                    data.groupby('PlayerID').apply(\n                        lambda group: group.apply(lambda game_player: calculatePointsGame(\n                            game_player[row['Target']], \n                            row['Min Value'], row['Max Value'], \n                            key, game_player['Position'],\n                            row['PCC Reward'], \n                            game_player['Starter'],\n                            row['Started']\n                        ), axis=1).sum()\n                    ),\n                    fill_value=0\n                )\n\n"
}
{
    "Id": 73876937,
    "PostTypeId": 1,
    "Title": "What is the difference between keyword pass and ... in python?",
    "Body": "Is there any significant difference between the two Python keywords (...) and (pass) like in the examples\ndef tempFunction():\n    pass \n\nand\ndef tempFunction():\n    ...\n\nI should be aware of?\n",
    "AcceptedAnswerId": 73877007,
    "AcceptedAnswer": "The ... is the shorthand for the Ellipsis global object in python. Similar to None and NotImplemented it can be used as a marker value to indicate the absence of something.\nFor example:\nprint(...)\n# Prints \"Ellipsis\"\n\nIn this case, it has no effect. You could put any constant there and it would do the same. This is valid:\ndef function():\n    1\n\nOr\ndef function():\n    'this function does nothing'\n\nNote both do nothing and return None. Since there is no return keyword the value won't be returned.\npass explicitly does nothing, so it will have the same effect in this case too.\n"
}
{
    "Id": 73597456,
    "PostTypeId": 1,
    "Title": "What is the python-poetry config file after 1.2.0 release?",
    "Body": "I have been using python-poetry for over a year now. \nAfter poetry 1.2.0 release, I get such an info warning:\nConfiguration file exists at ~/Library/Application Support/pypoetry,\nreusing this directory.\n\nConsider moving configuration to ~/Library/Preferences/pypoetry,\nas support for the legacy directory will be removed in an upcoming release.\n\nBut in docs, it is still indicated for macOS: ~/Library/Application Support/pypoetry \nhttps://python-poetry.org/docs/configuration/\nMy question is that if ~/Library/Preferences/pypoetry is the latest decision what should I do for moving configuration to there? \nIs just copy-pasting enough?\n\n",
    "AcceptedAnswerId": 73632457,
    "AcceptedAnswer": "Looks like it is as simple as copy/pasting to the new directory.\nI got the same error after upgrading to Poetry 1.2.  So I created a pypoetry folder in the new Preferences directory, copy/pasted the config.toml to it, and just to be safe, I renamed the original folder to:\n~/Library/Application Support/pypoetry_bak\nAfter doing this and running poetry -V, the error is gone.\n"
}
{
    "Id": 74599713,
    "PostTypeId": 1,
    "Title": "Merge two dictionaries in python",
    "Body": "I'm trying to merge two dictionaries based on key value. However, I'm not able to achieve it. Below is the way I tried solving.\ndict1 = {4: [741, 114, 306, 70],\n         2: [77, 325, 505, 144],\n         3: [937, 339, 612, 100],\n         1: [52, 811, 1593, 350]}\ndict2 = {1: 'A', 2: 'B', 3: 'C', 4: 'D'}\n\nMy resultant dictionary should be\noutput = {'D': [741, 114, 306, 70],\n          'B': [77, 325, 505, 144],\n          'C': [937, 339, 612, 100],\n          'A': [52, 811, 1593, 350]}\n\nMy code\ndef mergeDictionary(dict_obj1, dict_obj2):\n    dict_obj3 = {**dict_obj1, **dict_obj2}\n    for key, value in dict_obj3.items():\n        if key in dict_obj1 and key in dict_obj2:\n               dict_obj3[key] = [value , dict_obj1[key]]\n    return dict_obj3\n\ndict_3 = mergeDictionary(dict1, dict2)\n\nBut I'm getting this as output\ndict_3={4: ['D', [741, 114, 306, 70]], 2: ['B', [77, 325, 505, 144]], 3: ['C', [937, 339, 612, 100]], 1: ['A', [52, 811, 1593, 350]]}\n\n",
    "AcceptedAnswerId": 74599751,
    "AcceptedAnswer": "Use a simple dictionary comprehension:\noutput = {dict2[k]: v for k,v in dict1.items()}\n\nOutput:\n{'D': [741, 114, 306, 70],\n 'B': [77, 325, 505, 144],\n 'C': [937, 339, 612, 100],\n 'A': [52, 811, 1593, 350]}\n\n"
}
{
    "Id": 73693104,
    "PostTypeId": 1,
    "Title": "Python 3.10.7 - ValueError: Exceeds the limit (4300) for integer string conversion",
    "Body": "\n>>> import sys\n>>> sys.set_int_max_str_digits(4300)  # Illustrative, this is the default.\n>>> _ = int('2' * 5432)\nTraceback (most recent call last):\n...\nValueError: Exceeds the limit (4300) for integer string conversion: value has 5432 digits.\n\n\nPython 3.10.7 introduced this breaking change for type conversion.\nDocumentation: Integer string conversion length limitation\nActually I don't understand why\n\nthis was introduced and\nwhere does the default value of 4300 come from? Sounds like an arbitrary number.\n\n",
    "AcceptedAnswerId": 73693178,
    "AcceptedAnswer": "See github issue CVE-2020-10735: Prevent DoS by large intstr conversions #95778:\n\nProblem\nA Denial Of Service (DoS) issue was identified in CPython\nbecause we use binary bignum\u2019s for our int implementation. A huge\ninteger will always consume a near-quadratic amount of CPU time in\nconversion to or from a base 10 (decimal) string with a large number\nof digits. No efficient algorithm exists to do otherwise.\nIt is quite common for Python code implementing network protocols and\ndata serialization to do int(untrusted_string_or_bytes_value) on input\nto get a numeric value, without having limited the input length or to\ndo log(\"processing thing id %s\", unknowingly_huge_integer) or any\nsimilar concept to convert an int to a string without first checking\nits magnitude. (http, json, xmlrpc, logging, loading large values into\ninteger via linear-time conversions such as hexadecimal stored in\nyaml, or anything computing larger values based on user controlled\ninputs\u2026 which then wind up attempting to output as decimal later on).\nAll of these can suffer a CPU consuming DoS in the face of untrusted\ndata.\nEveryone auditing all existing code for this, adding length guards,\nand maintaining that practice everywhere is not feasible nor is it\nwhat we deem the vast majority of our users want to do.\nThis issue has been reported to the Python Security Response Team\nmultiple times by a few different people since early 2020, most\nrecently a few weeks ago while I was in the middle of polishing up the\nPR so it\u2019d be ready before 3.11.0rc2.\nMitigation\nAfter discussion on the Python Security Response Team\nmailing list the conclusion was that we needed to limit the size of\ninteger to string conversions for non-linear time conversions\n(anything not a power-of-2 base) by default. And offer the ability to\nconfigure or disable this limit.\nThe Python Steering Council is aware of this change and accepts it as\nnecessary.\n\nFurther discussion can be found on the Python Core Developers Discuss thread Int/str conversions broken in latest Python bugfix releases.\nI found this comment by Steve Dower to be informative:\n\nOur apologies for the lack of transparency in the process here. The\nissue was first reported to a number of other security teams, and\nconverged in the Python Security Response Team where we agreed that\nthe correct fix was to modify the runtime.\nThe delay between report and fix is entirely our fault. The security\nteam is made up of volunteers, our availability isn\u2019t always reliable,\nand there\u2019s nobody \u201cin charge\u201d to coordinate work. We\u2019ve been\ndiscussing how to improve our processes. However, we did agree that\nthe potential for exploitation is high enough that we didn\u2019t want to\ndisclose the issue without a fix available and ready for use.\nWe did work through a number of alternative approaches, implementing\nmany of them. The code doing int(gigabyte_long_untrusted_string) could\nbe anywhere inside a json.load or HTTP header parser, and can run very\ndeep. Parsing libraries are everywhere, and tend to use int\nindiscriminately (though they usually handle ValueError already).\nExpecting every library to add a new argument to every int() call\nwould have led to thousands of vulnerabilities being filed, and made\nit impossible for users to ever trust that their systems could not be\nDoS\u2019d.\nWe agree it\u2019s a heavy hammer to do it in the core, but it\u2019s also the\nonly hammer that has a chance of giving users the confidence to keep\nrunning Python at the boundary of their apps.\nNow, I\u2019m personally inclined to agree that int->str conversions should\ndo something other than raise. I was outvoted because it would break\nround-tripping, which is a reasonable argument that I accepted. We can\nstill improve this over time and make it more usable. However, in most\ncases we saw, rendering an excessively long string isn\u2019t desirable\neither. That should be the opt-in behaviour.\nRaising an exception from str may prove to be too much, and could be\nreconsidered, but we don\u2019t see a feasible way to push out updates to\nevery user of int, so that will surely remain global.\n\n"
}
{
    "Id": 73708478,
    "PostTypeId": 1,
    "Title": "The git (or python) command requires the command line developer tools",
    "Body": "This knowledge post isn't a duplication of other similar ones, since it's related to 12/September/2022 Xcode update, which demands a different kind of solution\nI have come to my computer today and discovered that nothing runs on my terminal Every time I have opened my IDE (VS Code or PyCharm), it has given me this message in the start of the terminal.\nI saw so many solutions, which have said to uninstall pyenv and install python via brew, which was a terrible idea, because I need different python versions for different projects.\nAlso, people spoke a lot about symlinks, which as well did not make any sense, because everything was working until yesterday.\nFurthermore, overwriting .oh-my-zsh with a new built one did not make any difference.\n",
    "AcceptedAnswerId": 73709260,
    "AcceptedAnswer": "I was prompted to reinstall commandLine tools over and over when trying to accept the terms\nI FIXED this by opening xcode and confirming the new update information\n"
}
{
    "Id": 74605279,
    "PostTypeId": 1,
    "Title": "Python 3.11 worse optimized than 3.10?",
    "Body": "I run this simple loop with Python 3.10.7 and 3.11.0 on Windows 10.\nimport time\na = 'a'\n\nstart = time.time()\nfor _ in range(1000000):\n    a += 'a'\nend = time.time()\n\nprint(a[:5], (end-start) * 1000)\n\nThe older version executes in 187ms, Python 3.11 needs about 17000ms. Does 3.10 realize that only the first 5 chars of a are needed, whereas 3.11 executes the whole loop? I confirmed this performance difference on godbolt.\n",
    "AcceptedAnswerId": 74607850,
    "AcceptedAnswer": "TL;DR: you should not use such a loop in any performance critical code but ''.join instead. The inefficient execution appears to be related to a regression during the bytecode generation in CPython 3.11 (and missing optimizations during the evaluation of binary add operation on Unicode strings).\n\nGeneral guidelines\nThis is an antipattern. You should not write such a code if you want this to be fast. This is described in PEP-8:\n\nCode should be written in a way that does not disadvantage other implementations of Python (PyPy, Jython, IronPython, Cython, Psyco, and such). \nFor example, do not rely on CPython\u2019s efficient implementation of in-place string concatenation for statements in the form a += b or a = a + b. This optimization is fragile even in CPython (it only works for some types) and isn\u2019t present at all in implementations that don\u2019t use refcounting. In performance sensitive parts of the library, the ''.join() form should be used instead. This will ensure that concatenation occurs in linear time across various implementations.\n\nIndeed, other implementations like PyPy does not perform an efficient in-place string concatenation for example. A new bigger string is created for every iteration (since strings are immutable, the previous one may be referenced and PyPy does not use a reference counting but a garbage collector). This results in a quadratic runtime as opposed to a linear runtime in CPython (at least in past implementation).\n\nDeep Analysis\nI can reproduce the problem on Windows 10 between the embedded (64-bit x86-64) version of CPython 3.10.8 and the one of 3.11.0:\nTimings:\n - CPython 3.10.8:    146.4 ms\n - CPython 3.11.0:  15186.8 ms\n\nIt turns out the code has not particularly changed between CPython 3.10 and 3.11 when it comes to Unicode string appending. See for example PyUnicode_Append: 3.10 and 3.11.\nA low-level profiling analysis shows that nearly all the time is spent in one unnamed function call of another unnamed function called by PyUnicode_Concat (which is also left unmodified between CPython 3.10.8 and 3.11.0). This slow unnamed function contains a pretty small set of assembly instructions and nearly all the time is spent in one unique x86-64 assembly instruction: rep movsb byte ptr [rdi], byte ptr [rsi]. This instruction is basically meant to copy a buffer pointed by the rsi register to a buffer pointed by the rdi register (the processor copy rcx bytes for the source buffer to the destination buffer and decrement the rcx register for each byte until it reach 0). This information shows that the unnamed function is actually memcpy of the standard MSVC C runtime (ie. CRT) which appears to be called by _copy_characters itself called by _PyUnicode_FastCopyCharacters of PyUnicode_Concat (all the functions are still belonging to the same file). However, these CPython functions are still left unmodified between CPython 3.10.8 and 3.11.0. The non-negligible time spent in malloc/free (about 0.3 seconds) seems to indicate that a lot of new string objects are created -- certainly at least 1 per iteration -- matching with the call to PyUnicode_New in the code of PyUnicode_Concat. All of this indicates that a new bigger string is created and copied as specified above.\nThe thing is calling PyUnicode_Concat is certainly the root of the performance issue here and I think CPython 3.10.8 is faster because it certainly calls PyUnicode_Append instead. Both calls are directly performed by the main big interpreter evaluation loop and this loop is driven by the generated bytecode.\nIt turns out that the generated bytecode is different between the two version and it is the root of the performance issue. Indeed, CPython 3.10 generates an INPLACE_ADD bytecode instruction while CPython 3.11 generates a  BINARY_OP bytecode instruction. Here is the bytecode for the loops in the two versions:\nCPython 3.10 loop:\n\n        >>   28 FOR_ITER                 6 (to 42)\n             30 STORE_NAME               4 (_)\n  6          32 LOAD_NAME                1 (a)\n             34 LOAD_CONST               2 ('a')\n             36 INPLACE_ADD                             <----------\n             38 STORE_NAME               1 (a)\n             40 JUMP_ABSOLUTE           14 (to 28)\n\nCPython 3.11 loop:\n\n        >>   66 FOR_ITER                 7 (to 82)\n             68 STORE_NAME               4 (_)\n  6          70 LOAD_NAME                1 (a)\n             72 LOAD_CONST               2 ('a')\n             74 BINARY_OP               13 (+=)         <----------\n             78 STORE_NAME               1 (a)\n             80 JUMP_BACKWARD            8 (to 66)\n\nThis changes appears to come from this issue. The code of the main interpreter loop (see ceval.c) is different between the two CPython version. Here are the code executed by the two versions:\n        // In CPython 3.10.8\n        case TARGET(INPLACE_ADD): {\n            PyObject *right = POP();\n            PyObject *left = TOP();\n            PyObject *sum;\n            if (PyUnicode_CheckExact(left) && PyUnicode_CheckExact(right)) {\n                sum = unicode_concatenate(tstate, left, right, f, next_instr); // <-----\n                /* unicode_concatenate consumed the ref to left */\n            }\n            else {\n                sum = PyNumber_InPlaceAdd(left, right);\n                Py_DECREF(left);\n            }\n            Py_DECREF(right);\n            SET_TOP(sum);\n            if (sum == NULL)\n                goto error;\n            DISPATCH();\n        }\n\n//----------------------------------------------------------------------------\n\n        // In CPython 3.11.0\n        TARGET(BINARY_OP_ADD_UNICODE) {\n            assert(cframe.use_tracing == 0);\n            PyObject *left = SECOND();\n            PyObject *right = TOP();\n            DEOPT_IF(!PyUnicode_CheckExact(left), BINARY_OP);\n            DEOPT_IF(Py_TYPE(right) != Py_TYPE(left), BINARY_OP);\n            STAT_INC(BINARY_OP, hit);\n            PyObject *res = PyUnicode_Concat(left, right); // <-----\n            STACK_SHRINK(1);\n            SET_TOP(res);\n            _Py_DECREF_SPECIALIZED(left, _PyUnicode_ExactDealloc);\n            _Py_DECREF_SPECIALIZED(right, _PyUnicode_ExactDealloc);\n            if (TOP() == NULL) {\n                goto error;\n            }\n            JUMPBY(INLINE_CACHE_ENTRIES_BINARY_OP);\n            DISPATCH();\n        }\n\nNote that unicode_concatenate calls PyUnicode_Append (and do some reference counting checks before). In the end, CPython 3.10.8 calls PyUnicode_Append which is fast (in-place) and CPython 3.11.0 calls PyUnicode_Concat which is slow (out-of-place). It clearly looks like a regression to me.\nPeople in the comments reported having no performance issue on Linux. However, experimental tests shows a BINARY_OP instruction is also generated on Linux, and I cannot find so far any Linux-specific optimization regarding string concatenation. Thus, the difference between the platforms is pretty surprising.\n\nUpdate: towards a fix\nI have opened an issue about this available here. One should not that putting the code in a function is significantly faster due to the variable being local (as pointed out by @Dennis in the comments).\n\nRelated posts:\n\nHow slow is Python's string concatenation vs. str.join?\nPython string 'join' is faster (?) than '+', but what's wrong here?\nPython string concatenation in for-loop in-place?\n\n"
}
{
    "Id": 74454587,
    "PostTypeId": 1,
    "Title": "sentry sdk custom performance integration for python app",
    "Body": "Sentry can track performance for celery tasks and API endpoints\nhttps://docs.sentry.io/product/performance/\nI have custom script that are lunching by crone and do set of similar tasks\nI want to incorporated sentry_sdk into my script to get performance tracing of my tasks\nAny advise how to do it with\nhttps://getsentry.github.io/sentry-python/api.html#sentry_sdk.capture_event\n",
    "AcceptedAnswerId": 74481555,
    "AcceptedAnswer": "You don't need use capture_event\nI would suggest to use sentry_sdk.start_transaction instead. It also allows track your function performance.\nLook at my example\nfrom time import sleep\nfrom sentry_sdk import Hub, init, start_transaction\n\ninit(\n    dsn=\"dsn\",\n    traces_sample_rate=1.0,\n)\n\n\ndef sentry_trace(func):\n    def wrapper(*args, **kwargs):\n        transaction = Hub.current.scope.transaction\n        if transaction:\n            with transaction.start_child(op=func.__name__):\n                return func(*args, **kwargs)\n        else:\n            with start_transaction(op=func.__name__, name=func.__name__):\n                return func(*args, **kwargs)\n\n    return wrapper\n\n\n@sentry_trace\ndef b():\n    for i in range(1000):\n        print(i)\n\n\n@sentry_trace\ndef c():\n    sleep(2)\n    print(1)\n\n\n@sentry_trace\ndef a():\n    sleep(1)\n    b()\n    c()\n\n\nif __name__ == '__main__':\n    a()\n\nAfter starting this code you can see basic info of transaction a with childs b and c\n\n"
}
{
    "Id": 74660176,
    "PostTypeId": 1,
    "Title": "Using VisualStudio+ Python -- how to handle \"overriding stdlib module\" Pylance(reportShadowedImports) warning?",
    "Body": "When running ipynbs in VS Code, I've started noticing Pylance warnings on standard library imports. I am using a conda virtual environment, and I believe the warning is related to that. An example using the glob library reads:\n \"env\\Lib\\glob.py\" is overriding the stdlib \"glob\" modulePylance(reportShadowedImports)\nSo far my notebooks run as expected, but I am curious if this warning is indicative of poor layout or is just stating the obvious more of an \"FYI you are not using the base install of python\".\nI have turned off linting and the problem stills persists. And almost nothing returns from my searches of the error \"reportShadowedImports\".\n",
    "AcceptedAnswerId": 74675579,
    "AcceptedAnswer": "The reason you find nothing by searching is because this check has just been implemented recently (see Github). I ran into the same problem as you because code.py from Micropython/Circuitpython also overrides the module \"code\" in stdlib.\nThe solution is simple, though you then loose out on this specific check. Just add reportShadowedImports to your pyright config. For VS Code, that would be adding it to .vscode/settings.json:\n{\n  \"python.languageServer\": \"Pylance\",\n  [...]\n  \"python.analysis.diagnosticSeverityOverrides\": {\n      \"reportShadowedImports\": \"none\"\n  },\n  [...]\n}\n\n"
}
{
    "Id": 73888639,
    "PostTypeId": 1,
    "Title": "Why is this unpacking expression not allowed in python3.10?",
    "Body": "I used to unpack a long iterable expression like this:\nIn python 3.8.7:\n>>> _, a, (*_), c = [1,2,3,4,5,6]\n>>> a\n2\n>>> c\n6\n\nIn python 3.10.7:\n>>> _, a, (*_), c = [1,2,3,4,5,6]\n  File \"\", line 1\n    _, a, (*_), c = [1,2,3,4,5,6]\n           ^^\nSyntaxError: cannot use starred expression here\n\nI'm not sure which version of python between 3.8.7 and 3.10.7 introduced this backwards breaking behavior. What's the justification for this?\n",
    "AcceptedAnswerId": 73888752,
    "AcceptedAnswer": "There's an official discussion here. The most relevant quote I can find is:\n\n\nAlso the current behavior allows (*x), y = 1 assignment. If (*x) is to be totally disallowed, (*x), y = 1 should also be rejected.\n\n\n\nI agree.\n\nThe final \"I agree\" is from Guido van Rossum.\nThe rationale for rejecting (*x) was:\n\nHonestly this seems like a bug in 3.8 to me (if it indeed behaves like\nthis):\n>>> (*x), y (1, 2, 3)\n\nEvery time I mistakenly tried (*x) I really meant (*x,), so it's\nsurprising that (*x), y would be interpreted as (*x, y) rather than\nflagging (*x) as an error.\nPlease don't \"fix\" this even if it is a regression.\n\nAlso by Guido van Rossum. So it seems like (*x) was rejected because it looks too similar to unpacking into a singlet tuple.\n"
}
{
    "Id": 74583630,
    "PostTypeId": 1,
    "Title": "Why is Python saying modules are imported when they are not?",
    "Body": "Python 3.6.5\nUsing this answer as a guide, I attempted to see whether some modules, such as math were imported.\nBut Python tells me they are all imported when they are not.\n>>> import sys\n>>> 'math' in sys.modules\nTrue\n>>> 'math' not in sys.modules\nFalse\n>>> math.pi\nTraceback (most recent call last):\n  File \"\", line 1, in \nNameError: name 'math' is not defined\n>>> import math\n>>> 'math' in sys.modules\nTrue\n>>> math.pi\n3.141592653589793\n\n",
    "AcceptedAnswerId": 74583684,
    "AcceptedAnswer": "to explain this, let's define this function:\ndef import_math():\n    import math\n\nimport_math()\n\nthe above function will import the module math, but only in its local scope, anyone that tries to reference math outside of it will get a name error, because math is not defined in the global scope.\nany module that is imported is saved into sys.modules so a call to check\nimport_math()\nprint(\"math\" in sys.modules)\n\nwill print True, because sys.modules caches any module that is loaded anywhere, whether or not it was available in the global scope, a very simple way to define math in the global scope would then to\nimport_math()\nmath = sys.modules[\"math\"]\n\nwhich will convert it from being only in sys.modules to being in the global scope, this is just equivalent to\nimport math\n\nwhich defines a variable math in the global scope that points to the module math.\nnow if you want to see whether \"math\" exists in the global scope is to check if it is in the global scope directly.\nprint(\"math\" in globals())\nprint(\"math\" in locals())\n\nwhich will print false if \"math\" wasn't imported into the global or local scope and is therefore inaccessable.\n"
}
{
    "Id": 74798626,
    "PostTypeId": 1,
    "Title": "Why is log(inf + inf j) equal to (inf + 0.785398 j), In C++/Python/NumPy?",
    "Body": "I've been finding a strange behaviour of log functions in C++ and numpy about the behaviour of log function handling complex infinite numbers. Specifically, log(inf + inf * 1j) equals (inf + 0.785398j) when I expect it to be (inf + nan * 1j).\nWhen taking the log of a complex number, the real part is the log of the absolute value of the input and the imaginary part is the phase of the input. Returning 0.785398 as the imaginary part of log(inf + inf * 1j) means it assumes the infs in the real and the imaginary part have the same length.\nThis assumption does not seem to be consistent with other calculation, for example, inf - inf == nan, inf / inf == nan which assumes 2 infs do not necessarily have the same values.\nWhy is the assumption for log(inf + inf * 1j) different?\nReproducing C++ code:\n#include \n#include \n#include \nint main() {\n    double inf = std::numeric_limits::infinity();\n    std::complex b(inf, inf);\n    std::complex c = std::log(b);\n    std::cout << c << \"\\n\";\n}\n\nReproducing Python code (numpy):\nimport numpy as np\n\na = complex(float('inf'), float('inf'))\nprint(np.log(a))\n\nEDIT: Thank you for everyone who's involved in the discussion about the historical reason and the mathematical reason. All of you turn this naive question into a really interesting discussion. The provided answers are all of high quality and I wish I can accept more than 1 answers. However, I've decided to accept @simon's answer as it explains in more detail the mathematical reason and provided a link to the document explaining the logic (although I can't fully understand it).\n",
    "AcceptedAnswerId": 74799453,
    "AcceptedAnswer": "The value of 0.785398 (actually pi/4) is consistent with at least some other functions: as you said, the imaginary part of the logarithm of a complex number is identical with the phase angle of the number. This can be reformulated to a question of its own: what is the phase angle of inf + j * inf?\nWe can calculate the phase angle of a complex number z by atan2(Im(z), Re(z)). With the given number, this boils down to calculating atan2(inf, inf), which is also 0.785398 (or pi/4), both for Numpy and C/C++. So now a similar question could be asked: why is atan2(inf, inf) == 0.785398?\nI do not have an answer to the latter (except for \"the C/C++ specifications say so\", as others already answered), I only have a guess: as atan2(y, x) == atan(y / x) for x > 0, probably someone made the decision in this context to not interpret inf / inf as \"undefined\" but instead as \"a very large number divided by the same very large number\". The result of this ratio would be 1, and atan(1) == pi/4 by the mathematical definition of atan.\nProbably this is not a satisfying answer, but at least I could hopefully show that the log definition in the given edge case is not completely inconsistent with similar edge cases of related function definitions.\nEdit: As I said, consistent with some other functions: it is also consistent with np.angle(complex(np.inf, np.inf)) == 0.785398, for example.\nEdit 2: Looking at the source code of an actual atan2 implementation brought up the following code comment:\n\nnote that the non obvious cases are y and x both infinite or both zero. for more information, see Branch Cuts for Complex Elementary Functions, or Much Ado About Nothing's Sign Bit, by W. Kahan\n\nI dug up the referenced document, you can find a copy here. In Chapter 8 of this reference, called \"Complex zeros and infinities\", William Kahan (who is both mathematician and computer scientist and, according to Wikipedia, the \"Father of Floating Point\") covers the zero and infinity edge cases of complex numbers and arrives at pi/4 for feeding inf + j * inf into the arg function (arg being the function that calculates the phase angle of a complex number, just like np.angle above). You will find this result on page 17 in the linked PDF. I am not mathematician enough for being able to summarize Kahan's rationale (which is to say: I don't really understand it), but maybe someone else can.\n"
}
{
    "Id": 73902642,
    "PostTypeId": 1,
    "Title": "Office 365 IMAP authentication via OAuth2 and python MSAL library",
    "Body": "I'm trying to upgrade a legacy mail bot to authenticate via Oauth2 instead of Basic authentication, as it's now deprecated two days from now.\nThe document states applications can retain their original logic, while swapping out only the authentication bit\n\nApplication developers who have built apps that send, read, or\notherwise process email using these protocols will be able to keep the\nsame protocol, but need to implement secure, Modern authentication\nexperiences for their users. This functionality is built on top of\nMicrosoft Identity platform v2.0 and supports access to Microsoft 365\nemail accounts.\n\nNote I've explicitly chosen the client credentials flow, because the documentation states\n\nThis type of grant is commonly used for server-to-server interactions\nthat must run in the background, without immediate interaction with a\nuser.\n\nSo I've got a python script that retrieves an Access Token using the MSAL python library. Now I'm trying to authenticate with the IMAP server, using that Access Token. There's some existing threads out there showing how to connect to Google, I imagine my case is pretty close to this one, except I'm connecting to a Office 365 IMAP server. Here's my script\nimport imaplib\nimport msal\nimport logging\n\napp = msal.ConfidentialClientApplication(\n    'client-id',\n    authority='https://login.microsoftonline.com/tenant-id',\n    client_credential='secret-key'\n)\n\nresult = app.acquire_token_for_client(scopes=['https://graph.microsoft.com/.default'])\n\ndef generate_auth_string(user, token):\n  return 'user=%s\\1auth=Bearer %s\\1\\1' % (user, token)\n\n# IMAP time!\nmailserver = 'outlook.office365.com'\nimapport = 993\nM = imaplib.IMAP4_SSL(mailserver,imapport)\nM.debug = 4\nM.authenticate('XOAUTH2', lambda x: generate_auth_string('user@mydomain.com', result['access_token']))\n\nprint(result)\n\nThe IMAP authentication is failing and despite setting M.debug = 4, the output isn't very helpful\n  22:56.53 > b'DBDH1 AUTHENTICATE XOAUTH2'\n  22:56.53 < b'+ '\n  22:56.53 write literal size 2048\n  22:57.84 < b'DBDH1 NO AUTHENTICATE failed.'\n  22:57.84 NO response: b'AUTHENTICATE failed.'\nTraceback (most recent call last):\n  File \"/home/ubuntu/mini-oauth.py\", line 21, in \n    M.authenticate(\"XOAUTH2\", lambda x: generate_auth_string('user@mydomain.com', result['access_token']))\n  File \"/usr/lib/python3.10/imaplib.py\", line 444, in authenticate\n    raise self.error(dat[-1].decode('utf-8', 'replace'))\nimaplib.IMAP4.error: AUTHENTICATE failed.\n\nAny idea where I might be going wrong, or how to get more robust information from the IMAP server about why the authentication is failing?\nThings I've looked at\n\nNote this answer no longer works as the suggested scopes fail to generate an Access Token.\n\nThe client credentials flow seems to mandate the https://graph.microsoft.com/.default grant. I'm not sure if that includes the scope required for the IMAP resource\nhttps://outlook.office.com/IMAP.AccessAsUser.All?\n\nVerified the code lifted from the Google thread produces the SASL XOAUTH2 string correctly, per example on the MS docs\n\n\nimport base64\n\nuser = 'test@contoso.onmicrosoft.com'\ntoken = 'EwBAAl3BAAUFFpUAo7J3Ve0bjLBWZWCclRC3EoAA'\n\nxoauth = \"user=%s\\1auth=Bearer %s\\1\\1\" % (user, token)\n\nxoauth = xoauth.encode('ascii')\nxoauth = base64.b64encode(xoauth)\nxoauth = xoauth.decode('ascii')\n\nxsanity = 'dXNlcj10ZXN0QGNvbnRvc28ub25taWNyb3NvZnQuY29tAWF1dGg9QmVhcmVyIEV3QkFBbDNCQUFVRkZwVUFvN0ozVmUwYmpMQldaV0NjbFJDM0VvQUEBAQ=='\n\nprint(xoauth == xsanity) # prints True\n\n\nThis thread seems to suggest multiple tokens need to be fetched, one for graph, then another for the IMAP connection; could that be what I'm missing?\n\n",
    "AcceptedAnswerId": 74131277,
    "AcceptedAnswer": "The imaplib.IMAP4.error: AUTHENTICATE failed Error occured because one point in the documentation is not that clear.\nWhen setting up the the Service Principal via Powershell you need to enter the App-ID and an Object-ID. Many people will think, it is the Object-ID you see on the overview page of the registered App, but its not!\nAt this point you need the Object-ID from \"Azure Active Directory -> Enterprise Applications --> Your-App --> Object-ID\"\nNew-ServicePrincipal -AppId  -ServiceId  [-Organization ]\n\nMicrosoft says:\n\nThe OBJECT_ID is the Object ID from the Overview page of the\nEnterprise Application node (Azure Portal) for the application\nregistration. It is not the Object ID from the Overview of the App\nRegistrations node. Using the incorrect Object ID will cause an\nauthentication failure.\n\nOfcourse you need to take care for the API-permissions and the other stuff, but this was for me the point.\nSo lets go trough it again, like it is explained on the documentation page.\nAuthenticate an IMAP, POP or SMTP connection using OAuth\n\nRegister the Application in your Tenant\nSetup a Client-Key for the application\nSetup the API permissions, select the APIs my organization uses tab and search for \"Office 365 Exchange Online\" -> Application permissions -> Choose IMAP and IMAP.AccessAsApp\nSetup the Service Principal and full access for your Application on the mailbox\nCheck if IMAP is activated for the mailbox\n\nThats the code I use to test it:\nimport imaplib\nimport msal\nimport pprint\n\nconf = {\n    \"authority\": \"https://login.microsoftonline.com/XXXXyourtenantIDXXXXX\",\n    \"client_id\": \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXXX\", #AppID\n    \"scope\": ['https://outlook.office365.com/.default'],\n    \"secret\": \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\", #Key-Value\n    \"secret-id\": \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\", #Key-ID\n}\n    \ndef generate_auth_string(user, token):\n    return f\"user={user}\\x01auth=Bearer {token}\\x01\\x01\"    \n\nif __name__ == \"__main__\":\n    app = msal.ConfidentialClientApplication(conf['client_id'], authority=conf['authority'],\n                                             client_credential=conf['secret'])\n\n    result = app.acquire_token_silent(conf['scope'], account=None)\n\n    if not result:\n        print(\"No suitable token in cache.  Get new one.\")\n        result = app.acquire_token_for_client(scopes=conf['scope'])\n\n    if \"access_token\" in result:\n        print(result['token_type'])\n        pprint.pprint(result)\n    else:\n        print(result.get(\"error\"))\n        print(result.get(\"error_description\"))\n        print(result.get(\"correlation_id\"))\n        \n    imap = imaplib.IMAP4('outlook.office365.com')\n    imap.starttls()\n    imap.authenticate(\"XOAUTH2\", lambda x: generate_auth_string(\"target_mailbox@example.com\", result['access_token']).encode(\"utf-8\"))\n\nAfter setting up the Service Principal and giving the App full access on the mailbox, wait 15 - 30 minutes for the changes to take effect and test it.\n"
}
{
    "Id": 74206978,
    "PostTypeId": 1,
    "Title": "Why does this specific code run faster in Python 3.11?",
    "Body": "I have the following code in a Python file called benchmark.py.\nsource = \"\"\"\nfor i in range(1000):\n    a = len(str(i)) \n\"\"\"\n\nimport timeit\n\nprint(timeit.timeit(stmt=source, number=100000))\n\nWhen I tried to run with multiple python versions I am seeing a drastic performance difference.\nC:\\Users\\Username\\Desktop>py -3.10 benchmark.py\n16.79652149998583\n\nC:\\Users\\Username\\Desktop>py -3.11 benchmark.py\n10.92280820000451\n\nAs you can see this code runs faster with python 3.11 than previous Python versions. I tried to disassemble the bytecode to understand the reason for this behaviour but I could only see a difference in opcode names (CALL_FUNCTION is replaced by PRECALL and CALL opcodes).\nI am quite not sure if that's the reason for this performance change. so I am looking for an answer that justifies with reference to cpython\nsource code.\npython 3.11 bytecode\n  0           0 RESUME                   0\n\n  2           2 PUSH_NULL\n              4 LOAD_NAME                0 (range)\n              6 LOAD_CONST               0 (1000)\n              8 PRECALL                  1\n             12 CALL                     1\n             22 GET_ITER\n        >>   24 FOR_ITER                22 (to 70)\n             26 STORE_NAME               1 (i)\n\n  3          28 PUSH_NULL\n             30 LOAD_NAME                2 (len)\n             32 PUSH_NULL\n             34 LOAD_NAME                3 (str)\n             36 LOAD_NAME                1 (i)\n             38 PRECALL                  1\n             42 CALL                     1\n             52 PRECALL                  1\n             56 CALL                     1\n             66 STORE_NAME               4 (a)\n             68 JUMP_BACKWARD           23 (to 24)\n\n  2     >>   70 LOAD_CONST               1 (None)\n             72 RETURN_VALUE\n\npython 3.10 bytecode\n  2           0 LOAD_NAME                0 (range)\n              2 LOAD_CONST               0 (1000)\n              4 CALL_FUNCTION            1\n              6 GET_ITER\n        >>    8 FOR_ITER                 8 (to 26)\n             10 STORE_NAME               1 (i)\n\n  3          12 LOAD_NAME                2 (len)\n             14 LOAD_NAME                3 (str)\n             16 LOAD_NAME                1 (i)\n             18 CALL_FUNCTION            1\n             20 CALL_FUNCTION            1\n             22 STORE_NAME               4 (a)\n             24 JUMP_ABSOLUTE            4 (to 8)\n\n  2     >>   26 LOAD_CONST               1 (None)\n             28 RETURN_VALUE\n\nPS: I understand that python 3.11 introduced bunch of performance improvements but I am curios to understand what optimization makes this code run faster in python 3.11\n",
    "AcceptedAnswerId": 74220032,
    "AcceptedAnswer": "There's a big section in the \"what's new\" page labeled \"faster runtime\". It looks like the most likely cause of the speedup here is PEP 659, which is a first start towards JIT optimization (perhaps not quite JIT compilation, but definitely JIT optimization).\nParticularly, the lookup and call for len and str now bypass a lot of dynamic machinery in the overwhelmingly common case where the built-ins aren't shadowed or overridden. The global and builtin dict lookups to resolve the name get skipped in a fast path, and the underlying C routines for len and str are called directly, instead of going through the general-purpose function call handling.\nYou wanted source references, so here's one. The str call will get specialized in specialize_class_call:\n    if (tp->tp_flags & Py_TPFLAGS_IMMUTABLETYPE) {\n        if (nargs == 1 && kwnames == NULL && oparg == 1) {\n            if (tp == &PyUnicode_Type) {\n                _Py_SET_OPCODE(*instr, PRECALL_NO_KW_STR_1);\n                return 0;\n            }\n\nwhere it detects that the call is a call to the str builtin with 1 positional argument and no keywords, and replaces the corresponding PRECALL opcode with PRECALL_NO_KW_STR_1. The handling for the PRECALL_NO_KW_STR_1 opcode in the bytecode evaluation loop looks like this:\n        TARGET(PRECALL_NO_KW_STR_1) {\n            assert(call_shape.kwnames == NULL);\n            assert(cframe.use_tracing == 0);\n            assert(oparg == 1);\n            DEOPT_IF(is_method(stack_pointer, 1), PRECALL);\n            PyObject *callable = PEEK(2);\n            DEOPT_IF(callable != (PyObject *)&PyUnicode_Type, PRECALL);\n            STAT_INC(PRECALL, hit);\n            SKIP_CALL();\n            PyObject *arg = TOP();\n            PyObject *res = PyObject_Str(arg);\n            Py_DECREF(arg);\n            Py_DECREF(&PyUnicode_Type);\n            STACK_SHRINK(2);\n            SET_TOP(res);\n            if (res == NULL) {\n                goto error;\n            }\n            CHECK_EVAL_BREAKER();\n            DISPATCH();\n        }\n\nwhich consists mostly of a bunch of safety prechecks and reference fiddling wrapped around a call to PyObject_Str, the C routine for calling str on an object.\nPython 3.11 includes many other performance enhancements besides the above, including optimizations to stack frame creation, method lookup, common arithmetic operations, interpreter startup, and more. Most code should run much faster now, barring things like I/O-bound workloads and code that spent most of its time in C library code (like NumPy).\n"
}
{
    "Id": 74307236,
    "PostTypeId": 1,
    "Title": "Python: Why do functools.partial functions not become bound methods when set as class attributes?",
    "Body": "I was reading about how functions become bound methods when being set as class atrributes. I then observed that this is not the case for functions that are wrapped by functools.partial. What is the explanation for this?\nSimple example:\nfrom functools import partial\n\ndef func1():\n    print(\"foo\")\n\nfunc1_partial = partial(func1)\n\nclass A:\n    f = func1\n    g = func1_partial\n\na = A()\n\n\na.f() # TypeError: func1() takes 0 positional arguments but 1 was given\n\na.g() # prints \"foo\"\n\n\nI kind of expected them both to behave in the same way.\n",
    "AcceptedAnswerId": 74307329,
    "AcceptedAnswer": "The trick that allows functions to become bound methods is the __get__ magic method.\nTo very briefly summarize that page, when you access a field on an instance, say foo.bar, Python first checks whether bar exists in foo's __dict__ (or __slots__, if it has one). If it does, we return it, no harm done. If not, then we look on type(foo). However, when we access the field Foo.bar on the class Foo through an instance, something magical happens. When we write foo.bar, assuming there is no bar on foo's __dict__ (resp. __slots__), then we actually call Foo.bar.__get__(foo, Foo). That is, Python calls a magic method asking the object how it would like to be retrieved.\nThis is how properties are implemented, and it's also how bound methods are implemented. Somewhere deep down (probably written in C), there's a __get__ function on the type function that binds the method when accessed through an instance.\nfunctools.partial, despite looking a lot like a function, is not an instance of the type function. It's just a random class that happens to implement __call__, and it doesn't implement __get__. Why doesn't it? Well, they probably just didn't think it was worth it, or it's possible nobody even considered it. Regardless, the \"bound method\" trick applies to the type called function, not to all callable objects.\n\nAnother useful resource on magic methods, and __get__ in particular: https://rszalski.github.io/magicmethods/#descriptor\n"
}
{
    "Id": 74500614,
    "PostTypeId": 1,
    "Title": "Python Decimal - multiplication by zero",
    "Body": "Why does the following code:\nfrom decimal import Decimal\nresult = Decimal('0') * Decimal('0.8881783462119193534061639577')\nprint(result)\n\nreturn 0E-28 ?\nI've traced it to the following code in the module:\nif not self or not other:\n    ans = _dec_from_triple(resultsign, '0', resultexp)\n    # Fixing in case the exponent is out of bounds\n    ans = ans._fix(context)\n    return ans\n\nThe code appears to follow Decimal Arithmetic Specification, which doesn't explicitly suggest what to do when we multiply by zero, referring to 'special numbers' from another standard, which also doesn't specify what we do when we multiply an integer by zero  :)\nSo the decimal library does the thing that is explicitly specified:\n\nThe coefficient of the result, before rounding, is computed by multiplying together the coefficients of the operands.\nThe exponent of the result, before rounding, is the sum of the exponents of the two operands.\nThe sign of the result is the exclusive or of the signs of the operands.\n\nQuestion: what is the need to return the coefficient and exponent (i.e, 0E-28) if one of the operands is a zero? We already know what that coefficient is when calling the multiplication function. Why not just return zero?\n",
    "AcceptedAnswerId": 74515870,
    "AcceptedAnswer": "Raymond Hettinger has given a comprehensive explanation at cpython github:\nIn Arithmetic Operations, the section on Arithmetic operations rules tells us:\n\nTrailing zeros are not removed after operations.\n\nThere are test cases covering multiplication by zero. Here are some from multiply.decTest:\n-- zeros, etc.\nmulx021 multiply  0      0     ->  0\nmulx022 multiply  0     -0     -> -0\nmulx023 multiply -0      0     -> -0\nmulx024 multiply -0     -0     ->  0\nmulx025 multiply -0.0   -0.0   ->  0.00\nmulx026 multiply -0.0   -0.0   ->  0.00\nmulx027 multiply -0.0   -0.0   ->  0.00\nmulx028 multiply -0.0   -0.0   ->  0.00\nmulx030 multiply  5.00   1E-3  ->  0.00500\nmulx031 multiply  00.00  0.000 ->  0.00000\nmulx032 multiply  00.00  0E-3  ->  0.00000     -- rhs is 0\nmulx033 multiply  0E-3   00.00 ->  0.00000     -- lhs is 0\nmulx034 multiply -5.00   1E-3  -> -0.00500\nmulx035 multiply -00.00  0.000 -> -0.00000\nmulx036 multiply -00.00  0E-3  -> -0.00000     -- rhs is 0\nmulx037 multiply -0E-3   00.00 -> -0.00000     -- lhs is 0\nmulx038 multiply  5.00  -1E-3  -> -0.00500\nmulx039 multiply  00.00 -0.000 -> -0.00000\nmulx040 multiply  00.00 -0E-3  -> -0.00000     -- rhs is 0\nmulx041 multiply  0E-3  -00.00 -> -0.00000     -- lhs is 0\nmulx042 multiply -5.00  -1E-3  ->  0.00500\nmulx043 multiply -00.00 -0.000 ->  0.00000\nmulx044 multiply -00.00 -0E-3  ->  0.00000     -- rhs is 0\nmulx045 multiply -0E-3  -00.00 ->  0.00000     -- lhs is 0\n\nAnd this from the examples:\nmulx053 multiply 0.9 -0 -> -0.0\n\nIn the Summary of Arithmetic section, the motivation is explained at a high level:\n\nThe arithmetic was designed as a decimal extended floating-point arithmetic, directly implementing the rules that people are taught at\nschool. Up to a given working precision, exact unrounded results are\ngiven when possible (for instance, 0.9 \u00f7 10 gives 0.09, not\n0.089999996), and trailing zeros are correctly preserved in most operations (1.23 + 1.27 gives 2.50, not 2.5). Where results would\nexceed the working precision, floating-point rules apply.\n\nMore detail in given in the FAQ section Why are trailing fractional zeros important?.\n"
}
{
    "Id": 71248521,
    "PostTypeId": 1,
    "Title": "Why \" NumExpr defaulting to 8 threads. \" warning message shown in python?",
    "Body": "I am trying to use the lux library in python to get visualization recommendations. It shows warnings like NumExpr defaulting to 8 threads..\nimport pandas as pd\nimport numpy as np\nimport opendatasets as od\npip install lux-api\nimport lux\nimport matplotlib\n\nAnd then:\nlink = \"https://www.kaggle.com/noordeen/insurance-premium-prediction\"\nod.download(link) \ndf = pd.read_csv(\"./insurance-premium-prediction/insurance.csv\")\n\nBut, everything is working fine. Is there any problem or should I ignore it?\nWarning shows like this:\n\n",
    "AcceptedAnswerId": 74656206,
    "AcceptedAnswer": "This is not really something to worry about in most cases. The warning comes from this function, here the most important part:\n...\n    env_configured = False\n    n_cores = detect_number_of_cores()\n    if 'NUMEXPR_MAX_THREADS' in os.environ:\n        # The user has configured NumExpr in the expected way, so suppress logs.\n        env_configured = True\n        n_cores = MAX_THREADS\n...\n    if 'NUMEXPR_NUM_THREADS' in os.environ:\n        requested_threads = int(os.environ['NUMEXPR_NUM_THREADS'])\n    elif 'OMP_NUM_THREADS' in os.environ:\n        requested_threads = int(os.environ['OMP_NUM_THREADS'])\n    else:\n        requested_threads = n_cores\n        if not env_configured:\n            log.info('NumExpr defaulting to %d threads.'%n_cores)\n\nSo if neither NUMEXPR_MAX_THREADS nor NUMEXPR_NUM_THREADS nor OMP_NUM_THREADS are set, NumExpr uses so many threads as there are cores (even if the documentation says \"at most 8\", yet this is not what I see in the code).\nYou might want to use another number of threads, e.g. while really huge matrices are calculated and one could profit from it or to use less threads, because there is no improvement. Set the environment variables either in the shell or prior to importing numexpr, e.g.\nimport os\nos.environ['NUMEXPR_MAX_THREADS'] = '4'\nos.environ['NUMEXPR_NUM_THREADS'] = '2'\nimport numexpr as ne \n\n"
}
{
    "Id": 74717007,
    "PostTypeId": 1,
    "Title": "Why does a python function work in parallel even if it should not?",
    "Body": "I am running this code using the healpy package. I am not using multiprocessing and I need it to run on a single core. It worked for a certain amount of time, but, when I run it now, the function healpy.projector.GnomonicProj.projmap takes all the available cores.\nThis is the incriminated code block:\ndef Stacking () :\n\n    f = lambda x,y,z: pixelfunc.vec2pix(xsize,x,y,z,nest=False)\n    map_array = pixelfunc.ma_to_array(data)\n    im = np.zeros((xsize, xsize))\n    plt.figure()\n\n    for i in range (nvoids) :\n        sys.stdout.write(\"\\r\" + str(i+1) + \"/\" + str(nvoids))\n        sys.stdout.flush()\n        proj = hp.projector.GnomonicProj(rot=[rav[i],decv[i]], xsize=xsize, reso=2*nRad*rad_deg[i]*60/(xsize))\n        im += proj.projmap(map_array, f)\n\n    im/=nvoids\n    plt.imshow(im)\n    plt.colorbar()\n    plt.title(title + \" (Map)\")\n    plt.savefig(\"../Plots/stackedMap_\"+name+\".png\")\n\n    return im\n\nDoes someone know why this function is running in parallel? And most important, does someone know a way to run it in a single core?\nThank you!\n",
    "AcceptedAnswerId": 74717228,
    "AcceptedAnswer": "In this thread they recommend to set the environment variable OMP_NUM_THREADS accordingly:\n\nWorked with:\nimport os\nos.environ['OMP_NUM_THREADS'] = '1'\nimport healpy as hp\nimport numpy as np\n\nos.environ['OMP_NUM_THREADS'] = '1' have to be done before import numpy and healpy libraries.\n\nAs to the why: probably they use some parallelization techniques wrapped within their implementation of the functions you use. According to the name of the variable, I would guess OpenMP it is.\n"
}
{
    "Id": 74717893,
    "PostTypeId": 1,
    "Title": "How to efficiently search for similar substring in a large text python?",
    "Body": "Let me try to explain my issue with an example, I have a large corpus and a substring like below,\ncorpus = \"\"\"very quick service, polite workers(cory, i think that's his name), i basically just drove there and got a quote(which seems to be very fair priced), then dropped off my car 4 days later(because they were fully booked until then), then i dropped off my car on my appointment day, then the same day the shop called me and notified me that the the job is done i can go pickup my car. when i go checked out my car i was amazed by the job they've done to it, and they even gave that dirty car a wash( prob even waxed it or coated it, cuz it was shiny as hell), tires shine, mats were vacuumed too. i gave them a dirty, broken car, they gave me back a what seems like a brand new car. i'm happy with the result, and i will def have all my car's work done by this place from now.\"\"\"\n\nsubstring = \"\"\"until then then i dropped off my car on my appointment day then the same day the shop called me and notified me that the the job is done i can go pickup my car when i go checked out my car i was amazed by the job they ve done to it and they even gave that dirty car a wash prob even waxed it or coated it cuz it was shiny as hell tires shine mats were vacuumed too i gave them a dirty broken car they gave me back a what seems like a brand new car i m happy with the result and i will def have all my car s work done by this place from now\"\"\"\n\nBoth the substring and corpus are very similar but it not exact,\nIf I do something like,\nimport re\nre.search(substring, corpus, flags=re.I) # this will fail substring is not exact but rather very similar\n\nIn the corpus the substring is like below which is bit different from the substring I have because of that regular expression search is failing, can someone suggest a really good alternative for similar substring lookup,\nuntil then), then i dropped off my car on my appointment day, then the same day the shop called me and notified me that the the job is done i can go pickup my car. when i go checked out my car i was amazed by the job they've done to it, and they even gave that dirty car a wash( prob even waxed it or coated it, cuz it was shiny as hell), tires shine, mats were vacuumed too. i gave them a dirty, broken car, they gave me back a what seems like a brand new car. i'm happy with the result, and i will def have all my car's work done by this place from now\n\nI did try difflib library but it was not satisfying my use-case.\nSome background information,\nThe substring I have right now, is obtained some time ago from pre-processed corpus using this regex re.sub(\"[^a-zA-Z]\", \" \", corpus).\nBut now I need to use that substring I have to do the reverse lookup in the corpus text and find the start and ending index in the corpus.\n",
    "AcceptedAnswerId": 74719826,
    "AcceptedAnswer": "You don't actually need to fuzzy match all that much, at least for the example given; text can only change in spaces within substring, and it can only change by adding at least one non-alphabetic character (which can replace a space, but the space can't be deleted without a replacement). This means you can construct a regex directly  from substring with wildcards between words, search (or finditer) the corpus for it, and the resulting match object will tell you where the match(es) begin and end:\nimport re\n\n# Allow any character between whitespace-separated \"words\" except ASCII\n# alphabetic characters\nssre = re.compile(r'[^a-z]+'.join(substring.split()), re.IGNORECASE)\n\nif m := ssre.search(corpus):\n    print(m.start(), m.end())\n\n    print(repr(m.group(0)))\n\nTry it online!\nwhich correctly identifies where the match began (index 217) and ended (index 771) in corpus; .group(0) can directly extract the matching text for you if you prefer (it's uncommon to need the indices, so there's a decent chance you were asking for them solely to extract the real text, and .group(0) does that directly). The output is:\n217 771\n\"until then), then i dropped off my car on my appointment day, then the same day the shop called me and notified me that the the job is done i can go pickup my car. when i go checked out my car i was amazed by the job they've done to it, and they even gave that dirty car a wash( prob even waxed it or coated it, cuz it was shiny as hell), tires shine, mats were vacuumed too. i gave them a dirty, broken car, they gave me back a what seems like a brand new car. i'm happy with the result, and i will def have all my car's work done by this place from now\"\n\nIf spaces might be deleted without being replaced, just change the + quantifier to * (the regex will run a little slower since it can't short-circuit as easily, but would still work, and should run fast enough).\nIf you need to handle non-ASCII alphabetic characters, the regex joiner can change from r'[^a-z]+' to the equivalent r'[\\W\\d_]+' (which means \"match all non-word characters [non-alphanumeric and not underscore], plus numeric characters and underscores\"); it's a little more awkward to read, but it handles stuff like \u00e9 properly (treating it as part of a word, not a connector character).\nWhile it's not going to be as flexible as difflib, when you know no words are removed or added, it's just a matter of spacing and punctuation, this works perfectly, and should run significantly faster than a true fuzzy matching solution (that has to do far more work to handle the concept of close matches).\n"
}
{
    "Id": 74948525,
    "PostTypeId": 1,
    "Title": "FutureWarning: save is not part of the public API in Python",
    "Body": "I am using Python to convert Pandas df to .xlsx (in Plotly-Dash app.). All working well so far but with this warning tho:\n\"FutureWarning:\nsave is not part of the public API, usage can give unexpected results and will be removed in a future version\"\nHow should I modify the code below in order to keep its functionality and stability in future? Thanks!\n writer = pd.ExcelWriter(\"File.xlsx\", engine = \"xlsxwriter\")\n\n workbook  = writer.book\n\n df.to_excel(writer, sheet_name = 'Sheet', index = False)\n  \n writer.save()\n\n",
    "AcceptedAnswerId": 74948596,
    "AcceptedAnswer": "just replace save with close.\n writer = pd.ExcelWriter(\"File.xlsx\", engine = \"xlsxwriter\")\n\n workbook  = writer.book\n\n df.to_excel(writer, sheet_name = 'Sheet', index = False)\n  \n writer.close()\n\n"
}
